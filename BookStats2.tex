\documentclass[oneside,10pt]{book}
\usepackage{amsmath} % for "\cfrac" macro

\usepackage{tabularray}
\usepackage{makecell}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{geometry}
\usepackage{multirow}

\usepackage[export]{adjustbox}
\usepackage{relsize}
\usepackage{array}
\usepackage{enumitem}
\usepackage{relsize}


\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
\pagestyle{plain}
\newcommand\Chapter[2]{
  %\chapter[#1: {\itshape#2}]{#1\\[2ex]\Large\itshape#2}
  \chapter[#1]{#1\\[2ex]\Large\itshape#2}
}

%---
\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{shapes.geometric}
\tikzset{
  %level/.style   = { ultra thick, blue },
  %connect/.style = { dashed, red },
  %notice/.style  = { draw, rectangle callout, callout relative pointer={#1} },
  label/.style   = { text width=2.5cm }
}
%---

%\usepackage{fontspec}
%\setmainfont{Times New Roman} %Times New Roman
%\setmonofont{Consolas}

%\usepackage{selinput}
%\SelectInputMappings{Euro={â‚¬}}

%\usepackage[utf8]{inputenc}
\usepackage[titles]{tocloft}
\setlength{\cftbeforechapskip}{7pt} %%%%%%%%%% 6pt
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{amsmath}    % need for subequations
\usepackage{amsfonts}
\usepackage{amssymb}  % needed for mathbb  OK
\usepackage{bigints}
\usepackage{graphicx}   % need for figures
\usepackage{subfig}
\usepackage{verbatim}   % useful for program listings
\usepackage{color}      % use if color is used in text
%\usepackage{subfigure}  % use for side-by-side figures
\usepackage{parskip}
\usepackage{float}
\usepackage{courier}
%\usepackage{artemisia} %%%
\usepackage{exercise}
\usepackage{sistyle}
\usepackage{textcomp} 
%

%%%\usepackage[utf8]{luainputenc}
%\usepackage{luatextra}
%
%%\usepackage[utf8]{inputenc}
%%\usepackage[T1]{fontenc}
%%\usepackage{textcomp,upgreek}
%\usepackage{fontspec} %,xltxtra}
%\usepackage{unicode}
\usepackage[euler]{textgreek}
%%\DeclareUnicodeCharacter{3B8}{\ensuremath{\uptheta}}
%

\SIthousandsep{,}
%\usepackage{numprint}
\setlength\parindent{0pt}

\newtheorem{prop}{Proposition}

\renewcommand{\DifficultyMarker}{}
\newcommand{\AtBeginExerciseHeader}{\hspace{-21pt}}  %-0.2pt
\renewcommand{\ExerciseHeader}{\AtBeginExerciseHeader\textbf{\ExerciseName~\ExerciseHeaderNB} \ExerciseTitle}
\renewcommand{\AnswerHeader}{\large\textbf{\AnswerName~\ExerciseHeaderNB}\smallskip\newline}
\setlength\AnswerSkipBefore{1em}

\usepackage{xspace}
\usepackage{imakeidx}
\makeindex

\usepackage[nottoc]{tocbibind}

\usepackage[colorlinks = true,
          linktocpage=true,
            pagebackref=true, % add back references to bibliography
            linkcolor = red,
            urlcolor  = blue,
            citecolor = red,
 %           refcolor  =red,
            anchorcolor = blue]{hyperref}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{gray2}{rgb}{0.35,0.35,0.35}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{index}{rgb}{0.88,0.32,0}

%------- source code settings
\usepackage{listings} 
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

%-----------------------------------------------------------------

\usepackage{blindtext}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }


\setlength{\baselineskip}{0.0pt} 
\setlength{\parskip}{3pt plus 2pt}
\setlength{\parindent}{20pt}
\setlength{\marginparsep}{0.0cm}
\setlength{\marginparwidth}{0.0cm}
\setlength{\marginparpush}{0.0cm}
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.4} %%%
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

\usepackage[symbols,nogroupskip,acronym]{glossaries-extra}
%\usepackage[xindy,symbols,nogroupskip,sort=def,acronym]{glossaries}
\makenoidxglossaries %%%%%%%%%%%%%%%
%\setlength{\glsdescwidth}{1.3\hsize}

\newglossary*{gloss}{Glossary}

\newglossaryentry{gls:armodels}{type=gloss,name={Autoregressive process},description={\textcolor{index}{Auto-correlated time series}\index{auto-regressive process}, as described in section~\ref{linearar}. 
 Time-continuous versions include \textcolor{index}{Gaussian processes}\index{Gaussian process} and 
\textcolor{index}{Brownian motions}\index{Brownian motion}, while \textcolor{index}{random walks}\index{random walk} are a discrete example; two-dimensional versions exist. These processes are essentially integrated \textcolor{index}{white noise}\index{white noise}.  See pages },text={auto-regressive}}

\newglossaryentry{gls:binning}{type=gloss,name={Binning},description={Feature binning consists of aggregating the values of a feature into a small number of bins, to avoid \gls{gls:overfitting}\index{overfitting} and reduce the number of 
\textcolor{index}{nodes}\index{node (decision tree)} in methods such as \textcolor{index}{naive Bayes}\index{naive Bayes}, 
\glspl{gls:neuralnet},  or \glspl{gls:decisiontree}. Binning can be applied to two or more features simultaneously. I discuss \textcolor{index}{optimum binning}\index{binning!optimum binning} in this book. See pages },text={binning}}

\newglossaryentry{gls:boosted}{type=gloss,name={Boosted model},description={Blending of several models to get the best of each one, also referred to as \glspl{gls:ensembles}. The concept is illustrated with 
\textcolor{index}{hidden decision trees}\index{hidden decision trees} in this book. Other popular examples are \textcolor{index}{gradient boosting}\index{gradient boosting} and \textcolor{index}{AdaBoost}\index{AdaBoost}. See pages },text={boosting}}

\newglossaryentry{gls:bootstrap}{type=gloss,name={Bootstrapping},description={A data-driven, model-free technique to estimate parameter values, to optimize \gls{gls:goodnessoffit} metrics. Related to resampling in the context of \gls{gls:crossvalid}. In this book, I discuss \textcolor{index}{parametric bootstrap}\index{parametric bootstrap} on \gls{gls:syntheticdata} that mimics the actual observations. See pages },text={bootstrapping}}

\newglossaryentry{gls:cr}{type=gloss,name={Confidence Region},description={A confidence region of  level $\gamma$ is a 2D set of minimum area covering a proportion $\gamma$ of the mass of a bivariate probability distribution. It is a 2D generalization of 
\textcolor{index}{confidence intervals}\index{confidence interval}. In this book, I also discuss \textcolor{index}{dual confidence regions}\index{confidence region!dual region} -- the analogous of \textcolor{index}{credible regions}\index{credible region (Bayesian)} in Bayesian inference. See pages },text={confidence region}}

\newglossaryentry{gls:crossvalid}{type=gloss,name={Cross-validation},description={Standard procedure used in \gls{gls:bootstrap}, and to test and validate a model, by splitting your data into training and \glspl{gls:validset}. Parameters are estimated based on \gls{gls:trainingset} data. An alternative to cross-validation is testing your model on \gls{gls:syntheticdata} with known response. See pages },text={cross-validation}}

\newglossaryentry{gls:decisiontree}{type=gloss,name={Decision trees},description={A simple, intuitive non-linear modeling techniques used in classification problems. It can handle missing and categorical data, as well as a large number of features, but requires appropriate feature binning. Typically one blends multiple binary trees each with a few \textcolor{index}{nodes}\index{node (decision tree)}, to boost performance. See pages },text={decision tree}}

\newglossaryentry{gls:dimreduct}{type=gloss,name={Dimension reduction},description={A technique to reduce the number of features in your dataset while minimizing the loss in predictive power. The most well known are \textcolor{index}{principal component analysis}\index{principal component analysis} and \gls{gls:featureselection} to maximize \gls{gls:goodnessoffit} metrics. See pages },text={dimensionality reduction}}

\newglossaryentry{gls:empdistr}{type=gloss,name={Empirical distribution},description={Cumulative frequency histogram attached to a statistic (for instance, nearest neighbor distances), and based on observations. When the number of observations tends to infinity and the bin sizes tend  to zero, this step function tends to the theoretical cumulative distribution function of the statistic in question. See pages },text={empirical distribution}}

\newglossaryentry{gls:ensembles}{type=gloss,name={Ensemble methods},description={A technique consisting of blending multiple models together, such as many \glspl{gls:decisiontree} with \gls{gls:logreg}, to get the best of each method and outperform each method taken separately. Examples include \gls{gls:boosted}, bagging, and AdaBoost. In this book, I discuss \textcolor{index}{hidden decision trees}\index{hidden decision trees}. See pages },text={ensemble method}}

\newglossaryentry{gls:explainableai}{type=gloss,name={Explainable AI},description={Automated machine learning techniques that are easy to interpret are referred to as interpretable machine learning or explainable artificial intelligence. As much as possible, the methods discussed in this book belong to that category. The goal is to design black-box systems less likely to generate unexpected results with unintended consequences. See pages },text={explainable AI}}

\newglossaryentry{gls:featureselection}{type=gloss,name={Feature selection},description={Features -- as opposed to the model response -- are also called independent variables or predictors. Feature selection, akin to \gls{gls:dimreduct}, aims at finding the minimum subset of variables with enough \gls{gls:predictivepower}. It is also used to eliminate redundant features and find \textcolor{index}{causality}\index{causality} (typically using \textcolor{index}{hierarchical Bayesian models}\index{Bayesian inference!hierarchical models}), as opposed to mere correlations. Sometimes, two features have poor predictive power when taken separately, but provide improved predictions when combined together. See pages },text={feature selection}}

\newglossaryentry{gls:gm}{type=gloss,name={Generative model},description={Bayesian Gaussian mixtures 
(\textcolor{index}{GMM}\index{GMM (Gaussian mixture model)}) 
combined with kernel density estimation and the \textcolor{index}{EM algorithm}\index{EM algorithm} is a classic modeling tool. In this book, I used 
\textcolor{index}{$m$-interlacings}\index{$m$-interlacing} instead. Generative adversarial networks (\textcolor{index}{GAN}\index{GAN (generative adversarial networks)}) work as follows: the generator 
creates new observations and the discriminator tests whether the new observations are statistically indistinguishable from training set data. When this goal is achieved, the new observations is your synthetic data. New observations can also be generated via
 \textcolor{index}{parametric bootstrap}\index{parametric bootstrap}.  See pages },text={generative model}}

\newglossaryentry{gls:goodnessoffit}{type=gloss,name={Goodness-of-fit},description={A \textcolor{index}{model fitting}\index{model fitting} criterion or metric to assess how a model or sub-model fits to a dataset, or to measure its \gls{gls:predictivepower} on a \gls{gls:validset}. Examples include \gls{gls:rsquared}, Chi-squared, Kolmogorov-Smirnov, error rate such as false positives and other metrics discussed in this book. See pages },text={goodness-of-fit}}

\newglossaryentry{gls:gradient}{type=gloss,name={Gradient methods},description={Iterative optimization techniques to find the minimum of maximum of a function, such as the \textcolor{index}{maximum likelihood}\index{maximum likelihood estimation}. When there are numerous local minima or maxima, use \textcolor{index}{swarm optimization}\index{swarm optimization}. Gradient methods (for instance, stochastic gradient descent or Newton's method) assume that the function is differentiable. If not, other techniques such as \textcolor{index}{Monte Carlo simulations}\index{Monte Carlo simulations} or the 
\textcolor{index}{fixed-point algorithm}\index{fixed-point algorithm} can be used. Constrained optimization involves using 
\textcolor{index}{Lagrange multipliers}\index{Lagrange multiplier}. See pages },text={gradient}}

\newglossaryentry{gls:graphmodel}{type=gloss,name={Graph structures},description={Graphs are found in \glspl{gls:decisiontree}, in  \glspl{gls:neuralnet} (connections between \textcolor{index}{neurons}\index{neural network!neuron}), in \textcolor{index}{nearest neighbors methods}\index{nearest neighbors}  (NN graphs), in \textcolor{index}{hierarchical Bayesian models}\index{Bayesian inference!hierarchical models}, and more. See pages },text={graph}}

\newglossaryentry{gls:hyperparam}{type=gloss,name={Hyperparameter},description={An hyperparameter is used to control the learning process: for instance, the dimension, the number of features, parameters, layers (neural networks) or clusters (clustering problem), or the width of a  filtering window in image processing. By contrast, the values of other parameters (typically node weights in \glspl{gls:neuralnet} or regression coefficients) are derived via training. See pages },text={hyperparameter}}

\newglossaryentry{gls:linkf}{type=gloss,name={Link function},description={A link function maps a nonlinear relationship to a linear one so that a linear model can be fit, and then mapped back to the original form using the inverse function. For instance, the \textcolor{index}{logit link function}\index{logit function} is used in \gls{gls:logreg}. Generalizations include \textcolor{index}{quantile}\index{quantile} functions and inverse \textcolor{index}{sigmoids}\index{sigmoid function} in \gls{gls:neuralnet} to work with additive (linear) parameters. See pages },text={link function}}


\newglossaryentry{gls:logreg}{type=gloss,name={Logistic regression},description={A generalized linear \gls{gls:regression} method where the binary response  (fraud/non-fraud or cancer/non-cancer) is modeled as a probability via the logistic link function. Alternatives to the iterative maximum likelihood solution are discussed in this book. See pages },text={logistic regression}}


\newglossaryentry{gls:neuralnet}{type=gloss,name={Neural network},description={A blackbox system used for predictions, optimization,  or pattern recognition especially in computer vision. It consists of layers, neurons in each layer, \glspl{gls:linkf} to model non-linear interactions, parameters (weights associated to the connections between neurons) and \glspl{gls:hyperparam}. Networks with several layers are called \textcolor{index}{deep neural networks}\index{deep neural network}. Also, \textcolor{index}{neurons}\index{neural network!neuron} are sometimes called nodes. See pages },text={neural network}}

\newglossaryentry{gls:nlp}{type=gloss,name={NLP},description={Natural language processing is a set of techniques to deal with unstructured text data, such as emails, automated customer support, or webpages downloaded with a crawler. The example discussed in section~\ref{nlp21} deals with creating a keyword taxonomy based on parsing Google search result pages. Text
 generation is referred to as NLG or \textcolor{index}{natural language generation}\index{natural language generation}\index{NLG (natural language generation)}, using
 \textcolor{index}{large language models}\index{large language models}\index{LLM (large language model)} (LLM). See pages },text={natural language processing}}


\newglossaryentry{gls:numericalstability}{type=gloss,name={Numerical stability},description={This issue occurring in unstable optimization problems typically with multiple minima or maxima, is frequently overlooked and leads to poor predictions or high volatility. It is sometimes referred to as \textcolor{index}{ill-conditioned problems}\index{ill-conditioned problem}. I explain how to fix it in several examples in this book, for instance in section~\ref{vandervg}. Not to be confused with numerical precision. See pages },text={numerical stability}}

\newglossaryentry{gls:overfitting}{type=gloss,name={Overfitting},description={Using too many unstable parameters resulting in excellent performance on the \gls{gls:trainingset}, but poor performance on future data or on the \gls{gls:validset}. It typically occurs with numerically unstable procedures such as regression (especially polynomial regression) when the training set is not large enough, or in the presence of \textcolor{index}{wide data}\index{wide data} (more features than observations) when using a method not suited to this situation. The opposite is underfitting. See pages },text={overfitting}}

\newglossaryentry{gls:predictivepower}{type=gloss,name={Predictive power},description={A metric to assess the \gls{gls:goodnessoffit} or performance of a model or subset of features, for instance in the context of \gls{gls:dimreduct} or \gls{gls:featureselection}. Typical metrics include \gls{gls:rsquared}, or \textcolor{index}{confusion matrices}\index{confusion matrix} in classification. See pages },text={predictive power}}

\newglossaryentry{gls:rsquared}{type=gloss,name={R-squared},description={A \gls{gls:goodnessoffit} metric to assess the predictive power of a model, measured on a \gls{gls:validset}. Alternatives include adjusted R-squared, mean absolute error and other metrics discussed in this book. See pages },text={R-squared}}

\newglossaryentry{gls:prng}{type=gloss,name={Random number},description={Pseudo-random numbers are sequences of binary digits, usually grouped into blocks, satisfying properties of independent Bernoulli trials. In this book, the concept is formally defined, and strong pseudo-number generators are built and used in computer-intensive simulations. See pages },text={pseudo-random number}}


\newglossaryentry{gls:regression}{type=gloss,name={Regression methods},description={I discuss a unified approach to all regression problems in chapter~\ref{chap1v}. Traditional techniques include linear, logistic, Bayesian, polynomial and \textcolor{index}{Lasso regression}\index{Lasso regression} (to deal with numerical instability and \gls{gls:overfitting}), solved using optimization techniques, \textcolor{index}{maximum likelihood}\index{maximum likelihood estimation} methods, linear algebra (\textcolor{index}{eigenvalues}\index{eigenvalue} and \textcolor{index}{singular value decomposition}\index{singular value decomposition}) or stepwise procedures. See pages },text={regression}}

\newglossaryentry{gls:supervisedlearning}{type=gloss,name={Supervised learning},description={Techniques dealing with labeled data (\textcolor{index}{classification}\index{classification}) or when the response is known (\gls{gls:regression}). The opposite is \textcolor{index}{unsupervised learning}\index{unsupervised learning}, for instance \textcolor{index}{clustering}\index{clustering} problems. In-between, you have \textcolor{index}{semi-supervised learning}\index{semi-supervised learning} and \textcolor{index}{reinforcement learning}\index{reinforcement learning} (favoring good decisions). The technique described in chapter~\ref{chap1v} fits into unsupervised regression. \textcolor{index}{Adversarial learning}\index{adversarial learning} is testing your model against extreme cases intended to make it fail, to build better models. See pages },text={supervised learning}}

\newglossaryentry{gls:syntheticdata}{type=gloss,name={Synthetic data},description={Artificial data simulated using a 
%\textcolor{index}{generative model}
\gls{gls:gm}\index{generative model}, typically a \textcolor{index}{mixture model}\index{mixture model}, to enrich existing datasets and improve the quality of \glspl{gls:trainingset}. Called \textcolor{index}{augmented data}\index{augmented data} when blended with real data. See pages },text={synthetic data}}

\newglossaryentry{gls:tensor}{type=gloss,name={Tensor},description={Matrix generalization with three of more dimensions. A matrix is a two-dimensional tensor. A triple summation with three indices is represented by a three-dimensional tensor, while a double summation involves a standard matrix. See pages },text={tensor}}

\newglossaryentry{gls:trainingset}{type=gloss,name={Training set},description={Dataset used to train your model in \gls{gls:supervisedlearning}. 
Typically, a portion of the training set is used to train the model, the other part is used as \gls{gls:validset}. See pages },text={training set}}

\newglossaryentry{gls:validset}{type=gloss,name={Validation set},description={A portion of your \gls{gls:trainingset}, typically $20\%$, used to measure the actual performance of your predictive algorithm outside the training set. In cross-validation and bootstrapping, the training and validation sets are split into multiple subsets to get a better sense of variations in the predictions. See pages },text={validation set}}

%----------------
% Top KW


% DISCUSS in LAST SECTION
%
% dummy variable
% exploratory analysis
% SVM
% recursivity
% computational complexity
% markov chain
% distributed architecture
% bayesian inference [dual confidence regions]
%  test of hypothesis, 
% *** prediction interval [?? in bayesian stats], 
% *** hash table / dictionary / associative array / list
% *** monte carlo methods, 

% give a fish... teach how to fish.. teach how to learn... install python... pip install... search the web / stackoverflow, post on forum questions, look for answer reviews/date, search doc on librarry functions...known instructors will all my varied experience, provide recommendations

% clustering, classification,  

% IGNORE
% robust method, covariance matrix
% black box system, nosql

% Create courses page on MLT


%% Add section "automated data cleaning/exploratory analysis" -- Well, not something I did overnight but after years of cleaning data and noticing I was always facing the same issues. The first step is to create a summary table for all the features: type (numbers, categories, text, mixed), for each feature get number of unique values and list top 5 popular values with count for each of these values  (could be a wrong zip-code like 99999). Compute min, max, median some percentiles if numeric feature. Compute cross-correlations b/w features. Check misalignments. Look for special stuff like NaN, N/A etc. Check for duplicate IDs or same IDs with 2 names (suggesting typos). Look for special character (accented and so on). Some negative or out-of-range or non-numeric or missing values when it should not happen? Duplicate features? The list is not short, indeed long enough that it feels there's a new problem each time, but in the end, yes there are several dozens (not several hundreds) of things to look for and fix, but it is manageable. Automation can help fix most of them. 
%structure unstructure data or use dictionary of typos or guided field capturing [KW: exploratory analysis] xxx Wells fargo broken sessions  simulations

%add NBCi , polynomial regression? data_cleaning, density estimation on the grid, long-range autocorrel time series ?? other concepts [adversarial models, change point detection]

% Other KW

%curve fitting,similarity metric, fixed point algorithm, stochastic processes
%shape recognition, lagrange multiplier, interpolation, augmented data, PCA, data video [and sound] ccc

%--------




\newglossaryentry{gls:cc}{type=gloss,name={Connected Component},description={A set of vertices in a graph that are connected to each other by paths. 
See also \gls{gls:nng}. See pages },text={connected component}}


\newglossaryentry{gls:ergo}{type=gloss,name={Ergodicity},description={A statistic such as the interarrival times is ergodic if it has the same asymptotic distribution, whether it is computed on many observations from a single realization of the process, or averaged across many realizations, each with few observations. See pages },text={ergodicity}}

\newglossaryentry{gls:homo1}{type=gloss,name={Homogeneity},description={A property of a point process, characterized by an homogeneous intensity function, that is, constant or independent of the location. See pages },text={homogeneous}}

\newglossaryentry{gls:mi}{type=gloss,name={Identifiability},description={A models is identifiable if it is uniquely defined by its parameters. Then it is possible to estimate each parameter separately. A trivial example of non-identifiability is when we have two parameters, say $\alpha,\beta$, but they only occur in a product $\alpha\beta$. In that case, if $\alpha\beta=6$, it is impossible to tell whether $\alpha=2,\beta=3$ or $\alpha=1,\beta=6$. See pages },text={model identifiability}}



\newglossaryentry{gls:ia}{type=gloss,name={Interarrival Time},description={In one dimension, random variable measuring the distance between a point of the process and its closest neighbor to the right, on the real axis. Interarrival times are also called {\em increments}. See pages },text={interarrival times}}

\newglossaryentry{gls:lattice1}{type=gloss,name={Lattice Space},description={In two dimensions, it consists of the locations $(h/\lambda,k/\lambda)$ with $h,k\in\mathbb{Z}$. The distribution of a point $(X_h,Y_k)$ is centered at $(h/\lambda,k/\lambda)$. The concept can be extended to any dimension. See pages },text={lattice space}}

\newglossaryentry{gls:lsc}{type=gloss,name={Location-scale},description={A random variable $X$ has a location-scale distribution with two parameters, the scale $s$ and location $\mu$, if any linear transformation $a+bX$ has a distribution of the same family, with parameters respectively $b^2s$ and $\mu+a$. Here $\mu$ is the expectation and $s$ is proportional to the variance of the distribution. See pages },text={location-scale distributions}}

\newglossaryentry{gls:modulo}{type=gloss,name={Modulo Operator},description={Sometimes, it is useful to work with point ``residues" modulo $\frac{1}{\lambda}$, instead of the original points, due to the nature of the underlying lattice. It magnifies the patterns of the point process. By definition, 
$X_k \bmod{\frac{1}{\lambda}}=X_k-\frac{1}{\lambda}\lfloor \lambda X_k \rfloor$ where the brackets represent the integer part function. See pages },text={modulo}}

\newglossaryentry{gls:nng}{type=gloss,name={NN Graph},description={Nearest neighbor graph. The vertices are the points of the process. Two vertices (the points they represent) are connected if at least one of the two points is nearest neighbor to the other one. This graph is undirected. See pages },text={nearest neighbor graph}}

\newglossaryentry{gls:pc}{type=gloss,name={Point Count},description={Random variable, denoted as $N(B)$, counting the number of points of the process in a particular set $B$, typically an interval $[a, b]$ in one dimension, and a square or circle in two dimensions. See pages },text={point count}}

\newglossaryentry{gls:pb55}{type=gloss,name={Point Distribution},description={Random variable representing how a point of the process is distributed in a domain $B$; for instance, for a stationary Poisson process, points are uniformly distributed on any compact domain $B$ (say, an interval in one dimension, or a square in two dimensions). See pages },text={point distribution}}

\newglossaryentry{gls:quant}{type=gloss,name={Quantile function},description={Inverse of the cumulative distribution function (CDF) $F$, denoted as $Q$. 
Thus if $P(X<x)=F(x)$, then $P(X<Q(x))=x$. See pages },text={quantile function}}


\newglossaryentry{gls:sf}{type=gloss,name={Scaling Factor},description={Core parameter of the Poisson-binomial process. Denoted as $s$, proportional to the variance of the distribution $F$ attached to the points of the process. It measures the level of repulsion among the points (maximum if $s=0$, minimum if $s=\infty$). In $d$ dimensions, the process is stationary Poisson of intensity $\lambda^d$ if $s=\infty$, and coincides with the fixed {\em lattice space} if $s=0$. See pages },text={scaling factor}}


\newglossaryentry{gls:statio}{type=gloss,name={Stationarity},description={Property of a point process: the point distributions in two sets of same shape and area, are identical. The process is stochastically invariant under translations. See pages },text={stationary}}


\begin{document}

\hypersetup{linkcolor=blue}
%inserting a glossary entry in gloss: \gls{gls:keyword1} \\



\baselineskip=2\baselineskip
\thispagestyle{empty}
\hspace{0pt}
\vfill
%\hrulefill
\begin{center}
\rule{0.90\textwidth}{.4pt}
\end{center}

\begin{center}
{\Huge \bf{Statistical Optimization for AI and\\\vspace{0.5ex}Machine Learning} }  
\end{center}


\baselineskip=0.5\baselineskip
\addvspace{1cm}
\begin{center}
%\includegraphics[width=0.7\textwidth]{linear.png}  \\
%\addvspace{1cm}
%\includegraphics[width=0.6\textwidth]{imgpyRiemannFinalOrbits-v2-small2.jpg} 
\includegraphics[width=0.91\textwidth]{interpol1.png}  
\end{center}
\addvspace{1cm}
\begin{center}
\rule{0.90\textwidth}{.4pt}
\end{center}
\begin{center}
Vincent Granville, Ph.D. $|$ \href{https://mltechniques.com/}{www.MLTechniques.com} $|$ Version 2.2, November 2023 
\end{center}
%\hrulefill

\hypersetup{linkcolor=red} % red %

\vfill
\hspace{0pt}
\pagebreak

\chapter*{Preface} %\clearpage

This book covers 
 optimization techniques pertaining to machine learning and generative AI, with an emphasis on producing better synthetic data with faster methods, some not even involving neural networks. NoGAN for tabular data is described in detail, along with full Python code, and case studies in healthcare, insurance, cybersecurity, education, and telecom. This low-cost technique is a game changer: it runs 1000x faster than generative adversarial networks (GAN) while consistently producing better results. Also, it leads to replicable results and auto-tuning.

Many evaluation metrics fail to detect defects in synthesized data, not because they are bad, but because 
 they are poorly implemented: due to the complexity, the full multivariate version is absent from vendor solutions. In this book, I describe an implementation
 of the full version, tested on numerous examples. Known as the multivariate Kolmogorov-Smirnov distance (KS), it is based on the joint empirical distributions attached to the datasets, and works in any dimension on categorical and numerical features. Python libraries, both for NoGAN and KS, are now available and presented in this book.

A very different synthesizer also discussed,  namely NoGAN2, is based on resampling, model-free hierarchical methods, auto-tuning, and explainable AI. It minimizes a particular loss function, also without gradient descent. While not based on neural networks, it nevertheless shares many similarities with GAN. Thus you can use it as a sandbox to quickly test various features and hyperparameters before adding the ones that work best, to GAN. 
Even though NoGAN and NoGAN2 don't use traditional optimization, gradient descent is the topic of the first chapter. Applied to data rather than math functions, there is no assumption of differentiability, no learning parameter, and essentially no math. The second chapter introduces a generic class of regression methods covering all existing ones and more, whether your data has a response or not, for supervised or unsupervised learning. I use gradient descent in this case.

One chapter is devoted to NLP, featuring an efficient technique to process large amounts of text data: hidden decision trees, presenting some similarities with XGBoost. A similar
 technique is used in NoGAN. Then I discuss other GenAI methods and various optimization techniques, including feature clustering, data thinning, smart grid search and more. Multivariate interpolation is used for time series and geospatial data, while agent-based modeling applies to complex systems. 


Methods are accompanied by enterprise-grade Python code, also available on GitHub. Chapters are mostly independent from each other, allowing you to read in random order. 
The style is very compact, and suitable to business professionals with little time. Jargon and arcane theories are absent, replaced by simple English to facilitate the reading by non-experts, and to help you discover topics usually made inaccessible to beginners.  While state-of-the-art research is presented in all chapters, the prerequisites to read this book are minimal: an analytic professional background, or a first course in calculus and linear algebra. 



%\noindent {\bf About the Author}\vspace{1ex}

%\begin{figure}%[H]
%\centering
%\includegraphics[width=0.2\textwidth]{vgr3.png}
%\caption{xxxx}
%\label{fig:linearbvvg}
%\end{figure}

\section*{About the author}

Vincent Granville is a pioneering GenAI scientist and machine learning expert, co-founder of Data Science Central 
(acquired by a publicly traded company in 2020), 
 Chief AI Scientist at \href{https://mltechniques.com/}{MLTechniques.com}, former VC-funded executive, author and patent owner -- one related to LLM. 
Vincentâ€™s past corporate experience includes Visa, Wells Fargo, eBay, NBC, Microsoft, and CNET. 


\begin{wrapfigure}{l}{2.5cm}
%\caption{A wrapped figure going nicely inside the text.}\label{wrap-fig:1}
\includegraphics[width=2.5cm]{vgr3.png}
\end{wrapfigure}

\noindent Vincent is also a former post-doc at Cambridge University, and the National Institute of Statistical Sciences (NISS).  
He  published in {\em Journal of Number Theory}, {\em Journal of the Royal Statistical Society} (Series B), and {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}. He is the author of multiple books, available \href{https://mltechniques.com/resources/}{here}, 
 including ``Synthetic Data and Generative AI'' (Elsevier, 2024). Vincent lives  in Washington state, and enjoys doing research on stochastic processes, dynamical systems, experimental math and probabilistic number theory. He recently launched a GenAI certification program,
 offering state-of-the-art, enterprise grade projects to participants. The program, based on his books, is discussed \href{https://mltblog.com/3pWxvZK}{here}.


%


\hypersetup{linkcolor=red}
%\renewcommand{\baselinestretch}{0.98}\normalsize
\tableofcontents 
%\renewcommand{\baselinestretch}{1.0}\normalsize

%\hypersetup{linkcolor=blue}

%\renewcommand{\baselinestretch}{0.97}\normalsize
%%%%%%%%%%%%% \listoffigures
%\renewcommand{\baselinestretch}{1.00}\normalsize
%\listoftables

%-------------------------------------------------------------------------------------------------------------------
\chapter{Math-free, Parameter-free Gradient Descent Algorithm}{}\label{gentil}

I discuss techniques related to the gradient descent method in 2D. 
The goal is to find the minima of a target function, called the cost function. The values of the function are computed at evenly spaced locations on a grid and stored in memory. Because of this, the approach is not directly based on derivatives, and there is no calculus involved. It implicitly uses discrete derivatives, but foremost, it is a simple geometric algorithm. The learning parameter typically 
 attached to gradient descend is explicitly specified here: it is equal to the granularity of the mesh and does not need fine-tuning. In addition to gradient descent and ascent, I also show how to build contour lines and orthogonal trajectories, with the exact same algorithm.

I apply the method to investigate one of the most famous unsolved problems in mathematics: the Riemann Hypothesis. The functions studied here are defined on the complex plane. However, no advanced knowledge of complex calculus is required as I use the standard 2D space in my illustrations. I show how the distribution of the minima of $|\zeta(\sigma +it)|$ can be studied by looking at (say) $\sigma=2$ rather than $\sigma=\frac{1}{2}$. These minima are the non-trivial roots of the Riemann zeta function and all of them are conjectured to have $\sigma=\frac{1}{2}$. It is a lot easier to work with $\sigma>1$ due to
 accelerated convergence. In the process, I introduce synthetic functions with arbitrary infinite Hadamard products (the most well known is the sine function) to assess non-Dirichlet functions that may behave like $\zeta$, and gain more insights and generalization about the problem. My presentation is mostly in simple English and accessible to first year college students. 

\section{Introduction}

The purpose is to test the generic gradient algorithm and related methods introduced in this chapter. I focus on a particular type of functions that mimic the Riemann zeta function $\zeta(\sigma,t)$ in the complex plane. For the argument of $\zeta$, I may use the notation $\sigma+it$ or 
$(\sigma, t)$ interchangeably. In particular, I am interested in the modulus (also called radius) 
 $|\zeta(\sigma,t)|$. Looking at the behavior when $\sigma<1$ is significantly more difficult than when $\sigma$ is large. I show how the minima of the modulus around $\sigma=\frac{1}{2}$ (or anywhere  where $\sigma<1$) create ripple effects propagating beyond $\sigma> 1$, for any $t$. This allows you to focus on the much easier part of the plane where $\sigma>1$, to know what is happening at $\sigma=\frac{1}{2}$. To achieve this goal, I look at very simple functions with similar behavior, and add extra minima with $\frac{1}{2} < \sigma < 1$, to analyze the impact. It is conjectured that $|\zeta(\sigma,t)|\geq 0$ has no zero in that region: this is the 
\textcolor{index}{Riemann Hypothesis}\index{Riemann Hypothesis} (RH) [\href{https://en.wikipedia.org/wiki/Riemann_hypothesis}{Wiki}]. %Based on my empirical observations, I go one step further by conjecturing that it does not even have any basin in that area, besides those created by the roots on the
 %critical line $\sigma=\frac{1}{2}$. Saddle points between two adjacent valleys created by two adjacent roots, do exist. They look like mountain passes. 

To study the functions in question, I developed visualization and optimization tools that will be of interest to machine learning practitioners.
 My approach to gradient descent (to find minima or maxima) does not use any calculus. I extend it to find orthogonal trajectories. I believe it is simple enough to be taught in high school. There is nothing truly novel in my technique, but I could not find any references explaining what I am doing. Thus the first part of this chapter focuses on my version of the gradient descent algorithm. It is especially useful for bivariate real-valued  functions
 that do not have any easy-to-compute derivatives (called gradient, in 2D). Orthogonal trajectories (the red curves in Figure~\ref{fig:ortho1}) are more difficult to obtain than contour lines: you typically need to solve differential equations to find them. Yet again, I show how to do it  without math.

Section~\ref{hg21} explores machine learning techniques to find the minima. Then in section~\ref{po0xz}, I explain how it applies to the functions of interest. I share the Python code in section~\ref{pc991}: it relies on the Mpmath library, to handle special functions such as $\zeta$ in the complex plane. This chapter shows once again how RH leads to the development of new machine learning / AI techniques, and how these 
 new exploration techniques lead to new insights about RH in a symbiotic, feedback loop. Earlier research on RH with the same dual purpose can be found in chapters 9 and 15 in my recent book~\cite{vgelsevier}, with applications ranging from testing and generating strong randomness
 to new clustering algorithms and animated data visualizations. In this chapter, an interesting machine learning concept is that of non-trivial  synthetic function, in the context of synthetic data and generative~AI.  
 

\section{Gradient descent and related optimization techniques}\label{hg21}

The easiest way to understand the methodology is to look at Figure~\ref{fig:ortho2}. You have a real-valued bivariate function 
 $|\zeta(\sigma+it)|$ with two arguments $\sigma, t$, with $0.25\leq \sigma \leq 1.60$ and $201.0\leq t \leq 203.3$ respectively on the vertical and horizontal axes. The argument is actually the complex number $\sigma + it$, but here we treat it as a vector $(\sigma, t)$. 
 The function $\zeta$ is the complex-valued \textcolor{index}{Riemann zeta function}\index{Riemann zeta function} [\href{https://en.wikipedia.org/wiki/Riemann_zeta_function}{Wiki}], and $|\cdot|$ stands for the 
 \textcolor{index}{modulus}\index{modulus (complex number)} [\href{https://en.wikipedia.org/wiki/Absolute_value#Complex_numbers}{Wiki}]. 
The modulus of a complex number $\sigma + it$ is defined as $\sqrt{\sigma^2 + t^2}$.
In particular, 
$|\zeta(\sigma,t)|$ is always positive and equal to zero (the minimum potential value) if and only if $(\sigma, t) = \sigma + it$ is a root of $\zeta$.


The dashed line corresponding to $\sigma=\frac{1}{2}$ is called the \textcolor{index}{critical line}\index{critical line (number theory)} [\href{https://en.wikipedia.org/wiki/Riemann_hypothesis#Zeros_on_the_critical_line}{Wiki}] in number theory: RH states that all the infinitely many zeros of $\zeta$ stand on that line: they must have $\sigma=\frac{1}{2}$. In Figure~\ref{fig:ortho2}, two of them are visible and represented by the green dots. In the rectangular area shown in the picture, $|\zeta(\sigma,t)|$ takes on values between zero (dark blue) and a little over $2.0$ (red).  
 For a 3D version, see Figure~\ref{fig:zeta3d}. An animated 3D version, with views from different angles, can be found 
 \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Images/gradient_3D_rotate_zeta.mp4}{here}.

\noindent Two types of curves are shown in Figure~\ref{fig:ortho2}:\vspace{1ex}
\begin{itemize}
\item \textcolor{index}{Contour lines}\index{contour lines} [\href{https://en.wikipedia.org/wiki/Contour_line}{Wiki}]: their color ranges from blue to red depending on the value of $|\zeta(\sigma,t)|$ that they represent. In fluid dynamics and electromagnetism, they are called equipotential curves. The color represents the \textcolor{index}{contour level}\index{contour level} attached to a specific contour line. 
\item \textcolor{index}{Orthogonal trajectories}\index{orthogonal trajectory} [\href{https://en.wikipedia.org/wiki/Orthogonal_trajectory}{Wiki}]: these are perpendicular to the contour lines. They are called streamlines in fluid dynamics. If the image represented an elevation map, each raindrop hitting a contour line would follow the streamline it is attached to, and move downstream until it hits a minimum: a green dot in our example. 
\end{itemize}\vspace{1ex}

\noindent The yellow dots in Figure~\ref{fig:ortho2} are sampled on a specific but arbitrary contour line. If you pick up a yellow dot, and follow downstream the orthogonal trajectory (in red) that it is attached to, you will end up at one of the two minima of $|\zeta(\sigma+it)|$ in the picture.  That's how the \textcolor{index}{gradient descent method} [\href{https://en.wikipedia.org/wiki/Gradient_descent}{Wiki}] works. The difference with traditional approaches is that here, a finite number of values of $|\zeta(\sigma+it)|$ are computed on a 
 rectangular grid. Thus, the gradient descent is performed on a discretized version of the target function. It does not even require the function to be differentiable, and derivatives are not explicitly computed. This is why I call the method math-free.

If you look closely at Figure~\ref{fig:ortho2}, you will notice that there is small cluster of nearby green dots attached to each of the two minima (the roots of $|\zeta(\sigma+it)|$ in this case). The reason is because each yellow dot -- a starting point in the gradient descent -- may lead to a different end-point on the grid: the same root maybe, but up to the first few digits. For instance, in my example the precision is about $0.005$.
Increasing the granularity of the grid 
 fixes the issue. But the price to pay is a lot more memory and time requirements, proportional to the number of cells in the grid.  In practice, one can start with a rather coarse grid 
 covering a large rectangular area to get
 a rough approximation. Then work with a finer grid covering a small area centered around the previously obtained approximation to get a better one, and so on.   

My method is particularly useful when you want to see the convergence paths (reaching a green dot) for a large number of initial points (the yellow dots) at once. The bottleneck is computing the values of $|\zeta(\sigma,t)|$ for over $\num{50000}$ vectors $(\sigma, t)$ to 
 populate the grid. It is slow because the series to compute $\zeta$ convergences slowly especially when $\sigma<1$ and $t$ is large. Using a product of sine functions even in the complex plane, instead of $\zeta$, is at least 10 times faster. In all cases though, finding all the paths at once is fast.
 Of course, you can compute just one convergence path and not use the large grid: instead, you would compute the limited data you need about $|\zeta(\sigma,t)|$  on the fly, rather than access it from the 2D arrays \texttt{za}, \texttt{dx}, and \texttt{dy}. These arrays contain a lot more than what you need, and are a waste of time and memory if you are interested in just one path.

\begin{figure}%[H]
\centering
\includegraphics[width=0.8\textwidth]{RH4_ortho_sinh2.png}
%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Orthogonal trajectories (red) and contour lines for the Sinh2 function}
\label{fig:ortho1}
\end{figure}


\begin{figure}%[H]
\centering
\includegraphics[width=0.8\textwidth]{RH4_ortho_zeta.png}
%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Orthogonal trajectories (red) and contour lines for $|\zeta(\sigma,t)|$}
\label{fig:ortho2}
\end{figure}

%One of the goals was to find whether there are micro-basins (extra green dots in the middle of the rectangle), besides the large ones corresponding to the roots at $\sigma=\frac{1}{2}$.  
%None were found after testing many rectangular windows, with limited precision. The next step is to actually prove that none exists for the class of functions that $\zeta$ belongs to: this would complete the proof of RH. If some do indeed exist but are hard to find (presumably occurring if $t$ is extremely large and possibly invisible to the naked eye) then my conjecture is wrong. If in addition at least one of these micro-basins reaches the absolute bottom, then the Riemann Hypothesis itself is wrong. The values of $t$ that could cause this to happen are not random; studying them will be the subject of another article, heavily focused on approximating irrational numbers by rational ones. The potential values for $t$, 
 %in order for $t$ to become threatening, are truly gigantic and not within reach of modern computers: multiple unfortunate coincidences would have to occur for a specific $t$ to make RH to fail.

\subsection{Implementation details}\label{porbug}

Using the parameter names in the Python code, 
the real-valued function $|\zeta(\sigma, t)|$ is first tabulated on a regular rectangular grid with $t$ between
\texttt{min\_t} and \texttt{max\_t}, and $\sigma$ between \texttt{min\_sigma} and \texttt{max\_sigma}. Increments for
 $t$ and $\sigma$ are respectively \texttt{incr\_t} and \texttt{incr\_sigma}. The 2D grid is named \texttt{za} in the Python code;
 it maps the evenly distributed vertices (points where the function is tabulated) onto 2D integer coordinates,  for easy use in contour plots with Matplotlib.  
 The mapping is performed using the \texttt{meshgrid} Numpy function. Also, \texttt{za} is a global variable (array). 

This step does not involve mathematics. From there, there will be no mathematics involved either. All operations are performed using simple manipulations of the values stored in the grid \texttt{za}, without calling the function~$\zeta$ ever again. Given
 a location represented by a 2D index \texttt{[h,k]} in \texttt{za}, the gradient is  the following vector: the first component \texttt{dx[h,k]} is the difference between the next and current value on the grid in the horizontal direction; the second component \texttt{dy[h,k]} is computed in the vertical direction. This grid-based gradient must be 
 adjusted to represent a real gradient in the actual space: divide \texttt{dx[h,k]} by \texttt{incr\_t}, and \texttt{dy[h,k]} by \texttt{incr\_sigma}.
Note that \texttt{za[h,k]} is equal to $|\zeta(\sigma, t)|$ with \texttt{[h,k]} representing the location of $(\sigma, t)$ on the grid.

The grid-based gradient for all grid locations is obtained with the Numpy function \texttt{gradient}, with one line of code: \texttt{dy,dx=np.gradient(za)}. It is used to plot the orthogonal trajectories. To produce the contour lines (which are orthogonal to the orthogonal trajectories)
 swap \texttt{dx}, \texttt{dy} and use \texttt{-dx} instead of \texttt{dx}. The very core of the Python program resides in the following code snippet \vspace{1ex}

\begin{lstlisting}
if mode == 'Descent': 
    sign = +1
elif mode == 'Ascent':
    sign = -1
iter = 0
while iter < n_iter:
    if type == 'Gradient':
        t = t - learn_t * sign * dx[h,k]/incr_t
        sigma = sigma - learn_sigma * sign * dy[h,k]/incr_sigma    
    elif type == 'Contour':
        t = t - learn_t * sign * dy[h,k]/incr_sigma  
        sigma = sigma + learn_sigma * sign * dx[h,k]/incr_t   
if t>min_t and t<max_t and sigma>min_sigma and sigma<max_sigma: 
    x.append(t)
    y.append(sigma)
old_z = za[h, k]
h = int(0.5+(sigma - min_sigma)/incr_sigma)
k = int(0.5+(t - min_t)/incr_t)             
if h<h_steps-2 and k<k_steps-2 and h>0 and k>0: 
        z = za[h, k]
    else:
        iter = 99999999999
        showEnd = False
    iter = iter + 1
\end{lstlisting}\vspace{1ex}

This piece of code produces the various curves that you see for instance in Figure~\ref{fig:ortho1}. The red and blue curves are obtained respectively
 with \texttt{type='Gradient'} and \texttt{type='Contour'}. The former is associated to orthogonal trajectories. Points of the curve under construction are added to the arrays \texttt{x} and \texttt{y}  for plotting purposes. The parameter \texttt{mode} specifies in which direction a curve is drawn, given a starting point. For the gradient, \texttt{Descent} means going down towards a minimum, while \texttt{Ascent} means going up. Note that \texttt{[h,k]} represents the location (2D index) of $(t,\sigma)$ on the grid \texttt{za}. 


Figure~\ref{fig:ortho2} shows a number of points -- the yellow dots -- on a same contour line. Each of these points is attached to one orthogonal trajectory by construction. Given one of these starting points, if you follow the associated orthogonal trajectory downward, you end up to a minimum of the function in
 question. If you follow it upward, you go up and depending on the starting point, you eventually comes close to a 
\textcolor{index}{saddle point}\index{saddle point} [\href{https://en.wikipedia.org/wiki/Saddle_point}{Wiki}] (the equivalent of a mountain pass), then continue upward following a ridge separating two valleys. You can use my method to find yellow dots all located on a same contour line. However the successive iterations very slightly deviate from the initial contour line over time. Instead, I used a contour line generated by the Matplotlib function \texttt{countourf} using the 
\texttt{collections} attribute of the contour object \texttt{CS}. 

A core parameter in the algorithm is the 
\textcolor{index}{learning rate}\index{learning rate} [\href{https://en.wikipedia.org/wiki/Learning_rate}{Wiki}]. Here I use two of them: one for the horizontal, and one for the vertical direction (\texttt{learn\_t} and \texttt{learn\_sigma} in the code). However, unlike in traditional implementations, you don't need to fine-tune them: they are set to the increments in each direction 
(respectively \texttt{incr\_t} and \texttt{incr\_sigma}). This makes my methodology more suitable for black-box implementations. 

Finally, another important parameter is the number of iterations \texttt{n\_iter} in the main algorithm implemented in the 
\texttt{gradient\_descent} function. If too low, it will generate the beginning of the path towards the minimum, but not the full path. If too high,
 the last iterations do not bring additional value and are a waste of time. The smaller \texttt{incr\_t} and \texttt{incr\_sigma}, the more accurate the solution. But reducing the increments by a factor 2 increases memory usage (size of arrays \texttt{za}, \texttt{dx}, \texttt{dy}) and time by a factor 4. It also means more iterations needed in the gradient descent. A good compromise is to not be too granular: then the
 curves won't be perfectly smooth, but you can smooth them out with the parameter 
\texttt{smooth} in the main part of the program. 

Further speed improvement is obtained by using a stopping rule. Instead of running a fixed, large number of iterations (the parameter \texttt{n\_iter}), stop after the maximum number of iterations is reached, or when the error becomes low enough and can not be improved -- whichever comes first. Due to the finite size and limited resolution of the grid, you will always reach a point where no further improvement is possible, unless you increase the granularity.
 









\subsection{General comments about the methodology and parameters}\label{ppzsdxvb}

Despite using the standard notation $(\sigma, t)=\sigma +it$ for the Riemann zeta function $\zeta$, all the figures show $\sigma$ on the vertical axis, and $t$ on the horizontal axis. The reason is because I look at $\zeta$ the other way around, to gain new insights. This is especially 
 useful in Figure~\ref{fig:spectrum}, which mimics a spectrum image.

Most of the Python code deals with visualization aspects: the gradient function represents a small portion of the code. In particular, I also generate 3D data animations of the target functions, seen from various angles. You can download the videos 
\href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Images/gradient_3D_rotate_zeta.mp4}{here} for $|\zeta(\sigma,t)|$, and \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Images/gradient_3D_Rotate.mp4}{here} for the function referred to as \texttt{Sinh2} in the code. There is also a video showing how the gradient descent progresses and converges, 
 starting with 100 random locations, again for the \texttt{Sinh2} function. You can see it \href{https://www.youtube.com/watch?v=pqQsLpPkvbw}{here} on YouTube, or you can download it on GitHub, \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Images/RH4_ortho.mp4}{here}. It is based on the code featured in section~\ref{p100}.

A number of functions, beside $|\zeta(\sigma, t)|$, are available in the code. It includes functions that have a similar behavior, to study the impact of a root, how it propagates vertically when $\sigma$ increases, and to see if roots can be detected far away from their actual locations. The parameter  
 \texttt{Function} lets you choose which function you want to test. For that purpose, you can obtain a broader view of the function, covering a much larger interval for $t$, by choosing the option \texttt{View='Global'}: it produces spectrum plots such as the one featured in
 Figure~\ref{fig:spectrum}.

\begin{figure}%[H]
\centering
\includegraphics[width=0.60\textwidth]{zeta3db.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Now a 3D view of $|\zeta(\sigma,t)|$, showing the two roots}
\label{fig:zeta3d}
\end{figure}
 
It is interesting to note that Python plots or libraries featuring orthogonal trajectories are hard to find, possibly because the mathematical way to compute them is not straightforward: it involves solving a partial differential equation, see section~\ref{tretra}.  The closest I found is quiver and
 stream plots, that indicate the direction of a field with arrows (magnetic, fluid dynamics, atmospheric gradients and so on), computed locally at a number of locations, with the length of the arrow 
 representing the local velocity or intensity. See for instance  
 \href{https://problemsolvingwithpython.com/06-Plotting-with-Matplotlib/06.15-Quiver-and-Stream-Plots/}{here}.

\begin{figure}%[H]
\centering
\includegraphics[width=0.70\textwidth]{spectrum.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{$|\zeta(\sigma,t)|$ with $200<t<300$ (horizontal axis) and $0.5 < \sigma < 1.1$ (vertical axis)}
\label{fig:spectrum}
\end{figure}
 
I now discuss some useful parameters of the method, that I haven't covered yet. The parameter names correspond  to those in the Python code featured in section~\ref{iudwlmn}. \vspace{1ex}

\begin{itemize}
\item In Figure~\ref{fig:ortho2}, the number of possible starting points (the yellow dots) is automatically computed and denoted as \texttt{n}. To make the
 starting points visible, set \texttt{showStart=True}. Likewise, to make the end points (blue dots) visible, set \texttt{showEnd=False}. 
 To show the orthogonal trajectories (the red curves), set \texttt{showPath=True}.
\item  The yellow dots are picked up on a contour line generated within the \texttt{contour} Maplotlib environment. The contour in question 
 is specified by the parameter \texttt{level}. Its value can be anywhere between 1 (the contour with coldest color) and \texttt{nlevels} (the 
 contour with the warmest color). The parameter \texttt{nlevels} is set by the user. You can choose not to include all the \texttt{n} yellow dots 
 found on the contour line, but instead a fraction of them:  this is controlled by the parameter \texttt{step}. 
\item If instead you want to start with \texttt{n} random points, control {n} and the locations of the yellow dots, use the code in section~\ref{p100}.
\item With the option \texttt{View='Global'}, the program generates the spectrum plot in Figure~\ref{fig:spectrum} instead of the
 more granular view in Figure~\ref{fig:ortho2}. It also produces the output data set \texttt{gradient\_LH.txt}. The later is used to create a plot of
 $f(\sigma, t)$ for various values of $\sigma$ away from where the roots are located, that is, away from $\sigma=\frac{1}{2}$ 
 if $f(\sigma,t)=|\zeta(\sigma,t)|$. This one-dimensional plot is for fixed values of $\sigma$, with $t$ being the variable. It is illustrated in Figure~\ref{fig:ts}. The goal is to see how far the dip created by the roots propagates. Or whether you can find the roots by focusing on larger values of $\sigma$, where computations are a lot easier.
\end{itemize}

% https://problemsolvingwithpython.com/06-Plotting-with-Matplotlib/06.15-Quiver-and-Stream-Plots/

% https://towardsai.net/p/tutorials/the-gradient-descent-algorithm 
% https://realpython.com/gradient-descent-algorithm-python/  logistic regression 1D

\subsection{Mathematical version of gradient descent and orthogonal trajectories}\label{tretra}

The purpose of this section is to explain the traditional mathematical approach to 
 \textcolor{index}{gradient descent}\index{gradient descent} optimization [\href{https://en.wikipedia.org/wiki/Gradient_descent}{Wiki}],
 also known as \textcolor{index}{steepest descent}\index{steepest descent}. In the end, the method described 
 in section~\ref{porbug} is a discretized version of this technique, although it does not require the target function -- in this case $|\zeta(\sigma,t)|$ --
 to be differentiable. In addition, I also explain how the orthogonal trajectories are typically obtained with a mathematical formulation. Finally, I show how the two methods -- no math versus math -- are related.

In the traditional math formulation, one starts with a rough estimate or guess for the location of the optimum. In other words, one of the yellow dots
 in Figure~\ref{fig:ortho2}. Then after successive iterations following downward the red curve attached to the yellow point, you end up at the location of a minimum in case of convergence, here a blue dot. As in Figure~\ref{fig:ortho2}, which location you end up with may depend on your starting location, unless the function is concave and has only one minimum. 

One major difference is that you can have an infinite number
 of iterations and indefinitely increase the accuracy of the solution. In the discrete version, you are limited by the granularity of the grid: to increase accuracy, you need a finer grid. This is why you see a few overlapping blue dots very close to each other in Figure~\ref{fig:ortho2}: we are getting very close to the optimum regardless of where we start from, but to get further improvements (so that the distinct blue dots are indistinguishable to the naked eye), you need a finer grid.  In my example, the precision is about 0.005 and determined by the parameters  
\texttt{incr\_t} and \texttt{incr\_sigma}. After getting a good approximation, it is possible to start over again, but this time with a much narrower grid around the optimum, and increased precision.

Now, here is how iteration $n+1$ looks like when trying to solve the problem in mathematical terms, starting with $(\sigma_0,t_0)$. To be as general as possible, I replaced $|\zeta(\sigma,t)|$ by an arbitrary function $f(\sigma,t)$.

$$
\begin{aligned}
      t_{n+1} & = t_n - \lambda \frac{\partial f}{\partial t} (\sigma_n, t_n)\\
      \sigma_{n+1} & = \sigma_n - \lambda' \frac{\partial f}{\partial \sigma} (\sigma_n, t_n)
\end{aligned}
$$
where $\lambda,\lambda'$ are the learning rate parameters corresponding to \texttt{learn\_t}, \texttt{learn\_sigma} in section~\ref{porbug}.
The iterated $(\sigma_n,t_n)$ may or may not converge depending on the learning rates: this is an issue you don't have to face with my
 discretized version.

One of the features of my method is the computation of contour lines and orthogonal trajectories at once, allowing you to plot the convergence paths for a large number of points, in little time. The time-consuming step is creating the \texttt{za} array that contains the values of the function  computed at a large number of evenly spaced locations. There is little literature about how to obtain the equations of the orthogonal trajectories, yet the problem is rather elementary. It requires solving a differential equation. See \href{https://math24.net/orthogonal-trajectories.html}{here} for a simple explanation. This tutorial explains  why the orthogonal trajectories to a family of concentric circles are straight lines going through the common center. It then shows that for a family of ellipses, the orthogonal trajectories are confocal hyperbolas satisfying a specific parametric equation.

The principle is simple. Each contour line satisfies an equation $f(\sigma, t) =\gamma$ where $\gamma$ is a parameter called the
 \textcolor{index}{contour level}\index{contour level}. Let $g(\sigma,t) = \gamma$ define an orthogonal trajectory. The orthogonality between the two means that $\nabla^T f(\sigma, t) \cdot  \nabla g(\sigma, t) = 0$, where $\nabla$ is the gradient operator, $^T$ is the transpose operator turning a column into a row vector, and the product is a dot product [\href{https://en.wikipedia.org/wiki/Dot_product}{Wiki}] between vectors. 
Thus, you need to solve the following differential equation, where $g$ is the unknown:
$$
 \frac{\partial f}{\partial t} \frac{\partial g}{\partial t} + \frac{\partial f}{\partial \sigma} \frac{\partial g}{\partial \sigma} = 0.
$$ 
The solution depends on a parameter, in the same way that a contour line depends on the contour level. The way it is implemented in the discretized version is as follows. The vector $(dx,dy)^T$ 
is the grid-based  gradient of $f$, used to produce the orthogonal trajectories. Then 
  $(dy, -dx)^T$ is the grid-base gradient of $g$, used to produce the contour lines. The dot product is $dx \cdot dy - dy \cdot dx = 0$. For the actual gradient, divide $dx$ and $dy$ respectively by  $dt$ and $d\sigma$. 
%  (representing using the notation in the Python code). 
In the Python code,  $dt$ and $d\sigma$ are represented by the variables \texttt{incr\_t} and by \texttt{incr\_sigma}.
Also, $dx$ and $dy$ are represented by \texttt{dx} and \texttt{dy}. This leaves the dot product unchanged and equal to zero.

A benefit of my method is that it is easy to implement even if you don't have a function $f$ to begin with, but instead data points with a measurement at each location: in short, a real-life dataset.



\section{Distribution of minima and the Riemann Hypothesis}\label{po0xz}

The purpose of my investigations was to study the minima of 
 $|\zeta(\sigma+it)|$ in a specific region, typically $\frac{1}{2}\leq \sigma \leq 2$. Not from a mathematical point of view, but using empirical methods and exploratory analysis, hoping to gain some insights. This section describes my approach and findings in more details.
 The main results are shown in Figures~\ref{fig:ortho2}, \ref{fig:spectrum}, and~\ref{fig:ts}. 

Figure~\ref{fig:ortho2} shows that $|\zeta(\sigma+it)|$ does not have basins other than those attached to a root. In other words, if it has a local minimum, this minimum must be a global minimum as well; that is, a root of $\zeta$. This is empirically confirmed by looking at various rectangular regions. The question is whether this is a unique feature of $\zeta$, or not. It turns out that this is a widespread feature shared  by all \textcolor{index}{analytic functions}\index{analytic function} [\href{https://en.wikipedia.org/wiki/Analytic_function}{Wiki}]. Another consequence is that at $\sigma=\frac{1}{2}$, the univariate function $|\zeta(\sigma,t)|$ -- a function of $t$ -- only has global minima (the roots) and no local minima. In an nutshell, an analytic function in the complex plane is one that has a Taylor series for both real and complex arguments. This is the case for $\zeta$. Then the  
\textcolor{index}{minimum modulus principle}\index{minimum modulus principle} [\href{https://en.wikipedia.org/wiki/Maximum_modulus_principle}{Wiki}] guarantees that all minima are global and correspond to a root. Another property of analytic function is
 that they satisfy the \textcolor{index}{Cauchy-Riemann equations}\index{Cauchy-Riemann equations} [\href{https://en.wikipedia.org/wiki/Cauchy\%E2\%80\%93Riemann_equations}{Wiki}]. As a result, the imagimary and real parts (both are real-valued functions) are orthogonal: you can use the real part for contour lines, and the imaginary part for orthogonal trajectories, or the other way around. For details, see
 \cite{conformalmap} pages 24--26.

The modulus of $\zeta$ has no local minima, but it has 
 \textcolor{index}{saddle points}\index{saddle point} [\href{https://en.wikipedia.org/wiki/Saddle_point}{Wiki}] as evidenced in Figure~\ref{fig:ortho2}. The absence of local minima in the middle of the rectangular window means that roots at $\sigma=\frac{1}{2}$ 
 have ripple effects reverberating well beyond $\sigma = 1$. This is illustrated in Figures~\ref{fig:spectrum} and~\ref{fig:ts}: values of
 $t$ producing a minimum at $\sigma=\frac{1}{2}$ tend to produce a minimum as well at (say) $\sigma=1.1$, though that minimum 
 may be very slightly shifted to the left or right. If not a minimum, at least an inflection point in $|\zeta(\sigma+it)|$ at $\sigma=1.10$. The effect is even more 
 pronounced at (say) $\sigma=0.80$. 
%Indeed the function $\zeta$, and thus all its zeroes, is 
% entirely determined by the values that it takes at integer arguments greater than $1$. This is best illustrated in the following formula:





\begin{figure}%[H]
\centering
\includegraphics[width=0.65\textwidth]{ts.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{$|\zeta(\sigma,t)|$  at $\sigma=1.2$ ($200<t<300$); vertical bars correspond to roots at $\sigma=\frac{1}{2}$}
\label{fig:ts}
\end{figure}


\subsection{Root taxonomy}\label{oaqwzx}

Roots are usually categorized based on whether they are simple of multiple, real or complex, conjugate or not, or whether their moduli is larger than one, smaller than one, or equal to one. The later case can be broken down in two subcategories: the root is equal to one or not. This has various implications in many areas, see for instance chapter~3 in my book~\cite{vgelsevier}. However, for 
$|\zeta(\sigma,t)|$ and all the other test functions investigated here, they all belong to a same category: simple, conjugate, complex, with modulus strictly larger than one. So how could we possibly categorize them?

In our context, there are infinitely many roots, all with the real part $\sigma = \frac{1}{2}$. They are somewhat randomly distributed. Some are close to each other, sometimes there is a large gap with no root. In the test functions, I also added roots with $\sigma = 0.7$ to check whether it has any impact in my analysis. The results discussed here were obtained with \texttt{View='Global'} in the Python code. The
 output file \texttt{gradient\_LH.txt} contains all the data needed. Rather than focusing on $\sigma=\frac{1}{2}$, to categorize the roots I look at
 $\sigma = 0.7$, $\sigma = 1.0$ and $\sigma =1.2$. The results are featured in Figure~\ref{fig:ts}. 

It turns out that what is happening at $\sigma=\frac{1}{2}$ has a strong impact on the behavior at (say) $\sigma=1.2$, and even more pronounced at (say) $\sigma=0.8$. Conversely, what happens at $\sigma=1.2$ can be used to detect the roots: at least their general behavior, and the values of their imaginary part $t$ with some accuracy. The real part $\sigma$ is known to be $\frac{1}{2}$, but even if this fact was not known -- say you know that the real part is somewhere between $\sigma=0.4$ and $\sigma=0.6$ -- this analysis would still be valid: you would be able to detect the imaginary part $t$, and unusually large dips in the function $|\zeta(\sigma, t)|$ observed at $\sigma=1.2$ would suggest not only the presence of a root down below, but it would also suggest that its real part $\sigma$ could be rather large (thus closer to $\sigma=1.2$), compared to other roots.  

All of this is visible in Figure~\ref{fig:ts}. Indeed, a root at $(\sigma,t)=(\frac{1}{2},t_0)$ could be categorized using three metrics: the value of $|\zeta(\sigma,t)|$ at the corresponding dip  or inflection point around $t =t_0$, observed at $\sigma=1.2$, $\sigma=1.0$ and $\sigma=0.8$. In 
 Figure~\ref{fig:ts} all the roots generate a dip or inflection point in the orange function 
$|\zeta(1.2,t)|$. 
The value of $t$ where the dip occurs almost matches the imaginary part of the corresponding root. In addition, if two roots are close to each other, as it was one root with multiplicity 2, 
 the corresponding dip is more pronounced.



\subsection{Studying root propagation with synthetic math functions}

In some ways, the methodology in section~\ref{oaqwzx} is similar to analyzing the spectrum of a star to identify its chemical composition.
Here the star is $|\zeta(\sigma,t)|$, and its composition is its root system. The darkest blue lines in the spectrum plot in Figure~\ref{fig:spectrum} 
 identify the roots. Likewise, the vertical blue lines in Figure~\ref{fig:ts} also identify the roots. In this case, directly observing the roots at $\sigma=\frac{1}{2}$ is not trivial. The borderline $\sigma=1$ is what separates
 simple math from advanced complex analysis. Indeed, you need to be able to build the unique analytic continuation of $\zeta(\sigma,t)$ to
 analyze what is happening when $\sigma\leq 1$. To the contrary, when $\sigma>1$, elementary techniques work.

The obvious next step is to see how the methodology works with functions other than $\zeta(\sigma,t)$, that do have a similar root system. 
 It leads to the same conclusions. To achieve this goal, I checked a few simple functions that mimic the root system in question, 
 but that can be computed a lot faster. The synthetic function that I tested is the following:
$$
f(\sigma, t) = \Big| \cosh\Big(\sigma-\frac{1}{2} +\frac{1}{4} it\log t\Big) \cdot \cosh\Big(\sigma - \frac{7}{10} + i t\log 2\Big)\Big|.
$$
It is denoted as \texttt{Sinh2} in the Python code. In addition to roots at $\sigma=\frac{1}{2}$, I added roots at $\sigma = 0.7$. As expected, roots with $\sigma=0.7$ create a bigger dip at $\sigma=1.2$  than those with $\sigma=\frac{1}{2}$.  Again,
 $|\cdot|$ represents the modulus. Here $\cosh$  is the hyperbolic cosine. The function is real-valued as I am interested in the modulus only. However, it has complex arguments. The MPmath Python library, dealing with special math functions in the complex plane,  makes all these computations easy. 

In general, functions that have an infinite product, called 
\textcolor{index}{Hadamard product}\index{Hadamard product} [\href{https://en.wikipedia.org/wiki/Riemann_zeta_function#Hadamard_product}{Wiki}] in the case of $\zeta$, are good candidates to test the method. The most well-known examples are the sine and cosine functions with real roots. The hyperbolic sine and cosine have a similar product, but with roots in the complex plane. I chose them because of this feature and the fact that all their roots have the same $\sigma$, just like for $\zeta$. Also, all their roots are known exactly, in contrast to $\zeta$. 

%p = np.log(2)
   %         z = mpmath.cosh(complex(sigma-0.5, t*np.log(t)/4))* \
      %             mpmath.cosh(complex(sigma-0.7, p*t)) 


\section{Python code}\label{pc991}

Section~\ref{iudwlmn} features the main program that generates all the figures and videos discussed in this chapter.  The starting points to illustrate convergence paths are all sampled on a contour line. If instead you are interested in starting points at arbitrary, random locations, you
 need to replace the main section of the program by the code in section~\ref{p100}. The latter produces a cool video: you can watch it 
 \href{https://www.youtube.com/watch?v=pqQsLpPkvbw}{here}. 


\subsection{Contours and orthogonal trajectories}\label{iudwlmn}

The parameters are described in section~\ref{porbug} and~\ref{ppzsdxvb}. The Python code \texttt{gradient.py} is also on my 
GitHub repository, \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/gradient.py}{here}. \vspace{1ex}

\begin{lstlisting}
import matplotlib.pyplot as plt
from matplotlib import cm # color maps
import numpy as np
import mpmath

View = 'Local'       # options: 'Local' or 'Global'
Function = 'Sinh2'   # options: 'Zeta', 'Eta', 'Sinh1', 'Sinh2'
Contour = 'Lines'    # options: 'Lines', 'Surface'
Video = True         # options: True or False
Granularity = 'Low'  # options: 'Low' or 'High'

if View == 'Local':
    min_t = 201.4    # for zeta, choose 201.0 
    max_t = 202.601  # for zeta, choose 203.301 
    min_sigma = 0.26 # for zeta, choose 0.25 
    max_sigma = 0.85 # for zeta, choose 1.60 
    if Granularity == 'High':
        incr_t = 0.0025 # slow, also requires 4x more memory
    elif Granularity == 'Low':
        incr_t = 0.005  # 4x faster than 'High', less accurate 
    incr_sigma = incr_t*(max_sigma - min_sigma)/(max_t - min_t)    
elif View == 'Global':
    min_t = 200
    max_t = 300
    min_sigma = 0.50
    max_sigma = 1.10
    incr_t = 0.05 
    incr_sigma = 0.05
nlevels = 180 # number of contour levels (120 for zeta)

#--- Store values of bivariate function in grid za[( , )]
#     xa[], y[a] are the x and y coordinates

xa = np.arange(min_t, max_t, incr_t)
ya = np.arange(min_sigma, max_sigma, incr_sigma)
xa, ya = np.meshgrid(xa, ya)
k_steps = 1 + int((max_t - min_t)/incr_t)
h_steps = 1 + int((max_sigma - min_sigma)/incr_sigma)
za = np.empty(shape=(len(xa),len(ya)))  # set dimensions for za

k=0
for t in np.arange(min_t, max_t, incr_t):
    print("t=",t) 
    h = 0
    for sigma in np.arange(min_sigma, max_sigma, incr_sigma):    
        if Function == 'Zeta':
            z = mpmath.zeta(complex(sigma, t)) 
        elif Function == 'Eta':
            z = mpmath.altzeta(complex(sigma, t))
        elif Function == 'Sinh2':
            p = np.log(2)
            z = mpmath.cosh(complex(sigma-0.5, t*np.log(t)/4))* \
                   mpmath.cosh(complex(sigma-0.7, p*t)) 
        elif Function == 'Sinh1':
            z = mpmath.cosh(complex(sigma-0.5, t*np.log(t)/4))
        modulus=np.sqrt(z.real*z.real + z.imag*z.imag)
        za[h,k]=modulus
        h = h + 1
    k = k + 1

#--- 3D surface plot 

fig = plt.figure(figsize =(12, 8), dpi=240)
axes = plt.axes(projection ='3d')
axes.set_xlabel('t', fontsize=6)
axes.set_ylabel('sigma',fontsize=6)
axes.set_zlabel('Eta Modulus',fontsize=6)
axes.tick_params(axis='both', which='major', labelsize=6)
axes.tick_params(axis='both', which='minor', labelsize=6)
axes.set_xlim3d(min_t, max_t)
axes.set_ylim3d(min_sigma, max_sigma)
axes.set_zlim3d(0.0, float(np.max(za)))

surf = axes.plot_surface(xa, ya, za, cmap=cm.coolwarm,
                       linewidth=1, antialiased=True)

#--- Create video of 3D surface plot (rotate the plot)

if Video:

    import moviepy.video.io.ImageSequenceClip  # to produce mp4 video
    from PIL import Image  # for some basic image processing

    Nframes = 250 
    flist=[]               # list of image filenames for the video
    w, h, dpi = 4, 3, 300  # width and heigh in inches
    fps=10                 # frames per second

    for frame in range(0,Nframes): 
        image='RH4_'+str(frame)+'.png' # filename of image in current frame
        print("Creating image",image) # show progress on the screen
        angle_vertical = 30 + 30 * np.sin(12*frame/Nframes)
        angle_horizontal = 80+ frame * 360 / Nframes
        axes.view_init(angle_vertical, angle_horizontal)
        plt.savefig(image,bbox_inches='tight')
        im = Image.open(image)
        if frame == 0:  # all images must have the same size
            width, height = im.size
            width=2*int(width/2)
            height=2*int(height/2)
            fixedSize=(width,height)
        im = im.resize(fixedSize) 
        im.save(image,"PNG")
        flist.append(image)

    # output video 
    clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(flist, fps=fps) 
    clip.write_videofile('RH4.mp4')

#--- Create contour map

fig = plt.figure(figsize =(12, 8), dpi=240)
axes = plt.axes()
fig.colorbar(surf, ax = axes, shrink = 0.5, aspect = 5)
# Add horizontal dashed line at sigma = 0.5 (the 'critical line') 
#     https://matplotlib.org/stable/gallery/lines_bars_and_markers/linestyles.html
plt.plot((min_t,max_t),(0.5,0.5),color='black',linestyle=(0,(15,15)),linewidth=0.2)

if View == 'Global':
    CS = axes.contourf(xa, ya, za, levels=nlevels, cmap=cm.coolwarm) 
    plt.savefig('RH4_contours.png',bbox_inches='tight')
    sigma_low  = 0.5 # must be > min_sigma
    sigma_mid  = 0.7
    sigma_high = 1.0   # must be < max_sigma (h_high < h_steps -1)
    sigma_max  = max_sigma
    h_low  = int(0.5+(sigma_low - min_sigma)/incr_sigma)
    h_mid  = int(0.5+(sigma_mid - min_sigma)/incr_sigma)
    h_high = int(0.5+(sigma_high - min_sigma)/incr_sigma)
    h_max  = h_steps-2
    OUT=open("gradient_LH.txt","w")
    header="\tsigma=%f\tsigma=%f\tsigma=%f\tsigma=%f\n" \
            % (sigma_low,sigma_mid,sigma_high,sigma_max)
    OUT.write(header)
    for k in range(k_steps-1):
        t = min_t + incr_t*k
        z_low  = za[h_low, k]
        z_mid  = za[h_mid, k]
        z_high = za[h_high, k]
        z_max  = za[h_max, k]
        line = str(t)+"\t"+str(z_low)+"\t"+str(z_mid)+"\t"+str(z_high)+"\t"+str(z_max)+"\n"
        OUT.write(line)
    OUT.close()
    exit() 
else:
    CS = axes.contour(xa, ya, za, levels=nlevels, cmap=cm.coolwarm, linewidths=0.75) 
    plt.savefig('RH4_contours.png',bbox_inches='tight')

#--- Steepest descent on contour map: sample paths to minimum (a root of |Zeta|)
#    requires: Granulary = 'High', View = 'Local'

def gradient_descent(t, sigma, showStart, showEnd, showPath, mode, n_iter, \
    learn_t, learn_sigma, type):

#   mode = Ascent or Descent, starting point is (t, sigma)
#   type = Gradient (orthogonal trajectory) or Contour
#   learn = learning rate in gradient method; n_iter = number of iterations
#   showStart, showEnd display start/end points if true, on the plot
#   in all cases, the full path is displayed in red on the plot

    x = []
    y = []
    x.append(t)
    y.append(sigma)
    h = int((sigma - min_sigma)/incr_sigma)
    k = int((t - min_t)/incr_t)
    old_z = za[h,k]
    if showStart:
        plt.plot(t, sigma, marker="o", markersize=5, markerfacecolor="yellow",color="black")
    if mode == 'Descent': 
        sign = +1
    elif mode == 'Ascent':
        sign = -1
         
    iter = 0
    while iter < n_iter:
        if type == 'Gradient':
            t = t - learn_t * sign * dx[h,k]/incr_t    
            sigma = sigma - learn_sigma * sign * dy[h,k]/incr_sigma     
        elif type == 'Contour':
            t = t - learn_t * sign * dy[h,k]/incr_sigma  
            sigma = sigma + learn_sigma * sign * dx[h,k]/incr_t 
        if t>min_t and t<max_t and sigma>min_sigma and sigma<max_sigma:   
            x.append(t)
            y.append(sigma)
        old_z = za[h, k]
        h = int(0.5+(sigma - min_sigma)/incr_sigma)
        k = int(0.5+(t - min_t)/incr_t)             
        if h<h_steps-2 and k<k_steps-2 and h>0 and k>0: 
            z = za[h, k]
        else:
            iter = 99999999999
            showEnd = False
        iter = iter + 1

    # smooth the path x, y
    n = len(x)
    if smooth > 0:
        for i in range(1,n-2):
            x[i] = np.mean(x[max(0,i-smooth):min(n-1,i+smooth)])
            y[i] = np.mean(y[max(0,i-smooth):min(n-1,i+smooth)])

    # plot path from intitial point to convergence
    if showPath:
        plt.plot(x,y,color='red',linewidth=0.2)
    if showEnd:   
        # show where the iteration converged
        sigma=int(0.5+100*sigma)/100
        t=int(0.5+100*t)/100
        if t>min_t and t<max_t and sigma>min_sigma and sigma<max_sigma:
            plt.plot(t, sigma, marker="o", markersize=5, markerfacecolor="palegreen",color="black")

    return(x, y)  

#--- Steepest descent: main part 


dy, dx = np.gradient(za)      # matrices with same dim as za 
norm = np.sqrt(dx*dx + dy*dy) # matrix with same dim as za
angle = np.arctan2(dx,dy)     # angle of descent (unused)

smooth = 0                    # integer, to smooth tajectories (0 = no smoothing)
learn_t = incr_t              # learning parameter in gradient method  
learn_sigma = incr_sigma      # learning parameter in gradient method 

showStart = False
showEnd   = False
showPath  = False

# sample points on a contour line
#     need Ascent and Descent for full loop of contour line

level = 40 # level=1 is the one with lowest za[,]. For zeta, set level=36 
           # level must be an integer between 1 and nlevels
lines = []  
x = []
y = []
for line in CS.collections[level].get_paths():
    lines.append(line.vertices)
x = lines[0][:, 0]
y = lines[0][:, 1]
n = len(x)
showStart = True
showEnd   = False
showPath  = True
n_iter = 3000 
step = 5 # for zeta choose step=10

for i in range(0, n, step):  
    t = x[i]
    sigma = y[i]
    # need Ascent and Descent for full path up and down
    gradient_descent(t, sigma, showStart, showEnd, showPath, 'Descent', n_iter, \
       learn_t, learn_sigma, 'Gradient')
    gradient_descent(t, sigma, showStart, showEnd, showPath, 'Ascent', n_iter, \
       learn_t, learn_sigma, 'Gradient')

plt.savefig('RH4_ortho.png',bbox_inches='tight')
\end{lstlisting}

\subsection{Animated gradient descent starting with 100 random points}\label{p100}

This code produces the video available \href{https://www.youtube.com/watch?v=pqQsLpPkvbw}{here}. To run it, use the main program in section~\ref{iudwlmn}, and replace the section
``Steepest descent: main part" by the code below, also available on GitHub \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/gradient_animated.py}{here}.\vspace{1ex}

\begin{lstlisting}
#--- Steepest descent: main part 

# import moviepy.video.io.ImageSequenceClip 

dy, dx = np.gradient(za)      # matrices with same dim as za 
smooth = 0                    # integer, to smooth tajectories (0 = no smoothing)
learn_t = incr_t              # learning parameter in gradient method  
learn_sigma = incr_sigma      # learning parameter in gradient method 

n = 100
np.random.seed(101)
xx = np.random.uniform(min_t,max_t,n)
yy = np.random.uniform(min_sigma,max_sigma,n)
showEnd   = False
showPath  = True
n_iter = 1 
flist = []
fps = 4

for frame in range(200):
    
    print("frame",frame)
    image='RH4_ortho'+str(frame)+'.png'
    if frame == 0:
        showStart = True
    else:
        showStart = False

    for i in range(0, n):  
        t = xx[i]
        sigma = yy[i]
        (x, y) = gradient_descent(t, sigma, showStart, showEnd, showPath, 'Descent', n_iter, \
           learn_t, learn_sigma, 'Gradient')
        m = len(x)
        xx[i] = x[m-1]  # new t attached to starting point i
        yy[i] = y[m-1]  # new sigma attached to starting point i
    plt.savefig(image,bbox_inches='tight')
    flist.append(image)

# output video 
clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(flist, fps=fps) 
clip.write_videofile('RH4_ortho.mp4')
\end{lstlisting}

%-------------------------------------------------------------------------------------------------------------------

\Chapter{Machine Learning Cloud Regression and Optimization}{The Swiss Army Knife of Optimization} \label{chap1v}
%%%%%%%%%%%%5\begin{abstract} 
%The Swiss Army Knife of Optimization

This chapter is not about regression performed in the cloud. It is about considering your data set as a cloud of points or observations, where the
 concepts of dependent and independent variables (the response and the features) are blurred. It is a very general type of regression, offering
 backward-compatibility with existing methods. Treating a variable as the response amounts to setting a constraint on the multivariate parameter, and results in an optimization algorithm with  Lagrange multipliers. The originality comes from unifying and bringing under a same umbrella, a number of disparate methods each solving a part of the general problem and originating from various fields. I also 
 propose a novel approach to logistic regression, and a generalized R-squared adapted to shape fitting, model fitting, \textcolor{index}{feature selection} 
and  \textcolor{index}{dimensionality reduction}.  In one example, I show how the technique can perform unsupervised clustering, with \textcolor{index}{confidence regions} for
 the cluster centers obtained via parametric bootstrap.

Besides ellipse fitting and its importance in \textcolor{index}{computer vision}\index{computer vision}, an interesting application is non-periodic sum of periodic time series. While rarely discussed in machine learning circles, such models explain many phenomena, for instance ocean tides. It is particular useful in time-continuous situations where the error is not a white noise, but instead smooth and continuous everywhere. For instance, granular temperature forecast.  Another curious application is modeling meteorite shapes. Finally, my methodology is model free and data driven, with a focus on \textcolor{index}{numerical stability}. Prediction intervals and confidence regions
 are obtained via bootstrapping. I provide Python code and \textcolor{index}{synthetic data} generators for replication purposes. 
%%%%%%%%%%%%%%%%%%\end{abstract}

\hypersetup{linkcolor=red}


%\listoffigures


\section{Introduction: circle fitting}\label{circ1}

The goal is to unify all regression techniques and present a simple, generic  framework to solve
 most problems dealing with fitting an equation to a data set. Currently, there are dozens of types of regressions, each with its own
 methodology and algorithm. Here I propose a single methodology and a single algorithm to solve all these problems.

The originality of my technique resides in my approach and methodology, rather than in the type of math or algorithm being used. Like all generic methods, it is rather abstract and  one would think more difficult to learn and describe. To the contrary, I believe it is 
 actually more intuitive and easier to grasp. First,
 the dependent variable and independent features are interchangeable: the concept of dependent variables is even absent in my methodology. Thus I call it ``cloud regression", as the data set is viewed as a cloud of points, with no particular axis or dimension being privileged unless explicitly required. 
 Then the technique is model-free: it uses resampling and bootstrap to build prediction intervals, or confidence intervals for the 
 regression coefficients. 

A judicious choice of notations makes my methodology backward-compatible with all existing techniques. The concept of \textcolor{index}{R-squared} is slightly modified  
 to offer extra possibilities: measuring the quality of the fit for the full model versus a sub-model of your choice. 
 In standard regression, the sub-model is a constant and referred to as the base model. Here the sub-model could be fitting a circle if the full model 
 is about ellipses, or fitting a plane versus an hyperplane in standard linear regression.

All regression books and chapters for beginners start with fitting a line. Here the easiest example -- the first one to be taught -- is fitting a circle centered at the origin. Think about it for a moment: intuitively, the estimated radius is the average radius computed on the data points. Indeed, this is the solution
 produced by my technique. The second easiest case is then fitting a line involving a slope and an intercept. Both examples are a particular case of fitting a quadratic form (ellipsoid). 

This presentation is intended to beginners. There are examples, just as in standard regression, where the solution is not unique. In my opinion, non-uniqueness should be embraced rather than avoided: in real life one would expect that multiple, different shapes can fit to a particular data set. Finding several of them provides more insights about your data. However, conditions needed for uniqueness are not discussed here: this is the topic of a more advanced presentation. 

In many cases, thanks to an appropriate re-parameterization, the solution is obtained using simple constrained optimization and Lagrange multipliers. It has more to do with elementary calculus than advanced matrix algebra. In particular, 
there is no explicit mention of \textcolor{index}{eigenvalues}\index{eigenvalue} 
[\href{https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors}{Wiki}] or 
 \textcolor{index}{singular value decomposition}\index{singular value decomposition} [\href{https://en.wikipedia.org/wiki/Singular_value_decomposition}{Wiki}]. Also, the shape does not need to have derivatives, though if it does, a faster implementation is possible, with a Newton-like algorithm. Indeed, the shape may be differentiable nowhere: think about fitting a Brownian motion to a set of observations. 

I provide examples using \textcolor{index}{synthetic data}\index{synthetic data} [\href{https://en.wikipedia.org/wiki/Synthetic_data}{Wiki}] and Python code. One of them involves time series forecasting with two periods $p,q$ where $p/q$ is not a rational number. Since $p$ and $q$ are among the parameters to be estimated, this is a true non-linear problem that can not be transformed into something linear via a \textcolor{index}{link function}\index{link function} [\href{https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function}{Wiki}], unlike (say) logistic regression.  
A real life application, to benchmark the performance of the method, is predicting ocean tides: large, granular geospatial data sets are available to test
 the prediction algorithm in this non-linear context. 


Finally, ``cloud regression" encompasses the \textcolor{index}{general linear model}\index{general linear model} [\href{https://en.wikipedia.org/wiki/General_linear_model}{Wiki}], 
the \textcolor{index}{generalized linear model}\index{generalized linear model} [\href{https://en.wikipedia.org/wiki/Generalized_linear_model}{Wiki}] (and thus logistic regression), 
 as well as \textcolor{index}{weighted least squares}\index{weighted least squares} [\href{https://en.wikipedia.org/wiki/Generalized_least_squares#Weighted_least_squares}{Wiki}]
 (and thus Bayesian regression). Via the mapping $z\mapsto w$ discussed in section~\ref{prevmeth}, it can accommodate splines as in  
\textcolor{index}{adaptive regression splines}\index{regression splines} [\href{https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline}{Wiki}].   
 Both cloud regression and the \textcolor{index}{total least squares}\index{total least squares} method [\href{https://en.wikipedia.org/wiki/Total_least_squares}{Wiki}]  minimize the sum of the squared distances between the data points and the shape, though my method does not give the  response (called 
 the dependent variable by statisticians) a particular status: in other words, it also works in the standard situation where there is no response, but just a cloud of points instead. 
 In addition, my technique handles truly non-linear situations, unlike the generalized linear model. For that reason, I call it the mother of all regressions.

This is not the first time a regression technique does not discriminate between dependent and independent variables: \textcolor{index}{partial least squares}\index{partial least squares} [\href{https://en.wikipedia.org/wiki/Partial_least_squares_regression}{Wiki}] 
  also allows for that.  See also~\cite{fit2015}. 


\subsection{Previous versions of my method}\label{prevmeth}\label{mapmp}

The current version is much more general, simpler and radically different from the first implementation. However, it may help to provide
 some historical context. Initially, the goal was to compute the sum of the squared distances between a set of points (the observations, or the ``cloud"), and
 a pre-specified shape $\Gamma_\theta$ (a line, plane or circle) belonging to a parametric family driven by a multidimensional parameter $\theta$.   

The idea was as follows. Let $P_\infty$ be a fixed point located extremely far away from the shape, and $P$ be a point of the \textcolor{index}{training set}. Draw the line
 $L_P$ that goes through $P$ and $P_\infty$, and find the intersection $\Gamma_\theta \cap \L_P$ closest to $P$, between the shape and the line. Let $Q_\theta$ be this point. The point in question may not be unique or may not exist (depending on $\theta$), but the distance $\Delta_\theta(P)=||P-Q_\theta||$ is, assuming there is an intersection. Then find $\theta^*$ that 
minimizes the sum of $\Delta_\theta(P)$ computed over all training set points $P$. This $\theta^*$, if unique, determines the shape that best fits the data. Traditional projection-based techniques compute the exact distance between a point and a shape, and therefore require the shape to be differentiable. The method based on
 $P_\infty$ works with shapes that are not differentiable. Some particular cases in the new implementation produce similar or identical results to those obtained with the $P_\infty$ method.

If the shape in question is an hyperplane and the context is traditional multivariate linear regression, then the shape is defined by 
$g(w,\theta)=0$ where $w=(y,x_1,\dots,x_m)$ and $g(w,\theta)=\theta_0 y+(\theta_1 x_1+\cdots +\theta_m x_m)$. Here $y$ corresponds to the dependent variable, $x_1,\dots, x_m$ to the features, and $\theta_0, \theta_1,\dots,\theta_m$ are the regression coefficients, with the constraint 
$\theta_0=-1$. In the new methodology, the constraint $\theta_0=-1$ is handled using a Lagrange multiplier, but other than that, it leads to the same traditional solution. If there is an intercept, then $x_1=1$. In the end, the goal is to propose a technique that is both general and intuitive,
 following the modern trend of \textcolor{index}{explainable AI}\index{explainable AI} [\href{https://en.wikipedia.org/wiki/Explainable_artificial_intelligence}{Wiki}].

In a second version of my methodology (not the current version), I introduced a mapping system, which essentially is a change of coordinates 
 associated to a link function. The
 goal was to transform the data so that after the mapping, it is more amenable to a simple solution. Also, it is an attempt at
 obtaining a scale-independent solution: whether your unit is a mile or a kilometer should have no impact on the solution. In its most general form, the observations and parameters are denoted as $z$ and $\varphi$. The shape satisfies the equation $h(z,\varphi)=0$. The mapping is defined as 
$g(w,\theta)=\xi(h(z,\varphi))$ where $\xi : \mathbb{R} \rightarrow \mathbb{R}$ is a strictly monotonic function, with $w=\nu(z)$ and $\theta
 = \phi(\varphi)$, for some multivariate one-to-one mappings $\nu$ and $\phi$. All the computations are done in the $(w,\theta)$-space, thought it is possible to revert back to the original $(z,\varphi)$ when computations are done, if ever needed. 

I eventually dropped both $\xi$ and simply ignored $\varphi$ and $\phi$, leading to a less abstract model, yet covering all practical cases. 
Thus in the current version, $h(z,\varphi)=g(w,\theta)$, and we don't care about $\varphi$. We may as well use $\varphi=\theta$. The mapping $\nu$ gives rise to spline regression in the new method. However, when splines are used, they are pre-specified rather than estimated, to avoid over-fitting. Typically, they are chosen to simplify the computations.

Finally, I was interested in some original dimension reduction technique. Not a true data reduction technique, but it allows you to reduce the number of parameters by a factor two: consider $w$ and $\theta$ to be complex, rather than real numbers, for instance via a mapping $z \mapsto w$ from $\mathbb{R}^2$ to 
$\mathbb{C}$, with $w=\nu(z)$. A benefit is the possibility to use
 a \textcolor{index}{conformal map}\index{conformal map} [\href{https://en.wikipedia.org/wiki/Conformal_map}{Wiki}] for $\nu$, thus preserving angles. Such an example is the \textcolor{index}{log-polar map}\index{log-polar map} [\href{https://en.wikipedia.org/wiki/Log-polar_coordinates}{Wiki}] $z=\exp(w)$ with
$g(w,\theta)=z^\theta=\exp(\theta w)$, which corresponds to using the polar coordinate system with $\theta,z,w\in\mathbb{C}$: it makes things easier when dealing with circular data. The next step was to look at quaternions to reduce the number of parameters by a factor four, but there are a number of challenges. Anyway, I promised to keep things simple in this introductory 
 presentation, so I won't discuss complex or quaternion mappings here. This is the topic of future research. 

It is interesting to note that the problem of circle fitting has been quite extensively studied, see~\cite{ieee200y}. Essentially, these
 methods solve the problem using $\varphi$ and they are not trivial. The solution based on my method 
 involves working with $\theta$ and leads to a very classic algorithm with a simple solution. The price to pay is that the $\theta$
 parameters are less obvious to interpret: they are the coefficients of a quadratic form. To the contrary, the direct solution 
 involves $\varphi$ parameters that have obvious meaning: the radius of the circle, its center and (in case of an ellipse) the rotation angle. 
However, my approach makes it a lot easier to generalize to ellipses
 and even far more complicated shapes, or hyperplanes for that matter, while at the same time having a solution that is even simpler than those discussed in~\cite{ieee200y} and applicable to
 the circle only. Of course, in this case there is a one-to-one mapping between $\varphi$ and $\theta$, see 
 \href{https://math.stackexchange.com/questions/1810677/center-and-axis-of-quadratic-surface}{here}. So you can always retrieve
 $\varphi$ from $\theta$. 

  
\section{Methodology, implementation details and caveats}

I encourage you to first read section~\ref{prevmeth}, as it provides a good amount of context. This section describes the details of the methodology. 
For simplicity, I do not describe the most general case, but a case that is general enough to cover all practical applications. I start by introducing the concept of data (called point cloud), parameter, and shape.

The {\bf data} set is denoted 
 as $W$, and consists of $m+1$ variables and $n$ observations. Thus $W$ is a $n \times (m+1)$ matrix as usual. The $k$-th row 
 corresponds to the $k$-th observation $W_k=(W_{k,0},W_{k,1},\dots,W_{k, m+1})$. For backward compatibility with traditional models, I use the notation $W_{k,0}=Y_k$ for the dependent variable or response (if there is one), and $(X_{k,1},\dots,X_{k,m})=(W_{k,1},\dots,W_{k, m+1})$ for
 the independent variables of features. The column vector corresponding to the response is denoted as $Y$, and the $n\times m$ matrix 
 representing the independent variables is denoted as $X$. The whole data set $W$ is referred to as the point cloud. 

The {\bf parameter} is a multivariate
 column vector denoted as $\theta=(\theta_0,\theta_1,\dots,\theta_d)$, with $d+1$ components. Typically, $d=m$ and $\theta$ satisfies some  constraint, specified by $\eta(\theta)=0$
 for some function $\eta$. The most common functions are $\eta(\theta)=\theta^T\theta-1$, $\eta(\theta)=\theta_0+1$, and
  $\eta(\theta)=(\theta_0+\cdots + \theta_d) - 1$. Here $^T$ denotes the matrix/vector transposition operator.

The purpose is to fit a {\bf shape} to the point cloud. The most typical shapes, after proper mapping, are hyperplanes or quadratic forms (ellipsoids). The
 former is a particular case of the latter. The shape belongs to a parametric family of equations driven by the multivariate parameter $\theta$. The equation of the shape is $g(w,\theta)=0$, for some function $g$. Typical examples include $g(w,\theta)=w\theta$ and $g(w,\theta)=w\theta-1$, with $d=m$. 
The former usually involves an intercept: $X_{k,1}=1$ for all $k=1,\dots,n$. Keep in mind that $w$ and $\theta$ are vectors, but $g(w,\theta)$ is a real number, not a vector. Thus $w\theta$ represents a \textcolor{index}{dot product}\index{dot product} [\href{https://en.wikipedia.org/wiki/Dot_product}{Wiki}]. 

\subsection{Solution, R-squared and backward compatibility}

The shape that best fits the data corresponds to $\theta=\theta^*$, obtained by minimizing the squares:
\begin{equation}
\theta^* = \underset{\theta}{\arg\min} \sum_{k=1}^n g^2(W_k,\theta).\label{tyrefd}
\end{equation}
The solution may not be unique. Uniqueness and \textcolor{index}{numerical stability} will be addressed in a future article, but the basics are covered in this document. The constraint $\eta(\theta)=0$ guarantees that the solution requires solving a (sometimes non-linear) 
 system of $d+2$ equations with $d+2$  unknowns. In some cases, $d\leq m$ to avoid \textcolor{index}{model identifiability}\index{model identifiability} issues
 [\href{https://en.wikipedia.org/wiki/Identifiability}{Wiki}]. Also, a large $d$ may result in \textcolor{index}{overfitting}\index{overfitting} 
 [\href{https://en.wikipedia.org/wiki/Overfitting}{Wiki}].  Then, you want $n > d$ otherwise the solution may  not be unique unless you add more 
 constraints on $\theta$. The solution $\theta^*$ is obtained by solving the system
\begin{equation}
\centering
\left\{\begin{split}
 \sum_{k=1}^n \nabla_\theta [g^2(W_k,\theta)] & =\lambda \nabla_\theta [\eta(\theta)],\\
 \eta(\theta) & =0 \\
\end{split}\right. \label{bgvcx}
\end{equation}
where $\nabla_\theta$ is the \textcolor{index}{gradient}\index{gradient operator} operator with respect to $\theta$ [\href{https://en.wikipedia.org/wiki/Gradient}{Wiki}], and $\lambda$ is called the \textcolor{index}{Lagrange multiplier}\index{Lagrange multiplier} [\href{https://en.wikipedia.org/wiki/Lagrange_multiplier}{Wiki}]. This is a classic constrained convex optimization problem. The top part of~(\ref{bgvcx}) consists of a system of $d+1$ equations with $d+2$ unknowns 
  $\theta_0,\dots\theta_d$ and $\lambda$. The bottom part is a single equation with $d+1$ unknowns  $\theta_0,\dots\theta_d$. Combined together,
 it constitutes a system of $d+2$ equations with $d+2$ unknowns. Note the analogy with \textcolor{index}{Lasso regression}\index{Lasso regression} 
 [\href{https://en.wikipedia.org/wiki/Lasso_(statistics)}{Wiki}] when $\eta(\theta)=\theta^T\theta - 1$, that is, when $\theta^T\theta=1$. 

\noindent The \textcolor{index}{mean squared error}\index{mean squared error} (MSE) relative to a particular $\theta$ is defined as  
\begin{equation}
\text{MSE}(\theta)=\frac{1}{n}\sum_{k=1}^n g^2(W_k,\theta) \geq \text{MSE}(\theta^*). \label{vgbe4} 
\end{equation}
The inequality in~(\ref{vgbe4})  is an immediate consequence of~(\ref{tyrefd}). Now define the 
 \textcolor{index}{R-squared}
\index{R-squared} with respect to 
 $\theta$ as 
\begin{equation}
R^2(\theta)=1 - \frac{\text{MSE}(\theta^*)}{\text{MSE}(\theta)}. \label{rsqwa}
\end{equation}
It follows immediately that $0\leq R^2(\theta)\leq 1$. A perfect fit corresponds to $\text{MSE}(\theta^*)=0$ (the whole cloud residing on the shape). In that case,  if $\theta\neq \theta^*$ and the optimum $\theta^*$ is unique, then $R^2(\theta)=1$.

In traditional linear regression, the R-squared is defined as $R^2(\theta_*)$ where $\theta_*$ is the optimum $\theta$ for the base model. 
The base model corresponds to all the coefficients $\theta_i$ attached to the independent variables set to zero, except the one attached to the intercept. In other words, in the base model, the predicted $Y$ is constant, equal to the empirical mean of $Y$. As a result,  $\text{MSE}(\theta_*)=\text{Var}[Y]$, the empirical variance of $Y$. A consequence is that $R^2(\theta_*)$ is the square of the
 correlation between the observed response $Y$, and the predicted response of the full model. 


Backward compatibility with traditional linear regression works as follows. The standard univariate regression corresponds to 
$g(w,\theta) = w \theta =\theta_0 y +\theta_1 x + \theta_2$, with the constraint $\theta_0=-1$.  Thus $g(w,\theta)=0$ if and only if
 $y= \theta_1 x + \theta_2$. This generalizes to multivariate regression as well.
A more elegant formulation in the new methodology is to replace the constraint $\theta_0=-1$ by the symmetric constraint $\theta_0^2+\theta_1^2+\theta_2^2=1$. 
 Note that $w$ is a row vector and $\theta$ is a column vector. 

\subsection{Upgrades to the model}

By model, I mean the general setting of the method: there is no probabilistic model involved in this discussion. 
\textcolor{index}{Prediction intervals}\index{prediction interval} [\href{https://en.wikipedia.org/wiki/Prediction_interval}{Wiki}] for the individual error $g(W_k,\theta^*)$ at each data point $W_k$ (or for the estimated response attached to $Y_k$, if there is an independent variable) and 
 \textcolor{index}{confidence regions}\index{confidence region} [\href{https://en.wikipedia.org/wiki/Confidence_region}{Wiki}] for $\theta^*$ can be obtained via re-sampling and \textcolor{index}{bootstrapping}\index{bootstrapping} [\href{https://en.wikipedia.org/wiki/Bootstrapping_(statistics)}{Wiki}]. This is also true for points outside the training set. 

Also, the squares 
 can be replaced by absolute values, as in \textcolor{index}{quantile regression}\index{quantile regression} [\href{https://en.wikipedia.org/wiki/Quantile_regression}{Wiki}], to minimize the impact of outliers and for scale preservation: if a variable is measured in years, then squares are expressed in squared years, a metric that is meaningless. This leads to a modified, better metric to assess the quality of the fit, replacing the R-squared.  
The \textcolor{index}{goodness-of-fit} (say, the R-squared) should be measured on the \textcolor{index}{validation set}\index{validation set}
 [\href{https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets}{Wiki}] even though $\theta^*$ is computed on a subset of the \textcolor{index}{training set}: this is a standard practice, called \textcolor{index}{cross-validation}\index{cross-validation} [\href{https://en.wikipedia.org/wiki/Cross-validation_(statistics)}{Wiki}].

Now, let's get back to the 
\textcolor{index}{R-squared}\index{R-squared}. In standard linear regression, the R-squared is defined as $R^2(\theta_*)$ via 
 Formula~(\ref{rsqwa}), where 
 $\theta_*$ is the optimum $\theta$ for the base model (the predicted response is constant, equal the mean of $Y$ for the base model). In the
 new methodology, there may be no response. Still, the definition of $R^2$ extends to that situation, and is compatible with the traditional version.
 What's more, it leads to many possible $R^2$, one for each sub-model (not just the base model), and this is true too for the standard regression. 
A sub-model corresponds to adding constraints on the parameter vector $\theta$, or in other words, working with a subset of the parameter space. Let $\theta_*$ be the optimum for a specific sub-model, while $\theta^*$ is the optimum for the full model. Then the definition of $R^2$, depending on $\theta_*$, is unchanged. It could not be any simpler! 

Now you can use $R^2$ for model comparison purposes and even for \textcolor{index}{feature selection}\index{feature selection} [\href{https://en.wikipedia.org/wiki/Feature_selection}{Wiki}]. You can test the improvement obtained by using the full model over a sub-model,  
 with the metric $S(\theta_*)= R^2(\theta^*)-R^2(\theta_*)$. Here $\theta_*$ is the optimum $\theta$ attached to the sub-model. Obviously, $0\leq S(\theta_*)\leq 1$. The larger $S(\theta_*)$, the bigger the improvement. Conversely, the smaller, the better the performance of the sub-model. Examples include fitting an ellipse (full model) versus fitting a circle (sub-model) or using all the features (full model) versus using a subset (sub-model). 
You can compare sub-models and rank them according to $S(\theta_*)$. This allows you to identify the smallest set of features that achieve a good
 enough $S(\theta_*)$, for 
%\textcolor{index}{dimensionality reduction}
\textcolor{index}{dimensionality reduction}
\index{dimensionality reduction} purposes [\href{https://en.wikipedia.org/wiki/Dimensionality_reduction}{Wiki}]. 


Finally, another update consists of using positive weights $\psi_k(\theta)$ in Formula~(\ref{tyrefd}). This amounts to performing
 \textcolor{index}{weighted regression}\index{weighted regression} [\href{https://en.wikipedia.org/wiki/Generalized_least_squares#Weighted_least_squares}{Wiki}].
 For instance, data points far away from the optimum shape, that is observations with a large $g^2(W_k,\theta^*)$, may be discarded to reduce the
 impact of outliers. Or the weights can be used to balance the coefficients $\theta_i$, in an effort to achieve scale-invariance in the expression
 $w\theta$.  Then the top system in~(\ref{bgvcx}) becomes
\begin{equation}
 \sum_{k=1}^n \psi_k(\theta)\nabla_\theta [g^2(W_k,\theta)] +\sum_{k=1}^n g^2(W_k,\theta)\nabla_\theta[\psi_k(\theta)]  =\lambda \nabla_\theta [\eta(\theta)].  \label{bgvcx2_1228}
\end{equation}



\section{Case studies}

In section~\ref{2ways}, I show how to solve the logistic regression. The first version is standard least squares, to further illustrate
backward compatibility with the traditional method. The second one illustrates how it could be done if you want to follow the spirit of the new methodology.  Then I discuss two fundamental examples based on synthetic data.

\subsection{Logistic regression, two ways}\label{2ways}

In the traditional setting, $w=(y,x)$ where $y$ is the response, and $x$ the features. For the 
\textcolor{index}{logistic regression}
\index{logistic regression} [\href{https://en.wikipedia.org/wiki/Logistic_regression}{Wiki}],
we have 
$$
g(w,\theta)=g(y,x)= y-F(x\theta), \quad \text{with } \text{ } x\theta=\theta_1 x_1 + \dots \theta_m x_m.
$$
Here $x_1=1$ corresponds to the intercept, thus we have $m-1$ actual features $x_2,x_3,\dots,x_m$. There is no constraint on the parameter $\theta$,
 thus there is no function $\eta(\theta)$. In Formula~(\ref{bgvcx}), $\eta(\theta)=0$ should be ignored, and $\lambda=0$. The function $F$ is
 a cumulative distribution function with a symmetric density around the origin. In this case, $F(x\theta)=1/(1+\exp[-x\theta])$ is the
  standard \textcolor{index}{logistic distribution}\index{logistic distribution}\index{distribution!logistic} [\href{https://en.wikipedia.org/wiki/Logistic_distribution}{Wiki}].

In the new methodology, one would proceed as follows. First, the original data is denoted as $z=(v,u)$. The logistic regression applies to the original data. Here $v$ is the response, 
 and $u$ the feature vector. The parameter $\theta$ is unchanged (not subject to a mapping), and still denoted as $\theta$.  This regression can be stated as
$$
g(z,\theta)=g(v,u)= v-F(u\theta), \quad \text{with } \text{ } u\theta=\theta_1 u_1 + \dots \theta_m u_m.
$$

The first step is to map $z=(v,u)$ onto $w=(y,x)$, with the hope of simplifying the problem, as discussed in section~\ref{mapmp}. This is done
 via the \textcolor{index}{link function}\index{link function} $y=F^{-1}(v)=\log[v/(1-v)]$ and $u=x$. Now we are back to 
$$
g(z,\theta)=g(w,\theta)=g(y,x;\theta)= y-x\theta, \quad \text{with } \text{ } x\theta=\theta_1 x_1 + \dots \theta_m x_m.
$$
This is how standard linear regression is expressed in the new framework. But it is still the traditional linear regression, with nothing new. The final step
 consists in extending $\theta$, adding one component $\theta_0$ to $\theta_1,\dots,\theta_m$. With the new $\theta$ (still denoted as $\theta$) we have $g(w,\theta)=w\theta=\theta_0 w_0+\cdots + \theta_m w_m$. You need to add one constraint on $\theta$. The constraint $\theta_0=-1$, 
 that is $\eta(\theta)=\theta_0+1$, yields the exact same solution as traditional linear regression. But $\theta^T\theta=1$, that is $\eta(\theta)=\theta^T\theta-1$, makes the problem somehow symmetric, and more elegant.

However, in many applications, the response $v$ in the original space is either $0$ or $1$, such as cancer versus non-cancer, or fraud versus non-fraud.  In this case, the link function is undefined. The mapping with the link function works if the response is a proportion, strictly between zero and one. Otherwise, the standard logistic regression  is the best approach. 
 A possible workaround is to use for $F$ a distribution with a finite support, such as uniform on $[a,b]$. Afterall, the observed values (the features) are always bounded anyway. Then, intuitively, given $\theta$, estimates of $a$ and $b$ are proportional respectively to the minimum and maximum of $U_k\theta$, over $k=1,\dots,n$. 

This suggests a new approach to logistic regression. First, use the model $v=F_\theta(u\theta)$ in the $(v,u)$-space, where $0\leq v\leq 1$ and 
$F_\theta$ is the
 \textcolor{index}{empirical distribution}\index{empirical distribution} [\href{https://en.wikipedia.org/wiki/Empirical_distribution_function}{Wiki}] of $u\theta$ given $\theta$. Then choose $\theta^*$ that minimizes the sum of squared residuals:
$$
\theta^* =\underset{\theta}{\arg\min}\sum_{k=1}^n g^2(V_k,U_k;\theta)=\underset{\theta}{\arg\min}\sum_{k=1}^n (V_k - F_\theta(U_k\theta))^2.
$$
 Remember, $U_k$ is a row vector, and $\theta$ is a column vector; the dot product $U_k\theta$ is a real number. Also,
 $V_k$ is the binary response attached to the $k$-th observation, while $U_k$ is the corresponding $m$-dimensional feature vector, both in the original 
 $(v,u)$-space. The empirical distribution $F_\theta$ is computed as follows: $F_\theta(t)$ is the proportion of observed feature vectors, among $U_1,\dots,U_n$, satisfying  $U_k\theta\leq t$.
Such a method could be called \textcolor{index}{CDF regression}\index{CDF regression}. You can use the methodology presented here to solve it, but it would be very computer-intensive, because $F_\theta$ depends on $\theta$
 in a non-obvious way. The predicted value for $V_k$, is $F_{\theta^*}(U_k\theta^*)$ in this case.
 

\subsection{Ellipsoid and hyperplane fitting}

This is a fundamental example, with hyperplanes being a particular case of ellipsoids. I illustrate the methodology with an example based on synthetic data, in a small dimension. The idea is to represent the shape with a quadratic form. In two dimensions, the equations is 
$$
\theta_0 x^2 + \theta_1 xy + \theta_2 y^2 + \theta_3 x + \theta_4 y + \theta_5=0.
$$
The trick is to re-write it with artificial variables $w_0=x^2, w_1=xy,w_2=y^2, w_3=x,w_4=y,w_5=1$, so that we can use the general framework
 with $g(w,\theta)=w\theta$. Again, $w\theta$ is the dot product. To avoid the trivial solution $\theta^*=0$, let's add the constraint
 $\theta^T\theta=1$, that is, $\eta(\theta)=\theta^T\theta-1$. Then, $\theta^*$ is solution of  the system
\begin{equation}
\centering
\left\{\begin{split}
( W^TW -\lambda I)\theta & =0, \\
 \theta^T\theta & =1. \\
\end{split}\right. \label{bgvcx2} %xxxxxx bgvcx2b
\end{equation}
The above solution is correct in any dimension. It is a direct application of~(\ref{bgvcx}). Here $W$ is the $n \times 6$ matrix containing the $n$ observations. Thus, $W_{k0}=X_k^2, W_{k1}=X_kY_k, W_{k2}=Y_k^2,  W_{k3}=X_k, W_{k4}=Y_k, W_{k5}=1$. The Python code and additional details, for a slightly different version with a slightly different $\eta(\theta)$, can be found \href{https://scipython.com/blog/direct-linear-least-squares-fitting-of-an-ellipse/}{here}. I use it in my own code, available on my GitHub repository, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/fittingEllipse.py}{here}, under the name \texttt{fittingEllipse.py}.  It is based on Halir's article about fitting ellipses~\cite{Halir98numericallystable}.

The Python code checks if the fitted shape is actually an ellipse. However, in the spirit of my methodology, it does not matter if it is an ellipse, a parabola, an hyperbola or even a line. The uniqueness of the solution is unimportant: indeed, if two very different solutions (say an ellipse and a 
 parabola) yield the same minimum mean squared error and are thus both optimal, it says something about the data set, 
 something interesting to know.   However, it would be interesting to compute $R^2(\theta_*)$ using
 Formula~(\ref{rsqwa}), where $\theta_*$ corresponds to a circle. It would tell
 whether the full model (ellipse) over a significant improvement over the circle sub-model.

\noindent Ellipsoid fitting shares some similarities with multivariate polynomial regression~\cite{vaccari2007}.  The differences are: 
\begin{itemize}
\item Ellipse fitting is a ``full" model;  in the polynomial regression $y=\theta_1+\theta_2 x + \theta_3 x^2$, the terms $y^2$ and $xy$ are always missing.
\item Polynomial regression fits a curve that is unbounded such as $y=x^2$, resulting in poor fitting; to the contrary in ellipse fitting (if the solution is actually an ellipse) the solution is bounded.
 \item To get as many terms in polynomial regression as in ellipse fitting, the only way is to increase the degree of the polynomial, which further increases the instability of the solution.
\end{itemize}  

\noindent Finally, Umbach~\cite{ieee200y} proposes a different approach to ellipse fitting. It is significantly more complicated, and indeed, they stopped at the circle. In short, their method directly estimates the center, semi-axis lengths and rotation angle via least squares, as opposed to estimating the coefficients in the quadratic form that represents the ellipse. 

\subsubsection{Curve fitting: 250 examples in one video}

Ellipse fitting is performed by setting \texttt{mode='CurveFitting'} in the Python code. The program automatically creates a number of ellipses, specified by their parameters (center, lengths of semi-axes, and rotation angle), then generates a different training set for each ellipse, and outputs the result of the fitting procedure as an image. The images are then bundled together to produce a video, and an animated gif. Each image features a particular 
 ellipse and training set, as well as the fitted ellipse based on the training set. The ellipses parameters are set by the variable \texttt{params} in the code: it is an array with five components. The number of ellipses is set by the parameter \texttt{nframes}, which also determines the number of frames in the output video.

%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.8\textwidth]{ellipse1c.png}  
\caption{Fitted ellipse (blue), given the training set (red) distributed around a partial arc}
\label{fig:ellipse11b}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

Actually, the program does a little more: it works with ellipse arcs. Using the centroid of the training set to estimate the center of the ellipse does not work in this case. So the program retrieves the original, unknown ellipse even if the training set consists of points randomly distributed around a portion of that ellipse. The arc in question is determined by a lower and upper angle in polar coordinates,
 denoted respectively as \texttt{tmin} and \texttt{tmax} in the code, with \texttt{tmin=0} and \texttt{tmax=2$\pi$} corresponding to the full ellipse.  

The training set consists of $n$ observations generated as follows. 
First sample $n$ points on the ellipse (or the arc you are interested in). Then perturb these points by adding some noise. You have two options:
 \texttt{noise\_CDF='Uniform'} and \texttt{noise\_CDF='Normal'}. The amount of noise is specified by the parameter 
 \texttt{noise} in the code. For the uniform distribution on the square $[-a,a] \times [-a,a ]$, \texttt{noise} represents $a$. For the bivariate normal distribution with covariance matrix $\sigma^2 I$ where $I$ is the identity matrix, it represents $\sigma^2$. There are various ways of sampling points on an ellipse. Three options are offered here, set by the parameter \texttt{sampling}. 
They are described in section~\ref{pipybv}, in the paragraph ``Sampling points on an ellipse arc". The option \texttt{'Enhanced'} is the only one performing  stochastic sampling (points randomly picked up on the ellipse), and used in Figure~\ref{fig:meteor}. 


In Figure~\ref{fig:meteor}, the size of the training set is $n=30$ while in Figure~\ref{fig:ellipse11b}, $n=250$. In the code, $n$ is represented by the variable \texttt{npts}. The training set is colored in red, the fitted ellipse in blue, and if present on the image as in Figure~\ref{fig:meteor}, the
 true ellipse is in black. The latter appears as a polygon rather than an ellipse because the sampled points on the true ellipse are joined by segments, 
 and $n$ is small. Typically, the true and fitted ellipses are very close to each other, although there is a systematic bias too small to be noticed to the naked eye unless the ellipse eccentricity is high. More on this soon.

Table~\ref{ellipar} compares the exact parameter values (set by the user) of the true ellipse in
 Figure~\ref{fig:meteor},  to a few sets of estimated values obtained by least squares. Each set of estimates is computed using
 a different training set. All training sets are produced with same amount and type of noise, to give an idea of the variance of the parameter estimates  
 at a specific level of noise.  The five parameters are the ellipse center $(x_0,y_0)$, the lengths of the semi axes $(a_p, b_p)$, and the ellipse orientation
 (the rotation angle $\phi$). 

In some cases, the solution may not be unique, or could be an hyperbola or parabola rather than an ellipse. For instance, if the ellipse is reduced to a circle, any value for the rotation angle is de facto correct, though the estimated curve is still unique and correctly identified. Also, if the true ellipse has a high eccentricity, the generated white (unbiased) noise 
 forces the training set points inside the ellipse more often than they should, as opposed to outside the boundary. This is because inside the ellipse, the noise from the North side strongly overlaps with the noise from the South side, assuming the long axis is the horizontal one. The result is biased estimates for $a_p$ and $b_p$, smaller than the actual ones. In the end, the fitted curve has a higher eccentricity than the true one. The effect is more pronounced the higher the eccentricity. If the variance of the noise is small enough, there is almost no bias.  

I posted a video featuring $250$ fitted ellipses with the associated training sets, \href{https://youtu.be/ReyA9NWyjso}{here}  on YouTube. 
It is also on GitHub, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Images/ellipseFitting300dpi.mp4}{here}. The accompanying animated gif is also on GitHub, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Images/ellipse100dpi.gif}{here}. All were produced with the Python code. In the video, the transition from one ellipse to the next one is very smooth. While I use $250$ different combinations of arcs, rotation angles, eccentricities and noises to feature a large collection of very different cases, these configurations slowly progress from one frame to the next one in the video. But the $250$ frames eventually cover a large spectrum of situations. The last one shows a perfect fit, where the training set points are all on the true ellipse.


\subsubsection{Confidence region for the fitted ellipse: application to meteorite shapes}\label{rt543erzxswa}

The computation of \textcolor{index}{confidence regions} is performed by setting \texttt{mode='ConfidenceRegion'} in the Python code. This time the program automatically creates a number of training sets (determined by the parameter \texttt{nframes}), for the same ellipse  
specified by its parameters \texttt{params}: center, lengths of semi-axes, and rotation angle. 
 Then it estimates the ellipse parameters, and thus the true ellipse, uniquely determined by the parameters in question. Figure~\ref{fig:meteor} shows the confidence region for the example outlined in Table~\ref{ellipar}.

\renewcommand{\arraystretch}{1.2} %%%
\begin{table}[H]
\[
\begin{array}{lccccc}
\hline
   & x_0 & x_1  & a_p & b_p & \phi  \\
\hline
\text{Exact values} & 3.00000 & -2.50000 & 7.00000 & 4.00000 & 0.78540\\
\text{Training set } 1 & 2.61951 & -2.41818 & 6.44421 & 3.82838 & 0.72768 \\
\text{Training set } 2 & 2.77270 & -2.32346 & 6.59185 & 4.24624 & 0.59971 \\
\text{Training set } 3 & 3.29900 & -2.60532 & 6.71834 & 4.15181 & 0.87760 \\
\text{Training set } 4 & 2.71936 & -2.42349 & 7.15562 & 4.52900 & 0.80404 \\
\hline
\end{array}
\]
\caption{\label{ellipar} Estimated ellipse parameters vs true values ($n=30$), for shape in Figure~\ref{fig:meteor}}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%

Actually I decided to display a polygon instead of the fitted ellipse, by selecting the option \texttt{sampling=} \texttt{'Enhanced'}. The polygon consists of the predicted locations of the $n=30$ training set points on the fitted ellipse. These locations are obtained in the exact same way that predicted values are obtained in a linear regression problem and then shown on the fitted line. After all, ellipse fitting as presented in this section is a particular case of the general cloud regression technique. I then joined these points using segments, resulting in one polygon per training set. The superimposition of these polygons is the confidence region. 

The reason for using polygons rather than ellipses is for a particular application: estimating the shape of a small, far away celestial body based on a low resolution image.  This is particularly useful when creating a taxonomy of these bodies: the shape parameters are used to classify them and understand their history as well as gravity interactions, and can be used as features
 in a machine learning algorithm. Then, for a small meteorite, people expect to see it as a polyhedron (the 3D version of a polygon) rather than an ellipsoid. Of course, if the number $n$ of points in the training set is large, then the polyhedron is indistinguishable  from the fitted ellipsoid. But in practice, with low resolution images,  $n$ is usually pretty small. 


%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{meteorite.png}  
\caption{Confidence region in blue, $n=30$ training set points; $50$ training sets (left) vs $150$ (right)}
\label{fig:meteor}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------




\subsubsection{Python code}\label{pipybv}

The main parameters in the code are highlighted in red in this high level summary. The program 
  is listed in this section and also available on GitHub 
\href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/fittingEllipse.py}{here},
 under the name \texttt{fittingEllipse.py}.

The least square optimization is performed using an implementation of  the Halir and Flusser algorithm~\cite{Halir98numericallystable}, adapted from a version posted 
 \href{https://scipython.com/blog/direct-linear-least-squares-fitting-of-an-ellipse/}{here} by Christian Hill, the author of the book
``Learning Scientific Programming with Python" \cite{chsp2016}. The optimization -- minimizing the sum of squared errors between the observed points and the fitted ellipse -- is performed on the coefficients of the quadratic equation representing the ellipse.
 This is the easiest way to do it, and it is also the approach that I use elsewhere in this chapter. 
 The function \texttt{fit\_ellipse} does the job, while \texttt{cart\_to\_pol} converts these coefficients into meaningful features: the center, rotation angle, eccentricity and the major and minor semi-axes of the ellipse [\href{https://simple.wikipedia.org/wiki/Semi-major_and_semi-minor_axes}{Wiki}]. \vspace{1ex}

%\pagebreak
 
\noindent {\bf Sampling points on an ellipse arc}\vspace{1ex}

\noindent The Python code also integrates other components written by various authors. First, it offers three options to sample points on an ellipse or a partial arc of an ellipse, via the parameter \textcolor{red}{\texttt{sampling}} in the main section of the code: 
\begin{itemize}
\item Evenly spaced on the perimeter, via the function \texttt{sample\_from\_ellipse\_even}. The code is adapted from an anonymous version posted 
\href{https://math.stackexchange.com/questions/3710402/generate-random-points-on-perimeter-of-ellipse}{here}. It requires the evaluation of
 \textcolor{index}{elliptic integrals} [\href{https://en.wikipedia.org/wiki/Elliptic_integral}{Wiki}]. 
\item Randomly chosen on the perimeter, in such a way that on average, the distance between two consecutive sampled points on the ellipse is constant. 
 It involves sampling from a multinormal distribution, rescaling the points and then sorting the sampled points so that they are ordered on the perimeter. This 
 also requires sorting an array according to another array.  It is done in the function
 \texttt{sample\_from\_ellipse}.
\item The standard, easiest but notoriously skewed sampling. It consists of choosing equally spaced angles in the polar representation of the ellipse. For curve fitting, it is good enough with very little differences compared to the two other methods.
\end{itemize}

\noindent For sampling on a partial arc rather then the full ellipse, set the parameters 
\textcolor{red}{\texttt{tmin}} and \textcolor{red}{\texttt{tmax}} to the appropriate values, in the main loop.
These are angles in the polar coordinate system, and should lie between $0$ and $2\pi$. The full ellipse corresponds to
 \texttt{tmin} set to zero, and \texttt{tmax} set to $2\pi$. \vspace{1ex}

\noindent{\bf Training set and ellipse parameters}\vspace{1ex}

\noindent Then, to create the training set, perturb the sampled points on the ellipse via uniform or Gaussian noise. 
 The choice is set by the parameter \textcolor{red}{\texttt{noise\_CDF}} in the main section of the code. The parameter 
\textcolor{red}{\texttt{noise}} determines the amount of noise, or in other words, the noise variance. Points, be it on the ellipse or in the training set,
 are arrays with names \texttt{x} and \texttt{y} (respectively for the X and Y coordinates). The number of points in the  training set is determined
 by the parameter \textcolor{index}{\texttt{npts}}.

The shape of the ellipse is set by the $5$-dimensional parameter vector \textcolor{red}{\texttt{params}}. Its components, denoted
 as \texttt{x0}, \texttt{y0}, \texttt{ap}, \texttt{bp}, \texttt{phi} throughout the code, are respectively the center of the ellipse, the length of the semi-axes, and the orientation of the ellipse (the rotation angle). \vspace{1ex}

\noindent {\bf Confidence regions versus curve fitting} \vspace{1ex}

\noindent The program creates \textcolor{red}{\texttt{nframes}} ellipses, one at a time in the main loop.
 At each iteration, the created ellipse and training set is saved as a PNG image, for inclusion in a video or animated gif (see next paragraph). This is why the variable controlling the main loop is called  \texttt{frame}. At each iteration the true parameters of the ellipse (the ones you chose), and their least squares estimates are displayed on the screen. 

If the parameter \textcolor{red}{\texttt{mode}} is set to \texttt{'ConfidenceRegion'}, then the amount of noise and all ellipse parameters are kept constant throughout the iterations. The fitted shapes varies from one iteration to the next depending on the training set (itself depending on the noise), creating a \textcolor{index}{confidence region} for a specific ellipse, given a specific amount of noise. New fitted ellipses keep being added to the image without erasing older ones, to display the confidence region under construction. Highly eccentric ellipses result in biased confidence regions.
 The method used to build the confidence region is known as \textcolor{index}{parametric bootstrap}\index{parametric bootstrap} [\href{https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Parametric_bootstrap}{Wiki}]. 

To the contrary, if \texttt{mode} is set to \texttt{'FittingCurves'}, a different ellipse with different parameters and different amount of noise is generated at each iteration, erasing the previous one in the new image. The purpose in this case is to assess the quality of the fit depending on the amount of noise and 
 the shape of the ellipse (the eccentricity and whether you use a full or partial arc for training, in particular).\vspace{1ex}

\noindent {\bf Creating videos and animated gifs} \vspace{1ex}

\noindent At each iteration in the main loop, the program creates and saves an image in your local folder, featuring the training set in red (a cloud of dots distributed around the true ellipse arc) and the fitted ellipse in blue. The name of the image is 
\texttt{ellipsexxx.png}, where \texttt{xxx} is
 the current frame number.  At the last iteration (the last frame in the video), the true ellipse -- the one with the parameters set in the main loop -- is added to the image, in black:  it allows you to assess the bias when choosing
 the option \texttt{mode='ConfidenceRegion'}. 

The video is saved as
  \texttt{ellipseFitting.mp4}, and the animated gif as \texttt{ellipseFitting.gif}.
The parameter \textcolor{red}{\texttt{DPI}} (dots per inch) sets the resolution of the images. For videos, I recommend to set it to $300$. For animated gifs, I recommend using $100$. At the bottom of the code, when creating the video with a Moviepy function, you are free  
 to change \texttt{fps=20} to any other value. This parameter sets the number of frames per second. 
\textcolor{index}{Color transparency}\index{color transparency} [\href{https://en.wikipedia.org/wiki/Alpha_compositing}{Wiki}] is used throughout the plots, to improve the rendering when multiple curves overlap. The transparency level is denoted as \texttt{alpha} in the code. You are not supposed to play with it unless
 you don't like my choice. I mention it just in case you are wondering what \texttt{alpha} represents.

Finally, if the parameter \textcolor{red}{\texttt{ShowImage}} is set to \texttt{True}, each 
 frame is also displayed on your screen. The default value is \texttt{False}. Turn it on only
 if you produce a very small number of frames, say \texttt{nframes=10} or less.\\


\begin{lstlisting}
import numpy as np
import matplotlib.pyplot as plt
import moviepy.video.io.ImageSequenceClip  # to produce mp4 video
from PIL import Image  # for some basic image processing

def fit_ellipse(x, y):

    # Fit the coefficients a,b,c,d,e,f, representing an ellipse described by
    # the formula F(x,y) = ax^2 + bxy + cy^2 + dx + ey + f = 0 to the provided
    # arrays of data points x=[x1, x2, ..., xn] and y=[y1, y2, ..., yn].

    # Based on the algorithm of Halir and Flusser, "Numerically stable direct
    # least squares fitting of ellipses'.

    D1 = np.vstack([x**2, x*y, y**2]).T
    D2 = np.vstack([x, y, np.ones(len(x))]).T
    S1 = D1.T @ D1
    S2 = D1.T @ D2
    S3 = D2.T @ D2
    T = -np.linalg.inv(S3) @ S2.T
    M = S1 + S2 @ T
    C = np.array(((0, 0, 2), (0, -1, 0), (2, 0, 0)), dtype=float)
    M = np.linalg.inv(C) @ M
    eigval, eigvec = np.linalg.eig(M)
    con = 4 * eigvec[0]* eigvec[2] - eigvec[1]**2
    ak = eigvec[:, np.nonzero(con > 0)[0]]
    return np.concatenate((ak, T @ ak)).ravel()

def cart_to_pol(coeffs):

    # Convert the cartesian conic coefficients, (a, b, c, d, e, f), to the
    # ellipse parameters, where F(x, y) = ax^2 + bxy + cy^2 + dx + ey + f = 0.
    # The returned parameters are x0, y0, ap, bp, e, phi, where (x0, y0) is the
    # ellipse centre; (ap, bp) are the semi-major and semi-minor axes,
    # respectively; e is the eccentricity; and phi is the rotation of the semi-
    # major axis from the x-axis.

    # We use the formulas from https://mathworld.wolfram.com/Ellipse.html
    # which assumes a cartesian form ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0.
    # Therefore, rename and scale b, d and f appropriately.
    a = coeffs[0]
    b = coeffs[1] / 2
    c = coeffs[2]
    d = coeffs[3] / 2
    f = coeffs[4] / 2
    g = coeffs[5]

    den = b**2 - a*c
    if den > 0:
        raise ValueError('coeffs do not represent an ellipse: b^2 - 4ac must'
                         ' be negative!')

    # The location of the ellipse centre.
    x0, y0 = (c*d - b*f) / den, (a*f - b*d) / den

    num = 2 * (a*f**2 + c*d**2 + g*b**2 - 2*b*d*f - a*c*g)
    fac = np.sqrt((a - c)**2 + 4*b**2)
    # The semi-major and semi-minor axis lengths (these are not sorted).
    ap = np.sqrt(num / den / (fac - a - c))
    bp = np.sqrt(num / den / (-fac - a - c))

    # Sort the semi-major and semi-minor axis lengths but keep track of
    # the original relative magnitudes of width and height.
    width_gt_height = True
    if ap < bp:
        width_gt_height = False
        ap, bp = bp, ap

    # The eccentricity.
    r = (bp/ap)**2
    if r > 1:
        r = 1/r
    e = np.sqrt(1 - r)

    # The angle of anticlockwise rotation of the major-axis from x-axis.
    if b == 0:
        phi = 0 if a < c else np.pi/2
    else:
        phi = np.arctan((2.*b) / (a - c)) / 2
        if a > c:
            phi += np.pi/2
    if not width_gt_height:
        # Ensure that phi is the angle to rotate to the semi-major axis.
        phi += np.pi/2
    phi = phi % np.pi

    return x0, y0, ap, bp, phi

def sample_from_ellipse_even(x0, y0, ap, bp, phi, tmin, tmax, npts):

    npoints = 1000
    delta_theta=2.0*np.pi/npoints
    theta=[0.0]
    delta_s=[0.0]
    integ_delta_s=[0.0]
    integ_delta_s_val=0.0
    for iTheta in range(1,npoints+1):
        delta_s_val=np.sqrt(ap**2*np.sin(iTheta*delta_theta)**2+ \
                            bp**2*np.cos(iTheta*delta_theta)**2)
        theta.append(iTheta*delta_theta)
        delta_s.append(delta_s_val)
        integ_delta_s_val = integ_delta_s_val+delta_s_val*delta_theta
        integ_delta_s.append(integ_delta_s_val)
    integ_delta_s_norm = []
    for iEntry in integ_delta_s:
        integ_delta_s_norm.append(iEntry/integ_delta_s[-1]*2.0*np.pi)    
    
    x=[]
    y=[] 
    for k in range(npts):
        t = tmin + (tmax-tmin)*k/npts
        for lookup_index in range(len(integ_delta_s_norm)):
            lower=integ_delta_s_norm[lookup_index]
            upper=integ_delta_s_norm[lookup_index+1]
            if (t >= lower) and  (t < upper):
                t2 = theta[lookup_index]
                break    
        x.append(x0 + ap*np.cos(t2)*np.cos(phi) - bp*np.sin(t2)*np.sin(phi))
        y.append(y0 + ap*np.cos(t2)*np.sin(phi) + bp*np.sin(t2)*np.cos(phi))

    return x, y

def sample_from_ellipse(x0, y0, ap, bp, phi, tmin, tmax, npts): 

    x=np.empty(npts)
    y=np.empty(npts)
    x_unsorted=np.empty(npts)
    y_unsorted=np.empty(npts)
    angle=np.empty(npts)

    # sample from multivariate normal, then rescale 
    cov=[[ap,0],[0,bp]]
    count=0
    while count < npts:
        u, v = np.random.multivariate_normal([0, 0], cov, size = 1).T
        d=np.sqrt(u*u/(ap*ap) + v*v/(bp*bp))
        u=u/d
        v=v/d
        t = np.pi + np.arctan2(-ap*v,-bp*u)   
        if t >= tmin and t <= tmax:
            x_unsorted[count] = x0 + np.cos(phi)*u - np.sin(phi)*v
            y_unsorted[count] = y0 + np.sin(phi)*u + np.cos(phi)*v
            angle[count]=t
            count=count+1

    # sort the points x, y for nice rendering with mpl.plot
    hash={}
    hash = dict(enumerate(angle.flatten(), 0)) # convert array angle to dictionary
    idx=0
    for w in sorted(hash, key=hash.get):
        x[idx]=x_unsorted[w]
        y[idx]=y_unsorted[w]
        idx=idx+1

    return x, y

def get_ellipse_pts(params, npts=100, tmin=0, tmax=2*np.pi, sampling='Standard'):

    # Return npts points on the ellipse described by the params = x0, y0, ap,
    # bp, e, phi for values of the parametric variable t between tmin and tmax.

    x0, y0, ap, bp, phi = params
    
    if sampling=='Standard':
        t = np.linspace(tmin, tmax, npts)
        x = x0 + ap * np.cos(t) * np.cos(phi) - bp * np.sin(t) * np.sin(phi)
        y = y0 + ap * np.cos(t) * np.sin(phi) + bp * np.sin(t) * np.cos(phi)
    elif sampling=='Enhanced':
        x, y = sample_from_ellipse(x0, y0, ap, bp, phi, tmin, tmax, npts) 
    elif sampling=='Even':
        x, y = sample_from_ellipse_even(x0, y0, ap, bp, phi, tmin, tmax, npts) 

    return x, y

def vgplot(x, y, color, alpha, npts, tmin, tmax):

    plt.plot(x, y, linewidth=0.2, color=color,alpha=alpha) # plot exact ellipse 
    # fill gap (missing segment in the ellipse plot) if plotting full ellipse
    if tmax-tmin > 2*np.pi - 0.01:
        gap_x=[x[npts-1],x[0]]
        gap_y=[y[npts-1],y[0]]
        plt.plot(gap_x, gap_y, linewidth=0.2, color=color,alpha=alpha)
    return()

def main(npts, noise, seed, tmin, tmax, params, sampling):

    # params = x0, y0, ap, bp, phi (input params for ellipse)

    # Get points x, y on the exact ellipse and plot them
    x, y = get_ellipse_pts(params, npts, tmin, tmax, sampling)
    if frame == nframes-1 and mode == 'ConfidenceRegion':
        vgplot(x, y,'black', 1, npts, tmin, tmax)

    # perturb x, y on the ellipse with some noise, to produce training set
    np.random.seed(seed)
    if noise_CDF=='Normal':
      cov = [[1,0],[0,1]]  
      u, v = np.random.multivariate_normal([0, 0], cov, size = npts).T
      x += noise * u
      y += noise * v
    elif noise_CDF=='Uniform':
      x += noise * np.random.uniform(-1,1,size=npts) 
      y += noise * np.random.uniform(-1,1,size=npts)

    # get and print exact and estimated ellipse params
    coeffs = fit_ellipse(x, y) # get quadratic form coeffs
    print('True ellipse    :  x0, y0, ap, bp, phi = %+.5f %+.5f %+.5f %+.5f %+.5f' % params)
    fitted_params = cart_to_pol(coeffs)  # convert quadratic coeffs to params
    print('Estimated values:  x0, y0, ap, bp, phi = %+.5f %+.5f %+.5f %+.5f %+.5f' % fitted_params)
    print()

    # plot training set points in red
    if mode == 'ConfidenceRegion':
      alpha=0.1  # color transparency for Confidence Regions
    elif mode == 'CurveFitting':
      alpha=1
    plt.scatter(x, y,s=0.5,color='red',alpha=alpha)   
 
    # get points on the fitted ellipse and plot them
    x, y = get_ellipse_pts(fitted_params,npts, tmin, tmax, sampling) 
    vgplot(x, y,'blue', alpha, npts, tmin, tmax)

    # save plots in a picture [filename is image]
    plt.savefig(image, bbox_inches='tight',dpi=dpi)  
    if ShowImage:
        plt.show()
    elif mode=='CurveFitting':
        plt.close() # so, each video frame contains one curve only
    return()

#--- Main Part: Initializationa

noise_CDF='Normal'       # options:  'Normal' or 'Uniform'
sampling='Enhanced'      # options: 'Enhanced', 'Standard', 'Even'
mode='ConfidenceRegion'  # options: 'ConfidenceRegion' or 'CurveFitting' 
npts = 25                # number of points in training set

ShowImage = False # set to False for video production
dpi=100     # image resolution in dpi (100 for gif / 300 for video)
flist=[]    # list of image filenames for the video
gif=[]      # used to produce the gif image
nframes=50  # number of frames in video

# intialize plotting parameters
plt.rcParams['axes.linewidth'] = 0.5
plt.rc('axes',edgecolor='black') # border color
plt.rc('xtick', labelsize=6) # font size, x axis  
plt.rc('ytick', labelsize=6) # font size, y axis

#--- Main part: Main loop

for frame in range(0,nframes): 

    # Global variables: dpi, frame, image
    image='ellipse'+str(frame)+'.png' # filename of image in current frame
    print("Creating image",image) # show progress on the screen

    # params = (x0, y0, ap, bp, phi) : first two coeffs is center of ellipse, last one 
    #  is rotation angle, the two in the middle are the semi-major and semi-minor axes

    if mode=='ConfidenceRegion':
        seed=frame      # new set of random numbers for each image 
        noise=0.8       # amount of noise added to to training set
        # 0 <= tmin < tmax <= 2 pi
        tmin=0          # training set: ellipse arc starts at tmin
        tmax = 2*np.pi  # training set: ellipse arc ends at tmax
        params = 3, -2.5, 7, 4, np.pi/4 # ellipse parameters
    elif mode=='CurveFitting':
        seed = 100          # same seed (random number generator) for all images
        p=frame/(nframes-1) # assumes nframes > 1
        noise=3*(1-p)*(1-p) # amount of noise added to to training set
        # 0 <= tmin < tmax <= 2 pi
        tmin= (1-p)*np.pi   # training set: ellipse arc starts at tmin 
        tmax= 2*np.pi       # training set: ellipse arc ends at tmax  
        params = 4, -3.5, 7, 1+6*(1-p), 2*(p+np.pi/3) # ellipse parameters 

    # call to main function 
    main(npts, noise, seed, tmin, tmax, params, sampling)

    # processing images for video and animated gif production (using pillow library)
    im = Image.open(image)
    if frame==0:  
      width, height = im.size  # determines the size of all future images
      width=2*int(width/2)
      height=2*int(height/2)
      fixedSize=(width,height) # even number of pixels for video production 
    im = im.resize(fixedSize)  # all images must have same size to produce video
    gif.append(im)       # to produce Gif image [uses lots of memory if dpi > 100] 
    im.save(image,"PNG") # save resized image for video production
    flist.append(image)

# output video / fps is number of frames per second
clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(flist, fps=20) 
clip.write_videofile('ellipseFitting.mp4')

# output video as gif file 
gif[0].save('ellipseFitting.gif',save_all=True, append_images=gif[1:],loop=0)    
\end{lstlisting}

\subsection{Non-periodic sum of periodic time series: ocean tides}\label{tidesofheav}

In this section I consider the problem of fitting a \textcolor{index}{non-periodic trigonometric series}\index{time series!non-periodic} via least squares. One  well known example 
is the Dirichlet eta function with an infinite number of superimposed periods. Its modulus is pictured in Figure~\ref{fig:rhs3v2}.
Practical application are also numerous. A good exercise
 is to download ocean tide data, and use the methodology described in this section to predict low and high tides, at various locations: the tides are influenced mostly by two factors --
 gravitation from the moon and from the sun -- each with its own period. But the combination is not periodic. The fitting technique allows you
 to quantify the effect of each hidden component (the sun and the moon in this case) and retrieve their respective periods. 
The tide data is available 
 for free, \href{https://tidesandcurrents.noaa.gov/}{here}.

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.81\textwidth]{rhs3.png}  
\caption{Three non-periodic time series made of periodic terms}
\label{fig:rhs3v2}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------


With my notation, the problem is 
 defined by $w=(y,x)$ and
 \begin{equation}
g(y,x;\theta)= y - \Big[\theta_1 \cos(\theta_2 x + \theta_3) + \theta_4\cos (\theta_5 x + \theta_6)\Big]. \label{rtfer}
\end{equation}
There is no constraint on $\theta$, and thus no $\eta$ function and no $\lambda$ in~(\ref{bgvcx}). Here $y$ is the response, and $x$ represents the time. For this reason, I also use the notation $y=f_\theta(x)$, equivalent to $g(y,x;\theta)=0$. This type of problem is called \textcolor{index}{curve fitting}\index{curve fitting} 
 [\href{https://en.wikipedia.org/wiki/Curve_fitting}{Wiki}] in scientific computing. There are libraries to solve it: in Python,
 one can use the \texttt{optimize.curve\_fit} function from the Scipy library. For more details, see the
 \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html}{Python documentation}. 



Finally, if you are only interested in predicting $y$ given $x$ , but not in estimating the parameter vector $\theta$, then the following interpolation formula 
is sometimes useful:
\begin{equation}
y=f(x)\approx \frac{\sin\pi x}{\pi}\cdot \Bigg[ \frac{f(0)}{x} + 2x\sum_{k=1}^n (-1)^k \frac{f(k)}{x^2-k^2}\Bigg] \label{apcfv}
\end{equation}
I used it  to get a good approximation of $y=f_\theta(x)$ when $y$ is known (that is, observed) at
integer increments $x=0,1$ and so on, even though $\theta$ is not known. I applied it to a function $f_\theta(x)$ closely related
to those pictured in Figure~\ref{fig:rhs3v2}. The function in question (namely, the real part of the Dirichlet eta function) can be expressed as an infinite sum of cosine terms with different amplitudes and different periods.
 Thus, in this case, the dimension of the unobserved $\theta$ is infinite, and $\theta$ remains an hidden parameter in the prediction experiment, hidden to the experimenter (as in a blind test). Its components are the various period and amplitude coefficients. 
 The approximation formula~(\ref{apcfv}) works under certain conditions: see Exercise 22 in~\cite{vgelsevier} for details. 

\subsubsection{Numerical instability and how to fix it}

Consider the simpler case where $\theta_3=\theta_6=0$: let's drop these two coefficients from the model. Even then, 
the problem is \textcolor{index}{ill-conditioned}\index{ill-conditioned problem} [\href{https://en.wikipedia.org/wiki/Condition_number}{Wiki}]. In particular if $\theta^*=(\theta_1^*, \theta_2^*,\theta_4^*,\theta_5^*)$ is an optimum solution, so is $(\theta_4^*, \theta_5^*,\theta_1^*,\theta_2^*)$. At the very least, 
 without loss of generality, you need to add the
 constraint $|\theta_1|\geq |\theta_4|$. 

In Python, I used the \texttt{curve\_fitting} function from Scipy. It does a poor job for this problem, even if you specify bounds for the coefficients 
 $\theta_1, \theta_2,\theta_4,\theta_5$ and start the algorithm with a vector $\theta$ close to an optimum $\theta^*$. My test involved
\begin{itemize}
\item Finding an optimum $\theta$ if the fitting function is $y=f_\theta(x) =\theta_1 \cos \theta_2 x +  \theta_4 \cos \theta_5 x$,
\item Using a synthetic training set where  $y=a_1 \cos a_2 x +  a_4 \cos a_5 x + a_7 \cos a_8 x$.
\end{itemize}
 The gray curve in  Figure~\ref{fig:pyplot1}, called the ``model'', is $y=a_1 \cos a_2 x +  a_4 \cos a_5 x$, 
 while the blue one is the fitted curve (not necessarily unique), and the dots represent the observations (training set in red, validation set in orange). 
The observations points do not lie exactly on the gray curve because I introduced some noise: the third term $a_7 \cos a_8 x$ between the model and the data. Note that the observations are equally spaced with respect to the X-axis, but absolutely not with respect to the Y-axis. It is possible
 to use a different sampling mechanism to address this issue. 
 The figure was produced with the Python code in section~\ref{poihgf}. The values of $a_1,\dots,a_8$ are pre-specified. Evidently, if $a_7=0$, then an obvious optimum solution is
 $\theta_i^*=a_i$ for $i=1,2,4,5$. It provides a perfect fit. Also, the coefficient $a_7$ specifies the amount of noise in the data. 


Unfortunately,  \texttt{curve\_fitting} fails or performs very poorly in most cases. Figure~\ref{fig:pyplot1} shows one of the relatively rare cases where it
 works well in the presence of noise. This Python function is still very useful in many contexts, but not in our example. The default setting 
(\texttt{method='lm'}, to be avoided) uses a supposedly robust version of the Levenberg-Marquardt algorithm, dating back to 1977, 
 see \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html}{here}. Essentially, it gets stuck in
 local minima or fails to converge, and may even reject a manually chosen initial condition close to an optimum as ``not feasible". Surprisingly,
 increasing the amount of noise in the data, can provide improvements. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{cosines.png}  
\caption{Training set (red), validation set (orange), fitted curve (blue) and model (gray)}
\label{fig:pyplot1}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

To fix the numerical instability problem, one can use a more modern technique, such as \textcolor{index}{swarm optimization}\index{swarm optimization} [\href{https://en.wikipedia.org/wiki/Particle_swarm_optimization}{Wiki}]. The \texttt{PySwarms} library documented \href{https://pyswarms.readthedocs.io/en/latest/}{here} 
 is the Python solution. For a tutorial, see 
 \href{https://machinelearningmastery.com/a-gentle-introduction-to-particle-swarm-optimization/}{here}. A simpler home-made solution taking 
advantage of the fact that the fitting curve $f_\theta(x)$ (called regression function by statisticians) is linear both in $\theta_1$ and
 $\theta_4$,  consists in splitting the problem as follows.

\begin{itemize}
\item Step 1: Sample $(\theta_2,\theta_5)$ over a region large enough to encompass the optimum values.
\item Step 2:  Given $\theta_2,\theta_5$, find $\theta_1,\theta_4$ that minimizes $E(\theta)=\sum_{k=1}^n g^2(Y_k,X_k;\theta)$. 
\end{itemize}
Here $(Y_k,X_k)$ is the $k$-th observation in the training set, and $\theta=(\theta_1,\theta_2, \theta_4,\theta_5)$. Both steps are straightforward.  
You repeat them until you reach a point where the minimum $E(\theta)$ computed over the past iterations almost never decreases anymore. Step 2 is just a simple standard two-dimensional linear regression with no intercept, with an exact solution. One of the benefits is that if there are
 multiple minima, you are bound to find them all, without facing convergence issues. It also reduces a 4-D Monte-Carlo simulation to a 2-D one.



\subsubsection{Python code}\label{poihgf}

Despite the issues previously described, I decided to include the Python code. It shows how the \texttt{curve\_fitting} function works, beyond using the default settings.
 It is still a good optimization technique for many problems such as polynomial regression. Unfortunately, not for our problem. If you decrease
 the amount of noise from \texttt{a7=0.2} to \texttt{a7=0.1} in the code below, there is a dramatic drop in performance. Likewise, if you change the  initial $\theta_5$ (the last component in  \texttt{\textcolor{gray2}{Î¸}\_start}) 
%\texttt{{\fontspec{GFS Artemisia} Î¸}\_start} 
  from 
$1.8$ to $1.7$, the performance collapses. The exact value in this example is $\theta_5=2$ by construction; the estimated
 (final) value produced by the Python code is about $1.995$. 


\renewcommand{\arraystretch}{1.2} %%%
\begin{table}[H]
\[
\begin{array}{lrrrr}
\hline
   & \theta_1 & \theta_2  & \theta_4 & \theta_5  \\
\hline
\text{Start}	&	0.00000	&	1.00000	& 0.00000	 &  1.80000\\
\text{End}	&	0.52939	&	1.42184	&-0.67571	 &  1.99526\\
\text{Model}	&	0.50000	&	1.41421	& -0.70000	 & 2.00000  \\
\hline
\end{array}
\]
\caption{\label{tablibvc} First and last step of \texttt{curve\_fitting}, approaching the model.}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%

Table~\ref{tablibvc} shows the quality of the estimation, for the parameter vector $\theta=(\theta_1,\theta_2,\theta_4,\theta_5)$.
The procedure \texttt{curve\_fitting} starts with an initial guess \texttt{\textcolor{gray2}{Î¸}\_start} labeled ``Start" in the table, and
 ends  with the entry marked as ``End" in the table: supposedly, close to an optimum $\theta^*$. Because of the way the   \textcolor{index}{synthetic data}\index{synthetic data} is generated (within the Python code), the row marked ``Model" and consisting of the value 
 $a_1,a_2,a_4,a_5$ is always close to an optimum $\theta^*$,
 unless the amount of noise introduced in the training set is too large. The ``End" solution (the output of
 \texttt{curve\_fitting}) is based exclusively on the training set points (the red dots 
 in Figure~\ref{fig:pyplot1}), not on the \textcolor{index}{validation set} (the orange dots). Yet the approximation is unusually good, given the amount of noise.

 By noise, I don't mean a random or Gaussian noise. Here the noise is deterministic: the purpose of this test is to check how well we can predict a phenomena modeled by a superimposition of multiple cosine terms of arbitrary periods, phases and amplitudes -- for instance ocean tides over time -- if we only use a sum of two cosine terms as an approximation. This model (its generalization with more terms)  is particular useful in situations where the error is not a \textcolor{index}{white noise}\index{white noise} [\href{https://en.wikipedia.org/wiki/White_noise}{Wiki}], but instead smooth and continuous everywhere: for instance in granular temperature forecast.  

The curve fitting code, also producing Figure~\ref{fig:pyplot1}, is on my GitHub repository,
 \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/fittingCurve.py}{here},
 under the name \texttt{fittingCurve.py}, and also listed below. I use Greek letters in the code  to represent the $\theta$ vector and its
 components, for consistency reasons. Python digests them with no problem.  \\
 
\renewcommand{\arraystretch}{1.0} %%%
\renewcommand{\arraystretch}{1.4} %%%


%\lstinputlisting{fittingCurve3.py}

\begin{lstlisting}[escapechar=@]
import numpy as np
import matplotlib as mpl
from scipy.optimize import curve_fit
from matplotlib import pyplot, rc

# initializations, define functions

def fpred(x, @\textcolor{gray2}{Î¸}@1, @\textcolor{gray2}{Î¸}@2, @\textcolor{gray2}{Î¸}@4, @\textcolor{gray2}{Î¸}@5):
  y = @\textcolor{gray2}{Î¸}@1*np.cos(@\textcolor{gray2}{Î¸}@2*x)+ @\textcolor{gray2}{Î¸}@4*np.cos(@\textcolor{gray2}{Î¸}@5*x) 
  return y

def fobs(x, a1, a2, a4, a5, a7, a8):   
  y = a1*np.cos(a2*xobs)+a4*np.cos(a5*xobs)+a7*np.cos(a8*xobs)  
  return y

n=800
n_training=200  # first n_training points is training set
x=[]
y_obs=[]
y_pred=[]
y_exact=[]

# create data set (observations)

a1=0.5 
a2=np.sqrt(2)
a4=-0.7 
a5=2
a7=0.2 # noise (e=0 means no noise)
a8=np.log(2)

for k in range(n):
  xobs=k/20.0
  x.append(xobs)
  y_obs.append(fobs(xobs, a1, a2, a4, a5, a7, a8)) 

# curve fitting between f and data, on training set

@\textcolor{gray2}{Î¸}@_bounds=((-2.0, -2.5, -1.0, -2.5),(2.0, 2.5, 1.0, 2.5))
@\textcolor{gray2}{Î¸}@_start=(0.0, 1.0, 0.0, 1.8)
popt, _ = curve_fit(fpred, x[0:n_training], y_obs[0:n_training],\
    method='trf',bounds=@\textcolor{gray2}{Î¸}@_bounds,p0=@\textcolor{gray2}{Î¸}@_start) 
@\textcolor{gray2}{Î¸}@1, @\textcolor{gray2}{Î¸}@2, @\textcolor{gray2}{Î¸}@4, @\textcolor{gray2}{Î¸}@5 = popt
print('Estimates       : @\textcolor{mauve}{Î¸}@1=%.5f @\textcolor{mauve}{Î¸}@2=%.5f @\textcolor{mauve}{Î¸}@4=%.5f @\textcolor{mauve}{Î¸}@5=%.5f' % (@\textcolor{gray2}{Î¸}@1, @\textcolor{gray2}{Î¸}@2, @\textcolor{gray2}{Î¸}@4, @\textcolor{gray2}{Î¸}@5))
print('True values: @\textcolor{mauve}{Î¸}@1=%.5f @\textcolor{mauve}{Î¸}@2=%.5f @\textcolor{mauve}{Î¸}@4=%.5f @\textcolor{mauve}{Î¸}@5=%.5f' % (a1, a2, a4, a5))
print('Initial val: @\textcolor{mauve}{Î¸}@1=%.5f @\textcolor{mauve}{Î¸}@2=%.5f @\textcolor{mauve}{Î¸}@4=%.5f @\textcolor{mauve}{Î¸}@5=%.5f' % \
   (@\textcolor{gray2}{Î¸}@_start[0], @\textcolor{gray2}{Î¸}@_start[1], @\textcolor{gray2}{Î¸}@_start[2], @\textcolor{gray2}{Î¸}@_start[3]))

# predictions  

for k in range(n):
  xobs=x[k]
  y_pred.append(fpred(xobs, @\textcolor{gray2}{Î¸}@1, @\textcolor{gray2}{Î¸}@2, @\textcolor{gray2}{Î¸}@4, @\textcolor{gray2}{Î¸}@5))
  y_exact.append(fpred(xobs, a1, a2, a4, a5)) 

# show plot

mpl.rcParams['axes.linewidth'] = 0.5
rc('axes',edgecolor='black') # border color
rc('xtick', labelsize=6) # font size, x axis 
rc('ytick', labelsize=6) # font size, y axis
pyplot.scatter(x[0:n_training],y_obs[0:n_training],s=0.5,color='red')
pyplot.scatter(x[n_training:n],y_obs[n_training:n],s=0.5,color='orange')
pyplot.plot(x, y_pred, color='blue',linewidth=0.5)
pyplot.plot(x, y_exact, color='gray',linewidth=0.5)
pyplot.show()
\end{lstlisting}

\subsection{Fitting a line in 3D, unsupervised clustering, and other generalizations}

In three dimensions, a line is the intersection of two planes $A$ and $B$, respectively with equations
$g_1(w,\theta_A)=0$ and $g_1(w,\theta_B)=0$. For instance, $g_1(w,\theta_A)=\theta_0 w_0 + \theta_1 w_1 +\theta_2 w_2 
- \theta_3$. To fit the line, \vspace{1ex}
\begin{itemize}
\item let $\theta_A=(\theta_0,\theta_1,\theta_2,\theta_3)^T$ and $\theta_B=(\theta_4,\theta_5,\theta_6,\theta_7)^T$, 
\item use $\theta=(\theta_A,\theta_B)$ and $g(w,\theta)=g_1^2(w,\theta_A)+g_1^2(w,\theta_B)$ in Formula~(\ref{tyrefd}), 
\item use the constraints  $\theta_A^T\theta_A + \theta_B^T\theta_B=1$, or two constraints: $\theta_A^T\theta_A=1$ and $\theta_B^T\theta_B=1$. 
\end{itemize}
 With two constraints, we have two 
Lagrange multipliers $\lambda_A$ and $\lambda_B$ in Formula~(\ref{bgvcx}).

Likewise, if the data points are either in plane $A$ or plane $B$ and you want to find these planes based on unlabeled training set observations, proceed exactly as for fitting a line in 3D (the
 previous paragraph), but this time use $g(w,\theta)=g_1(w,\theta_A)g_1(w,\theta_B)$ instead. By ``unlabeled", I mean that you don't know which plane a training set point is assigned to.   This is actually an unsupervised clustering problem. The \textcolor{index}{training set} points (called cloud) don't have to reside 
 in two separate flat planes: the cloud consists of two sub-clouds $A$ and $B$, possibly overlapping, each with its own thickness.

This generalizes in various ways: replacing planes by ellipsoids, working in higher dimensions, or with multiple sub-clouds
 $A, B, C$ and so on. One interesting example is as follows. Training set points are distributed in two clusters $A$ and $B$, and you want to find the centers of these clusters. Typically, one uses a \textcolor{index}{mixture}\index{mixture model} [\href{https://en.wikipedia.org/wiki/Mixture_model}{Wiki}] to model this situation.
 In our model-free framework, with the convention that $w$ is a row vector and $\theta_A,\theta_B$ are column vectors, the problem is stated as 
\begin{equation}
g(w,\theta)=||w^T-\theta_A||^{p/2} \cdot ||w^T-\theta_B||^{p/2} \quad \label{gerdsa}
\end{equation}
where $w, \theta_A, \theta_B$ have same dimensions, and there is no constraint on $\theta=(\theta_A,\theta_B)$. Here, $p>0$ is an \textcolor{index}{hyperparameter}\index{hyperparameter} [\href{https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)}{Wiki}]. 
If you use an iterative algorithm to find an optimum solution $\theta^*=(\theta_A^*,\theta_B^*)$, that is, the two centers $\theta_A^*,\theta_B^*$, it makes sense to start with
 $\theta_A=\theta_B$ being the centroid of the whole cloud. The solution may not be unique. Obviously, the problem is symmetric in $\theta_A$ and 
$\theta_B$, but there may be more subtle types of non-uniqueness. 

A more general formulation, not discussed here, is to replace $w^T-\theta_A$ and $w^T-\theta_B$ respectively by 
$\Lambda(w^T-\theta_A)$ and $\Lambda(w^T-\theta_B)$, where $\Lambda$ is a square invertible matrix, considered and treated as an extra parameter,
 part of the general parameter $\theta=(\theta_A,\theta_B,\Lambda)$. As a pre-processing step, one can normalize the data, so that its center is the origin and its covariance matrix -- after a suitable rotation -- is diagonal. My method preserves the norm $||\cdot||$, under such transformations.  

\subsubsection{Example: confidence region for the cluster centers}\label{reserse}

I tested model~(\ref{gerdsa}) in one dimension with $n=1000$ observations, 
 $p=1$ and \textcolor{index}{synthetic data}\index{synthetic data} generated as a mixture of two normal distributions. The purpose was to identify the cluster centers. The results are pictured in Figure~\ref{fig:screen2}.  The centers are correctly identified, despite the huge overlap between the two clusters (the purple area in the histogram). 
The histogram shows the point distribution in cluster $A$ (blue) and $B$ (red), here for the test labeled ``Sample~$39$" in the screenshot.

I computed confidence intervals for the centers, using \textcolor{index}{parametric bootstrap}\index{parametric bootstrap} [\href{https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Parametric_bootstrap}{Wiki}]. The theoretical values for the center
 locations are $0.50$ and $1.00$. The $95\%$ confidence intervals are $[0.46,0.53]$ and $[1.00, 1.04]$. The small bias is due to the uneven point counts and variances in the generated clusters: $400$ points and $\sigma=0.3$ in $A$, versus $600$ points and $\sigma=0.2$ in $B$. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.56\textwidth]{screen2g.png}   
\caption{Finding the two centers $\theta_A^*, \theta_B^*$ in sample 39; $n=1000$}
\label{fig:screen2}
\end{figure}
%imgpy9979_2and3.PNG screen2e.png  
%-------------------------

The bias visible in Figure~\ref{fig:cr} could be exacerbated by the
\textcolor{index}{Mersenne twister}\index{Mersenne twister} pseudo-random number generator [\href{https://en.wikipedia.org/wiki/Mersenne_Twister}{Wiki}] used in \texttt{numpy.random}, especially in extensive simulations such as this one:
 see chapter~11 in~\cite{vgelsevier}. In this experiment, the twister was called $800$ million times.  Then, I used the most extreme estimates based on $40$ tests, to get the upper and lower bounds of the confidence intervals. This could have contributed to the bias as well, as it is not the most robust approach: running $400$ tests and building the confidence intervals based on test percentiles is more robust. But it requires $10$ times more computations.

Out of curiosity, I decided to plot the \textcolor{index}{confidence region}\index{confidence region} [\href{https://en.wikipedia.org/wiki/Confidence_region}{Wiki}] for $(\theta_A^*,\theta_B^*)$, this time using $\num{5000}$ tests, 
 based on one trillion \textcolor{index}{pseudo-random numbers}: $\num{5000}$ tests $\times$ $1000$ points per test $\times$ $\num{100000}$ iterations per test $\times$ two 
 coordinates. It took about two hours of computing time on my laptop. The result is displayed in Figure~\ref{fig:cr}. Not surprisingly, the confidence
 region is elliptic: see section 18.3.6 in~\cite{vgelsevier} for the explanation.

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.73\textwidth]{CR3.png}  
\caption{Biased confidence region for $(\theta_A^*,\theta_B^*)$;  same example as in Figure~\ref{fig:screen2}; true value is $(0.5,1.0)$}
\label{fig:cr}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

The implementation details are in the short
 Python code in section~\ref{pyclustrw}.  This unsupervised center-searching algorithm is told that there are two clusters, but it does not know what proportion of points belong to $A$ or $B$, nor the variances attached to
 each distribution, or which one is labeled $A$ or $B$. If the number of clusters is not specified, try different values. In section~18.6 in~\cite{vgelsevier}, I describe
 a blackbox solution to find the optimum number of clusters. 

\subsubsection{Exact solution and caveats}\label{exact5}

Let $W_k$ be the $k$-th observation ($k=1,\dots,n$) stored as a row vector, $W$ the data set (a matrix with $n$ rows) and $\theta=(\theta_A,\theta_B)$ where $\theta_A,\theta_B$ are column vectors, each with the same dimension as $W_k$. Then, according to~(\ref{tyrefd}),  any optimum solution satisfies
\begin{equation}
(\theta_A^*,\theta_B^*) = \underset{\theta}{\arg\min} \sum_{k=1}^n g^2(W_k,\theta)= \underset{\theta_A,\theta_B}{\arg\min} \sum_{k=1}^n ||W_k^T-\theta_A||^{p} \cdot ||W_k^T-\theta_B||^{p}. \label{optim1}
\end{equation}
 As usual, the \textcolor{index}{mean squared error}\index{mean squared error} (MSE) is the sum in~(\ref{optim1}) computed at $\theta^*=(\theta_A^*,\theta_B^*)$, and divided by $n$. 
It follows immediately that if $h$ is a distance-preserving mapping (rotation, symmetry or translation) and $\theta^*$ is an optimum solution for the data set $W$, then $h(\theta^*$) is optimum for $h(W)$, since MSE is invariant under such transformations. Thus, without loss of generality, one can assume that the data set $W$ is centered at the origin.

You can choose a different $p$ for each cluster -- say $p_A,p_B$ -- or a weighted sum as in Formula~(\ref{bgvcx2_1228}). 
If $p=2$ and there is only one cluster (thus no $\theta_B$), then $\theta_A^*$ is the centroid of the point
 cloud (the $W_k$'s). Now let the clusters be well separated to the point that each observation $W_k$ coincides either with the center of $A$, or the center of $B$. 
Then there is only one unique optimum: $\theta_A^*$ is the centroid of one cluster, $\theta_B^*$ is the centroid of the other cluster, and the MSE is zero.

If there are three clusters, formula (\ref{optim1})  becomes
\begin{equation}
(\theta_A^*,\theta_B^*,\theta_C^*) = \underset{\theta_A,\theta_B,\theta_C}{\arg\min} \sum_{k=1}^n ||W_k^T-\theta_A||^{p} \cdot ||W_k^T-\theta_B||^{p} \cdot ||W_k^T-\theta_C||^{p}. \label{optim2}
\end{equation}

If $p>0$ is an even integer (or both $p_A,p_B$ are even integers), then finding the optimum in~(\ref{optim1}) or~(\ref{optim2}) consists in solving a system of 
 multivariate polynomials, where the 
 unknowns are the components of $\theta_A$ and $\theta_B$. The more clusters, the higher the degrees of the polynomials. In particular, in one dimension with $p=p_A=p_B=2$, if the data set (the point cloud $W$) is centered at the origin, then the optimum $(\theta_A^*,\theta_B^*)$ satisfies 
\begin{equation}
\theta_A^* \theta_B^* =-\sigma^2_W, \quad \theta_A^*+\theta_B^*=\frac{1}{n\sigma^2_W}\sum_{k=1}^n W_k^3, 
\quad \text{with } \sigma^2_W=\frac{1}{n}\sum_{k=1}^n W_k^2. \label{hhggffd}
\end{equation}
Formula~(\ref{hhggffd}) can easily be generalized to any dimension. It is obtained by vanishing the gradient to find the minimum in~(\ref{optim1}). Unfortunately, no exact formula exists for $p=1$. However, in one dimension for $p=1$, we have
$$
|W_k-\theta_A|\cdot |W_k-\theta_B| = \frac{1}{2}\cdot | (W_k-\theta_A)^2 + (W_k-\theta_B)^2-(\theta_A -\theta_B)^2 |.
$$

Based on the few tests done so far in one dimension, in general $p=1$ works better than $p=2$. If the two clusters are moderately unbalanced as in Figure~\ref{fig:screen2}, then $p=1$ still does well. 
However, if they are strongly unbalanced as in Figure~\ref{fig:hard}, the method fails.  It is still fixable, by choosing two different $p$'s,  denoted as $p_A$ and $p_B$. Then the optimum 
corresponds to the larger $p$ attached to
 the smaller cluster: the blue one, in Figure~\ref{fig:hard}.  In this case $p_A=3,p_B=1$ works just as well  as $p_A=1, p_B=1$ does
 in Figure~\ref{fig:screen2}.
The blue cluster has $1500$ points in Figure~\ref{fig:hard}, the red one $8500$. The two centers are $0.5$ and $1.0$,
   and the standard deviations are $0.1$ and $0.2$ respectively for the small and large cluster.

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.72\textwidth]{hard2.png}  
\caption{Challenging mixture, requiring $p_A=3,p_B=1$ to identify the two cluster centers}
\label{fig:hard}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------


 It is a good practice to try $(1,1), (2,1)$ and $(3,1)$ for $(p_A,p_B)$, to see which one provides the best fit as illustrated
 in section~\ref{kmeans}. This is particularly useful in blackbox systems, when automatically processing thousands of datasets without a human being ever looking at any of them.  Because the 
 ``best" solution -- from a visual point of view -- is sometimes a local rather than a global minimum, I recommend to list all the local minima found during the search for an 
 optimum $(\theta_A^*,\theta_B^*)$. 

Of course, there is a limit to this methodology, as well as to any other classifiers or mode-searching algorithms: if the mixture
 has just one mode, it is impossible to find two distinct meaningful centers, no matter what method you use. This happens when the cluster centers are truly distinct, but the variances 
are huge, or if one cluster contains very few points. Another example is when the clusters have irregular, non-convex shapes, with multiple centers and holes. In the latter case,
 the methodology is still useful to find the local modes.

 \subsubsection{Comparison with K-means clustering}\label{kmeans}

I included the \textcolor{index}{K-means clustering}\index{K-means clustering} method [\href{https://en.wikipedia.org/wiki/K-means_clustering}{Wiki}] in the Python code in section~\ref{pyclustrw}. Here I compare Kmeans with my method, on two datasets, each 
 with $n=1000$ points and two overlapping clusters. The theoretical cluster centers based on the underlying mixture model are $0.5$ and $1.0$ respectively.  
The datasets are pictured in Figure~\ref{fig:screen2} and~\ref{fig:hard}.

On challenging data with significant cluster overlapping, my method frequently outperforms Kmeans. However, on very skewed data, you 
 need two exponents $p_A, p_B$ in Formula~(\ref{optim1}), rather than just $p$ to get the best performance. When using $(p_A, p_B)=(3,1)$, my method is denoted as $(3,1)$. Likewise, with $(p_A, p_B)=(1,1)$ or $(p_A, p_B)=(2,1)$, my method is denoted respectively as $(1, 1)$ and $(2,1)$.
Intuitively, model $(3, 1)$ -- compared to the default version $(1, 1)$ -- allows you to  reduce the influence of a highly dominant cluster $A$, by penalizing its contribution to MSE, with a small $p_A$. Due to the symmetry of the problem, model $(3, 1)$ and $(1, 3)$ yield the same optimum centers 
with labels swapped,
and the same MSE at the optimum.  
  The reference model with one 
 single cluster coinciding with the centroid of the whole dataset, is called the ``base" model. An alternative is to use 
the \textcolor{index}{medoid}\index{medoid} [\href{https://en.wikipedia.org/wiki/Medoid}{Wiki}] rather than the centroid in the base model. 

The remaining of this section, besides model comparison, focuses on automatically detecting whether the default model $(1, 1)$ is good enough for a 
specific dataset,   or whether you should use the cluster centers generated by $(2,1)$ or $(3, 1)$ instead. The decision is based on 
 the MSE defined at the beginning of section~\ref{exact5}. However comparing MSE$(1,1)$  and
MSE$(1, 3)$ even for the same $\theta$ and on the same dataset is meaningless. This is the challenge that we face. 

To solve this problem, I start by computing MSE$(1,1)$ and MSE$(3,1)$ for all methods and both data sets. 
I skipped MSE$(2,1)$ as it yields similar solutions to MSE$(3,1)$. The results are summarized in 
Table~\ref{mse111} for the first dataset, and Table~\ref{mse112} for the second dataset. The vector   
$\theta^*(1, 1)=(\theta_A^*(1, 1),\theta_B^*(1, 1))$ contains the two optimum centers according to model $(1,1)$, given a data set. The same notation  
$\theta^*(3, 1)$ 
is used for model $(3,1)$.

\renewcommand{\arraystretch}{1.2} %%%
\begin{table}[H]
\[
\begin{array}{lcccc}
\hline
  \text{Model} &  \theta_A  & \theta_B & \text{MSE}(1,1) & \text{MSE}(3,1)  \\
\hline
\theta^*(1,1)	&	\textcolor{red}{0.53554}&	\textcolor{red}{1.02221}&	\textcolor{red}{0.09804}&	0.02981\\
\theta^*(3,1)	&	\textcolor{red}{0.20602}&	\textcolor{red}{0.94284}&	0.12986&	\textcolor{red}{0.02147}\\
\text{Kmeans}	&	0.42525&	1.01235&	0.10086&	0.03049\\
\text{Base}	&	0.80392&	0.80392&	0.11673&	0.03661\\
\hline
\end{array}
\]
\caption{\label{mse111} MSE for different methods and $\theta$s, same data set as in Figure~\ref{fig:screen2}}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%
\renewcommand{\arraystretch}{1.2} %%%



\begin{table}[H]
\[
\begin{array}{lrrrr}
\hline
 \text{Model} &  \theta_A  & \theta_B & \text{MSE}(1,1) & \text{MSE}(3,1)  \\
\hline
\theta^*(1,1)	&	\textcolor{red}{0.72435} & \textcolor{red}{1.09296} & \textcolor{red}{0.05743} & 0.01092\\
\theta^*(3,1)	&	\textcolor{red}{1.06020} & \textcolor{red}{0.52378} & 0.06871 & \textcolor{red}{0.00686}\\
\text{Kmeans}	&	0.65856 & 1.09833 & 0.05857 & 0.01340\\
\text{Base}	&	0.92682 & 0.92682 & 0.06812 & 0.01156\\
\hline
\end{array}
\]
\caption{\label{mse112} MSE for different methods and $\theta$s, same data set as in Figure~\ref{fig:hard}}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%

The red entries in Tables~\ref{mse111} and \ref{mse112} correspond to an optimum for models $(1,1)$ and $(3,1)$. The centers for M$(1,1)$ in the first dataset (Table~\ref{mse111}), and for M$(3,1)$ in the second dataset (Tables~\ref{mse112}), are much closer to the true values 
$0.5$ and $1.0$ than those produced by Kmeans. However, to claim that my method is better than Kmeans, you need a mechanism to decide
 when M$(1,1)$ or M$(3,1)$ is the best fit.  This is still a work in progress. 
As a starting point, the following arguments provide empirical rules to decide.
\begin{itemize}
\item For the first data set, MSE$(1,1)$ evaluated at the centroid of the whole data set (the base model) 
 is better (lower) than when evaluated at $\theta^*(3,1)$, suggesting that $\theta^*(3,1)$ is not a great solution here. Thus the default model $(1,1)$ 
 should be
preferred to $(3,1)$ in this case.
\item For the second data set, MSE$(1,1)$ evaluated at the centroid 
 is about the same as when evaluated at $\theta^*(3,1)$, suggesting that $\theta^*(3,1)$ is not that bad, at least not as bad as in the previous case. Thus model $(3,1)$ should not automatically be ruled out in this case. It is also an indicator that this data set is more challenging than the previous one.
\item For the second data set, MSE$(3,1)$ evaluated at  $\theta^*(3,1)$ is better than when evaluated at $\theta^*(1,1)$. This does not mean
anything: of course MSE$(3,1)$ is always best at $\theta^*(3,1)$, by design. 
However the ratio of these two MSE's,  $\rho = 0.01092 / 0.00686 \approx 1.59$, is quite high here. To the contrary, for the first data set  
$\rho \approx 1.39$ is much smaller. Thus model $(3,1)$ is more justified for the second dataset than for the
 first one.
\end{itemize} 

\noindent Note that he computation of MSE$(1,1)$ or MSE$(3,1)$ is performed without knowing which cluster a point is assigned to. Indeed, point allocation is discussed nowhere in my method: you find the centers without allocating points to specific clusters. Once the two centers 
$\theta_A^*,\theta_B^*$ have been computed, each point $W_k$ is assigned to the closest cluster. Proximity is measured as the distance between the point and the cluster center. 
Then choose the model -- $(1,1)$ or $(3, 1)$ -- minimizing the sum of these distances. 


It is my guess that replacing $||W_k-\theta_A||^2$ and $||W_k-\theta_B||^2$ 
by $||W_k-\theta_A||^{p_A}$ and $||W_k-\theta_B||^{p_B}$ 
 in the Kmeans procedure will yield better results similar to my method. Again, $\theta_A, \theta_B$ are the two cluster centers, and $W_k$ is 
 the $k$-th observation. Even $p_A=p_B=1$ could lead to significant improvements in Kmeans in the presence of outliers (for instance outliers from
 a large cluster spilling over to a nearby smaller cluster). 
This approach is somewhat similar to \textcolor{index}{K-medians clustering}\index{K-means clustering} [\href{https://en.wikipedia.org/wiki/K-medians_clustering}{Wiki}]. Using linear rather than power weights may have the same effect. \vspace{1ex}

\noindent {\bf Conclusions}

\noindent My method frequently works better than Kmeans to detect the centers when clusters strongly overlap and are unbalanced. However, this assumes that you have a 
mechanism to choose between model $(1, 1)$ and $(2, 1)$ or $(3, 1)$.  If you know beforehand that your data is highly skewed as in
 Figure~\ref{fig:hard}, then model $(3, 1)$ is a good candidate to begin with. When the clusters are well separated and one or two of the distributions is asymmetric,
 model $(1, 1)$ tends to correctly identify the cluster medians, rather than the standard centers (the mean). 

The method is simple and fast:  it does not perform point allocation. You can use it to find initial center configurations as first
 approximations in more complex algorithms, or in the context of ``unsupervised" 
\textcolor{index}{logistic regression}\index{logistic regression!unsupervised} to detect the two clusters when there is no independent variable.  Exact solutions such as (\ref{hhggffd}) are available in any dimension for two and more clusters, for instance for the $(2, 2)$, $(2,2,2)$ and $(4, 2)$ models. Other original clustering algorithms are described in 
 chapter~5 in~\cite{vgelsevier}. 

Next steps: test on asymmetric synthetic data modeled as a mixture of normal and gamma distributions with unequal cluster sizes and variances, reduce volatility, investigate the model $(1, \frac{1}{3})$ -- the sister of $(3, 1)$ -- and generalize the method to two or three dimensions and more than two clusters. One of the goals is to identify when my method performs better than Kmeans, to learn more about Kmeans and further improve it. 


\subsubsection{Python code}\label{pyclustrw}

The Python code \texttt{mixture1D.py} for the one-dimensional case in section~\ref{reserse}, is also on GitHub \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/mixture1D.py}{here}. 
 Note that $W_A, W_B$ and $W$ are vectors, respectively with $n_A,n_B$ and $n$ elements. The vector operations (multiplications and so on) are  implicitly performed component-wise, without using a loop on the elements. When $p=2$, \texttt{Error} corresponds to the mean squared error. I use color transparency -- the parameter \texttt{alpha} in 
 the histogram function \texttt{plt.hist} -- to visualize the overlap between the two components of the Gaussian mixture: see result in left plot,
 Figure~\ref{fig:screen2}. 

The optimum $\theta_A^*,\theta_B^*$ (the cluster centers) are obtained via Monte-Carlo simulations, using
 $\num{100000}$ sampled $\theta_A,\theta_B$ per test. On the screenshot showing convergence to the optimum, you can see
 that $\theta_A$ and $\theta_B$ are randomly flipped back and forth within each test. The algorithm senses that there are two distinct centers; however, it can't tell which one is labeled $A$ or $B$.  Afterall, this is \textcolor{index}{unsupervised learning}\index{unsupervised learning}. To address this issue, when computing the confidence intervals, I use the notation $\theta_A$ for the center on the left, and $\theta_B$ for the other one. That is, $\theta_A < \theta_B$. 
Finally, I run $40$ tests to determine a $95\%$ confidence interval for the 
 optimum values. Results are displayed in the screenshot in Figure~\ref{fig:screen2}. Zoom in for a better view.  \\ 

\begin{lstlisting}[escapechar=@]
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm 
from sklearn.cluster import KMeans

N_tests = 5    # number of data sets being tested
n_A = 1500  # number of points in cluster A
n_B = 8500  # number of points in cluster B
n = n_A + n_B
Ones = np.ones((n)) # array with 1's
p_A = 3
p_B = 1
np.random.seed(438713)
min_@\textcolor{gray2}{Î¸}@_A =  99999999
min_@\textcolor{gray2}{Î¸}@_B =  99999999
max_@\textcolor{gray2}{Î¸}@_A = -99999999
max_@\textcolor{gray2}{Î¸}@_B = -99999999
CR_x=[]  # confidence region for (best_@\textcolor{dkgreen}{Î¸}@_A, best_@\textcolor{dkgreen}{Î¸}@_A), 1st coordinate  
CR_y=[]  # confidence region for (best_@\textcolor{dkgreen}{Î¸}@_A, best_@\textcolor{dkgreen}{Î¸}@_A), 2nd coordinate 

def compute_MSE(@\textcolor{gray2}{Î¸}@_A, @\textcolor{gray2}{Î¸}@_B, p_A, p_B, W):
    n = W.size
    MSE = (1/n) * np.sum((abs(W - @\textcolor{gray2}{Î¸}@_A * Ones)**p_A) * (abs(W - @\textcolor{gray2}{Î¸}@_B * Ones)**p_B))
    return MSE

for sample in range(N_tests):   # new dataset at each iteration

    # W_A  = np.random.normal(0.5, 2, size=n_A)
    # W_B  = 1 + np.random.gamma(8, 5, size=n_B)/4
    W_A  = np.random.normal(0.5, 0.1, size=n_A)
    W_B  = np.random.normal(1.0, 0.2, size=n_B)
    W    = np.concatenate((W_A, W_B))
    min_MSE=99999999
    print('Sample %1d:' %(sample))

    for iter in range(10000):

        @\textcolor{gray2}{Î¸}@_A = np.amin(W) + (np.amax(W)-np.amin(W))*np.random.rand()
        @\textcolor{gray2}{Î¸}@_B = np.amin(W) + (np.amax(W)-np.amin(W))*np.random.rand()
        MSE = compute_MSE(@\textcolor{gray2}{Î¸}@_A, @\textcolor{gray2}{Î¸}@_B, p_A, p_B, W)   # MSE for my method
        if MSE < min_MSE:
            min_MSE=MSE
            print('Iter = %5d  @\textcolor{mauve}{Î¸}@_A = %+8.4f  @\textcolor{mauve}{Î¸}@_B = %+8.4f  MSE = %+12.4f' \
                    %(iter,@\textcolor{gray2}{Î¸}@_A ,@\textcolor{gray2}{Î¸}@_B, MSE))
            best_@\textcolor{gray2}{Î¸}@_A = min(@\textcolor{gray2}{Î¸}@_A, @\textcolor{gray2}{Î¸}@_B)
            best_@\textcolor{gray2}{Î¸}@_B = max(@\textcolor{gray2}{Î¸}@_A, @\textcolor{gray2}{Î¸}@_B)

    if best_@\textcolor{gray2}{Î¸}@_A < min_@\textcolor{gray2}{Î¸}@_A:
        min_@\textcolor{gray2}{Î¸}@_A = best_@\textcolor{gray2}{Î¸}@_A
    if best_@\textcolor{gray2}{Î¸}@_A > max_@\textcolor{gray2}{Î¸}@_A:
        max_@\textcolor{gray2}{Î¸}@_A = best_@\textcolor{gray2}{Î¸}@_A
    if best_@\textcolor{gray2}{Î¸}@_B < min_@\textcolor{gray2}{Î¸}@_B:
        min_@\textcolor{gray2}{Î¸}@_B = best_@\textcolor{gray2}{Î¸}@_B
    if best_@\textcolor{gray2}{Î¸}@_B > max_@\textcolor{gray2}{Î¸}@_B:
        max_@\textcolor{gray2}{Î¸}@_B = best_@\textcolor{gray2}{Î¸}@_B
    CR_x.append(best_@\textcolor{gray2}{Î¸}@_A) 
    CR_y.append(best_@\textcolor{gray2}{Î¸}@_B) 
    print()

    # get centers and MSE from Kmeans method (for comparison purposes)  
    V    = W.copy()  
    km = KMeans(n_clusters=2) 
    km.fit(V.reshape(-1,1))   
    centers_kmeans=km.cluster_centers_ 
    kmeans_A=min(centers_kmeans[0,0],centers_kmeans[1,0])
    kmeans_B=max(centers_kmeans[0,0],centers_kmeans[1,0])
    MSE_kmeans = compute_MSE(centers_kmeans[0,0], centers_kmeans[1,0], p_A, p_B, V)  

    # get cluster centers, medians, global centroid and compute MSE on those, 
    # for comparison with my method and with Kmeans
    centroid=(1/n)*np.sum(W) 
    centroid_A=(1/n_A)*np.sum(W_A) 
    centroid_B=(1/n_B)*np.sum(W_B) 
    median_A=np.median(W_A)
    median_B=np.median(W_B)
    MSE_base = compute_MSE(centroid, centroid, p_A, p_B, W)  # MSE for base model
    MSE_tc1 = compute_MSE(centroid_A, centroid_B, p_A, p_B, W)  
    MSE_tc2 = compute_MSE(centroid_B, centroid_A, p_A, p_B, W)  
    MSE_true_centers = min(MSE_tc1,MSE_tc2)  
    MSE_tm1 = compute_MSE(median_A, median_B, p_A, p_B, W)  
    MSE_tm2 = compute_MSE(median_B, median_A, p_A, p_B, W)  
    MSE_true_medians = min(MSE_tm1,MSE_tm2)  # MSE for base model

    print('True centers  @\textcolor{mauve}{Î¸}@_A = %+8.4f  @\textcolor{mauve}{Î¸}@_B = %+8.4f  MSE = %+12.4f' \
           %(centroid_A,centroid_B,MSE_true_centers)) 
    print('model (%1d,%1d)   @\textcolor{mauve}{Î¸}@_A = %+8.4f  @\textcolor{mauve}{Î¸}@_B = %+8.4f  MSE = %+12.4f' \
           %(p_A,p_B,best_@\textcolor{gray2}{Î¸}@_A,best_@\textcolor{gray2}{Î¸}@_B,min_MSE)) 
    print('Kmeans        @\textcolor{mauve}{Î¸}@_A = %+8.4f  @\textcolor{mauve}{Î¸}@_B = %+8.4f  MSE = %+12.4f' \
           %(kmeans_A,kmeans_B,MSE_kmeans)) 
    print('True medians  @\textcolor{mauve}{Î¸}@_A = %+8.4f  @\textcolor{mauve}{Î¸}@_B = %+8.4f  MSE = %+12.4f' \
           %(median_A,median_B,MSE_true_medians)) 
    print('Base          @\textcolor{mauve}{Î¸}@_A = %+8.4f  @\textcolor{mauve}{Î¸}@_B = %+8.4f  MSE = %+12.4f' \
           %(centroid,centroid,MSE_base)) 
    print()
 
print('95 %% range for min(@\textcolor{mauve}{Î¸}@_A, @\textcolor{mauve}{Î¸}@_B): [%+8.4f, %+8.4f]' %(min_@\textcolor{gray2}{Î¸}@_A ,max_@\textcolor{gray2}{Î¸}@_A))
print('95 %% range for max(@\textcolor{mauve}{Î¸}@_A, @\textcolor{mauve}{Î¸}@_B): [%+8.4f, %+8.4f]' %(min_@\textcolor{gray2}{Î¸}@_B ,max_@\textcolor{gray2}{Î¸}@_B))

# intialize plotting parameters
plt.rcParams['axes.linewidth'] = 0.2
plt.rc('axes',edgecolor='black') # border color
plt.rc('xtick', labelsize=7) # font size, x axis  
plt.rc('ytick', labelsize=7) # font size, y axis

# plotting histogram and density
bins=np.linspace(min(W), max(W), num=100)
plt.hist(W_A, color = "blue", alpha=0.2, edgecolor='blue',bins=bins) 
plt.hist(W_B, color = "red", alpha=0.3, edgecolor='red',bins=bins) 
plt.hist(W, color = "green", alpha=0.1, edgecolor='green',bins=bins) 
# plt.plot(bins, 8*norm.pdf(bins,0.5,0.3),color='blue',linewidth=0.6) 
# plt.plot(bins, 12*norm.pdf(bins,1,0.2),color='red',linewidth=0.6) 
plt.show()

# plotting confidence region
if N_tests > 50:
    plt.scatter(CR_x,CR_y,s=6,alpha=0.3) 
    plt.show() 
\end{lstlisting}

%---
\section{Connection to synthetic data: meteorites, ocean tides}\label{psoriasisy}

The examples in this chapter are based on simulated data. Simulations are used in synthetic data as follows. Say you have a collection of meteorite images and the goal is to classify them based on their shape, summarized by the ellipse parameters discussed in section~\ref{rt543erzxswa}.
Each type of meteorite corresponds to a specific set of parameter values. For instance elongated meteorites have a high eccentricity. The eccentricity is determined by the ration of the parameters \texttt{ap} and \texttt{bp} in the Python code (respectively the length of the semi-major and semi-minor axes). To generate elongated meteorites, set a specific range in your simulations for the ratio in question. This \textcolor{index}{generative model}\index{generative model} technique is referred to as \textcolor{index}{parametric bootstrap}\index{parametric bootstrap}. 

Another approach is to generate a large collection of shapes, by sampling parameter values. Shapes that fit with the elongated type of meteorites constitute an artificial sample (\index{textcolor}{synthetic data}) representing this type, and can be added to your training set, creating what is called
 \textcolor{index}{augmented data}\index{augmented data}. In the context of neural networks, this technique is called \textcolor{index}{GAN}\index{GAN (generative adversarial networks)}, an abbreviation for 
\textcolor{index}{generative adversarial networks}\index{generative adversarial networks!see GAN}. To assess the quality of the fit, use a metric such as \textcolor{index}{R-squared}\index{R-squared} to measure the distance or similarity between a set of synthetic and a set of real shapes. With GAN's, one uses a classifier: if it can not discriminate between elongated synthetic shapes, and elongated meteorites in the real data, then your synthetic data for the category in question is deemed good.  Implementation of GAN in Python are discussed in~\cite{ganclouc}. Some authors use a 
utility metric~\cite{utiljrss} to measure the quality of the fit between synthetic data and the real data that it represents. 

In section~\ref{kmeans} dealing with clustering, I used a mixture of non-Gaussian distributions. The classic but less general approach is to use a 
\textcolor{index}{Gaussian mixture model}\index{Gaussian mixture model!see GMM} (GMM) and estimate the weights of each component using the \textcolor{index}{EM algorithm}\index{EM algorithm}.  

Finally, to generate synthetic time series similar to those described in section~\ref{tidesofheav} (a typical example being ocean tides), proceed as for the meteorite problem. After standardization (for instance, trend removal) your real data set may consist  of different categories: time series with 2, 3, or more periods, with large or small amplitudes, with high or low frequency (fast or slow oscillations), 
 smooth of chaotic. Each category corresponds to a subset of parameter values in the parameter space. To add synthetic data to a specific category, 
 simulate time series with parameter values in the parameter subset in question. Or simulate a large number of time series, and assign a category to each of them: the category that most closely matches a particular type of time series in your data set. Create new categories for simulated times series very different from anything you have in your training set. The comparison with real data is made based on the estimated parameter values in the training set, not with the time series themselves. The parameters summarize the time series and allows for easy clustering of time series. Their estimation is discussed in section~\ref{tidesofheav}. This leads to \textcolor{index}{Explainable AI}\index{explainable AI}, as opposed to methods where no one can explain why a specific time series is assigned to a specific category.  

%--------------------------------------------------------------------------------------------------------
\Chapter{A Simple, Robust and Efficient Ensemble Method}{Application to Natural Language Processing}\label{piereboul}

The method described here illustrates the concept of \textcolor{index}{ensemble methods}, applied to a real life NLP problem: ranking articles published on a website to 
predict performance of future blog posts yet to be written, and help decide on title and other features to maximize traffic volume and quality, and thus revenue.  
The method, called hidden decision trees (HDT), implicitly builds a large number of small usable (possibly overlapping)  \textcolor{index}{decision trees}. Observations that 
don't fit in any usable node are classified with an alternate method, typically simplified \textcolor{index}{logistic regression}. 

This hybrid procedure offers the best of both worlds: decision tree combos  and \textcolor{index}{regression} models.  It is intuitive and simple to implement. The code is written in Python, and I also offer a light version in basic Excel. The interactive Excel version is targeted to analysts interested in learning Python or machine learning. HDT fits in the same category as bagging, \textcolor{index}{boosting}, stacking and \textcolor{index}{AdaBoost}\index{AdaBoost}.  This chapter encourages you to understand all the details, upgrade the technique if needed, and play with the full code or spreadsheet as if you wrote it yourself. This is in contrast with using blackbox Python functions without understanding their inner workings and limitations. Finally, I discuss how to build model-free confidence intervals for the predicted values.

 \section{Introduction}

The technique presented here, called \textcolor{index}{hidden decision trees}\index{hidden decision trees}, blends non-standard, robust versions of 
 \textcolor{index}{decision trees}\index{decision tree} and regression. Compared to \textcolor{index}{adaptive boosting}\textcolor{index}{adaptive boosting} [\href{https://en.wikipedia.org/wiki/AdaBoost}{Wiki}], it is simpler to implement. I used it in credit card fraud detection while working at Visa, as well as for scoring Internet traffic quality and search keyword scoring and bidding at eBay. It is an \textcolor{index}{ensemble method}\index{ensemble methods} [\href{https://en.wikipedia.org/wiki/Ensemble_learning}{Wiki}], in the sense that it 
blends multiple techniques to get the best of each one, to make predictions.  Here I describe an NLP (\textcolor{index}{natural language processing}\index{natural language processing}\index{NLP (natural language processing)}) case study: optimizing website content. 
The purpose is to to predict the performance of articles published in media outlets or blogs, in particular to predict which types of articles do well.


Here the response (that is, what we are trying to predict, also called dependent variable by statisticians) is the traffic volume, measured in page views, unique page views, or number of users who read the article over some time period. Page view counts
can be influenced by robots, and ``unique page views" is a more robust metric. Also, older articles have accumulated more page views over time, while the most recent ones
 have yet to build traffic. We need to correct for this bias. 
Correcting for time is explained in section~\ref{timeab}. A simple approach is to use articles published within the last two years but that are at least six month old, in the \textcolor{index}{training set}. Due to
 the highly skewed distribution, I use the logarithm of unique page views as the core metric. 

The features, also called predictors or independent variables, are:\vspace{1ex} 

\renewcommand{\arraystretch}{1.2} %%%

\begin{table}%[H]
%\[
\begin{center}
\small
\begin{tabular}{lc}
\hline
   Feature & Comment \\
\hline
Title keywords &  binary\\
Article category & blog, event, forum\\
Publisher website or category & \\
Creation date & year/month\\
The title contains numbers & yes/no\\
The title is a question & yes/no \\
The title contains special characters & yes/no \\
Length of title & \\
Size of article & number of words \\
The article contains pictures & yes/no\\
Body keywords & binary \\
Author popularity & \\
First few words in the body & \\
\hline
\end{tabular}
%]
\caption{\label{fffdsa}List of potential features to use in the model}
%\end{array}
\end{center}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%


\noindent Each keyword is a binary feature in itself, also called \textcolor{index}{dummy variable}\index{dummy variable} [\href{https://en.wikipedia.org/wiki/Dummy_variable_(statistics)}{Wiki}]: it is set to ``yes" if the keyword is found (say) in the title, and to ``no" otherwise. I used
 a shortlist of top keywords (by volume) found in all the articles combined. Also, I used a subset, narrowed version  
of the features in Table~\ref{fffdsa}: for instance, title keywords only, and whether the article is a blog or not.

 The method takes into account at all potential \textcolor{index}{key-value pair}\index{key-value pair} combinations, where ``key" is a subset of features, and ``value" is the vector of corresponding values.
For instance \texttt{key=(keyword1,keyword2,category)} and \texttt{value=('Python','tutorial','Blog')}. It is important to appropriately bin the features 
 to prevent the number of key-value pairs from exploding, using \textcolor{index}{optimum binning}\index{binning!optimum binning} [\href{http://gnpalencia.org/optbinning/}{Python}]. 
See recent article \cite{binh2020} on this topic. Another mechanism described later  is also used to keep the key-value table, stored as an hash table or associate array, manageable. Finally, this can easily be implemented in a distributed environment.
A key-value pair is also called a \textcolor{index}{node}\index{node (decision tree)}, and plays the same role as a node in a decision tree. 


\section{Methodology} 

You want to predict $p$, the logarithm of unique page views for an article (over some time period), as a function of keywords found in the title, and whether the article in question is a blog or not.  You start by creating a list of all one-token and two-token keywords found in all the article titles, with the article category (blog versus non-blog), after cleaning the titles and eliminating some stop word such as ``that", ``and" or ``the". Do not eliminate all keywords made up of one or two letters: the one-letter keyword ``R", corresponding to the programming language R, has a high \textcolor{index}{predictive power}\index{predictive power}.
For each key-value pair, get the number of articles matching it, as well as the average, minimum and maximum $p$ across these articles.

For instance, say the  key-value pair \texttt{(keyword1='R',keyword2 ='Python',category='Blog')} has $6$ articles and  the following statistics: average $p$ is $8.52$, minimum 
 is $7.41$, and maximum is $10.45$. If the average $p$ across all articles in the training set is  $6.83$, then this specific key-value pair (also called node) generates $\exp(8.52 - 6.83) = 5.42$ times more traffic than an average article. It is thus a large node in terms of traffic. 

Even the worst article among the $6$ ones belonging to this node, with a $p$ of $7.41$, outperforms the average $6.83$ across all nodes. So not only this is a large node, but a stable one. Some nodes have a higher variance $\text{Var}[p]$, for instance when one of the keywords has different meanings, such as the word ``training" in "training set"  and in
 ``courses and training".


\subsection{How hidden decision trees (HDT) work}\label{algoaba}

The nodes are overlapping, allowing considerable flexibility. In particular, nodes with two keywords are sub-nodes of nodes with one keyword.  
The general idea behind this technique is to group articles into buckets that are large enough to provide good predictions, without explicitly building \textcolor{index}{decision trees}.  The nodes are simple and easy to interpret, and unstable ones (with high variance) can be discarded. There is no splitting/pruning involved as with classical decision trees, making this methodology simple and robust, and thus fit for artificial intelligence and black-box implementation. The method is called 
 \textcolor{index}{hidden decision trees}\index{hidden decision trees} and abbreviated as \textcolor{index}{HDT} because you don't create decision trees, but you indirectly rely on a large number of small ones that are hidden in the
 algorithm.



Whether you are dealing with predicting the popularity of an article, or the risk for a client to default on a loan, the basic methodology is identical. It involves training sets, \textcolor{index}{cross-validation}, \textcolor{index}{feature selection}, \textcolor{index}{binning}, and populating hash tables of key-value pairs, referred to as the nodes.
When you process a new observation outside the training set, you check which node(s) it belongs to. If the ``ideal" nodes it belongs to are stable and not too small, you use a weighted average score computed over these nodes, as predictor.  If this score (defined as $p$ here) is significantly above the global average, and other constraints are met, then you classify the observation -- in this case a potential article  you want to write -- as good. An ideal node has strong \textcolor{index}{predictive power}: its $p$ is either
  very high or very low.

Also, you need to update your training set and the nodes table, including automatically discovered new nodes, every six months or so.
Parameters must be calibrated to guarantee that the proportion of false positives  remains small enough. Ideally, you want to end up with
 less than $\num{3000}$ stable nodes, each with at least $10$ observations (articles), covering $80\%$ of the articles or more. 
I discuss the parameters of the technique, and how to fine-tune them, in section~\ref{parambana}. Fine-tuning can be automated or made more robust by testing (say) 
$\num{2000}$ sets of parameters and identify regions of stability minimizing the error rate in the parameter space.  Error rate is defined as the proportion of misclassification, and false positives in particular.  

A big question is what to do with observations not belonging to any usable node: they cannot be classified. A \textcolor{index}{usable node}\index{node (decision tree)!usable node} is one with enough articles, with average $p$ within the mode far away from the global mean of $p$ computed across all nodes.
One way to address this issue is to use two algorithms: the one described so far, applied to usable or ideal nodes (let's call it algorithm A) and another one called algorithm B that classifies all observations. Observations that can't be classified or scored with algorithm A are classified/scored with algorithm B. 
The resulting hybrid algorithm is called Hidden Decision Trees. 

%---

\subsection{NLP case study: summary and findings}


If you run the Python script listed in section~\ref{pythourew}, besides producing the table of key-value pairs (the nodes) as a text file for further automated processing, it displays summary statistics that look like the following:

\begin{lstlisting}[frame=none] 
    Average log pageview count (pv): 6.83
    Avg pv, articles marked as Good: 8.09
    Avg pv, articles marked as Bad : 5.95

    Number of articles marked as Good: 223 (real number is 1079)
    Number of articles marked as Bad : 368 (real number is 919)
    Number of false positives             : 25 (Bad marked as Good)
    Number of false negatives             : 123 (Good marked as Bad)
    Total number of articles              : 2616

    Proportion of False Positives:  11.2%
    Proportion of Unclassified            :  77.4%

    Aggregation factor (Good node): 29.1
    Number of feature values: 16711 (marked as good: 49)

    Execution time:  0.0630 seconds
\end{lstlisting}

\noindent In the code, $p$ -- the logarithm of the pageview count -- is represented by  \texttt{pv}. The number of nodes is the total number of key-value pairs found, including the small unstable ones, regardless as to whether they are classified as good, bad, or unclassified. An article with $p$ above the arbitrary  \texttt{pv\_threshold\_good=7.1} (see source code) is considered as good. This corresponds to articles having about $1.3$ times more traffic than average, since I use a log scale and the average $p$ is $6.83$. Articles classified as good have an average $p$ of $8.09$, that is, about $3.3$ times more traffic than average. 

\noindent Two important metrics are:
\begin{itemize}
\item Aggregation factor: it is an indicator of the average size of a useful node, in this case classified as Good. A value above $5$ is highly desirable.
\item The most important error rate is measured here as the number of bad articles wrongly classified as good. The goal is to detect very good articles and find the reasons that make  them popular, to be able to increase the proportion of good articles in the future. Avoiding bad articles is the second most important goal, so I am also interested in identifying what makes them bad.
\end{itemize}

\noindent Also note that the method correctly identifies a proportion of the good articles, but leaves many unclassified. I explain in section~\ref{pythourew} 
 how to improve this. Finally an article is marked as good if it meets some criteria specified  in section~\ref{parambana}. 

Now I share some interesting findings revealed by these hidden decision tress, on the data set investigated in this study. First, articles with the following title features do well:
	contains a number as in ``10 great deep learning articles",
	contains the current year,
	is a question  (how to),
	is a blog post or belongs to the book category. 

Then the following title keywords are a good predictor of popularity:
	everyone (as in ``10 regression techniques everyone should know"),
	libraries,
	infographic,
	explained, 
	algorithms,
	languages, 
	amazing, 
	must read,
	R Python,
	job interview questions,
	should know (as in ``10 regression techniques everyone should know"),
	NoSQL databases,
	versus, 
	decision trees,
	logistic regression,
	correlations,
	tutorials,
	code, 
	free. 

\subsection{Parameters}\label{parambana} % parameter tuning

Besides \texttt{pv\_threshold\_good} and \texttt{pv\_threshold\_bad}, the algorithm uses $12$ parameters to identify a usable, stable node classified as good. You can see them in action in the Python code
 in section~\ref{pythourew}, in the instruction

\begin{lstlisting}[frame=none]
     if n > 3 and n < 8 and Min > 6.9 and Avg > 7.6 or \
         n >= 8 and n < 16 and Min > 6.7 and Avg > 7.4 or \
         n >= 16 and n < 200 and Min > 6.1 and Avg > 7.2:
\end{lstlisting}

%if ( ((\$n > 3)&&(\$n < 8)&&(\$min > 6.9)&&(\$avg > 7.6)) || 
% ((\$n >= 8)&&(\$n < 16)&&(\$min > 6.7)&&(\$avg > 7.4)) ||
% ((\$n >= 16)&&(\$n < 200)&&(\$min > 6.1)&&(\$avg > 7.2)) ) 

\noindent Here, \texttt{n} represents the size (number of observations) of a node, while \texttt{Avg} and \texttt{Min} are the average and minimum \texttt{pv} for the node in question.  I tested many combinations of values for these parameters. Increasing the required size to qualify as usable node will do the following:\vspace{1ex}
\begin{itemize}
	\item[-] Decrease the number of good articles correctly identified as good
	\item[-] Increase the error rate
	\item[-] Increase the stability of the system
	\item[-] Decrease the predictive power
	\item[-] Increase the aggregation factor
\end{itemize}\vspace{1ex}



\subsection{Improving the methodology}\label{ccvcx}

Some two-token keywords should be treated as one-token. For instance ``San Francisco" must be treated as a one-token keyword. It is easy to automatically detect this: when you analyze the text data, ``San" and ``Francisco" are lumped together far more frequently than dictated by pure chance. 
Also, I looked at nodes  where the two keywords are adjacent in the text. If you allow the two keywords not to be adjacent, the number of key-value pairs (the nodes) increases significantly, but you don't get much more additional predictive power in return, and there is a risk of over-fitting. 


Another improvement consists of favoring nodes containing articles spread over several years, as opposed to concentrated on a few weeks or months. The latter category may be popular articles at some time, that faded away.
Finally, you cannot exclusively focus on articles with great potential. It is important to have many, less popular articles as well: they constitute the long tail. Without these less popular articles, you face excessive content concentration and readership attrition in the long term. 

\section{Implementation details}

This section contains the Python code and details about the Excel spreadsheet. Both the decision tree and the regression part of HDT (referred to as ``algorithm B" in section~\ref{algoaba}) are implemented in the spreadsheet. The Python version, though more comprehensive in many regards, is limited to the decision trees. But first
I start by discussing a possible improvement of the methodology: bias correction.

\subsection{Correcting for bias}\label{timeab}

In online rankings, the most popular books, authors, articles, restaurants, products and so on are usually those that have been around for a long time. 
Here I address this issue by creating adjusted scores. It allows you to make fair comparisons between new and old items.

For top time-insensitive articles, page views peak in the first three days, but popularity remains high for many years.  In short, page view decay is very low over time. Finally, the most popular topics (keywords) change over time; this type of analysis helps find the trends. It is also a good idea to use two different sources of data for pageview
 measurements, see how they differ, understand why, and check whether the discrepancy worsens over time.

The articles scored here span over a three-year period, covering over $\num{2600}$ pieces of content totaling $6$ million pageviews across three websites. 
The summary data is on GitHub, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/ArticlePopularity.txt}{here}. It features the top $46$
 articles ranked according to the time-adjusted score. The number in parenthesis attached to each article is the non-adjusted (old) score. The difference between the time-adjusted score and the old one, is striking. 

\subsubsection{Time-adjusted scores}

You measure the page view count for a specific article, and your time period is  $[t_0, t_1]$. Typical models use 
 an \textcolor{index}{exponential decay}\index{exponential decay} of rate $\lambda$. The adjustment factor is then
 $$q = \frac{1}{\lambda}\cdot \Big[\exp(-\lambda t_0) -  \exp(-\lambda t_1)\Big] > 0.$$


Now define the adjusted score as $p / q$, where $p$ is the observed page view count in $[t_0, t_1]$. If $\lambda = 0$ (no decay) then $q = t_1-t_0$. 

\subsection{Excel spreadsheet}\label{excerds}

The interactive spreadsheet named \texttt{HDTdata4Excel.xlsx} is on my GitHub repository, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Spreadsheets/HDTdata4Excel.xlsx}{here}. It uses a subset of $9$ binary features. The first three are respectively ``published after 2014", 
``article is a forum discussion", and ``article is a blog post". The next six ones are indicators of whether or not the title contains a specific character string. The six strings in question are ``python", ``r",  ``machine learning", ``data science", ``data",  and ``analy". The last string captures words such as ``analytic" or
 ``analyst". These strings must be surrounded by spaces, so ``r" clearly represents the R programming language. True/false are encoded as $1$ and $0$ respectively. 



\renewcommand{\arraystretch}{1.2} %%%
\begin{table}%[H]
%\[
\begin{center}
%\small
\begin{tabular}{lrcc}
\hline
node & size & pv & index \\
\hline
N-000-000000 & 8 & 7.12 & 1.33 \\
N-000-000001 & 5 & 6.87 & 1.04 \\
N-000-000010 & 8 & 6.86 & 1.02 \\
N-000-000011 & 3 & 6.49 & 0.71 \\
N-000-000110 & 3 & 7.18 & 1.42 \\
N-001-000000 & 313 & 6.88 & 1.05 \\
N-001-000001 & 75 & 6.71 & 0.88 \\
N-001-000010 & 276 & 7.14 & 1.35 \\
N-001-000011 & 44 & 7.16 & 1.38 \\
N-001-000110 & 130 & 7.68 & 2.34 \\
N-001-000111 & 5 & 8.05 & 3.37 \\
N-001-001000 & 5 & 8.07 & 3.45 \\
N-001-001010 & 1 & 7.58 & 2.11 \\
N-001-001110 & 1 & 7.35 & 1.67 \\
\hline
\end{tabular}
%]
\caption{\label{fffnode}Statistics for selected HDT nodes (Excel version)}
%\end{array}
\end{center}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%

For instance, node \texttt{N-001-001110} in Table~\ref{fffnode} corresponds to blog posts published in 2014, containing the keywords ``machine learning", ``data science" and ``data" in the title, but not ``python", ``r" or ``analy". The column ``size" tells us that the node in question has only one article.


Nodes with fewer than 10 articles are classified using the regression method via the \texttt{LINEST} Excel function, rather than the mini decision trees. Instead of standard regression, you can use a simplified logistic regression, 
 as described in section~\ref{2ways}. There are $\num{2616}$ observations (articles) and $74$ nodes in the training set. By grouping all nodes with less than $10$ observations into one node, we get down to $24$ nodes. Correlations between individual features and the response $p$ (logarithm of pageviews, denoted as
 \texttt{pv} in Table~\ref{fffnode}) is very low. Thus individual features have no predictive power. They must be combined together
 to gain predictive power. The full HDT method is superior to either the mini decision trees, or the regression model taken separately.

The index in Table~\ref{fffnode} is the \texttt{pv} of the node in question, divided by the average \texttt{pv} across all nodes. It measures the performance of the node. Finally, an article is classified as good or bad depending on whether its index is significantly larger or lower than one. The thresholds are user-defined. 


\begin{figure}%[H]
\centering
\includegraphics[width=0.85\textwidth]{hdt.png}
%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Output from the Excel version of HDT}
\label{fig:hdt}
\end{figure}



\subsection{Python code and dataset}\label{pythourew}


The input dataset \texttt{HDTdata4.txt} is on my GitHub repository, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/HDTdata4.txt}{here}. The Python program \texttt{HDT.py} is listed below and can also be found on my GitHub repository, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/HDT.py}{here}. The output file \texttt{hdt-out2.txt} contains the usable key-value pairs (nodes) corresponding to popular articles, and the list of article IDs for each of these nodes. Finally, the variable \texttt{pv} represents $p$, the logarithm of the pageview count. The bivariate combinations (title keyword, article category)  constitute the
 keys of the hash table \texttt{list\_pv}, while the \texttt{pv} are the hash table values. Keywords are either one- or two-token. For one-token keywords, the second token is marked as N/A. In short, keyword is a bivariate entity.

As for the error rate, since the focus is on producing good articles, I am interested only in minimizing the number of bad articles flagged as good: the false positives.  To reduce error rates or the proportion of unclassified nodes, use more features (for instance, more of those listed in Table~\ref{fffdsa}), three-token keywords, a larger training set, a better keyword cleaning mechanism, and fine-tune the parameters. 

Of course, if you choose the option \texttt{mode='perfect\_fit'} in the program, your false positive rate drops to $0\%$ on the training set, but doing may lower the
 performance on the \textcolor{index}{validation set}, and may leave many nodes unclassified.  On the plus side, you have much fewer parameters to fine-tune:
 \texttt{pv\_threshold\_good}, \texttt{pv\_threshold\_bad}, and the minimum size of a usable node (the variable \texttt{n} in the code). The first two can be set respectively to $5\%$ above and 
$10\%$ below 
 the global average \texttt{pv}. The minimum node size should be set above $2$, and ideally above $5$, though a large value results in more unclassified nodes.
 For \texttt{mode='robust method'}, the
 parameters in the conditional statements defining \texttt{good\_node} and \texttt{bad\_node} were set manually based on an average \texttt{pv} of $6.83$. These choices can be automated.

In the end, unclassified nodes are classified via regression in the spreadsheet (see section~\ref{excerds}), but this has not yet been implemented in the Python code.
 But for my purpose (identifying what makes an article good), I did not need to add the regression part, as the mini decision trees alone (the nodes) provide enough valuable insights. \\


\begin{lstlisting}
from math import log
import time

start = time.time()

# This method updates the dictionaries based on given ID, pv and word
def update_pvs(word, pv, id, word_count_dict, word_pv_dict, min_pv_dict, max_pv_dict, ids_dict):
    if word in word_count_dict:
        word_count_dict[word] += 1
        word_pv_dict[word] += pv
        if min_pv_dict[word] > pv:
            min_pv_dict[word] = pv
        if max_pv_dict[word] < pv:
            max_pv_dict[word] = pv
        ids_dict[word].append(id)
    else:
        word_count_dict[word] = 1
        word_pv_dict[word] = pv
        min_pv_dict[word] = pv
        max_pv_dict[word] = pv
        ids_dict[word] = [id]
# dictionaries to hold count of each key words, their page views, and the ids of the article in which used.
List = dict()
list_pv = dict()
list_pv_max = dict()
list_pv_min = dict()
list_id = dict()
articleTitle = list() # Lists to hold article id wise title name and pv
articlepv = list()
sum_pv = 0
ID = 0
in_file = open("HDTdata4.txt", "r")

for line in in_file:
    if ID == 0: # excluding first line as it is header
        ID += 1
        continue
    line = line.lower()
    aux = line.split('\t') # Indexes will have: 0 - Title, 1 - URL, 2 - data and 3 - page views
    url = aux[1]
    pv = log(1 + int(aux[3]))
    if "/blogs/" in url:
        type = "BLOG"
    else:
        type = "OTHER"
#   #--- clean article titles, remove stop words
    title = aux[0]
    title = " " + title + " " # adding space at the ends to treat stop words at start, mid and end alike
    title = title.replace('"', ' ')
    title = title.replace('?', ' ? ')
    title = title.replace(':', ' ')
    title = title.replace('.', ' ')
    title = title.replace('(', ' ')
    title = title.replace(')', ' ')
    title = title.replace(',', ' ')
    title = title.replace(' a ', ' ')
    title = title.replace(' the ', ' ')
    title = title.replace(' for ', ' ')
    title = title.replace(' in ', ' ')
    title = title.replace(' and ', ' ')
    title = title.replace(' or ', ' ')
    title = title.replace(' is ', ' ')
    title = title.replace(' in ', ' ')
    title = title.replace(' are ', ' ')
    title = title.replace(' of ', ' ')
    title = title.strip()
    title = ' '.join(title.split()) # replacing multiple spaces with one
    #break down article title into keyword tokens
    aux2 = title.split(' ')
    num_words = len(aux2)
    for index in range(num_words):
        word = aux2[index].strip()
        word = word + '\t' + 'N/A' + '\t' + type
        update_pvs(word, pv, ID - 1, List,list_pv, list_pv_min, list_pv_max, list_id) # updating single words

        if (num_words - 1) > index:
            word = aux2[index] + '\t' + aux2[index+1] + '\t' + type
            update_pvs(word, pv, ID - 1, List, list_pv, list_pv_min, list_pv_max, list_id) # updating bigrams

    articleTitle.append(title)
    articlepv.append(pv)
    sum_pv += pv
    ID += 1
in_file.close()

nArticles = ID - 1  # -1 as the increments were done post loop
avg_pv = sum_pv/nArticles
articleFlag = ["NA" for n in range(nArticles)]
nidx = 0
nidx_Good = 0
nidx_Bad  = 0
pv_threshold_good = 7.1
pv_threshold_bad = 6.2
mode = 'robust method'  # options are 'perfect fit' or 'robust method'
OUT = open('hdt-out2.txt','w')
OUT2 = open('hdt-reasons.txt','w')
for idx in List:
    n = List[idx]
    Avg = list_pv[idx]/n
    Min = list_pv_min[idx]
    Max = list_pv_max[idx]
    idlist = list_id[idx]
    nidx += 1
    if mode == 'perfect fit':
      good_node = n > 2 and Min > pv_threshold_good
      bad_node  = n > 2 and Max < pv_threshold_bad
    elif mode == 'robust method': 
        # below values are chosen based on heuristics and experimenting 
        good_node = n > 3 and n < 8 and Min > 6.9 and Avg > 7.6 or \
                n >= 8 and n < 16 and Min > 6.7 and Avg > 7.4 or \
                n >= 16 and n < 200 and Min > 6.1 and Avg > 7.2
        bad_node =  n > 3 and n < 8 and Max < 6.3 and Avg < 5.4 or \
                n >= 8 and n < 16 and Max > 6.6 and Avg < 5.9 or \
                n >= 16 and n < 200 and Max > 7.2 and Avg < 6.2 
    if good_node:
        OUT.write(idx + '\t' + str(n) + '\t' + str(Avg) + '\t' + str(Min) + '\t' + str(Max) + '\t' + str(idlist) + '\n')
        nidx_Good += 1
        for ID in idlist:
            title=articleTitle[ID]
            pv = articlepv[ID]
            OUT2.write(title + '\t' + str(pv) + '\t' +  idx + '\t' + str(n) + '\t' + str(Avg) + '\t' + str(Min) + '\t' + str(Max) + '\n')
            articleFlag[ID] = "GOOD"
    elif bad_node:
        nidx_Bad += 1
        for ID in idlist:
            articleFlag[ID] = "BAD"
# Computing results based on Threshold values
pv1 = 0
pv2 = 0
n1 = 0
n2 = 0
m1 = 0
m2 = 0
FalsePositive = 0
FalseNegative = 0
for ID in range(nArticles):
    pv = articlepv[ID]
    if articleFlag[ID] == "GOOD":
        n1 += 1
        pv1 += pv
        if pv < pv_threshold_good:
            FalsePositive += 1
    elif articleFlag[ID] == "BAD":
        n2 += 1
        pv2 += pv
        if pv > pv_threshold_bad:
            FalseNegative += 1
    if pv > pv_threshold_good: 
        m1 += 1
    elif pv < pv_threshold_bad:  
        m2 += 1
#
# Printing results
avg_pv1 = pv1/n1
avg_pv2 = pv2/n2
errorRate = FalsePositive/n1
UnclassifiedRate = 1 - (n1 + n2) / nArticles
aggregationFactor = (nidx/nidx_Good)/(nArticles/n1)
print ("Average log pageview count (pv):","{0:.2f}".format(avg_pv))
print ("Avg pv, articles marked as Good:","{:.2f}".format(avg_pv1))
print ("Avg pv, articles marked as Bad :","{:.2f}".format(avg_pv2))
print()
print ("Number of articles marked as Good: ", n1, " (real number is ", m1,")", sep = "" )
print ("Number of articles marked as Bad : ", n2, " (real number is ", m2,")", sep = "")
print ("Number of false positives        :",FalsePositive,"(Bad marked as Good)")
print ("Number of false negatives        :", FalseNegative, "(Good marked as Bad)")
print ("Total number of articles         :", nArticles)
print()
print ("Proportion of False Positives: ","{0:.1%}".format(errorRate))
print ("Proportion of Unclassified   : ","{0:.1%}".format(UnclassifiedRate))
print()
print ("Aggregation factor (Good node):","{:.1f}".format(aggregationFactor))
print ("Number of feature values: ", nidx," (marked as good: ", nidx_Good,")", sep = "")
print ()
print("Execution time: ","{:.4f}".format(time.time() - start), "seconds")
\end{lstlisting}

\section{Model-free confidence intervals and perfect nodes}


Node \texttt{N-100-000000} in the spreadsheet has an average 
\texttt{pv} of $5.85$. It consists of $10$ articles with  
the following \texttt{pv}: $5.10, 5.10, 5.56, 5.56, 5.66, 5.69, 6.01,  6.19, 6.80, 6.80$. The $15$th and $85$th percentiles are $5.26$ and $6.68$ respectively, when computed with the \texttt{Percentile} function in Excel. Thus, $[5.26, 6.68]$ is a $70\%$ \textcolor{index}{confidence interval}\index{confidence interval} (CI) for \texttt{pv}, for the node
 in question. 


The whole CI including its upper bound is below the average \texttt{pv} of $6.83$. In fact this node corresponds to articles posted after 2014, not a blog or forum question (it could be a video or event announcement), and with a title containing none of the keyword features in the spreadsheet 
(columns \texttt{K:P} in the \texttt{data} tab). This node  has a maximum predictive power, in the sense that $100\%$ of the articles that it contains are bad, and $0\%$ 
 are good. This would also be true if it was the other way around, with Good swapped with Bad. Such a node is called a \textcolor{index}{perfect node}\index{node (decision tree)!perfect node}. When selecting the option 
\texttt{mode='Perfect fit'} in the Python code,
the method looks at perfect nodes only. The concept of \textcolor{index}{predictive power}\index{predictive power} is further discussed in section~18.4.2 in~\cite{vgelsevier}.

\subsection{Interesting asymptotic properties of confidence intervals}

I focus here on traditional model-free confidence intervals, as computed in the above paragraphs. The reader should be aware that there are other ways to define them, for instance \textcolor{index}{credible intervals}\index{credible interval} in the context of
 \textcolor{index}{Bayesian inference}\index{Bayesian inference} [\href{https://en.wikipedia.org/wiki/Bayesian_inference}{Wiki}], or 
 Bayesian-like \textcolor{index}{dual confidence intervals}\index{confidence region!dual region} as in section~18.3 in~\cite{vgelsevier}.


\renewcommand{\arraystretch}{1.2} %%%
\begin{center}
\begin{table}[H]
\[
\begin{array}{lccc}
\hline 
\text{pv distribution} & \text{Type} &  \text{E}[R_n] & \text{Stdev}[R_n] \\
\hline 
\text{Uniform} & \text{short tail} & 1 & 1/n \\
\text{Gaussian} & \text{medium tail} & \sqrt{\log n} & 1/\sqrt{n} \\
\text{Exponential} & \text{fat tail} & \log n & 1\\
\hline
\end{array}
\]
\caption{\label{ffraged}Order of magnitude for the expectation and standard deviation of the range $R_n$}
\end{table}
\end{center}

\renewcommand{\arraystretch}{1.0} %%%

In almost all cases, as the number $n$ of observations becomes large within a node, the length of the confidence interval, in this case for the expected \texttt{pv}, is asymptotically
 $L_n\sim\alpha n^\beta$. I discuss in an upcoming paper how to estimate $\alpha$ and $\beta$. The order of magnitude of the range $R_n =\max(\texttt{pv}) - \min(\texttt{pv})$
  computed on a node with $n$ observations, depends on the distribution of \texttt{pv}, and more specifically, on the type of this distribution. The result is summarized in Table~\ref{ffraged}, and discussed in the same upcoming article. In practice, \texttt{pv} may have a \textcolor{index}{mixture  distribution}\index{mixture model}.


%----------------------------------------------------------------------------------------------------------------
\Chapter{New Interpolation Methods  for Synthetization and Prediction}{}\label{chapterInterpol}


  I describe little-known original interpolation methods with applications to real-life datasets. These simple techniques are easy to implement and can be used for regression or prediction. They offer an alternative to model-based statistical methods. Applications include interpolating ocean tides at Dublin, predicting temperatures in the Chicago area with geospatial data, and a problem in astronomy:  planet alignments and frequency of these events. In one example, the 5-min data can be replaced by 80-min measurements, with the 5-min increments reconstructed via interpolation, without noticeable loss. Thus, my algorithm can be used for data compression. 

The first technique has strong ties to Fourier methods. In addition to the above applications, I show how it can be used to efficiently interpolate complex mathematical functions such as Bessel and Riemann zeta. For those familiar with MATLAB or Mathematica, this is an opportunity to play with the MPmath library in Python and see how it compares with the traditional tools in this context. 
In the process, I also show how the methodology can be used to 
generate \textcolor{index}{synthetic data}\index{synthetic data} [\href{https://en.wikipedia.org/wiki/Synthetic_data}{Wiki}], be it time series or geospatial data.

Depending on the parameters, in the geospatial context, the interpolation is either close to nearest-neighbor methods, 
\textcolor{index}{kriging}\index{kriging} [\href{https://en.wikipedia.org/wiki/Kriging}{Wiki}] (also known as Gaussian process regression), or a truly original and hybrid mix of additive and multiplicative techniques. There is an option not to interpolate at locations far away from the training set, where regression or interpolation results may be meaningless, regardless of the technique used. 

The second technique is based on ordinary least squares -- the same method used to solve polynomial regression -- but instead of highly unstable polynomials leading to overfitting, I focus on generic functions that avoid these pitfalls, using an iterative 
\textcolor{index}{greedy algorithm}\index{greedy algorithm} [\href{https://en.wikipedia.org/wiki/Greedy_algorithm}{Wiki}] to find the optimum. In particular, a solution based on orthogonal functions leads to a particularly simple implementation with a direct solution.




\section{First method}

The general principle is simple. We want to interpolate a function $g(t)$ at certain points $t = \rho_1, \rho_2,\dots$ belonging to a set $R$ called the root set. These points  are the roots of some function $\psi$. We create a function $w(t,\rho)$ which is equal to zero only if $t=\rho$ and $\rho\in R$. The functions $\psi$ and $w$ are chosen so that when $t\rightarrow \rho\in R$, the limit
$\psi(t)/w(t,\rho)$ -- a quotient where both the numerator and denominator are zero -- exists and is different from zero. The limit in question is denoted as $\lambda(\rho)$. The interpolated function, denoted as $f(t)$ and defined by~(\ref{tarmac}), is by construction  identical to $g(t)$ when $t\in R$.  This leads to the formulation

\begin{equation}
f(t)=\psi(t)\cdot \sum_{\rho\in R} \frac{f(\rho)}{\lambda(\rho)}\cdot \frac{1}{w(t,\rho)}, \quad \text{ with } \lambda(\rho) =   \lim_{t\rightarrow \rho} \text{ } \frac{\psi(t)}{w(t,\rho)}.\label{tarmac}
\end{equation}
Here $w(t,\rho) = 0$ if and only if $t=\rho$. The functions $\psi$ and $w$ must be chosen so that the limit in
 Formula~(\ref{tarmac}) always exists and is different from zero. Typically, $w(t,\rho)$ measures how close $t$ and $\rho$ are to each other. If the summation is infinite and the series is 
\textcolor{index}{conditionally convergent}\index{convergence!conditional} [\href{https://en.wikipedia.org/wiki/Conditional_convergence}{Wiki}] -- as opposed to \textcolor{index}{absolutely convergent}\index{convergence!absolute} -- then the roots $\rho$ need to be properly ordered. This is discussed in section~\ref{puner}. Convergence of the series may also require that 
 $w(t,\rho) \rightarrow \infty$ fast enough as $|\rho|\rightarrow\infty$ and $t$ is fixed.

In one dimension, the limit can be computed using l'HÃ´pital's rule [\href{https://en.wikipedia.org/wiki/L\%27H\%C3\%B4pital\%27s_rule}{Wiki}]:   
$$\lambda(\rho) = \frac{\psi'(\rho)}{w'(\rho,\rho)}, \quad \text{ with } \psi'(t)=\frac{\partial \psi(t)}{\partial t} \text{ and } 
w'(t,\rho) = \frac{\partial w(t,\rho)}{\partial t}.$$
Multiple applications of l'HÃ´pital's rule may be required for \textcolor{index}{roots with multiplicity}\index{multiple root} [\href{https://en.wikipedia.org/wiki/Multiplicity_(mathematics)}{Wiki}]. 
The symbol $\partial$ stands for the 
 \textcolor{index}{partial derivative}\index{partial derivative} [\href{https://en.wikipedia.org/wiki/Partial_derivative}{Wiki}], here with respect to $t$. In higher dimensions, the limit usually does not exist except under certain circumstances, see \cite{13rob} and
 section~\ref{totor}. 

% \subsection{Example with finite summation} 

\subsection{Example with infinite summation} \label{puner}

I start with some mathematics leading to interesting formulas. Then I use the formulas to interpolate time series, with a cool 
 application. Let $\psi(t)=\sin\pi t$ and $R = \{\rho_0, \rho_1, \dots\} = \mathbb{N}$ so that $\rho_k = k$. With 
$w(t,\rho)=t^2 - \rho^2$, you get:
\begin{equation}
f(t) =\frac{\sin\pi t}{\pi} \cdot \Bigg[\frac{f(0)}{t} + 2t \sum_{k=1}^\infty (-1)^k \frac{f(k)}{t^2-k^2}\Bigg].\label{thor}
\end{equation}
Formula~(\ref{thor}) is valid for any even function $f$ that can be written as 
\begin{equation}
f(t) = \sum_{k=0}^\infty \alpha_k \cos \beta_k t \, \text{ with } |\beta_k|<\pi, \text{ or } 
f(t) = \int_{-\infty}^\infty \alpha(u)\cos(\beta(u)t )du \, \text{ with } |\beta(u)|<\pi. \label{pures}
\end{equation}
Convergence and the fact that the left-hand side of~(\ref{thor}) matches the right-hand side is rooted in the theory of Fourier series. 
For details, see \href{https://mathoverflow.net/questions/438157/convergence-of-series-related-to-partial-fraction-expansion-of-cotangent-functio}{here}. A similar formula exists for odd functions. Note that $f$ is even if $f(-t)=f(t)$, and $f$ is odd 
if $f(-t)=-f(t)$. By combining the two formulas for odd and even functions, you get a formula that works for all functions regardless of parity. Again, limitations apply for convergence towards $f(t)$: the function $f$ must be a sum of two terms, one involving cosines as in~(\ref{pures}), and a similar one involving sines. In my general solution, you must replace $\pi$ by $\pi/2$ 
 in~(\ref{pures}) -- and the same in the sine term -- for the generalized version of formula~(\ref{thor}) to be valid. 

The formula assumes that the \textcolor{index}{interpolation nodes}\index{node (interpolation)} 
are integers. But a different grid could be used with a transformation such as $t'=a+bt$. You then
 interpolate $f$ using known values of $f(t')$ where $(t'-a)/b$ is an integer, rather than interpolating $f$ using known values of $f(t)$ where $t$ is an integer. With an appropriate choice for $a$,  you can extend the interpolation formula beyond the limitation previously discussed. For unevenly spaced nodes, use a non-linear mapping
 $\varphi(t)$ instead of $a+bt$. The function $\varphi$ should be strictly monotonic, and thus invertible.

The Python implementation is in section~\ref{porewa}, and available as \texttt{interpol\_fourier.py} on my GitHub repository, 
\href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/interpol_fourier.py}{ here}. With the linear transformation $a+bt$, I use nodes that are not integers to interpolate the math functions. I included advanced complex-valued math functions (Bessel, Riemann zeta) with complex arguments for those interested in scientific computing. It also illustrates how to use the 
\href{https://mpmath.org/}{mpmath library} in Python, which is an alternative to Matlab. 

Figure~\ref{fig:zeta} shows the interpolation of the real part of the 
\textcolor{index}{Riemann zeta function}\index{Riemann zeta function}  
 $\zeta(\sigma +it)$ 
[\href{https://en.wikipedia.org/wiki/Riemann_zeta_function}{Wiki}] on the 
 \textcolor{index}{critical line}\index{critical line (number theory)} [\href{https://en.wikipedia.org/wiki/Riemann_hypothesis#Zeros_on_the_critical_line}{Wiki}], that is when $\sigma=\frac{1}{2}$. 
According to the famous \textcolor{index}{Riemann Hypothesis}\index{Riemann Hypothesis}, that's where all the non-trivial zeros lie.
 Actually, the first time I used my interpolation formula was in this context, with integer nodes. The approximation here is based on a more granular grid, and more accurate.  


Interestingly, this function looks quite similar to many real-life time series: in the end, it just a special combination of sine and cosine terms with various amplitudes and incompatible periods. It is thus a good candidate for time series synthetization, able to mimic many real examples by choosing the right interval and mapping for $t$. The ocean tide data and the distance between Earth and Venus (section~\ref{venus}) fit in that category, though they involve 
 a small number of terms (the number is infinite for $\zeta$).     


\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{zeta.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Interpolating the real part of $\zeta(\frac{1}{2}+it)$ based on orange points}
\label{fig:zeta}
\end{figure}




\subsection{Applications: ocean tides, planet alignment}\label{venus}

The framework introduced in section~\ref{puner} and the corresponding Python code can handle both math functions and time series datasets. In the code, I use the notation \texttt{g} for the exact function. The interpolated version 
is denoted as \texttt{interpolate}. Each interpolated value $f(t)$ is based on $2n+1$ integer nodes where the exact value $g(t)$ is assumed to be known, on both sides of $t$ on the horizontal axis representing the time (left and right).

Here the dataset (time series) consists of ocean tides at Dublin, measured in 5-min increments. The small data extract
 \texttt{tides\_Dublin.txt} is on my GitHub repository, \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/tides_Dublin.txt}{here}. You can download the full version \href{https://www.digitalocean.ie/Data/DownloadTideData}{here},
 on DigitalOcean.ie. I used it to test the methodology, since tides are easy to forecast without any statistical model.  
The conclusions are as follows: \vspace{1ex}
\begin{itemize}
\item You only need data in 80-min increments to reconstruct the 5-min time series. So you can compress the dataset by a factor 16 (keeping a small fraction of it), almost without loss of information.
\item The 5-min data and the 5-min interpolated values are very close to each other. When they differ, the interpolated values seem better than the observed ones. The interpolation removes the noise. 
\item What I did is known as time series \textcolor{index}{disaggregation}\index{disaggregation} in the literature. It is useful to recover unobserved 5-min pollution levels and rainfall data from hourly observations, and in other contexts.  
\item What I did also amounts to generating 16 shifted subsets of 80-min interpolated tides, the shift being 5 minutes. 
Each subset is a synthetic dataset in itself. It would be very useful if the 5-min data was not known, offering various synthetic copies
 of the 80-min dataset.  The reason I did it despite the fact that the 5-min data is known, is to test the accuracy of the interpolated values. All of them (regardless of time) were generated using one single subset of real observations with 80-min increments, as if no intermediate values were known. 
\end{itemize} \vspace{1ex}

\noindent The dataset is stored in the table \texttt{temp}. It is equivalent to an array where the index, rather than being an integer, is a multiple of $1/16$. For that reason, I used a dictionary rather than an array in Python. Interpolated values are computed using observations where the index -- representing the time -- is an integer (one out of 16 observations). This is also true for interpolating the math functions in section~\ref{puner}. Finally, each interpolated value is based on $2n + 1$ nodes (exact values where the index is an integer) with $n=8$, spread over a 
$(8 + 8)\times 80$ minutes time period (that is, about 21 hours). By design, every 80 minutes -- when the index is an integer -- the interpolated and exact values are perfectly identical: this corresponds to the orange dots in Figure~\ref{fig:tides}. 

In Figure~\ref{fig:tides}, the red curve represents the observed (exact) 5-min tides. 
The blue curve represents the interpolated values. Except in a few instances, they are indistinguishable to the naked eye. The small black bars at the bottom represents the error, in absolute value. The same black bars are found in Figure~\ref{fig:zeta} related to the Riemann zeta function. However in that case the error is so small the the minuscule bars look like a glitch in the picture.

Note that no statistical model is involved in my method. It is still possible to compute various confidence or prediction intervals for the 
 interpolated values, using bootstrapping techniques. It is discussed at length in my articles and books, and in the literature in general.
For a parametric model to predict ocean tides, see 
 section~\ref{tidesofheav}.

The whole method can be seen as a regression technique to predict values within the range of observations, for time series or for
 observations ordered in a certain way (here by time). In some sense, it is a model-free regression that 
uses only one feature: the time. Can it be generalized to handle multiple features, or in other words, multivariate data? The answer is yes. See section~\ref{totor} and~\ref{tptyr} for an application to temperature interpolation, based on two features: longitude and latitude. 

\begin{Exercise} -- \,{\bf When are planets aligned?} 

\noindent We first create a dataset with daily measurements of the distance between Earth and Venus, and interpolate the distance to test how little data is needed for good enough performance: can you reconstruct daily data from monthly observations? What about quarterly or yearly observations? Then, the purpose is to assess how a specific class of models is good at synthetizing not only this type of data, but at the same time other types of datasets like the ocean tides in Figure~\ref{fig:tides} or the Riemann zeta function in
 Figure~\ref{fig:zeta}.

The planetary fact sheet published by the NASA contains all the information needed. 
It is available \href{https://nssdc.gsfc.nasa.gov/planetary/factsheet/}{here}. I picked up Venus and Earth because they are among the planets with the lowest eccentricities in the solar system. For simplicity, assume that the two orbits are circular. Also assume that at a time denoted as $t=0$, the Sun, Venus and Earth were aligned and on the same side (with Venus between Earth and the Sun). 

Note that all the major planets revolve around the sun in the same direction. 
Let $\theta_V, \theta_E, R_V, R_E$ be respectively the orbital periods of Venus and Earth, and the  distances from Sun for Venus and Earth.  From the NASA table, these quantities are respectively 224.7 days, 365.2 days, $108.2\times 10^6$ km, and 
$149.6  \times 10^6$ km. Let $d_V(t)$ be the distance at time $t$, between Earth and Venus. You first need to convert the orbitial periods into angular velocities 
 $\omega_V = 2\pi/\theta_V$ and $\omega_E = 2\pi/\theta_E$ per day.  Then elementary trigonometry leads to the formula
\begin{equation}
d_V^2(t) = R_E^2\Bigg[1 + \Big(\frac{R_V}{R_E}\Big)^2 -2\frac{R_V}{R_E} \cos\Big((\omega_V-\omega_E)t\Big) \Bigg]. \label{resw}
\end{equation}
The distance is thus periodic, and minimum and equal to $R_E - R_V$ when  
$(\omega_V-\omega_E)t$ is a multiple of $2\pi$. This happens roughly every 584 days. 

%xxxxyyyyy

\noindent {\bf Steps to complete} 

\noindent The exercise consists of the following steps:
\begin{itemize}
\item[] {\bf Step 1}:  Use formula~(\ref{resw}) to generate daily values of $d_V(t)$, for 10 consecutive years, starting at $t=0$.
\item[] {\bf Step 2}:  Use the Python code in section~\ref{porewa} applied to your data. Interpolate daily data using one out of every 30 observations. Conclude whether or not using one measurement per month is good enough to reconstruct the daily observations. See how many nodes (the variable \texttt{n} in the code) you need to get a decent interpolation.
\item[] {\bf Step 3}:  Add planet Mars. The three planets (Venus, Earth, Mars) are aligned with the sun and on the same side when both $(\omega_V-\omega_E)t$ and $(\omega_M-\omega_E)t$ are almost exact multiples of $2\pi$, that is, when both the distance $d_M(t)$ between Earth and Mars, and $d_V(t)$ between Earth and Venus, are minimum. In short, it happens when 
$g(t) = d_V(t) + d_M(t)$ is minimum.  Assume it happened at $t=0$. Plot the function $g(t)$, for a period of time long enough to see a global minimum (thus, corresponding to an alignment). Here $\omega_M$ is the angular velocity of Mars, and its orbit is approximated by a circle.
\item[] {\bf Step 4}: Repeat steps 1 and 2 but this time for $g(t)$. Unlike $d_V(t)$, the function $g(t)$ is not periodic. Alternatively, use Jupiter instead of Venus, as this leads to alignments visible to the naked eye in the night sky: the apparent locations of the two planets coincide. 

\item[] {\bf Step 5}: A possible general model for this type of time series is
\begin{equation}
f(t) = \sum_{k=1}^m A_k \sin(\theta_kt + \varphi_k) + \sum_{k=1}^m A'_k \cos(\theta'_kt + \varphi'_k) \label{tyre}
\end{equation}
where the $A_k, A'_k, \theta_k,\theta'_k,\varphi_k,\varphi'_k$ are the parameters, representing amplitudes, frequencies and phases. Show that this parameter configuration is redundant:  you can simplify while keeping the full modeling capability, by setting
$\varphi_k = \varphi_k'=0$ and re-parameterize. Hint: use the angle sum formula (Google it).
\item[] {\bf Step 6}: Try $10^6$ parameter configurations of the simplified model based on formula~(\ref{tyre}) 
with $\varphi_k=\varphi'_k=0$, to
 synthetize time series via Monte-Carlo simulations. For each simulated time series, measure how close it is to the ocean tide data (obtained by setting \texttt{mode='Data'} in the Python code), the functions $g(t)$ and $d_V(t)$ in this exercise, and the Riemann zeta function pictured in Figure~\ref{fig:zeta} (obtained by setting
\texttt{mode='Math.Zeta'} in the Python code). Use a basic proximity metric of your choice to asses the quality of the fit, and use it 
on the transformed time series obtained after normalization (to get zero mean and unit variance). A possible comparison metric is
a combination of  lag-1, lag-2 and lag-3 auto-correlations.
\item[] {\bf Step 7}: Because of the \textcolor{index}{curse of dimensionality}\index{curse of dimensionality} [\href{https://en.wikipedia.org/wiki/Curse_of_dimensionality}{Wiki}], Monte-Carlo is a very poor technique here as we are dealing with $8$ parameters. On the other hand, you can get very good approximations with just 4 parameters, with a lower risk of overfitting. Read section~\ref{tidesofheav} about a better inference procedure, applied to ocean tides. 
Also read chapter~13 in~\cite{vgelsevier} on synthetic universes featuring non-standard gravitation laws to generate different types of  synthetic time series. Finally, read chapter 6 on shape generation and comparison: it features a different type of metric to measure the distance between two objects, in this case the time series (their shape: real versus synthetic version). 
\end{itemize}
\end{Exercise}


% xxxxyyyyy
\begin{figure}%[H] xxxxxyyyyyyyyy
\centering
\includegraphics[width=0.7\textwidth]{tides3.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Tides at Dublin (5-min data), with 80 mins between interpolating nodes}
\label{fig:tides}
\end{figure}


\subsection{Problem in two dimensions} \label{totor}

In this section I discuss a basic example with a finite summation, where everything works nicely. However, it is a fundamental and very important case, as it applies to all regression problems. Also, it easily generalizes to higher dimensions. I use the notation 
 $t=(x,y)$ and $z=f(t)=f(x, y)$. Let us assume that $\psi(x,y)$ has $n$ roots $\rho_k=(x_k,y_k)$ with $k=1,\dots,n$. The setting is as follows: we have a dataset with $n$ observations $(z_k,x_k,y_k)$ for $k=1,\dots,n$. Here $z_k$ is the response or
 dependent variable, and $x_k, y_k$ are the two features, also called independent variables or predictors. I use

$$
\psi(t) = \psi(x,y) = \prod_{k=1}^n w_k(x,y), \quad \lambda(\rho_k)=\lambda(x_k,y_k) = \prod_{i\neq k} w_i(x,y),  
$$
with the notation $w_k(x,y) = w(t,\rho_k) = w(x, y; x_k, y_k)$. I provide a specific example in formula~(\ref{miel}). For now, let us keep in mind that by construction, $w(x, y; x', y')=0$ if and only if $(x,y)=(x',y')$. It follows that

\begin{equation}
z = f(x,y) =  \sum_{k=1}^n   \gamma_k f(x_k,y_k), \quad \text{ with } \gamma_k = 
\prod_{i\neq k} \frac{w_i(x,y)}{w_i(x_k,y_k)}
=\prod_{i\neq k} \frac{w(x,y; x_i,y_i)}{w(x_k,y_k; x_i,y_i)}
. \label{qq2}
\end{equation}
Thus, $z_k = f(x_k,y_k)$, for $k=1,\dots,n$. Given a new observation $(x, y)$, the predicted response $z$, based on the $n$ data points in the training set, is provided
 by formula~(\ref{qq2}). If $(x,y)$ is already in the training set, then the predicted $z$ will be exact. 


\begin{figure}%[H]
\centering
\includegraphics[width=0.90\textwidth]{interpol1.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Temperature data: interpolation with my method (observed values at dots)}
\label{fig:interpol1}
\end{figure}

\begin{figure}%[H]
\centering
\includegraphics[width=0.90\textwidth]{interpol2.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{My method: round dots represent observed values, ``+" are interpolated}
\label{fig:interpol2}
\end{figure}

Unlike in traditional kernel-based methods, here the choice of the ``distance" or ``kernel" function $w$ is critical. Some adaptations preserve the fact that $z_k=f(x_k,y_k)$ for $k=1,\cdots,n$, while providing significantly better predictions and smoothness for observations outside the training set. This makes the method a suitable alternative to regression techniques. In particular, I implemented the following upgrades: \vspace{1ex}
\begin{itemize}
\item Replacing $\gamma_k$ by $\gamma'_k = \gamma_k /(1 + \gamma_k)$. It guarantees that these coefficients lie between 0 and 1.
\item Replacing $\gamma'_k$ by $\gamma^*_k = \gamma'_k / w_k^\kappa(x,y)$ where $\kappa\geq 0$ is an hyperparameter. This reduces the impact of the point $(x_k,y_k)$ if it is too far away from $(x,y)$.
\item Normalizing $\gamma^*_k$ so that their sum is equal to $1$. This eliminates additive bias outside the training set.
\end{itemize}\vspace{1ex}

\noindent These transformations make the technique somewhat hybrid: a combination of multiplicative, additive, and nearest neighbor methods. Further improvement is obtained by completely ignoring a point $(x_k,y_k)$ when interpolating $f(x,y)$, if 
$w_k(x,y)>\delta$. Here $\delta > 0$ is an hyperparameter. It may result in the inability to make a prediction for a point $(x,y)$ far away from all training set points: this is actually a desirable feature, not a defect.

\subsection{Spatial interpolation of the temperature dataset}\label{tptyr}

To test the method presented in section~\ref{totor}, I used the streaming high frequency temperature data in Chicago, retrieved from \href{https://arrayofthings.github.io/}{Array of Things}. The data was analyzed \href{https://cybergisxhub.cigi.illinois.edu/notebook/spatial-interpolation/}{here} 
in 2019 using CyberGISX, a set of \textcolor{index}{GIS}\index{GIS} tools 
[\href{https://en.wikipedia.org/wiki/Geographic_information_system}{Wiki}] developed in Python by the University of Illinois. They used ordinary kriging.  The dataset has 3 fields: latitude (shown on the vertical axis), longitude (horizontal axis) and temperature (the color). 

Figures~\ref{fig:interpol1} and~\ref{fig:spatial} show the results: my method versus ordinary kriging. The picture corresponding to
 kriging covers a larger area, with vast regions without training locations. This is extrapolation rather than interpolation, and the deep blue well and strong red dome are artifacts of the method. They are much less pronounced in my picture (Figure~\ref{fig:interpol1}). Indeed, I had to force my technique to cover an area away from the training set, beyond what is 
reasonable, to avoid blank (non-interpolated) zones in my image.  

\begin{figure}%[H]
\centering
\includegraphics[width=0.60\textwidth]{spatial.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Temperature dataset: interpolation using ordinary kriging}
\label{fig:spatial}
\end{figure}

Figure~\ref{fig:interpol2} used a smaller interpolation window: the green ``+" are non-interpolated locations due to their distance to the training set. Interpolating so far away from the training set, without additional information on how the real data behaves (heat domes, cold atmospheric depressions) is meaningless and does not generalize to other fields.  The 32 training set locations are represented by circular dots in all three pictures.  In Figure~\ref{fig:interpol1}, the color of these dots (exact temperature) does not match the color of the background (interpolated value on nearby location) despite the appearance. There are tiny differences, not visible to the naked eye: it proves that the method works! One location in the South East corner has four training set points in very close proximity, with vastly different temperatures (the red dot is almost hidden): there you can see that the interpolation averaged out the four temperatures in question.

For $w_k(x,y)$, I used the function defined by~(\ref{miel}), with $\alpha=1,\beta=2$. I did not make any efforts to find ideal parameter values, as this would defeat the purpose of designing a generic algorithm that works in many settings with as little fine-tuning as possible. For the same reason, the parameter $\kappa$ in section~\ref{totor} is set to $2$, and $\delta$ is automatically computed as the smallest value that guarantees all the tested locations can be interpolated.

\begin{equation}
w_k(x,y) = \Big(|x-x_k|^\beta + |y-y_k|^\beta\Big)^\alpha, \text{ with } \alpha,\beta > 0. \label{miel}
\end{equation}

The parameters $\alpha,\beta$ control the smoothness of the interpolated function. Choosing a small value for $\delta$ amounts to using a nearest neighbor type of interpolation. Choosing a high value for $\kappa$ amounts to performing kriging. Thus the method is eclectic and encompasses various types of interpolation. The Python implementation in section~\ref{pyif} follows best practices: the data is first normalized before interpolation, divisions by zero are properly handled, and you can choose not to interpolate at locations too far away from the training set by adjusting~$\delta$.

As seen in Figure~\ref{fig:interpol2}, the Python code also generates 4 copies of the training set; the number of copies is specified by the variable 
\texttt{ppo} in the code. In each copy, the location of each point is uniformly distributed in a circle around the original training set location that it represents. The radius of that circle is determined by the variable \texttt{radius} in the code. This 
\textcolor{index}{synthetic data}\index{synthetic data} is used to test the performance of the algorithm. It allows you to play with different values of the radius. The final line of code computes the average distance (temperature discrepancy) between exact values in the training set and the associated values in the synthetic data, interpolated at sampled locations. 



\section{Second method}

So far I managed to hide the underlying mathematics quite well to make the presentation easier to understand, without limiting its depth. Here the mathematics are significantly  more visible. Again, Fourier series make their apparition. I cover interpolation and regression in this section, both univariate and multivariate, not just for datasets but also for mathematical functions. There is no novelty in terms of mathematical research: the originality is in the angle took to present and use the methodology. It leads to remarkably simple, elegant, and robust multivariate regression methods.

\subsection{From unstable polynomial to robust orthogonal  regression}

A classic approach to approximate a function is to use 
\textcolor{index}{ordinary least squares}\index{ordinary least squares} [\href{https://en.wikipedia.org/wiki/Ordinary_least_squares}{Wiki}]. Indeed, regression techniques often rely, in one way or another, on this optimization technique. When the response is denoted as $f(x)$, and the feature vector denoted as $x$, the problem consists of finding coefficients $\alpha_1,\dots,\alpha_n$ such that
\begin{equation}
f_n(x) \equiv \sum_{k=0}^n \alpha_k p_k(x) \label{poinbg}
\end{equation}
is the ``best" approximation to $f(x)$. As in the time series application in section~\ref{venus}, the function $f$ can be a mathematical function, or represent observed values in a dataset. In the latter case, $f$ is known only for a finite number of arguments $x$ corresponding to an entry in the data set. Either way, the problem consists of interpolating $f$. This is usually called a regression problem when dealing with real data, and the function $f$ allows you to predict the response, given a new observation $x$ not in the training set, via formula~(\ref{poinbg}). Also, $n$ is not the number of observations, but the index in an iterative loop associated with an optimization algorithm. 

Traditionally, $p_k(x)=x^k$ in polynomial regression. In classic multivariate regression, $p_k(x)$ is the $k$-th component of the feature vector $x$, possibly after some transformation such as normalization, with $p_0(x)=1$ corresponding to the intercept.  Here, I am approaching the problem from a different angle. The idea is to build a sequence of functions $(f_n)$ converging to $f$ as $n\rightarrow \infty$, starting with $f_{-1}(x)=0$. At iteration $n$, the function $f_n$ is chosen to minimize 
\begin{equation}
\delta(\alpha_n)\equiv \int_D \Big(f_n(x)-f(x)\Big)^2 dx = \int_D \Big(\alpha_n p_n(x) + f_{n-1}(x) - f(x)\Big)^2 dx, \label{tupues}
\end{equation}
where $D$ is the domain where we want the approximation to be best. If $f$ is known only for integer values or we are dealing with a dataset, the integral is replaced by a sum, and $D$ is the discrete set of interpolation nodes (the integers), or the set of observed features in case of a dataset. It is convenient to introduce the following notations:
$$
\beta_k = \int_D p_k(x)f(x)dx, \quad \gamma_k = \int_D p_k^2(x)dx, \quad \beta_{ij} = \int_D p_i(x)p_j(x)dx.
$$
Then we have $\alpha_0 = \beta_0/\gamma_0$ and for $n>0$:
$$
\alpha_n = \frac{1}{\gamma_n} \Bigg[\beta_n - \sum_{k=0}^{n-1} \alpha_k\beta_{kn} \Bigg].
$$
\subsection{Using orthogonal functions}\label{poreese}

If the coefficients $\beta_{ij}$ are all zero, the formulas considerably simplify. This is the case if $(p_k)$ is
 a sequence of \textcolor{index}{orthogonal functions}\index{orthogonal function} [\href{https://en.wikipedia.org/wiki/Orthogonal_functions}{Wiki}].   The most well-known example is $p_{2k}(x) = \cos(k\pi x/L)$ combined
 with $p_{2k+1}(x) = \sin(k\pi x/L)$ for $k=0,1$ and so on. If $D=[-L,L]$, it corresponds to approximating or interpolating a periodic function~$f$ on $D$ using its  \textcolor{index}{Fourier series}\index{Fourier series} [\href{https://en.wikipedia.org/wiki/Fourier_series}{Wiki}]. In this case, $\gamma_k = L$ if $k>0$, with $\gamma_0=2L$.

Unfortunately, the polynomials $p_x(x) = x^k$ are not orthogonal on any interval.  However, there is a process called 
\textcolor{index}{Gram-Schmidt orthogonalization}\index{Gram-Schmidt orthogonalization} 
[\href{https://en.wikipedia.org/wiki/Gram\%E2\%80\%93Schmidt_process}{Wiki}]
 that turns any sequence of linearly independent functions into orthogonal ones. When applied to $1, x, x^2, x^3$ and so on, it leads to 
\textcolor{index}{Legendre polynomials} [\href{https://en.wikipedia.org/wiki/Legendre_polynomials}{Wiki}] for the $p_k(x)$'s. They are orthogonal on $D = [-1, 1]$.

As usual, rather than minimizing the distance between $f$ and $f_n$ in formula~(\ref{tupues}), it is possible to use a weighted distance. All the results can be adapted. In particular, many orthogonal functions involve a weight. For instance, 
the \textcolor{index}{Hermite polynomials}\index{Hermite polynomials} [\href{https://en.wikipedia.org/wiki/Hermite_polynomials}{Wiki}] involve the weight $w(x)=\exp(-x^2/2)$, and satisfy
$$
\int_{-\infty}^\infty p_i(x)p_j(x)w(x) dx = 0 \text { if } i\neq j.
 $$
In this case, $D$ is the entire real line, infinite in both directions. Finally, it is also possible to directly work with a discrete set $D$
 and 
\textcolor{index}{discrete orthogonal functions}\index{discrete orthogonal functions} [\href{https://en.wikipedia.org/wiki/Discrete_orthogonal_polynomials}{Wiki}]. 
The integrals become sums as usual. The general framework related to all these concepts
 is the \textcolor{index}{Sturm-Liouville theory}\index{Sturm-Liouville theory} [\href{https://en.wikipedia.org/wiki/Sturm\%E2\%80\%93Liouville_theory}{Wiki}].

\subsection{Application to regression}\label{oiuty}

You can apply the Fourier series method to multivariate regression as follows. It works best with continuous features. For discrete features, I advise to look at \textcolor{index}{discrete Fourier series}\index{discrete Fourier series} [\href{https://en.wikipedia.org/wiki/Discrete_Fourier_series}{Wiki}] instead. You want to transform each continuous feature separately so that the values are -- as closely as possible -- distributed uniformly on the interval $[-1, 1]$ after the transformation. This is accomplished in two steps: first apply the transformation $F_k$ to the $k$-th component of the feature vector. Then apply the transformation $Q$. Do this for each component $k=1,\dots,n$.  Here \vspace{1ex}

\begin{itemize}
\item $F_k$ is the 
\textcolor{index}{empirical distribution function}\index{empirical distribution} [\href{https://en.wikipedia.org/wiki/Empirical_distribution_function}{Wiki}] attached to feature $k$. In Python, use the function \texttt{ECDF} from the statsmodel library. 
\item $Q$ is the \textcolor{index}{quantile function}\index{quantile function} [\href{https://en.wikipedia.org/wiki/Quantile_function}{Wiki}] of a uniform distribution on $[-1, 1]$. Thus $Q(u) = -1 + 2u$.
\end{itemize} \vspace{1ex}

\noindent Now use formula~(\ref{poinbg}) on the transformed data, with the $p_k$ and $\alpha_k$ from the first paragraph 
in section~\ref{poreese}, together with $L=1$. Note that there is no matrix inversion in this procedure. It is a 
particular type of \textcolor{index}{spline regression}\index{spline regression} [\href{https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline}{Wiki}]. See Python code in section~\ref{orthofou}. For a different type of spline regression based on exact interpolation similar to the
 method discussed in section~\ref{totor}, see chapter~8 in~\cite{vgelsevier}. 

For polynomial regression, use Legendre polynomials, after transforming the features so that values stay within $[-1, 1]$, and uniformly distributed. Again, there is no matrix inversion and the procedure is fast and simple. For an other example of regression
 with Legendre polynomials, see~\cite{54re2022w}. 

The methods presented here have their roots in interpolating math functions, where successive terms in the summation formula -- in this case formula~(\ref{poinbg}) -- have on average a decreasing impact. Otherwise, the sum, usually involving an infinite number of terms, would not converge. So it makes sense to order the features by decreasing importance. The first feature with coefficient $\alpha_1$ may be chosen as the one with least residual error. The second feature with coefficient $\alpha_2$ being the one yielding the best improvement to the residual error, and so on.  

Also, for the same reason, in multivariate regression, the method is suited for datasets with a large $n$ (number of features)  well approximated by model~(\ref{poinbg}), even with a small number of observations, see~\cite{four2}. 
 Such datasets are sometimes referred to as \textcolor{index}{wide data}\index{wide data}. A more general version uses \textcolor{index}{multidimensional Fourier series}\index{multidimensional Fourier series}~\cite{mfour10}.

%### make a web app out of it
%update gradient.py: za = np.abs(0*xa + 0*ya) # set dimensions for za
 %          ---> za = np.empty(shape=(len(xa),len(ya)))


\section{Python code}

This section contains the code, both for the time series interpolation, and the geospatial temperature dataset. The time series version
  deals with the ocean tide dataset as well as interpolating advanced math functions (with complex values and complex arguments) using the MPmath library. You can use the code for your own datasets, and for synthetization purposes. In addition, I added some minimal code for the multivariate regression based on Fourier series.

\subsection{Time series interpolation}\label{porewa}

This program deals with the interpolation method in one dimension. The code is also on my 
GitHub repository, \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/interpol_fourier.py}{here}. For parameter description, see 
 sections~\ref{puner} and~\ref{venus}.  The code can interpolate a math function or a time series (dataset with observations ordered by time, with fixed time increments) depending on 
 the \texttt{mode} parameter. Either way, the ``object" to be interpolated (function or data) is represented by the function \texttt{g} in the code. The program computes  
 non-linear moving averages,  to interpolate the value of $g(t)$ at fractional arguments of the time, say $t = 1/16, 2/16, 3/16$ and so on, when the value is known  only for integer arguments. 

When $t$ is an integer, the interpolated and observed values are identical. In the case of math functions, under certain general conditions, the interpolated values are also exact when the number of nodes (determined by variable \texttt{n}) is infinite. In practice, very good approximations are obtained already with $n=8$.

The parameter $1/16$ is represented by the variable \texttt{incr} in the code. You can change it to the inverse of a power of two, say $1/8, 1/4$ or $1/2$. Other values
 such as $1/7$ may cause problems: due to computer arithmetic, the instruction \texttt{7*1/7} or \texttt{9*1/9} does not return an exact integer; however \texttt{8*1/8} does. It is 
 easy to  correct this issue if you need to. 

Finally, one way to reduce the number of operations is to use a hash table (dictionary in Python) to store values of $g(t)$ each time a new $t$ is encountered. Due to the moving average, the same value $g(t)$ may be computed multiple times on different occasions. The hash table will avoid double computations, and can save 
 time especially when computing the Zeta function. \vspace{1ex}

\begin{lstlisting}
# interpol_fourier.py (author: MLTechniques.com)
import numpy as np
import mpmath
import matplotlib as mpl
from matplotlib import pyplot as plt

# https://www.digitalocean.ie/Data/DownloadTideData

mode = 'Data' # options: 'Data', 'Math.Bessel', 'Math.Zeta' 

#--- read data

if mode == 'Data': 

    # one column: observed value 
    # time is generated by the algorithm; integer for interpolation nodes

    IN = open("tides_Dublin.txt","r") 
    table = IN.readlines()
    IN.close()

    temp={}
    t = 0  
    # t/t_unit is an integer every t_unit observations (node)
    t_unit = 16 # use 16 for ocean tides, 32 for planet data discussed in the classroom  
    for string in table: 
        string = string.replace('\n', '')
        fields = string.split('\t')
        temp[t/t_unit] = float(fields[0])    
        t = t + 1
    nobs = len(temp)

else:
    t_unit = 16

#--- function to interpolate

def g(t):
    if mode == 'Data':
        z = temp[t]
    elif mode == 'Math.Bessel':
        t = 40*(t-t_min)/(t_max-t_min) 
        z = mpmath.besselj(1,t) 
        z = float(z.real) # real part of the complex-valued function
    elif mode == 'Math.Zeta': 
        t = 4 + 40*(t-t_min)/(t_max-t_min) 
        z = mpmath.zeta(complex(0.5,t)) 
        z = float(z.real) # real part of the complex-valued function
    return(z)

#--- interpolation function

def interpolate(t, eps): 
    sum = 0
    t_0 = int(t + 0.5) # closest interpolation node to t
    pi2 = 2/np.pi  
    flag1 = -1  
    flag2 = -1  
    for k in range(0, n):
        # use nodes k1, k2 in interpolation formula
        k1 = t_0 + k
        k2 = t_0 - k
        tt = t - t_0
        if k != 0: 
            if k %2 == 0:
                z = g(k1) + g(k2) 
                if abs(tt**2 - k**2) > eps:
                    term = flag1 * tt*z*pi2 * np.sin(tt/pi2) / (tt**2 - k**2)
                else:    
                    # use limit as tt --> k
                    term = z/2
                flag1 = -flag1
            else: 
                z = g(k1) - g(k2) 
                if abs(tt**2 - k**2) > eps:
                    term = flag2 * tt*z*pi2 * np.cos(tt/pi2) / (tt**2 - k**2)
                else: 
                    # use limit as tt --> k
                    term = z/2
                flag2 = -flag2
        else: 
            z = g(k1)
            if abs(tt) > eps:
                term = z*pi2*np.sin(tt/pi2) / tt
            else:
                # use limit as tt --> k (here k = 0)
                term = z
        sum += term
    return(sum)

#--- main loop and visualizations 

n  = 8   
    # 2n+1 is number of nodes used in interpolation 
    # in all 3 cases tested (data, math functions), n >= 8 works 
if mode=='Data':
    # restrictions: 
    #     t_min >= n, t_max  <= int(nobs/t_unit - n)
    #     t_max > t_min, at least one node between t_min and t_max
    t_min  = n  # interpolate between t_min and t_max 
    t_max  = int(nobs/t_unit - n)  # must have t_max - t_min > 0
else:
    t_min = 0
    t_max = 100
incr   = 1/t_unit   # time increment between nodes 
eps    = 1.0e-12 

OUT = open("interpol_tides_Dublin.txt","w")

time = []
ze = []
zi = []

fig = plt.figure(figsize=(6,3))
mpl.rcParams['axes.linewidth'] = 0.2
mpl.rc('xtick', labelsize=6) 
mpl.rc('ytick', labelsize=6) 

for t in np.arange(t_min, t_max, incr):  
    time.append(t)
    z_interpol = interpolate(t, eps) 
    z_exact = g(t)
    zi.append(z_interpol)
    ze.append(z_exact)
    error = abs(z_exact - z_interpol)
    if t == int(t):
        plt.scatter(t,z_exact,color='orange', s=6)
    print("t = %8.5f exact = %8.5f interpolated = %8.5f error = %8.5f %3d nodes" % (t,z_exact,z_interpol,error,n))
    OUT.write("%10.6f\t%10.6f\t%10.6f\t%10.6f\n" % (t,z_exact,z_interpol,error))
OUT.close()

plt.plot(time,ze,color='red',linewidth = 0.5, alpha=0.5) 
plt.plot(time,zi,color='blue', linewidth = 0.5,alpha=0.5)
base = min(ze) - (max(ze) -min(ze))/10
for index in range(len(time)):
    # plot error bars showing delta between exact and interpolated values
    t = time[index]
    error = abs(zi[index]-ze[index])
    plt.vlines(t,base,base+error,color='black',linewidth=0.2) 
plt.savefig('tides2.png', dpi=200)
plt.show()
\end{lstlisting}

\subsection{Geospatial temperature dataset}\label{pyif}


The Python code \texttt{interpol.py} is also on my GitHub repository, \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/interpol.py}{here}. 
The functions and parameters are described in sections~\ref{totor} and~\ref{tptyr}. The main function, performing interpolation on a 2-dimensional grid applied 
to temperatures in the Chicago area, is rather simple.  The data is stored into the \texttt{data} array, mapped to the \texttt{npdata} Numpy array. It is then mapped onto a grid,  represented 
 by the \texttt{zgrid} array. The grid is used only to produce contour plots.

Four copies of the training set are generated (using \texttt{ppo=4}). They can be viewed as four synthetized versions of the training set, with locations and temperatures 
distributed just like in
 the original training set. The synthetized locations are stored in the arrays \texttt{xa} and \texttt{ya} (latitude and longitude); the synthetized temperatures obtained by interpolation are stored in the array \texttt{za}. 

Interpolated values computed on locations identical to a training set location are exact, by design. Note that before interpolating, the data is transformed: it is normalized to have zero mean and unit variance, a standard practice. It is de-normalized at to end to produce the contour plots. Each interpolated value is computed using a variable number of nodes. That number depends on how many nodes  are close enough to the target location. A node is a location in the training set with known temperature.

The number of nodes, for each synthetized location, is stored in the \texttt{npt} array. Using the default parameter value for \texttt{alpha} guarantees that there is always at least one node (the nearest neighbor) to compute the interpolated value. This can lead to meaningless interpolated values for locations far away from the training set. Reducing the default \texttt{alpha} results in some non-interpolated values marked as \texttt{NaN}, and it is actually recommended. The un-computed values show up as a green ``+" in Figure~\ref{fig:interpol2}.

Finally, the \texttt{interpolate} function accepts locations \texttt{x}, \texttt{y} that are either a single location or an array of locations. Accordingly, the returned value \texttt{z} -- the temperature -- can be a single value or an array.  The \texttt{audit} parameter is used internally for testing and monitoring purposes. 
\vspace{1ex}


\begin{lstlisting}
import numpy as np
import matplotlib as mpl
from matplotlib import pyplot as plt
from matplotlib import colors  
from matplotlib import cm # color maps

data = [
# (latitute, longitude, temperature)
# source = https://cybergisxhub.cigi.illinois.edu/notebook/spatial-interpolation/
(41.878377,-87.627678,28.24),
(41.751238,-87.712990,19.83),
(41.736314,-87.624179,26.17),
(41.722457,-87.575350,45.70),
(41.736495,-87.614529,35.07),
(41.751295,-87.605288,36.47),
(41.923996,-87.761072,22.45),
(41.866786,-87.666306,45.01), # 125.01 outlier changed to 45.01
(41.808594,-87.665048,19.82),
(41.786756,-87.664343,26.21),
(41.791329,-87.598677,22.04),
(41.751142,-87.712990,20.20),
(41.831070,-87.617298,20.50),
(41.788979,-87.597995,42.15),
(41.914094,-87.683022,21.67),
(41.871480,-87.676440,25.14),
(41.736593,-87.604759,45.01), # 125.01 outlier changed to 45.01
(41.896157,-87.662391,21.16),
(41.788608,-87.598713,19.50),
(41.924903,-87.687703,21.61),
(41.895005,-87.745817,32.03),
(41.892003,-87.611643,28.30),
(41.839066,-87.665685,20.11),
(41.967590,-87.762570,40.60),
(41.885750,-87.629690,42.80),
(41.714021,-87.659612,31.46),
(41.721301,-87.662630,21.35),
(41.692703,-87.621020,21.99),
(41.691803,-87.663723,21.62),
(41.779744,-87.654487,20.88),
(41.820972,-87.802435,20.55),
(41.792543,-87.600008,20.41)
]
npdata = np.array(data)

#--- top parameters 

n = len(npdata)   # number of points in data set
ppo = 4           # create ppo new points around each observed point
new_obs = n * ppo  
alpha = 1.0       # small alpha increases smoothing
beta  = 2.0       # small beta increases smoothing
kappa = 2.0       # high kappa makes method close to kriging
eps   = 1.0e-8    # make it work if sample locations same as observed ones
np.random.seed(6)
radius = 1.2
audit  = True     # so log monitoring info about the interpolation        

xa = []                 # latitute
ya = []                 # longitude
da = []                 # dist between observed and interpolated value
zd = []                 # observed z
za = np.empty(new_obs)  # interpolated z

#--- transform data: normalization 

mu = npdata.mean(axis=0)
stdev = npdata.std(axis=0)
npdata = (npdata - mu)/stdev

#--- interpolation for sampled locations

def w(x, y, x_k, y_k, alpha, beta):
    # distance function
    z = (abs(x - x_k)**beta + abs(y - y_k)**beta)**alpha
    return(z)

# create random locations for interpolation purposes  
for h in range(ppo):
    # sample points in a circle of radius "radius" around each obs 
    xa = np.append(xa, npdata[:,0] + radius * np.random.uniform(-1, 1, n))
    ya = np.append(ya, npdata[:,1] + radius * np.random.uniform(-1, 1, n))
    da = np.append(da, w(xa[-n:],ya[-n:],npdata[:,0],npdata[:,1],alpha,beta))
    zd = np.append(zd, npdata[:,2])

delta = eps + max(da)   # to ignore obs too far away from sampled point
npt = np.empty(new_obs) # number of points used for interpolation at location j

def interpolate(x, y, npdata, delta, audit):
    # compute interpolated z at location (x, y) based on npdata (observations)
    # also returns npoints, the number of data points used in the interpolation
    # data points (x_k, y_k) with w[(x,y), (x_k,y_k)] >= delta are ignored
    # note: (x, y) can be a location or an array of locations

    sum  = 0.0
    sum_coeff = 0.0
    npoints = 0
    for k in range(n):
        x_k = npdata[k, 0]
        y_k = npdata[k, 1]
        z_k = npdata[k, 2]
        coeff = 1
        for i in range(n):
            x_i = npdata[i, 0]
            y_i = npdata[i, 1]
            if i != k:
                numerator = w(x, y, x_i, y_i, alpha, beta)
                denominator = w(x_k, y_k, x_i, y_i, alpha, beta) 
                coeff *= numerator / (eps + denominator) 
        dist = w(x, y, x_k, y_k, alpha, beta)
        if dist < delta:
            coeff = (eps + dist)**(-kappa) * coeff / (1 + coeff) 
            sum_coeff += coeff
            npoints += 1
            if audit:
                OUT.write("%3d\t%3d\t%8.5f\t%8.5f\t%8.5f\n" % (j,k,z_k,coeff,dist))
        else:
            coeff = 0.0
        sum += z_k * coeff  
    if npoints > 0:
        z = sum / sum_coeff 
    else:
        z = 'NaN'  # undefined
    return(z, npoints)

OUT=open("audit.txt","w")   # output file for auditing / detecting issues
OUT.write("j\tk\tz_k\tcoeff\tdist\n")

for j in range(new_obs):
    (za[j], npt[j]) = interpolate(xa[j], ya[j], npdata, 0.5*delta, audit=True)

OUT.close()

#--- inverse transform (un-normalize) and visualizations

steps = 140  # to create grid with steps x steps points, to generate contours
xb = np.linspace(min(npdata[:,0])-0.50, max(npdata[:,0])+0.50, steps)
yb = np.linspace(min(npdata[:,1])-0.50, max(npdata[:,1])+0.50, steps)
xc = mu[0] + stdev[0] * xb
yc = mu[1] + stdev[1] * yb
xc, yc = np.meshgrid(xc, yc)
zgrid = np.empty(shape=(len(xb),len(yb)))   

# create grid and get interpolated values at grid locations
for h in range(len(xb)):
    for k in range(len(yb)):
        x = xb[h]
        y = yb[k]
        (z, points) = interpolate(x, y, npdata, 2.2*delta, audit=False)
        if z == 'NaN':
            zgrid[h,k] = 'NaN'
        else: 
            zgrid[h,k] = mu[2] + stdev[2] * z
zgridt = zgrid.transpose()

# inverse transform
xa = mu[0] + stdev[0] * xa
ya = mu[1] + stdev[1] * ya
za = mu[2] + stdev[2] * za
xb = mu[0] + stdev[0] * xb
yb = mu[1] + stdev[1] * yb
npdata = mu + stdev * npdata

def set_plt_params():
    # initialize visualizations
    fig = plt.figure(figsize =(4, 3), dpi=200) 
    ax = fig.gca()
    plt.setp(ax.spines.values(), linewidth=0.1)
    ax.xaxis.set_tick_params(width=0.1)
    ax.yaxis.set_tick_params(width=0.1)
    ax.xaxis.set_tick_params(length=2)
    ax.yaxis.set_tick_params(length=2)
    ax.tick_params(axis='x', labelsize=4)
    ax.tick_params(axis='y', labelsize=4)
    plt.rc('xtick', labelsize=4) 
    plt.rc('ytick', labelsize=4) 
    plt.rcParams['axes.linewidth'] = 0.1
    return(fig,ax)

# contour plot
(fig, ax) = set_plt_params() 
cs = plt.contourf(yc, xc, zgridt,cmap='coolwarm',levels=16) 
cbar = plt.colorbar(cs)
cbar.ax.tick_params(width=0.1) 
cbar.ax.tick_params(length=2) 
plt.scatter(npdata[:,1], npdata[:,0], c=npdata[:,2], s=8, cmap=cm.coolwarm,
      edgecolors='black',linewidth=0.3,alpha=0.8)
plt.show()
plt.close()
           
# scatter plot        
(fig, ax) = set_plt_params()
my_cmap = cm.get_cmap('coolwarm')
my_norm = colors.Normalize()
ec_colors = my_cmap(my_norm(npdata[:,2]))
plt.scatter(npdata[:,1], npdata[:,0], c='white', s=5, cmap=cm.coolwarm,
    edgecolors=ec_colors,linewidth=0.4)
sc=plt.scatter(ya[npt>0], xa[npt>0], c=za[npt>0], cmap=cm.coolwarm, 
    marker='+',s=5,linewidth=0.4) 

# show in green points not interpolated as they were too far away
plt.scatter(ya[npt==0], xa[npt==0], c='lightgreen', marker='+', s=5, 
    linewidth=0.4) 

cbar = plt.colorbar(sc)
cbar.ax.tick_params(width=0.1) 
cbar.ax.tick_params(length=2)
# plt.ylim(min(npdata[:,0]),max(npdata[:,0]))
# plt.xlim(min(npdata[:,1]),max(npdata[:,1]))
plt.show()

#--- measuring quality of the fit

error = np.mean(abs(za[npt>0] - zd[npt>0]))
print("Error=",delta)
\end{lstlisting}

\subsection{Regression with Fourier series}\label{orthofou}

The basic code here is provided to illustrate the methodology in section~\ref{oiuty}, for multivariate regression with sine and cosine splines. It is a minimal workable piece of code. The
 data is made up, and no transformer is necessary because the observed values are already in $[-1,1]$ for the feature vector, by construction. The code is also
 on my GitHub repository, 
\href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/interpol_ortho.py}{here}.
 Look for \texttt{interpol\_ortho.py}. \vspace{1ex}

\begin{lstlisting}
import numpy as np
import random

#---- make up data

data = []
nobs = 100 
random.seed(69)
for i in range(nobs):
    x1 = -1 + 2*random.random()           # feature 1
    x2 = -1 + 2*random.random()           # feature 2
    z  = np.sin(0.56*x1) - 0.5*np.cos(1.53*x2)  # response
    obs = [x1, x2, z]
    data.append(obs)

npdata = np.array(data)
transf_npdata = npdata  # no data transformer needed here

#--- the p_k functions 

def p_k(x, k):

    # if input x is an array, output z is also an array

    if k % 2 == 0:
        z = np.cos(k*x*np.pi)
    else: 
        z = np.sin(k*x*np.pi)
    return(z)

#--- beta_k, alpha_k, gamma_k coefficients

intercept = np.ones(nobs)   
p_0 = p_k(intercept, k = 0)
p_1 = p_k(transf_npdata[:,0], k = 1)  # feature 1
p_2 = p_k(transf_npdata[:,1], k = 2)  # feature 2 

gamma_0 = np.dot(p_0, p_0) # dot product
gamma_1 = np.dot(p_1, p_1)
gamma_2 = np.dot(p_2, p_2)

observed_temp =  npdata[:,2] 
beta_0 = np.dot(p_0, observed_temp)
beta_1 = np.dot(p_1, observed_temp)
beta_2 = np.dot(p_2, observed_temp)

alpha_0 = beta_0 / gamma_0
alpha_1 = beta_1 / gamma_1
alpha_2 = beta_2 / gamma_2

#--- interpolation 

predicted_temp = alpha_0 * p_0 + alpha_1 * p_1 + alpha_2 * p_2 

#--- print results: predicted vs observed

for i in range(nobs):
    print("%8.5f %8.5f" %(predicted_temp[i],observed_temp[i]))

correlmatrix = np.corrcoef(predicted_temp,observed_temp)
correlation = correlmatrix[0, 1]
print("corr between predicted/observed: %8.5f" % (correlation))

#--- interpolate for new observation (with intercept = 1)

x1 =  0.234
x2 = -0.541

z_predicted = alpha_0 * p_k(1,k=0) + alpha_1 * p_k(x1,k=1) + alpha_2 * p_k(x2,k=2)
print("test interpolation: z_predict = %8.5f" %(z_predicted))
\end{lstlisting}

%-------------------------------------------------------------------------------

\chapter{Synthetic Tabular Data: Copulas vs enhanced GANs}{}\label{newai}

  I covered many methods leading to interpretable machine learning and \textcolor{index}{explainable AI}\index{explainable AI}, throughout this book. For the sake of completeness, in this chapter, I describe copulas and GAN for data synthetization, as well as additional explainable AI topics and other usages of
synthetic data.

A key concept is \textcolor{index}{feature attribution}\index{feature attribution}. It indicates how much each feature in your model contributed to the predictions, for each individual observation. It is different from global feature attribution, applied to the dataset as a whole and measured using \textcolor{index}{predictive power}\index{predictive power} 
  or percentage of total variance attributed to specific features in 
\textcolor{index}{principal component analysis}\index{principal component analysis}, or based on combinatorial methods such as in  section~7.5 in~\cite{vgelsevier}. In the context of computer vision, it is sometimes called pixel
 attribution and represents the most influential pixels that explain the result of image classification performed by neural networks or other means. In this context, each pixel of an image is considered as a feature. See the book ``Interpretable Machine Learning" \cite{cmol}, page 254. For an introduction to feature attribution, comparing different methods, see 
 \href{https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview}{here}.

For regression, additive models and tabular data (as opposed to images), a popular feature attribution method referred to as SHAP is based on the 
\textcolor{index}{Shapley value}\index{Shapley value} [\href{https://en.wikipedia.org/wiki/Shapley_value}{Wiki}], originating from game theory. More about SHAP can be
 found 
\href{https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An\%20introduction\%20to\%20explainable\%20AI\%20with\%20Shapley\%20values.html}{here},
 in the document ``An introduction to explainable AI with Shapley values" posted in 2018 by Scott Lundberg, Senior Researcher at Microsoft Research.
 A simple example in the context of linear regression is posted 
\href{https://towardsdatascience.com/explainable-ai-xai-with-shap-regression-problem-b2d63fdca670}{here}. Another key concept is \textcolor{index}{feature importance}\index{feature importance}. It is used to score input features based on how useful they are at predicting a target variable. 
The term, possibly coined around 2020 by machine learning practitioners, is a different name for the 
\textcolor{index}{predictive power}\index{predictive power} of a feature, used throughout this book and covering many different metrics. It has become
 popular among Python developers, see for instance \href{https://machinelearningmastery.com/calculate-feature-importance-with-python/}{here}.

In addition to feature attribution and importance, another way to understand how a black-box system works (to increase explainability) is to assess the contribution of 
a subset of observations, and its predictive impact either on the whole system or on a specific observation. In particular, one would want to detect the observations with the greatest impact on specific predictions, to understand how these predictions were made, especially outside the training set. For instance, to see how a specific prediction is impacted by individual observations, 
you can remove the observation that least impacts that prediction, in your training set. Then re-run the model, and again remove the observation with minimum impact on that prediction.
 And again and again recursively until you are left with a tiny training set, yet big enough to compute with great precision the predicted value in question. Then see if the observations left in your tiny training set share common patterns.

 Another way to understand how your black-box works is to use rich synthetic or hand-made data to find observations that are not properly handled. For instance
 a specific word such as 2:39 pronounced with an accent. Alexa always confuses it with 2:59 when I ask her, with my French accent, to set an alarm at 2:39 to go pick up my son at school. A simple fix in this case would be for Alexa to learn directly from me, recognize her error as I train her, and then get it fixed for good. This has the benefit to let Alexa adapt to each customer, offering customized chats rather than relying only on a central training set. Finding counter-examples to your system, to make it fail, in essence to crack it, is referred to as \textcolor{index}{adversarial learning}\index{adversarial learning} [\href{https://en.wikipedia.org/wiki/Adversarial_machine_learning}{Wiki}]. It helps you better understand how your black-box works, and by integrating these tricky cases, it helps you improve your system.

Finally, \textcolor{index}{generative adversarial networks} (GANs)\index{GAN (generative adversarial networks)} [\href{https://en.wikipedia.org/wiki/Generative_adversarial_network}{Wiki}] are popular in computer vision. Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. 
To identify a fake GAN-generated picture from a real one of the same person based on iris parameters, see~\cite{gan2021}. 

\section{Sensitivity analysis, bias reduction and other uses of synthetic data}

\textcolor{index}{Synthetic data}\index{synthetic data} was originally developed as a method to replace missing values with synthetic ones: this is known 
as \textcolor{index}{imputation}\index{imputation (missing values)} [\href{https://en.wikipedia.org/wiki/Imputation_(statistics)}{Wiki}]. 
It did not work well as the missing values typically don't follow the underlying statistical model. A potential solution is as follows. 
Use real or synthetic data (ideally, both) and remove some values to emulate a data set with missing values. Then replace the missing values by synthetic ones. Try with a large number of parameter-driven simulations to see which parameter values are best at producing meaningful missing values. Another benefit of synthetic data is its ability to test model resilience: replace some real observations in your 
\textcolor{index}{validation set}\index{validation set} with 
 synthetic ones, and see how your predictions are sensitive to this change. Eliminate models that show lack of resilience. This is an easy way to reduce \textcolor{index}{overfitting}\index{overfitting}.  

Another use of synthetic data is for \textcolor{index}{bootstrapping}\index{bootstrapping} [\href{https://en.wikipedia.org/wiki/Bootstrapping_(statistics)}{Wiki}] or to compute \textcolor{index}{confidence regions}\index{confidence region} based on 
\textcolor{index}{parametric bootstrap}\index{parametric bootstrap}. Numerous examples are included in this book: see the keywords in question in the index. This simulation-heavy technique requires a large amount of
 data generated with the same parameter values as those estimated on your original data set. Also, synthetic data is used to benchmark and test 
 algorithms. Again, this book features numerous examples. It is also used to correct for imbalanced data or to reduce algorithm biases, by over-sampling from groups or segments with few observations or representing minorities. The example in section~\ref{piviiiurobvbc} shows
 how to generate synthetic data at the group level, rather than globally. It allows you to choose how many observations you want to generate, for each group.

The technique must be properly implemented to reduce \textcolor{index}{algorithmic bias}\index{algorithmic bias} [\href{https://en.wikipedia.org/wiki/Algorithmic_bias}{Wiki}] that penalizes minorities. When using copulas, see the issues in section~\ref{sdsvc}. A solution  is to add extra features (say, education level) to your real data to better capture the nuances of a population segment that is consistently penalized as a whole (high crime area), to detect sub-segments that do well. Then you can compute a separate copula for sub-segments such as college-educated in high crime area. 

Finally, for authors and publishers, synthetic images can mimic and replace copyrighted ones, eliminating licensing fees and authorization issues.  Synthesized art is called \textcolor{index}{AI art}\index{AI art}. The question is: who owns it? Can you own randomly generated patterns or numbers?

% yyyyyy yyy

\section{Using copulas to generate synthetic data}\label{piviiiurobvbc}

One method to generate data with the exact same correlation structure and same marginal distributions as in your real dataset is to use \textcolor{index}{copulas}\index{copula} [\href{https://en.wikipedia.org/wiki/Copula_(probability_theory)}{Wiki}]. The algorithm to produce $n$ synthesized observations is as follows: \vspace{1ex}
\begin{itemize}
\item Step 1: Compute the correlation matrix $W$ associated to your real data.
\item Step 2: Generate $n$ deviates from a multivariate Gaussian distribution with zero mean and covariance matrix $W$. Each deviate is a 
 vector $Z_i$ ($i=1,\dots,n$), with the components matching the features in the real data set.
\item Step 3: For each generated $Z_{ij}$ (the $j$-th feature in your $i$-th vector) compute $U_{ij}=\Phi(Z_{ij})$, where $\Phi$ is
 the CDF (cumulative distribution function) of a univariate standard normal distribution. Thus $0\leq U_{ij}\leq 1$.
\item Step 4: Compute $S_{ij}=Q_j(U_{ij})$ where $Q_j$ is the univariate
\textcolor{index}{empirical quantile distribution}\index{quantile!empirical} (the inverse of the \textcolor{index}{empirical distribution}\index{empirical distribution}) attached to the $j$-th feature, and computed on the real data.
\end{itemize}\vspace{1ex}
Assuming $W$ is non singular, your set of feature vectors $S_i$ ($i=1,\dots,n$) is your synthesized data, mimicking your real data set.
I implicitly used the \textcolor{index}{Gaussian copula}\index{copula!Gaussian} here, but other options exist, such as the 
\textcolor{index}{Frank copula}\index{copula!Frank}. This method is a direct application 
 of \textcolor{index}{Sklar's theorem}\index{Sklar's theorem} [\href{https://en.wikipedia.org/wiki/Copula_(probability_theory)#Sklar's_theorem}{Wiki}]. There are various ways to measure the similarity or distance between the synthetic and real version of the data. In this context, the \textcolor{index}{Hellinger distance}\index{Hellinger distance} 
[\href{https://en.wikipedia.org/wiki/Hellinger_distance}{Wiki}] is popular.  See also section~\ref{evrd7hg} 
 on comparing two datasets. However these metrics lead to \textcolor{index}{overfitting}\index{overfitting}: the best synthetic data being an exact replica of the real data. Having statistical summaries matching those in the real data as in Table~\ref{taburew}, combined with the worst Hellinger score, leads to richer synthetic data. In the end, the quality should be measured by the improvement obtained when making predictions for the \textcolor{index}{validation set}\index{validation set}, after adding your synthetic data to the training set.

The Hellinger distance is popular because it takes values between 0 and 1, with 0 being a perfect fit, and 1 being the worst case. It is based on the PDF (probability density function) instead of the CDF (cumulative distribution function). This leads to a bumpier metric more sensitive to noise, compared to (say) the 
\textcolor{index}{Kolmogorov-Smirnov distance}\index{Kolmogorov-Smirnov test}  [\href{https://en.wikipedia.org/wiki/Kolmogorov\%E2\%80\%93Smirnov_test}{Wiki}] between two CDFs.
Also, while the Hellinger distance can be computed globally by working with the joint, multivariate PDF involving all the features, it is based on  observed frequencies measured over a large number of small multivariate bins in the feature vector, each with very few observations and thus unstable.  A better solution is to compute the distance separately for each feature, and then take the maximum of these distances. In short, considering the best synthetization as the one minimizing the maximum distance across all features. On the plus side, the Hellinger distance can handle categorical data very well.

The formula to compare two probability density functions $P$ and $P'$ is as follows, illustrated for discrete distributions with $n$ levels or categories. In practice, it is applied to frequencies (the empirical PDF measured on each feature).
$$
H(P,P') = \frac{1}{\sqrt{2}} \sqrt{\sum_{k=1}^n \Big(\sqrt{p_i}-\sqrt{p_i'}\Big)^2}
$$

In Python, the four steps of the synthetization algorithm are performed respectively with the functions \texttt{np.corrcoef},
 \texttt{np.random.multivariate\_normal}, \texttt{norm.cdf}
 and \texttt{np.quantile}. Except for \texttt{norm.cdf} (CDF of standard Gaussian distribution) which comes from the Scipy library, the other ones are implemented in Numpy. To generate the exact same synthetic data each time you run the program, 
 use \texttt{np.random.seed} with the same \textcolor{index}{seed}\index{seed (random number generator)}. 

Finally, a few Python libraries 
  deal with copulas, for instance \href{https://pypi.org/project/copulalib/}{Copulalib} and \href{https://pypi.org/project/copulas/}{Copulas}. See also \href{https://pypi.org/project/sdv/}{SDV} (the Synthetic Data Vault, in Python) for more options including deep learning. The Python program \href{https://github.com/VincentGranville/Main/blob/main/copula.py}{\texttt{copula.py}} on my GitHup repository (main folder) 
shows how it works. It is also possible to avoid copulas and deal directly with the correlation matrix, as in section~7.2.1 in~\cite{vgelsevier}. Or ignore correlations altogether, and simply add uncorrelated white noise to each feature: this may be the easiest way to generate synthetic data. This approach is 
 significantly superior to copulas
  to generate synthetic values outside the range observed in the real data. It also preserves the correlation structure, and in some sense, generates richer data. Another popular method is \textcolor{index}{rejection sampling}\index{rejection sampling} [\href{https://en.wikipedia.org/wiki/Rejection_sampling}{Wiki}].

\subsection{The insurance dataset: Python code and results}\label{sdsvc}

I used the algorithm in section~\ref{piviiiurobvbc} to synthesize the insurance dataset shared on Kaggle, 
 \href{https://www.kaggle.com/datasets/teertha/ushealthinsurancedataset}{here}. The spreadsheet 
\href{https://github.com/VincentGranville/Main/blob/main/insurance.xlsx}{\texttt{insurance.xlsx}} on GitHub (main folder) summarizes all the findings, and contains three datasets: the real data, synthetic data that I produced with
 \href{https://github.com/VincentGranville/Main/blob/main/insurance.py}{\texttt{insurance.py}} (same folder), and synthetic data generated by Mostly.ai. The dataset has the following fields: age, sex, bmi (body mass index), number of children covered by plan, smoker (yes or no), 
 region (Northeast and so on), and charges incurred by the insurer for the customer in question. 

I don't know what algorithm Mostly.ai uses,
  but their synthetic copy of the real data is strikingly similar to mine. Both have all the hallmarks (quality and defects) of being copula-generated. My version is slightly better because I generated a different copula (and thus, a different correlation matrix) for each group of observations. I grouped the observations by sex, smoker status and region while Mostly.ai applied the same copula across all these groups. 
Automatically detecting the groups as large homogeneous \textcolor{index}{nodes}\index{node (decision tree)} in a decision tree, combined with using a separate copula for each node, would lead to an \textcolor{index}{ensemble method}\index{ensemble methods} not unlike the \textcolor{index}{boosted trees}\index{boosted trees} described in chapter~\ref{piereboul}.  By large node, I mean a node with many observations  (enough to compute a meaningful correlation matrix and empirical quantiles) but little depth to avoid overfitting. The nodes are detected on the real data.
 

\begin{table}%[H]
\[\def\arraystretch{1.1}
\begin{array}{lrrr}
\hline
\text{Statistic}	& \text{Real data}& \text{Synthetic 1} & \text{Synthetic 2}\\
\hline
\hline	
\text{Mean age} & 39.21 &	39.21	& 38.61  \\
\text{Min age} & 18 & 18 & 18 \\
\text{Max age} & 64 & 64 & 64 \\
\hline	
\text{Mean bmi} & 30.66	& 30.97 &	30.79\\
\text{Min bmi} & 15.96	& 17.29 &	16.11\\
\text{Max bmi} & 53.13	& 47.74 &	52.98\\
\hline	
\text{Mean charges} & \num{13270}	 & \num{13516}	& \num{13253} \\
\text{Min charges} & \num{1122} &	\num{1137}&	\num{1126} \\
\text{Max charges} & \num{63770} &	\num{49993}&	\num{59588} \\
\text{Stdev charges} &\num{12110}	& \num{12330} &	\num{12132}  \\
\hline	
\text{Correl age,  bmi} &0.11 &	0.06 &	0.09 \\
\text{Correl age,  children} & 0.04	&0.02	&0.03 \\
\text{Correl age,  charges} & 0.30 & 0.29 & 0.30\\
\text{Correl bmi, charges} & 0.20 & 0.14 & 0.18\\
\text{Stdev children} & 1.21 & 1.19 & 1.20 \\
\hline
\end{array}
\]
\caption{\label{taburew} Comparing real data with two different synthetic copies}
\end{table}

Table~\ref{taburew} provides some high-level summary statistics: Synthetic 1 is produced by Mostly.ai, and Synthetic 2 using the methodology described here. The correlation structure is well reconstituted. Synthetic 2 is better than Synthetic 1 for ``Max charges" (the maximum computed over all observations) because it generates separate copulas for each group. Also, this feature has a bimodal distribution.

The sore point is that copulas are unable to generate values outside the range observed in the real dataset: this is evident when looking at the maxima and minima. All data sets have the same number of observations. Even when I increase the number of synthesized observations by a factor 100, I am still stuck with a maximum charge less than $\$\num{63770}$, even though this ceiling is not an outlier in the real data. The same is true with ``age", although this is compounded by the fact that ages 18 and 64 are cut-off points. 
The issue is that the quantile functions $Q_j$ in section~\ref{piviiiurobvbc} generate values between the minimum and maximum observed in
 the real data, for each feature $j$. 
A workaround is to introduce uncorrelated white noise either in the real or synthetic data: see section~\ref{babel99}.

The Python code in this section can be optimized for speed as follows: pre-compute the empirical quantiles functions associated to each feature in the real dataset, as well as the CDF of the standard Gaussian distribution. In other words, use a table of pre-computed values. Note that the empirical quantiles in my method need to be computed separately for each group. 
Except for ``bmi", the features in the insurance dataset are highly non-Gaussian: ``charges" is bimodal, ``number of children" has a geometric (discrete) distribution, and ``age" is uniform except for the extremes. \vspace{1ex}


\begin{lstlisting}
import csv 
from scipy.stats import norm
import numpy as np

filename = 'insurance.csv' # make sure fields don't contain commas
# source: https://www.kaggle.com/datasets/teertha/ushealthinsurancedataset
# Fields: age, sex, bmi, children, smoker, region, charges

with open(filename, 'r') as csvfile:
    reader = csv.reader(csvfile)
    fields = next(reader) # Reads header row as a list
    rows = list(reader)   # Reads all subsequent rows as a list of lists

#-- group by (sex, smoker, region)

groupCount = {}
groupList = {}
for obs in rows:
    group = obs[1] +"\t"+obs[4]+"\t"+obs[5]
    if group in groupCount:
        cnt = groupCount[group]
        groupList[(group,cnt)]=(obs[0],obs[2],obs[3],obs[6]) 
        groupCount[group] += 1    
    else:
        groupList[(group,0)]=(obs[0],obs[2],obs[3],obs[6]) 
        groupCount[group] = 1

#-- generate synthetic data customized to each group (Gaussian copula)

seed = 453
np.random.seed(seed)
OUT=open("insurance_synth.txt","w")
for group in groupCount:
    nobs = groupCount[group]
    age = []
    bmi = []
    children = []
    charges = []
    for cnt in range(nobs):
        features = groupList[(group,cnt)]
        age.append(float(features[0]))       # uniform outside very young or very old
        bmi.append(float(features[1]))       # Gaussian distribution?
        children.append(float(features[2]))  # geometric distribution?
        charges.append(float(features[3]))   # bimodal, not gaussian 
    mu  = [np.mean(age), np.mean(bmi), np.mean(children), np.mean(charges)]
    zero = [0, 0, 0, 0] 
    z = np.stack((age, bmi, children, charges), axis = 0)
    corr = np.corrcoef(z) # correlation matrix for Gaussian copula for this group

    print("------------------")
    print("\n\nGroup: ",group,"[",cnt,"obs ]\n") 
    print("mean age: %2d\nmean bmi: %2d\nmean children: %1.2f\nmean charges: %2d\n" 
           % (mu[0],mu[1],mu[2],mu[3]))  
    print("correlation matrix:\n")
    print(np.corrcoef(z),"\n")
    nobs_synth = nobs  # number of synthetic obs to create for this group
    gfg = np.random.multivariate_normal(zero, corr, nobs_synth) 
    g_age = gfg[:,0]
    g_bmi = gfg[:,1]
    g_children = gfg[:,2]
    g_charges = gfg[:,3]

    # generate nobs_synth observations for this group
    print("synthetic observations:\n")
    for k in range(nobs_synth):   
        u_age = norm.cdf(g_age[k])
        u_bmi = norm.cdf(g_bmi[k])
        u_children = norm.cdf(g_children[k])
        u_charges = norm.cdf(g_charges[k])
        s_age = np.quantile(age, u_age)                # synthesized age 
        s_bmi = np.quantile(bmi, u_bmi)                # synthesized bmi
        s_children = np.quantile(children, u_children) # synthesized children
        s_charges = np.quantile(charges, u_charges)    # synthesized charges
        line = group+"\t"+str(s_age)+"\t"+str(s_bmi)+
                   "\t"+str(s_children)+"\t"+str(s_charges)+"\n"
        OUT.write(line)
        print("%3d. %d %d %d %d" %(k, s_age, s_bmi, s_children, s_charges))
OUT.close()
\end{lstlisting}

\subsection{Potential improvements}\label{babel99}

The copula method as implemented in section~\ref{sdsvc} has some limitations: it does not generate synthetic data outside the observed range in the real data. A simple solution is to add some parametric noise to each feature. The parameter can be the amount of noise added to a specific 
 feature, or in other words, the variance attached to the added noise. See section~7.2 in~\cite{vgelsevier} for an application of this method. For Gaussian-like features, it makes sense to use a white noise (uncorrelated
 Gaussian noise with zero mean). For some features, a noise generated using a two-parameter distribution may be a better fit. Noise can also be generated jointly for more than one feature at a time, with correlations among the noise components replicating those found in the real data, for the features in question. 
 Again, this is discussed in section~7.2~in~\cite{vgelsevier}.

Also, rather than using the empirical quantiles in step 4 (see beginning of section~\ref{piviiiurobvbc}), you may use the quantiles of some known distribution: one that is a good fit for the feature in question. The parameters attached to the distribution in question are estimated on the real data. So, when performing the simulation in step 4 for a specific feature, you use the distribution in question with its parameter(s) estimated on the real data. For instance, in the insurance dataset, the BMI feature (body mass index) is well approximated by a Gaussian distribution, while the number of children is
 well approximated by a geometric distribution. The ``charges" feature (cost to the insurance company for each policy holder) is bimodal:  
in this case, a mixture of two Gaussian's may be a good fit. Such mixtures are referred to as 
\textcolor{index}{Gaussian mixture models}\index{GMM (Gaussian mixture model)} (GMM) in this context.

Finally, you can produce a very large number of synthetic datasets to mimic the same real dataset. This is accomplished by using a different \texttt{seed} each time in the Python code. However, it is rather inefficient as each new synthetized version
 does not make any progress towards improving the fit with the real data: you need to try a very large number of seeds.  A better solution consists in using parametric noise or pre-specified distributions: Gaussian, geometric, or Gaussian mixture depending on the feature, as described earlier for this particular dataset. Thus you introduce parameters that can be estimated or fine-tuned based on the real data, rather than using parameter-free 
 empirical quantiles. Remember that the quantile distribution is the inverse of the cumulative distribution, whether empirical or parametric and model-based.  

Now, when moving from one synthetized version to the next one, this approach allows you to fine-tune the parameters in such a way that the fit with the real data -- measured via Hellinger, or a discriminator function if using GAN -- improves over time. This approach can use a gradient-descent path,
 where the target function to optimize is (say) the Hellinger distance, and the domain of this function (its arguments) is the parameter space.
 By optimization, I mean finding the minimum in the case of the Hellinger distance, as it corresponds to the best fit.
This is illustrated in Figure~\ref{fig:digdig}. It can be done without neural networks unlike traditional GAN. It may be a lot faster than
 training a neural network, easier to control, leading to more explainable AI and lower risk of over-fitting.
 In short, you get all the benefits of \textcolor{index}{generative adversarial networks}\index{GAN (generative adversarial networks)}, without the drawbacks.

\section{Synthetization: GAN versus copulas}

\textcolor{index}{Generative adversarial networks}\index{GAN (generative adversarial networks)} (GAN) have been very successful in some applications such as computer vision. Many computer-intensive AI platforms rely on them, partly because of its benefits, partly for marketing purposes as it tends to sell well. Here I describe my point of view. 
There are many good things in GAN, and many features that can be dropped or improved. In particular, it can be done outside of neural networks (and then renamed accordingly). But what is GAN to begin with?


In this context, GAN is used to mimic real datasets, create new ones and blend them with real data, to improve predictions, classification or any machine learning algorithm. Think about text generation in ChatGPT. It has two parts: data generation (the synthesizer) and checking how good the generated data is (the discriminator). In the process, it uses an iterative optimization algorithm to move from one set of synthetized data, to the next one: a better one hopefully, using a gradient technique minimizing a cost function involving many parameters. The cost function tells you how good or bad you are at mimicking the real data. 

\subsection{Parameterizing the copula quantiles combined with gradient descent}

The immediate drawbacks of GAN are the risk of over-fitting, the time it takes to train the algorithm, and the lack of explainability. There are solutions to the latter, based on feature importance. But if you work with traditional data (transactional, digital twins, tabular data), what are the benefits? Or more specifically, can you get most of the benefits without using an expensive, slow or full GAN implementation?
The answer is yes. Replicating the feature distribution and correlation structure present in your real data can be done efficiently with copulas.  Indeed, many GAN systems also use copulas. Parameter-free empirical quantiles used in copulas  can be replaced by parametric probability distributions fit to your real data. If a feature is bimodal, try a mixture of Gaussian distributions: 
now you are playing with GMMs (\textcolor{index}{Gaussian mixture models}\index{GMM (Gaussian mixture model)}), a technique sometimes incorporated in GAN. The parameters of your GMM (centers, variance and weight attached to each component) are estimated on the real data with the \textcolor{index}{EM algorithm}\index{EM algorithm}. It also allows you to sample outside the range in your real data.


But one of the big benefits of GAN is its ability to navigate through a high-dimensional parameter space to eventually get closer and closer to a good minimum (a good representation of the original data), albeit very slowly due to the curse of dimensionality. Parameter-free, copula-based methods would require a tremendous number of simulations, each using a different seed from a random generator, to compete with GAN. What GAN could do in several hours, these simulations may require months of computing power to achieve similar results. This true especially if the number of features is large.

But there is a solution to this, outside of GAN. First, use a parametric technique with your copula method (or other method such as noise injection). If you have many features, you may end up with as many parameters as a large GAN system, so you are stuck again. One workaround is to compress the feature space: use selected features for selected groups in your data. Another way is to optimize 2-3 parameters at a time (carefully selected) in a stepwise procedure. Start from various configurations as in swarm optimization and do it separately for data segments (groups) as in ensemble methods such as XGBoost. You may not reach a global optimum, but the difference with extensive neural network processing (GAN) may be very small. And then you have a more interpretable technique, faster and requiring fewer resources than GAN, and thus less expensive. And possibly home-made, so you have full control of the algorithm.

\subsection{Feature clustering to break a big problem into smaller ones}\label{fcv34}

One way to identify subsets of features to apply a separate copula to each of them is as follows. The method
 is called \textcolor{index}{feature clustering}\index{feature clustering}~\cite{fcnice}, as opposed 
to traditional clustering aimed at grouping observations. A Python implementation can be found \href{https://scikit-learn.org/stable/modules/generated/sklearn.cluster.FeatureAgglomeration.html}{here}.
Start by computing the correlation matrix
 attached to your real data. Rank all pairs of features $\{A, B\}$  by correlation between $A$ and $B$, starting with the largest correlation in absolute value, down to the lowest one that is statistically significant and/or at least above (say) $0.30$. 
The top pair constitutes your first group of features. Look at the second pair. If it contains a feature from the first group, merge the two groups to obtain a single group with 3 features. If not, you have two groups of features at this stage, each containing 2 features. Proceed iteratively until 
all pairs of features have been visited. 

However, the correlation structure may not always represent all the dependencies in the data: points distributed on a circle are not correlated, yet they are highly dependent! In this case, use 1 dimension (the angular position)  rather than 
 the 2 Cartesian coordinates. More generally, appropriate data transformations and reduction can fix the issue.


\section{Deep dive into generative adversarial networks (GAN)}

In this section I discuss GAN in details, with a Python implementation to synthesize tabular data. Unlike many neural networks, my code can generate replicable outputs. 
Other solutions and references are provided in section~\ref{xcvcx}. Later on, I discuss enhancements to the original model. Finally, I show how to blend GAN with copulas to get the best
 of both worlds. 


\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{sdccp.png}  
\caption{Synthetic versus real data, produced by SDV GAN + copula}
\label{fig:pictty}
\end{figure}

\subsection{Open source libraries and references}\label{xcvcx}

One of the most popular libraries for synthetization is SDV, which stands for 
\textcolor{index}{synthetic data vault}\index{SDV (Python library)}. You can check it out on GitHub, 
 \href{https://github.com/sdv-dev/SDV}{here}. For sample code,
 see \href{https://medium.com/@davide.gazze/sdv-generate-synthetic-data-using-gan-and-python-4c26a1e4b3c2}{here}. 
and \href{https://bobrupakroy.medium.com/gan-based-deep-learning-data-synthesizer-copulagan-a6376169b3ca}{here}. 
SDV comes with 28 real-life datasets. To see the list, with the number of tables, rows and columns for each data set, run the code below. \vspace{1ex}

\begin{lstlisting}
from sdv.demo import get_available_demos
from sdv.demo import load_tabular_demo
from sdv.tabular import CopulaGAN

demos = get_available_demos()
print(demos)  # show list of demo datasets

metadata, real_data = load_tabular_demo('student_placements_pii',metadata=True)
print("\nReal data:\n",real_data.head())
model = CopulaGAN()
model = CopulaGAN(primary_key='student_id',anonymize_fields={'address': 'address' })
model.fit(real_data)
synth_data1 = model.sample(200)
print("\nSynth. set 1:\n",synth_data1.head())

model.save('my_model.pkl')                # this shows how to save the model
loaded = CopulaGAN.load('my_model.pkl')   # load the model, and 
synth_data2 = loaded.sample(200)          # get new set of synth. data
print("\nSynth. set 2\n:",synth_data2.head())
\end{lstlisting}\vspace{1ex}

The first example is a YouTube dataset with 2 tables, 2735 rows and 10 columns. The number of rows ranges from 83 to over 6 million, and some datasets have over 300 features. The code in question also loads one dataset (\texttt{'student\_placements\_pii'}), and shows how to use a GAN model based on \textcolor{index}{Gaussian copulas}\index{copula!Frank}. See output in Figure~\ref{fig:pictty}. The code is on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/GAN_copula_SDV.py}{here}. Some of the fields such as the address are anonymized rather than synthesized. This is performed by
 implicitly calling the Python library Faker, described \href{https://pypi.org/project/Faker/}{here}. 
 % does not work -- For an example about how it works, see \href{https://medium.com/@davide.gazze/sdv-generate-synthetic-data-using-gan-and-python-4c26a1e4b3c2}{here}. 
% xxxxx copulaGAN https://bobrupakroy.medium.com/gan-based-deep-learning-data-synthesizer-copulagan-a6376169b3ca
Also, you can choose the option \texttt{FAST\_ML} for the optimizer (the gradient descent algorithm), for faster processing but with lower
 accuracy. This is done using the instruction 
\texttt{TabularPreset(name='FAST\_ML', metadata=metadata)} as explained \href{https://medium.com/@davide.gazze/sdv-generate-synthetic-data-using-gan-and-python-4c26a1e4b3c2}{here}. 

SDV is a black-box, so to illustrate the various steps of GAN in details, I use an 
implementation based on the Keras library instead.  Keras is easier to use than \textcolor{index}{Tensorflow}\index{TensorFlow} 
[\href{https://en.wikipedia.org/wiki/TensorFlow}{Wiki}], though it requires Tensorflow to be installed on your system. Another black-box alternative, allowing you to implement GAN for tabular data synthetization with just 3 lines of code, is 
\textcolor{index}{TabGAN}\index{TabGAN (Python library)}, available \href{https://github.com/Diyago/GAN-for-tabular-data}{here}. See~\cite{insaf2020}  for a discussion. 
I had to install the most recent version of Numpy to get it to work.  More generally, installing these libraries may also require 
 the most recent version of pip, which you can get via the command \texttt{pip install --upgrade pip}. TabGAN uses 
\textcolor{index}{LightGBM}\index{LightGBM} [\href{https://en.wikipedia.org/wiki/LightGBM}{Wiki}], 
 a fast version of gradient boosting, and it is thus a bit faster than my step-by-step version in section~\ref{pyganvg}.

Another implementation very similar to mine  and illustrated on time series, is 
discussed \href{https://towardsdatascience.com/hands-on-generative-adversarial-networks-gan-for-signal-processing-with-python-ff5b8d78bd28}{here}. Mine includes a new version of \textcolor{index}{correlation matrix distance}\index{correlation matrix distance} to assess quality; 
see \href{https://www.researchgate.net/publication/4194743_Correlation_Matrix_Distance_a_Meaningful_Measure_for_Evaluation_of_Non-Stationary_MIMO_Channels}{here} and~\cite{pcdxzaw} for a standard definition.
It also uses a seed for every single source of randomness in the algorithm, allowing for full replicability, as well as other specific features. If you run the code in GPU, there
 might be additional sources of randomness that you can't control. You can run the code (say \texttt{mycode.py}) with the following command line in that case, if replicability is important for your application:

 \texttt{> CUDA\_VISIBLE\_DEVICES="" PYTHONHASHSEED=0 python mycode.py}

\noindent Finally, there are various libraries to assess the quality of the synthetized data. I use TableEvaluator in my code
 in section~\ref{pyganvg}, along
 with home-made metrics that are more useful to me. TableEvaluator is described \href{https://pypi.org/project/table-evaluator/}{here}. There is not much documentation about it, but you can check out the full source code on GitHub, \href{https://github.com/Baukebrenninkmeijer/table-evaluator/blob/master/table_evaluator/table_evaluator.py}{here}. That is how I found out that the output metric  called 
``Base Statistics" is a correlation distance between the real versus synthesized data, computed on various bivariate indicators as data points (mean, median, correlation between features and so on both for real and synthesized). 

Additional reading on the subject includes the book ``Synthetic Data for Deep Learning"~\cite{sddl21},
``Pros and Cons of GAN Evaluation Measures"~\cite{procons21}, 
``Survey on Synthetic Data Generation, Evaluation Methods and GANs"~\cite{gan18pobt},
and ``Are GANs Created Equal? A Large-Scale Study" by Google Brain~\cite{gbrain18}. 
See also Lei Xu's master thesis (MIT, 2017) available
 \href{https://dai.lids.mit.edu/wp-content/uploads/2020/02/Lei_SMThesis_neo.pdf}{here}
 and the related article on ArXiv~\cite{lw18}, 
 as well as \href{https://www.maskaravivek.com/post/ctgan-tabular-synthetic-data-generation/}{this article}.
 For the Keras models used in my implementation, see \href{https://www.activestate.com/resources/quick-reads/what-is-a-keras-model/}{here}.

\subsection{Synthesizing medical data with GAN}

Here I summarize my implementation of GAN applied to the Kaggle diabetes data set. First, I discuss the
\textcolor{index}{hyperparameters}\index{hyperparameter}, then the main steps in the methodology. The
 data is processed ``as is", without normalization or transformation. Possible transformations (preprocessing) are discussed in
section~\ref{gantrasd}. However, I removed all the observations with missing values, for better comparison with the copula method.
 Anyway, it makes sense to treat these observations separately, as a different segment, by re-running GAN on them only. Indeed, it produces better results when they are separated from the complete observations. After removing observations with missing values, we are left with 392 rows.

The dataset has 9 features. One of them called \texttt{Outcome} is the response: it indicates whether or not the patient had cancer. Thus the problem is predicting -- based on the remaining 8 features -- the chance of getting cancer. It is a supervised classification problem with two groups:
 cancer versus non-cancer. 

The first part of the code (section~\ref{cfpadgfvew}) imports the libraries, reads the data and removes observations with missing values, and then 
performs the classification with the \textcolor{index}{random forest} algorithm [\href{https://en.wikipedia.org/wiki/Random_forest}{Wiki}]. 
It also defines some global variables such as the \textcolor{index}{learning rate}\index{learning rate} [\href{https://en.wikipedia.org/wiki/Learning_rate}{Wiki}] in the Adam gradient descent, and \textcolor{index}{\texttt{seed}}\index{seed (random number generator)} which allows for replicability by using the same value in each run. 
 The quality of the classification is displayed on the screen, as the \texttt{Base Accuracy} metric. 

The last part of the code (section~\ref{lasravc}) evaluates the quality of the synthetic data. It also performs the classification on the synthetic data, for comparison with the results obtained 
on the real data. It would be interesting to augment the real data by adding the synthetic data into it, and see if we get more robust predictions. 
The augmented data is not expected to increase accuracy; instead it is expected to increase robustness and reduce  \textcolor{index}{overfitting}\index{overfitting}.

The dataset \texttt{diabetes.csv} is on my GitHub directory, \href{https://github.com/VincentGranville/Main/blob/main/diabetes.csv}{here}. 
The original can be found on Kaggle, \href{https://www.kaggle.com/uciml/pima-indians-diabetes-database}{here}. It should be
noted that all the features are treated as continuous, even the binary \texttt{Outcome}. Thus the number of pregnancies (an integer) or the ``outcome" generated by GAN
 are real numbers, that must be mapped onto integers to make sense. Other implementations such as copulas or copula-based GANs do not have
 this limitation. An alternative is to use one GAN for \texttt{Outcome==0}, and another one for \texttt{Outcome==1}. Also, categorical features (absent here except for Outcome) can be replaced
 by \textcolor{index}{dummy variables}\index{dummy variable} [\href{https://en.wikipedia.org/wiki/Dummy_variable_(statistics)}{Wiki}]. 
 

\subsubsection{Hyperparameters}\label{fgdloo}

There is a surprisingly large number of hyperparameters that can be fine-tuned. This is one of the reasons why these systems take a lot of time
 to train and optimize. In addition, the gradient descent present in all GANs (with its own parameters), is the bottleneck. Self-identification of good parameters by the algorithm
 itself -- and possibly adjusted over time --  is one way to at least automate the process. But it can lead to overfitting. The best
 solution is to use standard hyperparameters that work well in many contexts, without trying to over-adjust. 

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{history2.png}  
\caption{Loss function (in orange) for $10^4$ successive epochs; enhanced GAN on the right plot}
\label{fig:pictty2xsvv}
\end{figure}

All these parameters are set and used in the GAN part of the code, in section~\ref{xcxxsdzs}. The only exception is the learning rate parameter.  \vspace{1ex}
\begin{itemize}
\item Epochs: One iteration of GAN consists of processing the full data set: this is done using a number of small samples (batches), one at a time.
  The number of iterations is the number of \textcolor{index}{epochs}\index{epoch}\index{neural network!epoch}, set to \texttt{n\_epochs=10000} (a typical value) in the \texttt{train} function used to train the GAN.
\item Batch size: see Epochs. Here it is set to \texttt{n\_batch=128} in the \texttt{train} function. Half of the batch is used to sample from the real data, and the other half to generate latent data.
\item Latent data: here \textcolor{index}{latent variables}\index{latent variables} [\href{https://en.wikipedia.org/wiki/Latent_and_observable_variables}{Wiki}] are univariate random Gaussian deviates with zero mean and unit variance; typically their number matches the number of features. Their role is similar to the Gaussian deviates in the copula method. However, using a uniform distribution on $[-1, 1]$ is worth exploring as many GAN functions
 (for instance ReLU) return values between 0 and 1. Such restrictions are also used in Fourier regression in section~\ref{orthofou}.
\item Activation function: 
 in neural network, the \textcolor{index}{activation function}\index{activation function}\index{neural network!activation function} [\href{https://en.wikipedia.org/wiki/Activation_function}{Wiki}] decides whether a \textcolor{index}{neuron}\index{neural network!neuron} should be activated or not. Classic examples used in the code are \textcolor{index}{ReLU}\index{ReLU function} [\href{https://en.wikipedia.org/wiki/Rectifier_(neural_networks)}{Wiki}] and \textcolor{index}{sigmoid}\index{sigmoid function} [\href{https://en.wikipedia.org/wiki/Sigmoid_function}{Wiki}].  
\item Kernel initializer: defines the way to set the initial random weights of Keras layers. See documentation
 \href{https://keras.io/api/layers/initializers/}{here}.
\item Number of layers: the neural networks  (both the generator and discriminator) use 3 layers. 
A layer is added via the instruction \texttt{model.add}, with a number of options: activation function, kernel initializer, and so on. 
See 
\texttt{define\_generator}, \texttt{define\_discriminator}
 and \texttt{define\_gan} (the combination of both) in the code. With 3 layers, we are dealing with a
 \textcolor{index}{deep neural network}\index{deep neural network}.
\item Learning rate: attached to the gradient descent. I tried $0.01$ and $0.001$, and settled for the latter. Small values result in quite chaotic, faster behavior (at the beginning) and somewhat reduced accuracy. Large values result in slow steady progress but you can end up stuck in a bad local optimum. Think
 of it as the cooling schedule in a simulated annealing algorithm. 
\item Gradient descent method: I use \texttt{Adam}. The alternative \texttt{SGD} 
(\textcolor{index}{stochastic gradient descent}\index{stochastic gradient descent} [\href{https://en.wikipedia.org/wiki/Stochastic_gradient_descent}{Wiki}]) did no do well here, but does well in computer vision.
\item Loss function: in gradient descent or any optimization problem, the \textcolor{index}{loss function}\index{loss function} [\href{https://en.wikipedia.org/wiki/Loss_function}{Wiki}] specifies the type of distance to the optimum vector, used for minimization. Think of it as a regression problem solved by least squares -- the loss function being quadratic in this case. It is sometimes called the error function. 
\item Accuracy: calculates how often predictions equal labels in the context of classification. In this context, classification means assigning
 an observation to either real (truly real or excellent synthetization) or fake (poor synthetic data not close to the reality).
See Keras documentation \href{https://keras.io/api/metrics/accuracy_metrics/}{here}. There are various options to choose from, to measure accuracy.
\item Architecture: the type of neural network. In this example, set to \texttt{Sequential}. 
\end{itemize}



\subsubsection{GAN: Main steps}

The steps in this section corresponds to the actual GAN procedure in section~\ref{maindqq}. It does not include the pre-processing step: checking how good the real training set is at predicting cancer in a 
\textcolor{index}{cross-validation}\index{cross-validation}
  framework (section~\ref{cfpadgfvew}). It does not cover the post-processing step either: 
 GAN evaluation and how good the synthesized training set is at predicting cancer in the same cross-validation framework. This part of the code is covered 
 in section~\ref{lasravc}. It is assumed that all the GAN models have been created and compiled, with the hyperparameters discussed
in section~\ref{fgdloo}. So this section only covers the \texttt{train} function used to train GAN.  That said, it is the most important part of the code.
%\pagebreak

\begin{figure}[H]
\centering
\includegraphics[width=0.89\textwidth]{excelgan.png}  
\caption{Summary statistics, medical dataset (synth 1 and 2 correspond to GAN)}
\label{fig:pictty2xkuu}
\end{figure}

\noindent The following steps are repeated for each epoch in the \texttt{train} function: \vspace{1ex}\nopagebreak

\begin{itemize}\nopagebreak
\item Step 1: update the discriminator: get a new sample (half batch) from the real data and assign these points to \texttt{label=1}; get a new latent data sample (half batch, random Gaussian vectors) to generate fake data and assign these points to \texttt{label=0}. The fake sampling function also maps the latent data  into the space of the real data via the instruction \texttt{X=generator.predict(x\_input)}. Here \texttt{x\_input} represents the latent data, and
 \texttt{X} the mapped version. The \texttt{train\_on\_batch} function (one call for the real sample, one call for the fake one) also returns the losses for each  data label (fake / real). Note that the discriminator is set to ``non-trainable". 
\item Step 2: update the generator: get a full sample (full batch) of latent data, assigned this time to \texttt{label=1} to train the generator. 
Training aims at minimizing over time (on average) the loss function
 \texttt{g\_loss} (an average of the \texttt{d\_loss} values returned by the discriminator) until an equilibrium is reached: a local minimum in all likelihood. 
Note that \texttt{g\_loss} oscillates over time in order to not get stuck too quickly in a local minimum. The steepest gradient descent can result in this problem. So we use the \textcolor{index}{Adam gradient descent}\index{Adam gradient descent} (adaptive moment estimation) instead to avoid this issue. 
\item Step 3:  If the epoch iteration is a multiple of 200, output the summary statistics to show progress, in particular how 
 \texttt{g\_loss} is decreasing over time, on average. 
\end{itemize}\vspace{1ex}

\noindent Note that the output layer of the discriminator is activated by the sigmoid function  to discriminate between real and fake samples.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{correl3.png}  
\caption{Correlation matrix, real vs synthetic: GAN (synth 2) and copula-based}
\label{fig:pictty2xsds}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{diab4b.png}  
\caption{Copulas superior to GANs (synth 1, 2) to capture correlations in real data}
\label{fig:pictty2xs}
\end{figure}

\subsection{Initial results}

The first synthetized data using GAN on the medical dataset was disappointing. While GAN was able to replicate the mean, variance,
 and percentiles attached to each of the 9 features, it failed at replicating the correlation structure. In addition, GAN was very sensitive to the \textcolor{index}{seed}\index{seed (random number generator)} (denoted as \texttt{seed} in the code). Also, even for a single gradient path started with a specific seed,   the oscillations in the loss function over successive epochs, and thus the quality of the synthetized data, were still volatile even after 5000 epochs. I present the initial results here. It originates from a piece of code widely distributed over the Internet. 
 I show in section~\ref{enhgfd} how to significantly improve the algorithm using front-end modifications, with little changes to the hyperparameters. 

The left plot in Figure~\ref{fig:pictty2xsvv} shows the volatility in the loss function -- the orange curve. In the same figure, the contrast between the left and right plot shows the huge impact of the seed on the final results. In Figures~\ref{fig:pictty2xkuu}, 
 \ref{fig:pictty2xsds} and \ref{fig:pictty2xs}, ``synth 1" represents the initial version of GAN, while ``synth 2" corresponds to the enhanced version. Even the enhanced version is inferior to copulas to reconstruct the correlation structure. However, GAN, even with the initial version, does a decent job at reconstructing the statistical summaries of most features (mean, variance, percentiles p$_{.25}$ and p$_{.75}$).
 More results are in the spreadsheet \texttt{diabetes\_synthetic.xlsx}, available
 \href{https://github.com/VincentGranville/Main/blob/main/diabetes_synthetic.xlsx}{here} on GitHub.


\subsection{Fine-tuning the hyperparameters}

You can use a back-end or front-end approach to fine-tune the hyperparameters and other GAN components, or a combination of both. The back-end strategy consists of
 modifying components that are buried more deeply in the architecture, such as the loss function, the number and type of layers,
the type of neural network (sequential here), the gradient descent method (Adam here), the size of the batches and the ratio when splitting batches into real versus fake, the dimension and type of random deviates for the latent variables (Gaussian here, with dimension about the same as the number of features), 
the Keras metric used in the compile step, the learning rate, the type of activation function, and so on. I kept the original settings 
 as found \href{https://towardsdatascience.com/hands-on-generative-adversarial-networks-gan-for-signal-processing-with-python-ff5b8d78bd28}{here} and elsewhere, except for the learning rate and number of epochs.

Instead, I focused on front-end modifications. In particular, the enhanced version of my implementation produces a full synthetic
 dataset at each epoch, and computes the fit with the real data each time. It barely increases the amount of time needed to run a full GAN cycle.
The fit is measured as the correlation matrix distance between the
 real and synthesized data. This
 metric is always between 0 and 1, with 0 being best. The goal was to replicate the correlation structure in the real dataset, thus the choice of this particular metric.  The final synthetic data is the one obtained at the epoch where the correlation matrix distance is best (minimum).
In addition, it must not come from an early epoch. For instance if the number of epochs id \num{10000}, I check the
correlation matrix distance starting at epoch 7500.
 

In addition,  the \textcolor{index}{seed}\index{seed (random number generator)} is an integral and important part of my algorithm. I made the 
results replicable, so if you run the program twice with the same seed, you will get the same results, unlike in most other
 implementations.  Then, I test various seeds and pick up the one that produces the best results.

% xxx compare with svd as well
              
\subsection{Enhanced GAN: methodology and results}\label{enhgfd}

To achieve better results, I explained how to process missing data separately, or applying a different GAN to specific segments of your
 population. Or a different GAN for each group of features resulting from \textcolor{index}{feature clustering}\index{feature clustering}.

 Transforming or normalizing your data (for instance, decorrelate) may also lead to better synthetization. For instance, say your real
 data $X$ is an $n\times m$ array with $n$ rows -- the observations -- and $m$ columns -- the features. Assume that all the features have been normalized, thus having zero mean and unit variance. You can transform $X$ to obtain $Y=XW$, where
 $\text{Cov}[Y]= W^T C_X W$ is a $m \times m$ diagonal matrix, $^T$ denotes the transposition operator, $C_X=\text{Cov}[X]$ and $W$ has size $m\times m$. The transformed data $Y$ consists of uncorrelated features. To achieve this goal take $W=C^{-1/2}$. There are multiple possible square roots, and this transformation is discussed in section~\ref{fcv34}. You can use an iterative algorithm to compute the 
 \textcolor{index}{matrix square root}\index{square root (matrix)}. Then synthesize $Y$ (instead of $X$) and let $Z$ be the resulting data. Your final (un-transformed) synthesized data  is $X'=ZW^{-1}$, with the exact same correlation matrix as your real data $X$.
 This procedure is known as \textcolor{index}{decorrelation}\index{decorrelation} [\href{https://en.wikipedia.org/wiki/Decorrelation}{Wiki}], followed by recorrelation. 

However, the easiest solution is as follows. The GAN algorithm is very sensitive to the 
\textcolor{index}{seed}\index{seed (random number generator)}, which determines the initial configuration. In Figure~\ref{fig:pictty2xsvv}, I  compare two trajectories of the gradient descent
 based on two different seeds. Clearly, \texttt{seed=103} does a much better job than \texttt{seed=102}, attaining 
 and staying in regions of lower loss much faster than \texttt{seed=102}. Thus the solution consists in trying different seeds. Not only that, but even with a same seed, the iterates (called \textcolor{index}{epochs}\index{epoch}) oscillate wildly. In short, you could get
 a much better synthetization if you stop after 9800 epochs rather than (say) $\num{10000}$. The problem is
 further compounded by the fact that the \textcolor{index}{loss function}\index{loss function} may achieve an optimization goal different from what you are looking for.

I address these issues in my enhanced version of GAN. To use it, set \texttt{mode='Enhanced'} in the Python code
 in section~\ref{maindqq}. Given one instance of GAN corresponding to a specific seed, it will retain the best synthetic data (in other words, the best model produced by Keras) based on
 a distance function of your choice, rather than the one obtained at the last epoch and very dependent on the loss function. In my code, I was interested in synthetic data good a mimicking the correlation structure present in the real dataset, so I wrote my own function
\texttt{gan\_distance}, measuring the correlation distance between real and synthetic data at each epoch. It is based on
 the \textcolor{index}{correlation matrix distance}\index{correlation matrix distance}. Of course, you can modify that function to meet your own needs.

The enhancement techniques described so far are front-end: they do not modify the internal components of GAN. They are also easy to understand and implement, contributing to  
 explainable AI. But you can also dig into the GAN black-box internals and modify some of the low-level components. This is facilitated to some extend by the Keras library, which offers some tools, for instance to customize the loss function. These back-end enhancements require more knowledge about how GANs work. You can write your own
 function \texttt{custom\_distance} and have Keras ``digest" it by choosing the 
 option \texttt{model.compile(metrics=[custom\_distance])} in your GAN model. See additional documentation 
 \href{https://www.tensorflow.org/guide/keras/save_and_serialize}{here} and 
\href{https://datascience.stackexchange.com/questions/116811/define-a-custom-distance-between-classes-in-keras}{here}. 
Another possibility is to use an adaptive \textcolor{index}{learning rate}\index{learning rate}: see how to do it \href{https://towardsdatascience.com/learning-rate-schedule-in-practice-an-example-with-keras-and-tensorflow-2-0-2f48b2888a0c}{here}.
Finally, being able  with 
\textcolor{index}{reinforcement learning}\index{reinforcement learning} [\href{https://en.wikipedia.org/wiki/Reinforcement_learning}{Wiki}] to reward configurations minimizing your own front-end distance function (rather than the 
 default loss function) would be helpful, see~\cite{reinflr21}. By configuration, I mean the updated model and its set of 
updated weights obtained at the end of each epoch.


\noindent To summarize, the enhanced version of my implementation has the following upgrades: \vspace{1ex}

\begin{itemize}
\item Replicable results
\item Missing data treated separately
\item Run multiple versions each with a different seed, use the best version
\item Binary data: 0 and 1 processed with two different GANs (available in future version)
\item Stop when your customized distance between real and synthetic data is minimum 
\end{itemize}\vspace{1ex}

\noindent The last feature requires  \texttt{mode='Enhanced'} in the Python code. The increased performance of the enhanced version can be seen in the Figure~\ref{fig:pictty2xsvv}, comparing standard (left) with enhanced mode (right). Also, the GAN synthetization
 in Figure~\ref{fig:pictty2xsuu} (right plot) uses the enhanced mode. It features an example on tabular data where GAN outperforms copulas.  An additional upgrade is to blend copula methods with GAN. This is done in the SDV library: see code in
 section~\ref{xcvcx}. Finally, applying GAN on decorrelated data (followed by a re-correlation step) as discussed at the
 beginning of this section, is guaranteed to
 preserve the exact correlation structure in the real data. This operation is fully reversible.

\subsection{Feature clustering via hierarchical clustering or connected components}\label{dk6fb}

For those interested in the \textcolor{index}{feature clustering}\index{feature clustering} algorithm,
 the topic is well covered in the literature, see for instance~\cite{fcnice}.  Here I provide a simple method, consisting of finding the \textcolor{index}{connected components} 
\index{graph!connected components}\index{connected components} [\href{https://en.wikipedia.org/wiki/Component_(graph_theory)}{Wiki}] in the correlation matrix. The main program is a slight adaptation of the version used to detect
 connected components in nearest neighbor graphs~\cite{vgelsevier}. 
 Two features are connected if their correlation is above some parameter named \texttt{threshold} in the Python code.  A cluster of features is just a connected component of the 
 \textcolor{index}{undirected graph}\index{graph!undirected} in question.   

I applied the method to the medical data set (the real data), using the correlation matrix at the bottom
 of Figure~\ref{fig:pictty2xsds} with \texttt{threshold=0.4}. Five feature clusters are
 detected: $\{0, 7\}, \{1, 4, 8\}, \{3, 5\}, \{2\}, \{6\}$. For instance feature 0 corresponds to pregnancies, 1 to glucose, 2 to blood pressure, and so on. It means, thanks to the low correlations between these 5 clusters, that a separate copula or GAN can be applied to each of them, thus splitting a 9D problem into a number a small-dimensional problems, each with a dimension no larger than 3. The Python code 
\texttt{featureClustering.py} is also
 on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/featureClustering.py}{here}. For more details, see section~\ref{fcvgfd}.

Feature clustering via the correlation matrix is scale-invariant. You can also use this methodology for traditional clustering, by swapping features and observations. For instance, with 
\textcolor{index}{wide data}\index{wide data}, that is, a small number of observations (say less than $\num{10000}$) with a large number of features, as in clinical trials. 

I provide two implementations of the feature clustering procedure. The first one uses 
\textcolor{index}{hierarchical clustering}\index{hierarchical clustering} with the Scipy library. In 
 Figure~\ref{fig:picbhggg2xs}, $\rho$ is the correlation, the X-axis is the feature label. The code is available on GitHub,
 \href{https://github.com/VincentGranville/Main/blob/main/featureClusteringScipy.py}{here}. The second one is based on the connected components. The code  
 is also available \href{https://github.com/VincentGranville/Main/blob/main/featureClustering.py}{here}. \vspace{1ex}

\begin{lstlisting}
# feature correlation with hierarchical clustering and dendograms
# featureClusteringScipy.py

import matplotlib.pyplot as plt
import numpy as np
import scipy.spatial.distance as ssd
import scipy.cluster.hierarchy as hcluster

correlMatrix = [
      [1.0000,0.1983,0.2134,0.0932,0.0790,-0.0253,0.0076,0.6796,0.2566],
      [0.1983,1.0000,0.2100,0.1989,0.5812,0.2095,0.1402,0.3436,0.5157],
      [0.2134,0.2100,1.0000,0.2326,0.0985,0.3044,-0.0160,0.3000,0.1927],
      [0.0932,0.1989,0.2326,1.0000,0.1822,0.6644,0.1605,0.1678,0.2559],
      [0.0790,0.5812,0.0985,0.1822,1.0000,0.2264,0.1359,0.2171,0.3014],
      [-0.0253,0.2095,0.3044,0.6644,0.2264,1.0000,0.1588,0.0698,0.2701],
      [0.0076,0.1402,-0.0160,0.1605,0.1359,0.1588,1.0000,0.0850,0.2093],
      [0.6796,0.3436,0.3000,0.1678,0.2171,0.0698,0.0850,1.0000,0.3508],
      [0.2566,0.5157,0.1927,0.2559,0.3014,0.2701,0.2093,0.3508,1.0000]]

simMatrix = correlMatrix - np.identity(len(correlMatrix)) 
distVec = ssd.squareform(simMatrix)
linkage = hcluster.linkage(1 - distVec)

plt.figure()
axes = plt.axes()
axes.tick_params(axis='both', which='major', labelsize=8)
for axis in ['top','bottom','left','right']:
    axes.spines[axis].set_linewidth(0.5) 
with plt.rc_context({'lines.linewidth': 0.5}):
    dendro  = hcluster.dendrogram(linkage,leaf_font_size=8)
plt.show()
\end{lstlisting}

%image xxx99 ... xxxx

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{hcluster.png}  
\caption{Feature clustering using Scipy; Y-axis is $1-|\rho|$}
\label{fig:picbhggg2xs}
\end{figure}

\noindent The following implementation is based on connected components. It provides the same results as the
 previous one based on hierarchical clustering and dendograms. Figure~\ref{fig:picbhggg2xs} illustrates the
 hierarchical clustering version. \vspace{1ex}

\begin{lstlisting}
# feature correlation with connected components
# featureClustering.py

correlMatrix = [
      [1.0000,0.1983,0.2134,0.0932,0.0790,-0.0253,0.0076,0.6796,0.2566],
      [0.1983,1.0000,0.2100,0.1989,0.5812,0.2095,0.1402,0.3436,0.5157],
      [0.2134,0.2100,1.0000,0.2326,0.0985,0.3044,-0.0160,0.3000,0.1927],
      [0.0932,0.1989,0.2326,1.0000,0.1822,0.6644,0.1605,0.1678,0.2559],
      [0.0790,0.5812,0.0985,0.1822,1.0000,0.2264,0.1359,0.2171,0.3014],
      [-0.0253,0.2095,0.3044,0.6644,0.2264,1.0000,0.1588,0.0698,0.2701],
      [0.0076,0.1402,-0.0160,0.1605,0.1359,0.1588,1.0000,0.0850,0.2093],
      [0.6796,0.3436,0.3000,0.1678,0.2171,0.0698,0.0850,1.0000,0.3508],
      [0.2566,0.5157,0.1927,0.2559,0.3014,0.2701,0.2093,0.3508,1.0000]]

dim = len(correlMatrix)
threshold = 0.4  # two features with |correl|>threshold are connected 
pairs = {}

for i in range(dim):
    for j in range(i+1,dim):
        dist = abs(correlMatrix[i][j])
        if dist > threshold:
            pairs[(i,j)] = abs(correlMatrix[i][j])
            pairs[(j,i)] = abs(correlMatrix[i][j])

# connected components algo to detect feature clusters on feature pairs

#---
# PART 1: Initialization. 

point=[]
NNIdx={}
idxHash={}

n=0
for key in pairs:
    idx  = key[0]
    idx2 = key[1]
    if idx in idxHash:
        idxHash[idx]=idxHash[idx]+1
    else:
        idxHash[idx]=1
    point.append(idx)
    NNIdx[idx]=idx2
    n=n+1


hash={}
for i in range(n):
    idx=point[i]
    if idx in NNIdx:
        substring="~"+str(NNIdx[idx])
    string="" 
    if idx in hash:
        string=str(hash[idx])
    if substring not in string: 
        if idx in hash:
            hash[idx]=hash[idx]+substring 
        else:
            hash[idx]=substring    
    substring="~"+str(idx)
    if NNIdx[idx] in hash: 
        string=hash[NNIdx[idx]]
    if substring not in string: 
        if NNIdx[idx] in hash:
            hash[NNIdx[idx]]=hash[NNIdx[idx]]+substring 
        else:
            hash[NNIdx[idx]]=substring 

#---
# PART 2: Find the connected components 

i=0;
status={}
stack={}
onStack={}
cliqueHash={}

while i<n:

    while (i<n and point[i] in status and status[point[i]]==-1):    
        # point[i] already assigned to a clique, move to next point
        i=i+1

    nstack=1
    if i<n:
        idx=point[i]
        stack[0]=idx;     # initialize the point stack, by adding $idx 
        onStack[idx]=1;
        size=1    # size of the stack at any given time

        while nstack>0:    
            idx=stack[nstack-1]
            if (idx not in status) or status[idx] != -1: 
                status[idx]=-1    # idx considered processed
                if i<n:    
                    if point[i] in cliqueHash:
                        cliqueHash[point[i]]=cliqueHash[point[i]]+"~"+str(idx)
                    else: 
                        cliqueHash[point[i]]="~"+str(idx)
                nstack=nstack-1 
                aux=hash[idx].split("~")
                aux.pop(0)    # remove first (empty) element of aux
                for idx2 in aux:
                    # loop over all points that have point idx as nearest neighbor
                    idx2=int(idx2)
                    if idx2 not in status or status[idx2] != -1:     
                        # add point idx2 on the stack if it is not there yet
                        if idx2 not in onStack: 
                            stack[nstack]=idx2
                            nstack=nstack+1
                        onStack[idx2]=1

#---
# PART 3: Save results.

clusterID = 1
for clique in cliqueHash:
    cluster = cliqueHash[clique] 
    cluster = cluster.replace('~', ' ')
    print("Feature Cluster number %2d: features %s"  %(clusterID, cluster))
    clusterID += 1
clusteredFeature = {}
for feature in range(dim):
    for clique in cliqueHash:
        if str(feature) in cliqueHash[clique]: 
            clusteredFeature[feature] = True
for feature in range(dim):
    if feature not in clusteredFeature:
        cluster = " "+str(feature)
        print("Feature Cluster number %2d: features %s"  %(clusterID, cluster))
        clusterID += 1        
\end{lstlisting}

\section{Comparing GANs with the copula method}

On the medical data set, the copula method performs better as illustrated
 in Figures~\ref{fig:pictty2xkuu} and~\ref{fig:pictty2xsds}, and it is a lot faster. Unlike my copula method
 in section~\ref{sdsvc}, many GAN implementations do not
 produce replicable results. However, this problem is solved in my implementation in section~\ref{pyganvg}. Also GAN is very sensitive to
 the initial configuration (the seed), and oscillations from one epoch to the next one are rather large, even after $\num{10000}$ epochs.
 This latter issue may actually be an advantage: trying different seeds and/or using a stopping rule based on the quality of the synthetisation
 in any given epoch, leads to substantial improvements. 


GAN has  many hyperparameters that can be fine-tuned, even the loss function. This can lead to overfitting and makes the method less suitable as a black-box, compared to the parameter-free copula technique. Copulas are also less sensitive to outliers and small modifications of the real data, at least in this context (tabular data) and when using a parameter-free method based on empirical quantiles. But unlike GAN, they may not be able to generate synthetic data outside the range of observations. 
There are solutions to this problem: noise injection, or using parametric rather than empirical quantiles. Parametric quantiles are obtained by fitting a feature or pair of features to a known probability distribution such as a mixture of Gaussians (GMM). It can also lead to overfitting.
A nice feature of copulas is that it easily works with a mix of categorical, ordinal and continuous features.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{circle.png}  
\caption{Real data (left), copula (middle) and GAN synthetization (right)}
\label{fig:pictty2xsuu}
\end{figure}

Another issue with GAN, on this particular dataset, is the fact that the feature correlations in the synthetic data are exaggerated. This is true in the early epochs, and this phenomenon is attenuated in the last epochs, but it is still strongly noticeable, for instance on the left plot in Figure~\ref{fig:pictty2xsvv}.


\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{gdist.png}  
\caption{Loss function (orange) and distance (grey), circle dataset}
\label{fig:pictty2xsvvdd}
\end{figure}

The superiority of copulas, as seen in Figure~\ref{fig:pictty2xkuu} and~\ref{fig:pictty2xsds}, is due to the goal being achieved here: mimicking the correlation structure in the real data. Indeed, copulas are perfect at that. But what if the goal is different, or if the correlation matrix fails to capture the patterns in the real data? I tried with an artificial example, where most of the feature correlations are zero, but with strong non-linear dependencies instead. The dataset in question has 9 features; the last one is also a binary response for classification purposes, as in
 the medical dataset. The first two features represent points lying on two concentric circles: see Figure~\ref{fig:pictty2xsuu}. It is trivial to synthesize the whole 9-D dataset with any method. However, I use it for illustration purposes as some real-life datasets  have 
 similar undetected or invisible patterns buried in very high dimensions.  



In this case, it turns out that GAN does a better job. Both GAN and copula generate the correct correlation structure, though copula fails to recognize the circular patterns. Also, GAN iterations are very stable in this example, compared to the medical dataset.  
The copula method used in this example is based on two copulas, one for each group: the two concentric circles correspond to the two groups. This is not the case for GAN which is  somewhat at a disadvantage,
 because of using the same model for both groups. 

I computed the correlation between $X_1^2 + X_2^2$ (based on the first two features) and the binary response $X_9$. In the real data consisting of 400 observations, by design it is exactly 1 even though the correlation between $X_1$ and $X_2$ is zero. 
The copula method yields $0.014$, and GAN yields a dramatic improvement with a value of $0.723$, much closer to 1. Even when using a separate copula
 for each group, copula yields a much better correlation of $0.676$ but still worse than GAN, even though GAN  is based on  a single model for both groups. Another interesting correlation is between $X_1$ and the response $X_9$. In the real dataset,
 the value is $0.067$. The copula yields 
$0.128$, and GAN yields $0.070$.  So GAN is slightly better. 
Given the way the real dataset was built, the true value would be zero if it had an infinite number
of observations.  More correlations are displayed in Table~\ref{tabephuy}, and the full list is easy to obtain from the 
\href{https://github.com/VincentGranville/Main/blob/main/circle8D.xlsx}{Excel spreadsheet}.


\renewcommand{\arraystretch}{1.2} %%%
\renewcommand{\arraystretch}{1.2} %%%
\begin{table}[H]
%\small
\[
\begin{array}{crrr}
\hline
 \text{Feature pair} &  \text{Real data} & \text{Copula synth.} & \text{GAN synth.} \\
\hline
\hline
X_9, X_1^2+X_2^2  & 1.0000 & \text{\textcolor{red}{$0.0136$}} & 0.7235 \\
X_1, X_9 &  0.0662&	0.1281 &	0.0700\\
X_1, X_2 &  0.0641 &	0.1069 &	0.0660\\
X_1, X_3 & 1.0000 &	1.0000 &	0.9976 \\
 X_2, X_3 & 0.0641 &	0.1069 &	0.0495\\
 X_1, X_1^2+X_2^2 & 0.0662 &	0.1906 &	0.1186\\
 X_1, X_5 & -0.0278 & -0.0278 &	-0.0047\\

\hline
\end{array}
\]
\caption{\label{tabephuy} Correlations on 9D circle dataset: real vs copula and GAN}
\end{table}


I stopped GAN at epoch 7727 which achieves the minimum overall 
\textcolor{index}{correlation matrix distance}\index{correlation matrix distance} of $0.017$, a very good performance since the best possible value is zero, and the worst case is one. Epoch 10001 (the last one) yields  
$0.036$ and epoch 9998 yields $0.057$: it shows the benefit of using the enhanced version of GAN to capture the $0.017$ minimum.
The evolution of this GAN system is pictured in Figure~\ref{fig:pictty2xsvvdd}. The labels \texttt{d\_history}, 
\texttt{g\_history}, \texttt{g\_dist} are as in the Python code with ``d" standing for the discriminator model, and ``g" standing for the generator model in GAN (the one that creates the synthetization).
The details are in my spreadsheet \texttt{circle8d.xlsx} on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/circle8D.xlsx}{here}.

Note that even if GAN is not as good as copulas at replicating the correlation structure in the medical dataset, it is possible to first 
\textcolor{index}{decorrelate}\index{decorrelatation} the data, then apply GAN (or any method!) and then re-correlate, as explained in section~\ref{enhgfd}. With this transformation, any synthetization algorithm that
 generates uncorrelated data will perfectly replicate the correlations found in the real data, after the re-correlation step.


\subsection{Conclusion: getting the best out of copulas and GAN}\label{gantrasd}

I already discussed how to improve GANs in the context of tabular data, for instance by applying GAN to the 
\textcolor{index}{decorrelated}\index{decorrelation} real data, or using your own distance metric or 
\textcolor{index}{loss function}\index{loss function}, fine-tuning the \textcolor{index}{learning rate}\index{learning rate}, or using a faster version of the gradient descent such as \textcolor{index}{LightGBM}\index{LightGBM}. Some improvements apply both to GANs and copulas: using a separate GAN model or copula for specific groups of observations, or for specific groups of features based on 
\textcolor{index}{feature clustering}\index{feature clustering}. Or testing 10 different GANs (using different seeds) or 10 different copulas: the latter is a lot faster.
Some implementations blend GAN and copulas. See for instance the CopulaGAN module in the \textcolor{index}{SVD}\index{SVD (Python library)} library. 

To improve copulas methods specifically, you can replace the 
parameter-free \textcolor{index}{empirical quantiles}\index{empirical quantiles} by quantiles from a parametric family of distributions, fit to the data, with parameters estimated on the real data. For instance, a \textcolor{index}{Gaussian mixture model}\index{GMM (Gaussian mixture model)} (GMM) for features with multimodal distribution. Then generate synthetic data using an iterative process 
 based on the 
 \textcolor{index}{Hellinger distance}\index{Hellinger distance}. This distance measures the fit between the real data and the current synthesized version. And proceed iteratively as in GAN: successive iterations are obtained following the gradient path of the Hellinger distance, viewed as as multivariate function of the parameters of your model. In essence, this is very similar to the GAN approach, and can be done with or without neural networks.

In other words, you can improve GANs by integrating them with copulas and follow a gradient path that leads to an optimum of some discriminating function. And you can improve copulas by using a gradient descent algorithm (or stepwise procedure focusing on 2 parameters at a time) to navigate the parameter space until you optimize the Hellinger distance.
 In the end, the two techniques with the respective improvements may not be that different, especially when using multivariate parametric distributions spanning across multiple features, for the copula. 


%---
\section{Data synthetization explained in one picture}

Figure~\ref{fig:digdig} summarizing many of the elements discussed in this book, is organized as follows. Dashed blue lines are associated to GANs 
(\textcolor{index}{generative adversarial networks}\index{GAN (generative adversarial networks)}\index{GAN (generative adversarial networks)}), where the goal is to produce a sequence of synthetic datasets that get better and better at mimicking the structure present in the real data, over successive iterations. The diagram features 5 such iterations, with the synthetized datasets denoted as $S_1,S_2,\dots,S_5$. Typically, GANs follow the gradient of $h$ to reach an optimum configuration $q$ that can not be classified as  
non-real anymore. Synthetic data that gets closer to the real data gets rewarded in this 
\textcolor{index}{reinforcement learning}\index{reinforcement learning} technique. Like any
 simulation-intensive method, training the neural network can be time-consuming, and this black-box approach
 may lack \textcolor{index}{explainability}\index{explainable AI}.

Dashed pink lines are associated to modeling techniques (generative model, GMM) where synthetic data is obtained by simulating the underlying model using the parameter values estimated on the real data, that is, $q_k = p$ for all $k$. In case of GMM (\textcolor{index}{Gaussian mixture models}\index{GMM (Gaussian mixture model)}), the parameters are the cluster centers, the covariance matrix attached to each cluster, and the proportions of the mixture. For stationary time series, the parameter is typically the autocorrelation function (ACF). In some applications including when using \textcolor{index}{copulas}\index{copula}, the EDPD (empirical probability density function) is used instead. For GANs, the $q_k$'s are the weights attached to neuron connections.

The goal is to mimic the structure in the real data, not the real data itself. The structure is represented by a parametric configuration denoted as $p$ in the real data. I use the notation $p_1,\dots,p_5$ for the structures found in the 5 synthetic data sets. The quality $h_k$ of the synthetic data set $k$ is the distance between $p_k$ and $p$, based on the 
\textcolor{index}{Hellinger metric}\index{Hellinger distance} or some discriminating function in the case of GAN. It is assumed that the real data has been normalized (transformed) before synthesizing. ``Estim. param." stands for estimated parameters in the diagram, though sometimes the parameters can be a function or matrix rather than a set of elements.




%%%\noindent\rule{1.00\textwidth}{.4pt} 
\begin{figure}[H]
\centering
%\begin{center}
\resizebox{14cm}{!}{ %  the ! is to keep x and y ratios proportional
\begin{tikzpicture} %[scale=0.70]

   % Draw labels
    \node[label] at (0,2)  {Randomize (seed, GMM, param, noise)};
    \node[label] at (3,2)  {Synthetize \\ (model, GAN gener., copula)};
    \node[label] at (6.0,2) {Estim. param (ACF, EPDF, GMM param., summary stat)};
    \node[label] at (9,2) {Fit (Hellinger, GAN discrim.)};
    %\node[label] at (7.5,0,1.5) {Fit};
    \node[label] at (12.0,2) {Estim. param (ACF, EPDF, GMM param., summary stat)};
    \node[label] at (15,2) {Real Data};
 
   \begin{scope}[every node/.style={circle, thin,draw,fill=pink}]
        \node[fill=pink] (0) at (0.0,0.0) {$q_1$};
        \node (0) at (0.0,0.0) {$q_1$};
        \node (1) at (0.0,-2) {$q_2$};
        \node (2) at (0.0, -4.0) {$q_3$};
        \node (3) at (0.0,-6) {$q_4$};
        \node (3a) at (0.0,-8.0) {$q_5$};
    \end{scope}

   \begin{scope}%[every node/.style={rectangle,minimum size=1.2cm,thin,draw,fill=green}]
        \node[rectangle,minimum size=1.2cm,thin,draw,fill={rgb:green,1;white,2}] (4) at (3,0.0) {$S_1$}; %%%%%%%%%%%5
        \node (5)[regular polygon,regular polygon sides=5, minimum size = 1.5cm, thin,draw,fill={rgb:green,1;white,4}] at (3,-2) {$S_2$};
%[color={rgb:black,1;white,4}
        \node (6)[regular polygon,regular polygon sides=9,minimum size=1.0cm,thin,draw,fill=green]  at (3,-4.0) {$S_3$};
        \node (7)[regular polygon,regular polygon sides=7, minimum size = 1.7cm, thin,draw,fill={rgb:green,1;white,1}] at (3,-6) {$S_4$};
        \node (7a)[regular polygon,regular polygon sides=6, minimum size = 1.5cm, thin,draw,fill={rgb:green,5;white,4;red,1}] at (3,-8.0) {$S_5$};
       \node (9)[regular polygon,regular polygon sides=7, minimum size = 1.5cm, thin,draw,fill=green]  at (15,-4.0) {$R$};
%fill={rgb:red,50;green,20;white,140}
    \end{scope}

   \begin{scope}[every node/.style={circle, thin,draw,fill=white}]
        \node (20) at (9.0,0) {$h_1$};
        \node (21) at (9.0,-2){$h_2$};
        \node (22) at (9.0,-4){$h_3$};
        \node (23) at (9.0,-6){$h_4$};
        \node (23a) at (9.0,-8){$h_5$};
  \end{scope}

   \begin{scope}[every node/.style={circle,thin,draw,fill=yellow}]
        \node (10) at (6.0,0.0) {$p_1$};
        \node (11) at (6.0,-2) {$p_2$};
        \node (12) at (6.0, -4.0) {$p_3$};
        \node (13) at (6.0,-6) {$p_4$};
        \node (13a) at (6.0,-8.0) {$p_5$};
        \node (19) at (12.0,-4.0) {$p$};
    \end{scope}

    %\begin{scope} %[every node/.style={circle,thin,draw}]
     %   \node (15) at (9.0,0.0) {$h_1$};
      %  \node (16) at (9.0,-2) {$h_2$};
       % \node (17) at (9.0, -4.0) {$h_3$};
        %\node (18) at (9.0,-6) {$h_4$};
        %\node (18a) at (9.0,-8.0) {$h_5$};
    %\end{scope}

    \begin{scope}[>={Stealth},
        num/.style={text=black, fill=white,circle, inner sep=0pt,minimum size=1pt},
        every edge/.style={draw,thin}]
        %\path [->] (0) edge[bend right=30] (3);
        \path [->] (0) edge node {} (4);
        \path [->] (1) edge node {} (5);
        \path [->] (2) edge node {} (6);
        \path [->] (3) edge node  {} (7);
        \path [->] (3a) edge node  {} (7a);

        \path [->] (4) edge node {} (10);
        \path [->] (5) edge node {} (11);
        \path [->] (6) edge node {} (12);
        \path [->] (7) edge node  {} (13);
        \path [->] (7a) edge node  {} (13a);

       \path [<-] (19) edge node  {} (9);

        %\path [->] (3) edge node {} (5); 
        %\path [->] (4) edge node {} (6); 
        %\path [->] (5) edge node {}  (6); 
        %\path [->] (5) edge[bend right=30] (7); 
        %\draw [red, thin ,dashed, ->] (20)  to[out=275,in=175] (1) to[out=0,in=270] (1) ;
        \draw [blue, thin ,dashed, ->] (20)   to (1); % to[out=0,in=27] (1) ;
        \draw [blue, thin ,dashed, ->] (21)  to (2); % to[out=0,in=27] (2) ;
        \draw [blue, thin ,dashed, ->] (22)  to (3); % to[out=0,in=27] (3) ;
        \draw [blue, thin ,dashed, ->] (23)  to (3a); % to[out=0,in=27] (3a) ;
        %\node[num] at (8) {$y_j$};
        \path [blue, thin, dashed, ->] (0) edge node[num] {} (1);
        \path [blue, thin, dashed, ->] (1) edge node[num] {} (2);
        \path [blue, thin, dashed, ->] (2) edge node[num] {} (3);
        \path [blue, thin, dashed, ->] (3) edge node[num] {} (3a);


        %\path [color={rgb:black,1;white,4}, thin,<-] (20) edge node[num] {} (19);
        \path [black, thin,<-] (20) edge node[num] {} (19);
        \path [black, thin, <-] (21) edge node[num] {} (19);
        \path [black, thin, <-] (22) edge node[num] {} (19);
        \path [black, thin, <-] (23) edge node[num] {} (19);
        \path [black, thin, <-] (23a) edge node[num] {} (19);

        \path [black, thin,<-] (20) edge node[num] {} (10);
        \path [black, thin, <-] (21) edge node[num] {} (11);
        \path [black, thin, <-] (22) edge node[num] {} (12);
        \path [black, thin, <-] (23) edge node[num] {} (13);
        \path [black, thin, ->] (23a) edge node[num] {} (13a);

        \draw [pink, thin ,dashed, ->] (19)   to[out=57,in=7] (0); 
        \draw [pink, thin ,dashed, ->] (19)  to[out=57,in=17] (1); 
        \draw [pink, thin ,dashed, ->] (19)  to[out=37,in=7] (2); 
        \draw [pink, thin ,dashed, ->] (19)  to (3); 
        \draw [pink, thin ,dashed, ->] (19)  to[out=-57,in=17] (3a);  

        %\path [pink, thin, dashed, ->] (15) edge node[num] {{$t_j$}} (1);
    
    \end{scope}
\end{tikzpicture}
} % end resizebox
%%%\noindent\rule{1.00\textwidth}{.4pt} 
\caption{Data synthetization: general schema} \label{fig:digdig}
\end{figure}
%\end{center}


\section{Python code: GAN to synthesize medical data}\label{pyganvg}

I broke down the program into three pieces. First, reading the data and removing observations with missing values. During this step, I also run a classification algorithm (random forest) on the real data, as the goal is to discriminate between patients likely to get cancer, from the other ones. 
 The rightmost column in the tabular data set, called \texttt{Outcome}, is the cancer indicator (1 = yes, 0 = no).

The second step is the core of the GAN procedure, including the production of synthetic data. Finally, the last part performs model evaluation   -- the fit between real and synthetic data -- using the TableEvaluator library my matrix correlation distance defined in step 2. In the last part, I classify again the data with the random classifier, but this time the synthesized data, for comparison purposes with the classification on the real data obtained in the first step.

To run in enhanced mode, set \texttt{mode='Enhanced'}. The full program named \texttt{GAN\_diabetes.py} 
 is also on my GitHub repository, 
 \href{https://github.com/VincentGranville/Main/blob/main/GAN_diabetes.py}{here}. The real dataset
 \texttt{diabetes.csv} can be found \href{https://github.com/VincentGranville/Main/blob/main/diabetes.csv}{here}.


\subsection{Classification problem with random forests}\label{cfpadgfvew}

This step reads the data, removes observations with missing values, and performs a classification on the real data. It also imports all the libraries needed.
 and eliminates all sources of uncontrollable randomness by using a seed for all the random number generators involved (native Python,
 TensorFlow, Numpy). This leads to replicable results. The hyperparemeter \texttt{learning\_rate} is also initialized here.\vspace{1ex}
 %You may need the most recent version of Numpy, TensorFlow and Pip. \vspace{1ex}


\begin{lstlisting}
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import random as python_random
from tensorflow import random
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam    # type of gradient descent optimizer
from numpy.random import randn
from matplotlib import pyplot
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics

data = pd.read_csv('diabetes.csv')  
# data located at https://github.com/VincentGranville/Main/blob/main/diabetes.csv

# rows with missing data must be treated separately: I remove them here
data.drop(data.index[(data["Insulin"] == 0)], axis=0, inplace=True) 
data.drop(data.index[(data["Glucose"] == 0)], axis=0, inplace=True) 
data.drop(data.index[(data["BMI"] == 0)], axis=0, inplace=True) 
# no further data transformation used beyond this point
data.to_csv('diabetes_clean.csv')

print (data.shape)
print (data.tail())
print (data.columns)

seed = 102     # to make results replicable
np.random.seed(seed)     # for numpy
random.set_seed(seed)    # for tensorflow/keras
python_random.seed(seed) # for python

adam = Adam(learning_rate=0.001) # also try 0.01
latent_dim = 10
n_inputs   = 9   # number of features
n_outputs  = 9   # number of features


#--- STEP 1: Base Accuracy for Real Dataset

features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']
label = ['Outcome']  # OutCome column is the label (binary 0/1) 
X = data[features]
y = data[label] 

# Real data split into train/test dataset for classification with random forest

X_true_train, X_true_test, y_true_train, y_true_test = train_test_split(X, y, test_size=0.30, random_state=42)
clf_true = RandomForestClassifier(n_estimators=100)
clf_true.fit(X_true_train,y_true_train)
y_true_pred=clf_true.predict(X_true_test)
print("Base Accuracy: %5.3f" % (metrics.accuracy_score(y_true_test, y_true_pred)))
print("Base classification report:\n",metrics.classification_report(y_true_test, y_true_pred))
\end{lstlisting}


\subsection{GAN method}\label{xcxxsdzs}\label{maindqq}

The main function is \texttt{train}. Adding layers to the networks, combining the discriminator and generator models of GAN, selecting 
 the loss functions and so on, and compiling the models, is done in the satellite functions defined here.
In addition, my matrix correlation distance function is defined in this step. It is heavily used in the enhanced version, when
 \texttt{mode=='Enhanced'}. The last instruction saves the synthesized data \texttt{data\_fake} to a CSV file.\vspace{1ex}


\begin{lstlisting}
#--- STEP 2: Generate Synthetic Data

def generate_latent_points(latent_dim, n_samples):
    x_input = randn(latent_dim * n_samples) 
    x_input = x_input.reshape(n_samples, latent_dim)
    return x_input

def generate_fake_samples(generator, latent_dim, n_samples):
    x_input = generate_latent_points(latent_dim, n_samples) # random N(0,1) data
    X = generator.predict(x_input,verbose=0) 
    y = np.zeros((n_samples, 1))  # class label = 0 for fake data
    return X, y

def generate_real_samples(n):
    X = data.sample(n)   # sample from real data
    y = np.ones((n, 1))  # class label = 1 for real data
    return X, y

def define_generator(latent_dim, n_outputs): 
    model = Sequential()
    model.add(Dense(15, activation='relu',  kernel_initializer='he_uniform', input_dim=latent_dim))
    model.add(Dense(30, activation='relu'))
    model.add(Dense(n_outputs, activation='linear'))
    model.compile(loss='mean_absolute_error', optimizer=adam, metrics=['mean_absolute_error']) # 
    return model

def define_discriminator(n_inputs):
    model = Sequential()
    model.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))
    model.add(Dense(50, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy']) 
    return model

def define_gan(generator, discriminator):
    discriminator.trainable = False # weights must be set to not trainable
    model = Sequential()
    model.add(generator) 
    model.add(discriminator) 
    model.compile(loss='binary_crossentropy', optimizer=adam)  
    return model

def gan_distance(data, model, latent_dim, nobs_synth): 

    # generate nobs_synth synthetic rows as X, and return it as data_fake
    # also return correlation distance between data_fake and real data

    latent_points = generate_latent_points(latent_dim, nobs_synth)  
    X = model.predict(latent_points, verbose=0)  
    data_fake = pd.DataFrame(data=X,  columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'])
 
    # convert Outcome field to binary 0/1
    outcome_mean = data_fake.Outcome.mean()
    data_fake['Outcome'] = data_fake['Outcome'] > outcome_mean
    data_fake["Outcome"] = data_fake["Outcome"].astype(int)

    # compute correlation distance
    R_data      = np.corrcoef(data.T) # T for transpose
    R_data_fake = np.corrcoef(data_fake.T)
    g_dist = np.average(abs(R_data-R_data_fake))
    return(g_dist, data_fake) 

def train(g_model, d_model, gan_model, latent_dim, mode, n_epochs=10000, n_batch=128, n_eval=200):   
    
    # determine half the size of one batch, for updating the  discriminator
    half_batch = int(n_batch / 2)
    d_history = [] 
    g_history = [] 
    g_dist_history = []
    if mode == 'Enhanced':
        g_dist_min = 999999999.0  

    for epoch in range(0,n_epochs+1): 
                 
        # update discriminator
        x_real, y_real = generate_real_samples(half_batch)  # sample from real data
        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)
        d_loss_real, d_real_acc = d_model.train_on_batch(x_real, y_real) 
        d_loss_fake, d_fake_acc = d_model.train_on_batch(x_fake, y_fake)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # update generator via the discriminator error
        x_gan = generate_latent_points(latent_dim, n_batch)  # random input for generator
        y_gan = np.ones((n_batch, 1))                        # label = 1 for fake samples
        g_loss_fake = gan_model.train_on_batch(x_gan, y_gan) 
        d_history.append(d_loss)
        g_history.append(g_loss_fake)

        if mode == 'Enhanced': 
            (g_dist, data_fake) = gan_distance(data, g_model, latent_dim, nobs_synth=400)
            if g_dist < g_dist_min and epoch > int(0.75*n_epochs): 
               g_dist_min = g_dist
               best_data_fake = data_fake
               best_epoch = epoch
        else: 
            g_dist = -1.0
        g_dist_history.append(g_dist)
                
        if epoch % n_eval == 0: # evaluate the model every n_eval epochs
            print('>%d, d1=%.3f, d2=%.3f d=%.3f g=%.3f g_dist=%.3f' % (epoch, d_loss_real, d_loss_fake, d_loss,  g_loss_fake, g_dist))       
            plt.subplot(1, 1, 1)
            plt.plot(d_history, label='d')
            plt.plot(g_history, label='gen')
            # plt.show() # un-comment to see the plots
            plt.close()
       
    OUT=open("history.txt","w")
    for k in range(len(d_history)):
        OUT.write("%6.4f\t%6.4f\t%6.4f\n" %(d_history[k],g_history[k],g_dist_history[k]))
    OUT.close()
    
    if mode == 'Standard':
        # best synth data is assumed to be the one produced at last epoch
        best_epoch = epoch
        (g_dist_min, best_data_fake) = gan_distance(data, g_model, latent_dim, nobs_synth=400)
       
    return(g_model, best_data_fake, g_dist_min, best_epoch) 

#--- main part for building & training model

discriminator = define_discriminator(n_inputs)
discriminator.summary()
generator = define_generator(latent_dim, n_outputs)
generator.summary()
gan_model = define_gan(generator, discriminator)

mode = 'Enhanced'  # options: 'Standard' or 'Enhanced'
model, data_fake, g_dist, best_epoch = train(generator, discriminator, gan_model, latent_dim, mode)
data_fake.to_csv('diabetes_synthetic.csv') 
\end{lstlisting}

\subsection{GAN Evaluation and post-classification}\label{lasravc}

Evaluates the quality of the synthetic data with the TableEvaluator library
 and \texttt{g\_dist}, the matrix correlation distance obtained in the previous step. Also, performs classification, but this time 
on the synthetic data, to compare with the results obtained on the real data in Step 1.\vspace{1ex}



\begin{lstlisting}
#--- STEP 3: Classify synthetic data based on Outcome field

features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']
label = ['Outcome']
X_fake_created = data_fake[features]
y_fake_created = data_fake[label]
X_fake_train, X_fake_test, y_fake_train, y_fake_test = train_test_split(X_fake_created, y_fake_created, test_size=0.30, random_state=42)
clf_fake = RandomForestClassifier(n_estimators=100)
clf_fake.fit(X_fake_train,y_fake_train)
y_fake_pred=clf_fake.predict(X_fake_test)
print("Accuracy of fake data model: %5.3f" % (metrics.accuracy_score(y_fake_test, y_fake_pred)))
print("Classification report of fake data model:\n",metrics.classification_report(y_fake_test, y_fake_pred))


#--- STEP 4: Evaluate the Quality of Generated Fake Data With g_dist and Table_evaluator

from table_evaluator import load_data, TableEvaluator

table_evaluator = TableEvaluator(data, data_fake)
table_evaluator.evaluate(target_col='Outcome')
# table_evaluator.visual_evaluation() 

print("Avg correlation distance: %5.3f" % (g_dist))
print("Based on epoch number: %5d" % (best_epoch))
\end{lstlisting}


%--------------------------------------------------------------------------------------------------------------------
\chapter{Cost-effective Generative AI with NoGAN}\label{chnogan}

I now introduce a new, NoGAN alternative to standard tabular data synthetization. It is designed to run faster by several orders of magnitude, compared to training generative adversarial networks (GAN). In addition, the quality of the generated data is superior to almost all other products available on the market. The hyperparameters are intuitive, leading to explainable AI.

Many evaluation metrics to measure faithfulness have critical flaws, sometimes rating generated data as excellent, when it is actually a failure, due to relying on low-dimensional indicators. I fix this problem with the full multivariate empirical distribution (ECDF). As an additional benefit, both for synthetization and evaluation, all types of features â€” categorical, ordinal, or continuous â€” are processed with a single formula, regardless of type, even in the presence of missing values.

In real-life case studies, the synthetization was generated in less than 5 seconds, versus 10 minutes with GAN. It produced higher quality results, verified via cross-validation. Thanks to the very fast implementation, it is possible to automatically and efficiently fine-tune the hyperparameters. I also discuss next steps to further improve the speed, the faithfulness of the generated data, auto-tuning, Gaussian NoGAN, and applications other than synthetization.

\section{Introduction}
Neural network methods have overshadowed all other techniques in the last decade, to the point that alternatives are simply ignored. And for good reasons: techniques such as \textcolor{index}{generative adversarial networks}\index{generative adversarial networks}\index{GAN (generative adversarial networks)} (GAN) proved very successful in some contexts, especially computer vision. Indeed, there has been several attempts to turn 
 every problem and traditional method -- regression, supervised classification, reinforcement learning --  into \textcolor{index}{deep neural networks}\index{deep neural network} (DNN).
 Yet recently, some authors showed the equivalence between DNN and other techniques such as decision trees, initiating a trend in the opposite direction~\cite{caglar22}. 

 
After testing various GANs for tabular data synthetization, I realized that there has to be a better way~to solve the problem. My method,
 referred to as NoGAN,  is the result of several years of research on the topic.  It is inspired by the~exact multivariate interpolation technique discussed in 
 section~\ref{tptyr}, as well as the hidden decision tree framework discussed in chapter~\ref{piereboul}. The latter is an ensemble method based on a moderately large number of moderately small decision trees, a feature now present in NoGAN. The former is related in the following sense: NoGAN interpolates the \textcolor{index}{multivariate empirical distribution function}\index{empirical distribution} (ECDF). Another way to describe the new method is as a generalization of the \textcolor{index}{copula}\index{copula} technique discussed in section~\ref{piviiiurobvbc}, replicating not only the marginal distributions and the correlation structure, but also the full joint distribution.



The joint or {\em multivariate} ECDF has remained elusive to this day. It is a rather non-intuitive object,~hard to visualize and handle even in two dimensions, let alone in higher dimensions with categorical features. For that reason, the probability density function (PDF) is more popular. It leads to techniques such as
 \textcolor{index}{Gaussian mixture models}\index{Gaussian mixture model} (GMM), frequently embedded into neural networks,
  or the \textcolor{index}{Hellinger distance}\index{Hellinger distance} to evaluate the quality of synthesized data. However, the ECDF is more robust and avoids a number of issues such as non-differentiable PDFs. Then, while the Hellinger distance also generalizes to multivariate PDFs, in practice all the implementations are one-dimensional, with the distance computed for each feature separately.  The Hellinger equivalent for the cumulative distribution function (CDF) is the 
\textcolor{index}{Kolmogorov-Smirnov distance}\index{Kolmogorov-Smirnov distance} (KS). 

It is said that KS does not generalize to the multivariate case. Thus its 
 total absence in applications~when the dimension is higher than one, despite the fact that it is the best metric to fully capture all 
the~dependencies among features, especially the non-linear ones. Interestingly, NoGAN is the first   
 working implementation of the multivariate ECDF and KS in the context of synthetization, breaking what was previously considered as insurmountable barriers.
 Theoretical bounds for the multivariate KS distance were investigated only recently~\cite{gtepouc21}, while an algorithm 
 for fast computation of the multivariate ECDF is discussed in~\cite{putrider22}.


Core features of NoGAN are the extensive use of multivariate bins and 
\textcolor{index}{optimum binning}\index{binning!optimum binning} or bucketization, to store various counts
 attached to the ECDF in the feature space. Also, the number of bins is not larger than the number of observations: this explains
  the speed of the algorithm, with a computational complexity proportional to the number of observations, and just one loop over the dataset.
 This is in contrast to GAN,~which requires a full processing of the entire dataset at each \textcolor{index}{epoch}\index{epoch}. Future versions of NoGAN~will have multiple layers,
 with more granular bins, the deeper the layer. 

Memory requirements are also minimum. While dense layers in
 neural networks connect all the neurons, in NoGAN only meaningful connections are created and stored: the underlying sparse structure is very efficiently 
 represented. The higher the dimension, the sparser the feature space due to the curse of dimensionality. NoGAN exploits this fact. Another benefit is interpretability. Hyperparameters are intuitive, and the whole algorithm epitomizes \textcolor{index}{explainable AI}\index{explainable AI}.

\section{Description and architecture}\label{daberikh}

The NoGAN technique consists of four main steps: binning the feature space and populating the bins using the training set, synthesizing the data, computing the ECDF,  
 and finally evaluating the quality of the synthetization on a validation set.  I now describe these four steps. It is assumed that the data has been cleaned, and possibly transformed via a reversible mapping such as \textcolor{index}{principal component analysis}\index{principal component analysis} (PCA) to make the numerical features decorrelated, and thus orthogonal. Features may be categorical, ordinal, or continuous. Before proceeding, split the real data into two sets: one called \textcolor{index}{training set}\index{training set} to build the synthetic data, and the remaining part called the \textcolor{index}{validation set}\index{validation set} to evaluate the quality (faithfulness) of the synthetization. 

\subsection{Binning the feature space}

First, values or labels attached to categorical features are encoded as consecutive integers, although the ordering is unimportant. The classic way to do it is
 via \textcolor{index}{dummy variables}\index{dummy variable}. However, I recommend the following approach. %\vspace{1ex}

\begin{itemize}
\item Create a list of all flag vectors. A flag vector is a combination of \textcolor{index}{key-value pairs}\index{key-value pair},
 for instance \texttt{[gender=M, smoker=Yes, Region=North]}. Browse the training set to create and update the list of flag vectors, incrementing a counter by 1 each time a new flag vector is found. 
\item The counter, also called index, is the integer uniquely identifying any combination of categorical values. Use it as the key for the flag vector table, mapping an integer to a specific feature combination.
\item Now all categorical features have been collapsed into a single generic feature with integer values. Category labels are unimportant at this point. You may want to aggregate all flag-vectors with very few observations into a single, catch-all flag-vector for optimization purposes, assuming they represent a small proportion of the training set
 when combined together.
\end{itemize}\vspace{1ex}

\noindent Then, we move to the construction of the \textcolor{index}{quantile}\index{quantile} table. In the current version of NoGAN, it is done as follows. Each featured is binned into consecutive intervals, each with a lower and upper bound. Let $n_k$ be the number of 1D bins attached to the $k$-th feature. The binning is achieved so that each interval contains about the same number of observations. To do so, the lower bounds of each interval correspond to evenly spaced values of the empirical \textcolor{index}{quantile function}\index{quantile function} attached to the feature in question. For the categorical feature(s) turned into integers via encoding, choose $n_k$ large enough so that each category combination or flag vector (except
 possibly those with very few observations) is represented in the synthetic data. In the Python code, the lower and upper bounds are 
 represented respectively by the arrays \texttt{L\_bounds} and \texttt{U\_bounds}. 

In Python,
 use the  \texttt{quantile} function available in Numpy to compute the interval lower bounds. The quantile function  is the inverse of the cumulative distribution function (CDF). The upper bound of each interval corresponds to the lower bound of the next interval, or for the last interval, the maximum value for the feature in question. All intervals must be semi-open: containing the lower bound, but not the upper bound. This is important to properly handle the feature(s) representing categorical data.

Let us assume that we have $d$ features. Thus, the dimension of the feature space is $d$, after collapsing all the categorical features into one single ordinal feature. Now, a multivariate bin is defined as an $d$-dimensional hyperrectangle 
$I_1 \times I_2 \times \cdots\times I_d$, where $I_k$ is any of the $n_k$ intervals attached to the $k$-th feature.
 Thus, potentially, the total number of multivariate bins could be as high as $n_1\cdot n_2 \cdots  n_d$. The next step consists of browsing the data set,
 to create and update all the multivariate bins encountered, computing the number of observations in each visited bin.
 Bins may be stored in a hash table updated on the fly after visiting each observation sequentially. Thus, the creation of the
multivariate  bin structure takes 
 little time, and the maximum number of bins is no larger than the number of observations. Empty bins -- by far the most frequent type of bins in  
  high dimensional feature spaces -- are never created to begin with.

\subsection{Data synthetization}

The quantile table created in the previous step is named \texttt{pc\_table2} in the Python code. It is an array with 
 $d$ rows, one per feature. The $k$-th row has $n_k$ columns, thus the number of columns is variable.  The bin count in each multivariate bin
 is divided by the total number of observations, and the bin structure is named \texttt{bin\_count}. It is an hash table
 (dictionary in Python), with the multivariate key (named \texttt{key}) consisting of the lower bounds of the intervals $I_1,\dots,I_d$ 
  uniquely identifying the bin in question. Because Python does not allow dictionaries to be indexed by arrays, the key is turned into
 a string \texttt{skey}.

The core \textcolor{index}{hyperparameter}\index{hyperparameter} is the vector $[n_1,\dots,n_d]$ controlling the number of bins and the bin structure. See \texttt{bins\_per\_feature} in the Python code. High values of $n_1,\dots,n_d$ result in many small bins and a more faithful replication of the training set, up to a certain degree:  the risk is that the validation set may have many observations outside the training set bins.
When that happens, KS -- the quality of the synthetic data -- will have a higher value, indicating lower performance. In the telecom case study   
 discussed in section~\ref{hg4cvs}, there are four features. 
I set the hyperparameter to \texttt{[50,40,40,4]} with the last value corresponding to the binary categorical feature. It resulted in about 3500 non-empty but very small bins. Including
 empty bins, the total would be $\num{320000}$. Nevertheless, the performance based on the validation set was excellent.
In addition to \texttt{bin\_count}, another useful hash table indexed by the same key is \texttt{bin\_obs}. For each bin, 
it stores the list of training set observations lying inside. 

The actual synthetization proceeds as follows. For each multivariate bin $B$ in \texttt{bin\_count}, generate $N_B$ observations uniformly distributed
 on $B$. There are two options for the counts $N_B$. If \texttt{mode=='FixedCounts'}, we use the same counts as in the training set. If the bins are granular enough (the granularity is determined by the hyperparameter vector), the synthetic data will be an exact replication of the training set.
If instead \texttt{mode=='RandomCounts'}, specify the total number $N$  (\texttt{nobs\_synth} in the code) of synthetic observations that you want to
 generate. Then the counts $N_B$ are sampled from a \textcolor{index}{multinomial distribution}\index{multinomial distribution} of parameters $N, p$.
 Here $p$ is a probability vector adding to one, with one value for each bin $B$: the proportion of training set points that lie inside. In the code,
 $p$ is represented by \texttt{pvals}.

Due to the design of the algorithm, synthetic values for categorical feature(s) -- now represented as integers or more precisely indices in the validation set -- must be truncated with the \texttt{int} function. It will then result in the correct proportion of observations in each category, as well as correct interdepencies, linear or not, among all the features including the categorical ones. 

\subsection{Computing the multivariate ECDF}\label{iuc3id}

Given a location $z = (z_1,\dots, z_d)$ in the feature space, the ECDF $F(z)$ evaluated at $z$ is defined as
 the proportion of observations $x = (x_1,\dots,x_d)$ satisfying $x_k < z_k$ for $k=1,\dots,d$. These proportions are based on counts
 and computed with SQL queries in Pandas. In the code, an instruction such as 
\begin{center}
\texttt{countifs=len(data\_validation.query(query\_string))} 
\end{center}
illustrates the mechanism. The details are as follows: \vspace{1ex}
\begin{itemize}
\item \texttt{query\_string} is the SQL query, automatically created for a large number of $z$.
\item \texttt{query} is the Pandas function to run SQL queries against a dataset.
\item \texttt{data\_validation} is the Pandas dataframe representing the dataset, in this case the validation set.
\item \texttt{len} is the number of rows returned by the query (those meeting the condition in \texttt{query\_string}).
\end{itemize}\vspace{1ex}
This is the most time-consuming step in the code, and it is  needed to evaluate the quality of the synthetic data (the next step).
In typical GAN implementations, the opposite is true: training is slow, but evaluation is fast. 
However fast evaluation is due to using faulty quality metrics that fail to capture subtle yet highly visible interdependencies among the features.
 This is illustrated in my article on benchmarking synthesizers~\cite{vgvendors}, especially on the circle dataset. My multivariate ECDF and KS framework 
 was created precisely to address this issue. Of course you could run a GAN and use my KS metric instead, which I highly recommend.

Yet, despite the large number of SQL queries, this step is executed very fast, in a few seconds. I suspect that Pandas optimizes the queries 
 in one way or another. If not, there are different ways to accelerate the computations. First, we only need counts, not the full data returned by each query. Then there are techniques to optimize the computation of the ECDF, see~\cite{putrider22}. Finally, $F(z)$ is evaluated
 at a large number of locations $z$ in the working area of the feature space, where all the observations lie. In the code, that number is specified by the variable \texttt{n\_nodes}, and set to 1000. Ideally, you want a value larger by several orders of magnitude. This would greatly benefit from optimizing the SQL queries. Note that $F(z)$ only has a finite number of potential values, not necessarily very high depending on the number and types of features: this can be leveraged to minimize the number of queries.

 Finally, the locations $z$ are selected based on random quantiles $q$ that are uniformly distributed on $[0, 1]$, for each feature.  It results in 
significantly undersampling $z$ locations with a high $F(z)$ close to one. To correct for this bias, a second set of random locations is added, using
  $q^{1/d}$ instead of $q$, where $d$ is the number of features. Random quantile vectors are stored in \texttt{combo}. The second set
 of locations is obtained using \texttt{adjusted==True} in the \texttt{compute\_ecdf} function. Since ECDF is computed based on a large number
 of locations called ``nodes", its interpolated shape is only an approximation, converging to the real function as the number of nodes increases. 
 
\subsection{Evaluating the quality}\label{evrd7hg}

One of the original and powerful components of NoGAN is the multivariate 
\textcolor{index}{Kolmogorov-Smirnov distance}\index{Kolmogorov-Smirnov distance} (KS) to evaluate the quality of
 the synthetization. By multivariate, I mean that all the features are processed jointly, not separately. This helps detect unusual patterns spanning across multiple dimensions, far more complex than pairwise interactions. The KS distance is defined as
$$
\text{KS}(F_s, F_v) = \sup_{z} |F_s(z) - F_v(z)|,
$$
where $z$ is any location in the feature space, and $F_s, F_v$ are the ECDFs, respectively computed on the synthetic and validation set. 
In practice, $F_s,F_v$ are approximated using a number of locations called nodes, as described in section~\ref{iuc3id}. In the NoGAN version
 described here, these nodes are generated according to some stochastic process based on the quantiles of the original features. The goal is to guarantee
 that the nodes cover the sparse working area -- the tiny region of the feature space where the observations lie  -- with maximum efficiency.
 How small that area is, compared to the full potential feature space, depends to a large extent on the dimension: the number of features.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{nogan3.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{ECDF scatterplot, synthetic vs validation set}
\label{fig:nogan3}
\end{figure}

Try increasing \texttt{n\_nodes} (the number of nodes) from $10^3$ to $10^4$ and $10^5$, to see when KS stabilizes.  Also, because of the randomization, KS computed between the training and validation set may vary slightly between successive runs.
The problem occurs when using different hyperparameter vectors in each run. To fix this little issue and get perfect replicability, I introduced two seeds in the algorithm. When the second seed is activated by setting \texttt{reseed=True}, KS won't be impacted by how many times the random number generator was called earlier in the code.

Besides computing the KS distance between the synthetic and validation sets, it is useful to also compute KS between the training and validation sets. This distance is called ``Base KS" and named \texttt{KS\_base} in the~code. Since the synthetization is based on training data only, if the training and validation sets are very different, you should expect that the synthesized data will not be great at mimicking the real data, outside the training set. 
 This occurs when not properly splitting the real data into training and validation sets. To summarize, the absolute difference between
 KS and Base KS may be the best indicator of faithfulness.

Finally, because synthetic observations are uniformly generated in hyperrectangles (the numerous multivariate bins), if these bins are large enough, side effects will be noticeable to the naked eye, especially near the edges of the working area and when ordinal features are involved.
This is barely noticeable in the middle left plot 
in Figure~\ref{fig:nogan5}, at least if you stare at it for some time. To eliminate this effect, increase bin granularity. A more elegant solution consists
 of working with Gaussian rather uniform distributions to sample synthetic observations within each bin. However this may not improve KS much: 
 it just happens that the human brain detects sharp borders a lot easier than fuzzy ones. 


% add correl table for evaluation


\section{Results and discussion}\label{hg4cvs}

The main dataset comes from Kaggle. It contains 7044 observations, each one representing a customer from a telecom company,
 with a number of numerical and categorical features, including a response called \texttt{Churn}. The latter is binary Yes/No, with ``Yes" meaning the customer has left.
 There is no time stamp. The three numerical features are \texttt{tenure} (how long the customer has been with the company, measured in months), \texttt{MonthlyCharges} and \texttt{TotalCharges}. The last two are of course highly correlated. 

\begin{figure}[H]
\centering
\includegraphics[width=0.71\textwidth]{nogan4b.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Synthetic (top) versus validation set (bottom), for Churn = Yes}
\label{fig:nogan4}
\end{figure}



\subsection{Telecom dataset}

I synthesized the data with an home-made GAN and with the open source library SDV. Both performed poorly. To remove the high correlation, I created a new feature called \texttt{TotalChargesResidues} to replace \texttt{TotalCharges}. This change led to a big improvement. I then
 run the NoGAN synthesizer. The results were spectacular, even after cross-validation: using 50\% of the dataset to train the model and generate the synthetic data, and the other 50\% 
 to compare the synthetization with the real data not included for training, that is,  with the \textcolor{index}{validation set}\index{validation set}. The full dataset is available on my GitHub repository, \href{https://github.com/VincentGranville/Main/blob/main/Telecom.csv}{here}.




 
For illustration purposes, I created two sets of plots: one for \texttt{Churn='Yes'}, and one for \texttt{Churn='No'}. 
The histograms in Figures~\ref{fig:nogan4},~\ref{fig:nogan5} show that the continuous distributions heavily depend on the Churn category, with the difference well rendered by the synthetization. This remains true for pairwise interactions, as shown in the scatterplots
 in Figures~\ref{fig:nogan1},~\ref{fig:nogan2}.

To create Figure~\ref{fig:nogan3}, I used a large number of randomly distributed vectors  $z$ in the feature space and for each of them, I computed $[F_s(z), F_v(z)]$. These 2D arrays correspond to the blue dots in the scatterplot in Figure~\ref{fig:nogan3}. The closer to the main diagonal, the better the synthetization. Here $F_s, F_v$ are the ECDFs, respectively for the synthetic and validation datasets.
The vectors $z$ are also called nodes or locations in the Python code.  The feature space is called the grid because the working area (where the observations lie) is partitioned into non-overlapping rectangular bins arranged in a lattice configuration. The dimension of the space is the number of features. 

The final KS distance between the synthetic and validation data is the maximum value for 
$|F_s(z) - F_v(z)|$. Here, it is equal to $0.0313$, which is excellent. It would be interesting to compare it with the base KS distance
 between the training and validation data. One would expect, if the synthetization is very good, that both distances are very similar, with base KS
 being slightly smaller. Here, base KS is equal to 0.0293, confirming~the very good quality of the generated data.

The KS distance and the NoGAN algorithm are now available as open source Python libraries, see Appendix~\ref{aasdattt} for details. 
There you will find a test on a dataset with 21 features (numerical and categorical mix), sample code to call the two Python libraries, 
 illustration with Gaussian NoGAN, and the default hyperparameters.  The code
 in section~\ref{gomornogan} is the full, original implementation. 

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{nogan1.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Synthetic (left) versus validation set (right), for category Churn = `Yes'}
\label{fig:nogan1}
\end{figure}




\begin{figure}[H]
\centering
\includegraphics[width=0.71\textwidth]{nogan5b.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Synthetic (top) versus validation set (bottom), for category Churn = `No'}
\label{fig:nogan5}
\end{figure}







\subsection{The circle dataset}

This data set was created to illustrate weaknesses in the \textcolor{index}{copula synthesizer}\index{copula}. This method is very good at replicating
 the marginal distributions and the correlation structure. But in this case, it fails: the absence or correlation is perfectly rendered, but
 the non-linear patterns -- the points in the real data located on two concentric circles -- is completely missed, resulting in  a synthetization that looks strikingly bad to the naked eye. The synthetization in question and relevant pictures are found in~\cite{vgvendors} but not reproduced here.

Back then, the goal was to show the superiority of GAN to deal with this type of feature inter-dependencies. The results were mixed, with many GANs (including the open source library SDV) not doing any better than copula. Only YData.ai did well both on the circle dataset and other case studies involved.
Even worse, some vendors including SDV reported the result as excellent, according to their  own evaluation metrics. This is not surprising: even the worst synthetizations got the means, standard deviations and cross-correlations (all zero) were very well replicated. This is due to evaluation
 metrics that fail to detect patterns such as the double circle, or any complex non-linear structure. The multivariate KS introduced in this chapter fixes this issue.

\begin{figure}[H]
\centering
\includegraphics[width=0.35\textwidth]{NoGAN_circle.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Synthesizing the circle dataset}
\label{fig:nogancc}
\end{figure}



Despite the appearances, the circle dataset is not about computer vision and shape recognition. It is about tabular data 
 that happens to have some odd joint distribution. In the end, only NoGAN and YData were able to not only do well  on  the circle dataset,
 but also all the  examples, including the diabetes and insurance datasets.

As for NoGAN, the KS distance between synthetic and validation data is equal to 0.0950, while the KS distance between training and validation set is 0.0750. This is not as good as for the telecom dataset, because here, the training and validation sets are more different. The reason is because the circle dataset is~much smaller, making comparisons more difficult. Still, NoGAN wins here, despite using the same hyperparameter as for the telecom dataset. The synthetization is pictured in Figure~\ref{fig:nogancc}, with the real data (validation set) on the right.

As a final word, the focus in this chapter is only on achieving and measuring faithfulness. There is no discussion on increasing privacy or security, which are other important benefits   of data synthetizations. Some vendors and users focus mostly on these aspects and may not care as much about faithfulness. Also, fine-tuning a GAN to make it automatically work on any dataset is a lot more difficult than optimizing NoGAN to turn it into 
 a generic blackbox. A promising approach is the use of 
\textcolor{index}{Wasserstein GAN}\index{Wasserstein GAN} (WGAN), based on a different \textcolor{index}{loss function}\index{loss function}: see~\cite{ieeewgan}.

\section{Potential improvements}

The first easy change to the method is about how synthetic observations are generated in each multivariate bin. The bins are hyperrectangles, with known dimensions. We also know the training set points lying in each one. Generation is performed by uniform sampling on the hyperrectangle. Instead, one could try a multivariate Gaussian distribution, where the hyperrectangle covers a proportion $p$ of the normal distribution in question. The fixed parameter $p$ could be fine-tuned, with $p=95\%$ being a good starting value. This presents some analogy to 
 \textcolor{index}{diffusion models}\index{diffusion model} in machine learning. An implementation of Gaussian NoGAN can
be found \href{https://github.com/VincentGranville/Main/blob/main/NoGAN_gaussian.py}{here}. Look at lines 142--150 and 192--200 in the code in question. On the Telecom dataset, it produced better results, both visually (quality of the scatterplots) and in terms of KS distance 

Another enhancement consists of using quantiles that are not evenly spaced for the marginal distributions. In other words, use a transform of the quantiles. In this version, the increment between quantiles is specified in section [2.1] in the code, via \texttt{incr}. 
But the user could provide his own customized quantile table -- the array \texttt{pc\_table2}. Then, the hyperparameter
 \texttt{bins\_per\_feature}, specifying the number of quantile intervals per feature (thus, also \texttt{incr}) could be automatically fine-tuned to minimize the KS distance. This can be accomplished using the 
\textcolor{index}{smart grid search}\index{grid search}  technique described in section~\ref{smargsxxbv}.

Rather than transforming quantile parameters, it is possible to transform the data prior to using NoGAN, then apply the inverse transform
 after NoGAN to recover the original shape. This is similar to using \textcolor{index}{transformers}\index{transformer} in large language models. A transform that makes sense is \textcolor{index}{principal component analysis}\index{principal component analysis} (PCA) to
 get orthogonal (uncorrelated) features. In fact, since the hyperrectangles have their faces either parallel or perpendicular to the axes in the feature space (there is no rotation angle), PCA is a good candidate. Indeed I use such a transform to decorrelate two heavily correlated features in the telecom dataset,
 as it was causing issues with my own GAN (though not necessarily with NoGAN). See section [1.3] in the code.
Multiple transformations are possible, for instance logit then PCA before NoGAN, then inverse PCA followed by inverse logit after NoGAN. 

 Other improvements include testing multiple seeds to choose the one that yields the best KS distance, and further increasing the speed of the algorithm. 
 To improve speed without changing the results, two options are possible. First, reduce the number of SQL queries to a minimum, and optimize the query engine, possibly via a
 distributed architecture. Then use a dichotomic search instead of the current version of the \texttt{while} loop in section [2.2]. 
With minimum impact on the results, speed can also be improved by reducing the number of observations used for training, via data thinning
 (see section~\ref{aithing}), a 
 much faster alternative to \textcolor{index}{data distillation}\index{data distillation}. Or by reducing the number of features via 
 \textcolor{index}{feature clustering}\index{feature clustering}, as illustrated in section~\ref{fcv34}.  

Finally, it remains to be seen if adding deeper layers could improve NoGAN. The current version has only one layer. Adding a second layer consists
 of splitting all large multivariate bins (say with more than 100 observations) into sub-bins. The goal is to break down large bins, especially those
 with heterogeneous observations, into smaller hyperrectangles. Top level bins that are covered by sub-bins in the second layer, are simply ignored and replaced by the deeper
 sub-bins. One could add a third layer and so on. A second layer will help when using small values in
 the hyperparameter vector \texttt{bins\_per\_feature}.

\subsection{The original idea behind NoGAN}

Interestingly, my first version of NoGAN was very different. However I did not implement it due to computational complexity. Yet, the concept is remarkably simple. It consists of independent feature sampling (one at a time) followed by resampling to get the right feature interdependencies. If you want to give it a try, here is how it works: \vspace{1ex}
\begin{itemize}
\item First, sample values for each feature separately, based on the marginal (unidimensional) ECDFs. This is easy and very fast. 
\item Then reshuffle the sample values of the second feature to optimize the KS distance, measured on the first two features.
 You may try a large number of permutations,  or at each step decide whether or not to swap two values depending on whether it improves the KS distance or not. 
\item Then do the same but this time for the third feature, keeping the first two features unchanged. And so on until all the features are processed.  
\end{itemize}\vspace{1ex}
Each additional feature increases the
 dimension of the KS framework by one. To optimize speed, when deciding whether swapping values or not at any iteration, do not process the whole data each time to compute the new KS; instead update the KS formula by modifying 
 only the counts impacted by the change. Also, splitting the problem and processing one \textcolor{index}{batch}\index{batch} at a time -- a subset of training data with just 100 observations or so -- results in faster computations. This approach is similar to GAN, where the term {\em batch} comes from. 

Reschuffling the sampled values of a feature to minimize a cost function, may also be viewed as an \textcolor{index}{assignment problem}\index{assignment problem} [\href{https://en.wikipedia.org/wiki/Assignment_problem}{Wiki}], and solved using the \textcolor{index}{Hungarian algorithm}\index{Hungarian algorithm} [\href{https://en.wikipedia.org/wiki/Hungarian_algorithm}{Wiki}]. Here the cost functions are the successive~KS distances, with the underlying ECDFs increasing by one dimension each time a new feature is added.  The KS
 distance is closely related to the \textcolor{index}{total variation distance}\index{total variation distance}
 [\href{https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures}{Wiki}].  
You may want to first transform the training data via PCA, and start with the top transformed feature (with maximum
 information) down to the least significant one.
 A variant, even more computer-intensive, consists of reshuffling all features jointly, rather than the suggested stepwise procedure. The advantage is that it will reproduce the correct multivariate ECDF, for sure. 

\subsection{Auto-tuning the hyperparameters, missing values}\label{attrere}

Besides \textcolor{index}{smart grid search}\index{smart grid search} (see section~\ref{smargsxxbv}), one way to optimize the hyperparameter vector is to use NoGAN separately on each feature. It allows you to test different values of the hyperparameter vector, one feature at a time, thus focusing on one single value each time. The optimum value, given a feature, is the one that minimizes the KS distance. You may use a smaller subset of the training set for that purpose, to speed up the computations. This technique is known as \textcolor{index}{auto-tuning}\index{auto-tuning}.

As for \textcolor{index}{missing values}\index{missing values}, they are easy to handle in categorical features, by assigning them a separate category and using the encoding method. With continuous features, it is more tricky. One way to do it is to create a new binary feature called {\em label}, taking the value one if the value is missing in
 the parent feature, and zero otherwise. For simplicity, let us assume that a missing value is encoded as zero in the parent feature. In the parent feature, replace missing values by random values following the same distribution as the non missing values. Once the synthetization is completed, remove the extra feature, and replace the random values in the parent feature, by the missing value, encoded as zero in our example. Then compute KS. 







 

\section{Conclusion}

The superiority of NoGAN is substantial and unquestionable. After all, it allows for exact replication of the real data if bins are 
granular enough. This is easily achieved with barely any penalty in running time or memory requirements. 
No matter what, the final number of bins is no larger than the number of observations. GAN is not capable of such performance, making
 NoGAN a game changer. The \textcolor{index}{loss function}\index{loss function} is the KS distance between the multivariate ECDFs computed on the real and synthetic data. However,
 there is no \textcolor{index}{gradient descent}\index{gradient descent} algorithm involved, contributing to the speed and stability of the method, regardless of the type of features. In particular, the method is not subject to \textcolor{index}{mode collapse}\index{mode collapse} or divergence.
 Likewise, there is no discriminator model involved, unlike GAN. The method also leads to fully replicable results and simple parallel  implementation.



NoGAN was designed for tabular data and untested so far in other contexts. It would be interesting to see~how it performs in computer vision, text data, and
 other problems. Indeed, the binning algorithm (hidden decision trees) was first built years ago in the context 
 of text processing (NLP).  Without any adaptation, NoGAN can be used for clustering, competing with methods based on density estimation. Or to compute
 model-free confidence intervals, as a better alternative to \textcolor{index}{resampling}\index{resampling} techniques such as 
 \textcolor{index}{bootstrapping}\index{bootstrapping}. The latter consists or reshuffling observations, while NoGAN creates new ones.
 Finally, you can use it for supervised classification, with no risk of overfitting, by assigning a label to each bin. Rather than overfit, the technique
 will fail to produce a prediction for future observations located outside the bin configuration. Last but not least, NoGAN can be used as a data imputation method, by averaging across parent bins that are identical to the incomplete son, except for the missing values.



\section{Python implementation}\label{gomornogan}

Explanations about the different steps, including a description of the main variables and tables, can be found in section~\ref{daberikh}.
Compared to GAN, this implementation requires very few library functions and only three imports: Numpy, Statsmodels, and Pandas. This significantly reduces
 incompatibilities between library versions, and increases the chance that you can run it ``as is" on any platform, without
 impacting your own environment. The code \texttt{NoGAN.py} is also available on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/NoGAN.py}{here}. 

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{nogan2.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Synthetic (left) versus validation set (right), for Churn = No}
\label{fig:nogan2}
\end{figure}

This work was initiated as part of my \href{https://mltblog.com/3pWxvZK}{GenAI certification program}. I would like to thank 
\href{https://www.linkedin.com/in/shakti-chaturvedi-49aab9106/}{Shakti Chaturvedi} for all our interactions and feedback, leading to the current version of NoGAN.
\vspace{1ex}



\begin{lstlisting}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import pyplot
from statsmodels.distributions.empirical_distribution import ECDF

#--- [1] read data and only keep features and observations we want

#- [1.1] utility functions

def string_to_numbers(string):

    string = string.replace("[", "")
    string = string.replace("]", "")
    string = string.replace(" ", "")
    arr = string.split(',')
    arr = [eval(i) for i in arr]
    return(arr)

def category_to_integer(category):
    if category == 'Yes':
        integer = 1
    elif category == 'No':
        integer = 0
    else:
        integer = 2
    return(integer)

#- [1.2] read data

url = "https://raw.githubusercontent.com/VincentGranville/Main/main/Telecom.csv"
data = pd.read_csv(url)
features = ['tenure', 'MonthlyCharges', 'TotalCharges','Churn'] 
data['Churn'] = data['Churn'].map(category_to_integer) 
data['TotalCharges'].replace(' ', np.nan, inplace=True)
data.dropna(subset=['TotalCharges'], inplace=True)  # remove missing data
print(data.head()) 
print (data.shape)
print (data.columns)

#- [1.3] transforming TotalCharges to TotalChargeResidues, add to dataframe

arr1 = data['tenure'].to_numpy()
arr2 = data['TotalCharges'].to_numpy() 
arr2 = arr2.astype(float)
residues = arr2 - arr1 * np.sum(arr2) / np.sum(arr1)  # also try arr2/arr1
data['TotalChargeResidues'] = residues

#- [1.4] set seed for replicability

pd.core.common.random_state(None)
seed = 105
np.random.seed(seed)

#- [1.5] initialize hyperparameters (bins_per_feature), select features

features = ['tenure','MonthlyCharges','TotalChargeResidues','Churn'] 
bins_per_feature = [50, 40, 40, 4]   

bins_per_feature = np.array(bins_per_feature).astype(int)
data = data[features]
print(data.head())
print (data.shape)
print (data.columns)

#- [1.6] split real dataset into training and validation sets

data_training = data.sample(frac = 0.5)
data_validation = data.drop(data_training.index)
data_training.to_csv('telecom_training_vg2.csv')
data_validation.to_csv('telecom_validation_vg2.csv')

nobs = len(data_training)
n_features = len(features)
eps = 0.0000000001 


#--- [2] create synthetic data  

#- [2.1] create quantile table pc_table2, one row for each feature

pc_table2 = []
for k in range(n_features):
    label = features[k]
    incr = 1 / bins_per_feature[k]   
    pc = np.arange(0, 1 + eps, incr)
    arr = np.quantile(data_training[label], pc, axis=0)
    pc_table2.append(arr)

#- [2.2] create/update bin for each obs [layer 1]
#        Faster implementation: replace 'while' loop by dichotomic search

npdata = pd.DataFrame.to_numpy(data_training[features])
bin_count = {}   # number of obs per bin
bin_obs = {}     # list of obs in each bin, separated by "~", stored as a string
for obs in npdata:
    key = [] 
    for k in range(n_features):
        idx = 0
        arr = pc_table2[k]    # percentiles for feature k
        while obs[k] >= arr[idx] and idx < bins_per_feature[k]: 
            idx = idx + 1
        idx = idx - 1  # lower bound for feature k in bin[key] attached to obs
        key.append(idx)
        skey = str(key)
    if skey in bin_count:
        bin_count[skey] += 1
        bin_obs[skey] += "~" + str(obs)
    else:
        bin_count[skey] = 1
        bin_obs[skey] = str(obs)

#- [2.3] generate nobs_synth observations (if mode = FixedCounts, nobs_synth = nobs)

def random_bin_counts(n, bin_count):
    # generate multinomial bin counts with same expectation as real counts
    pvals = []
    for skey in bin_count:
        pvals.append(bin_count[skey]/nobs)
    return(np.random.multinomial(n, pvals))

def get_obs_in_bin(bin_obs, skey): 
    # get list of observations (real data) in bin skey, also return median
    arr_obs = []
    arr_obs_aux = (bin_obs[skey]).split('~')
    for obs in arr_obs_aux:
        obs = ' '.join(obs.split())
        obs = obs.replace("[ ", "")
        obs = obs.replace("[", "")
        obs = obs.replace(" ]", "")
        obs = obs.replace("]", "")
        obs = obs.split(' ')
        obs = (np.array(obs)).astype(float)
        arr_obs.append(obs)
    arr_obs = np.array(arr_obs)
    median = np.median(arr_obs, axis = 0)
    return(arr_obs, median) 

    
mode = 'RandomCounts'  # (options: 'FixedCounts' or 'RandomCounts')
if mode == 'RandomCounts':
    nobs_synth = nobs 
    bin_count_random = random_bin_counts(nobs_synth, bin_count)
    ikey = 0

data_synth = []
bin_counter = 0

for skey in bin_count:

    if mode == 'FixedCounts':
        count = bin_count[skey]
    elif mode == 'RandomCounts': 
        count = bin_count_random[ikey]
        ikey += 1
    key = string_to_numbers(skey)
    L_bounds = []
    U_bounds = []
    bin_counter += 1

    for k in range(n_features):
        arr = pc_table2[k] 
        L_bounds.append(arr[key[k]])
        U_bounds.append(arr[1 + key[k]])

    # sample new synth obs (new_obs) in rectangular bin skey, uniformily
    # try other distrib, like multivariate Gaussian around bin median 
    # the list of real observations in bin[skey] is stored in obs_list (numpy array)
    # median is the vector of medians for all obs in bin skey

    obs_list, median = get_obs_in_bin(bin_obs, skey) # not used in this version
    
    for i in range(count):        
        new_obs = np.empty(n_features) # synthesized obs
        for k in range(n_features):
            new_obs[k] = np.random.uniform(L_bounds[k],U_bounds[k])
        data_synth.append(new_obs)

    str_median = str(["%8.2f" % number for number in median])
    str_median = str_median.replace("'","")
    print("bin ID = %5d | count = %5d | median = %s | bin key = %s" 
              %(bin_counter, bin_count[skey], str_median, skey))

data_synth = pd.DataFrame(data_synth, columns = features)

# apply floor function (not round) to categorical/ordinal features
data_synth['Churn'] = data_synth['Churn'].astype('int') 
data_synth['tenure'] = data_synth['tenure'].astype('int')

print(data_synth)    
data_synth.to_csv('telecom_synth_vg2.csv')


#--- [3] Evaluation synthetization using joint ECDF & Kolmogorov-Smirnov distance

# dataframes: df = synthetic; data = real data,
# compute multivariate ecdf on validation set, sort it by value (from 0 to 1) 

#- [3.1] compute ecdf on validation set (to later compare with that on synth data)

def compute_ecdf(dataframe, n_nodes, adjusted):

    # Monte-Carlo: sampling n_nodes locations (combos) for ecdf
    #    - adjusted correct for sparsity in high ecdf, but is sparse in low ecdf  
    #    - non-adjusted is the other way around
    # for faster computation: pre-compute percentiles for each feature
    # for faster computation: optimize the computation of n_nodes SQL-like queries

    ecdf = {} 

    for point in range(n_nodes):

        if point % 100 == 0:
            print("sampling ecdf, location = %4d (adjusted = %s):" % (point, adjusted))
        combo = np.random.uniform(0, 1, n_features)
        if adjusted:
            combo = combo**(1/n_features)
        z = []   # multivariate quantile
        query_string = ""
        for k in range(n_features):
            label = features[k]
            dr = data_validation[label]
            percentile = combo[k] 
            z.append(eps + np.quantile(dr, percentile))
            if k == 0:
                query_string += "{} <= {}".format(label, z[k])
            else: 
                query_string += " and {} <= {}".format(label, z[k])

        countifs = len(data_validation.query(query_string))
        if countifs > 0: 
            ecdf[str(z)] = countifs / len(data_validation)
  
    ecdf = dict(sorted(ecdf.items(), key=lambda item: item[1]))

    # extract table with locations (ecdf argument) and ecdf values:
    #     - cosmetic change to return output easier to handle than ecdf 

    idx = 0
    arr_location = []
    arr_value = []
    for location in ecdf:
        value = ecdf[location]
        location = string_to_numbers(location)
        arr_location.append(location)
        arr_value.append(value)
        idx += 1

    print("\n")
    return(arr_location, arr_value)


n_nodes = 1000   # number of random locations in feature space, where ecdf is computed
reseed = False
if reseed:
   seed = 555
   np.random.seed(seed) 
arr_location1, arr_value1 = compute_ecdf(data_validation, n_nodes, adjusted = True)
arr_location2, arr_value2 = compute_ecdf(data_validation, n_nodes, adjusted = False)

#- [3.2] comparison: synthetic (based on training set) vs real (validation set)

def ks_delta(SyntheticData, locations, ecdf_ValidationSet):

    # SyntheticData is a dataframe
    # locations are the points in the feature space where ecdf is computed
    # for the validation set, ecdf values are stored in ecdf_ValidationSet
    # here we compute ecdf for the synthetic data, at the specified locations
    # output ks_max in [0, 1] with 0 = best, 1 = worst

    ks_max = 0
    ecdf_real = []
    ecdf_synth = []
    for idx in range(len(locations)):
        location = locations[idx]
        value = ecdf_ValidationSet[idx]
        query_string = ""
        for k in range(n_features):
            label = features[k]
            if k == 0:
                query_string += "{} <= {}".format(label, location[k])
            else: 
                query_string += " and {} <= {}".format(label, location[k])
        countifs = len(SyntheticData.query(query_string))
        synth_value = countifs / len(SyntheticData)
        ks = abs(value - synth_value)
        ecdf_real.append(value)
        ecdf_synth.append(synth_value)
        if ks > ks_max:
            ks_max = ks
        # print("location ID: %6d | ecdf_real: %6.4f | ecdf_synth: %6.4f"
        #             %(idx, value, synth_value))
    return(ks_max, ecdf_real, ecdf_synth)

df = pd.read_csv('telecom_synth_vg2.csv')
ks_max1, ecdf_real1, ecdf_synth1 = ks_delta(df, arr_location1, arr_value1)
ks_max2, ecdf_real2, ecdf_synth2 = ks_delta(df, arr_location2, arr_value2)
ks_max = max(ks_max1, ks_max2)
print("Test ECDF Kolmogorof-Smirnov dist. (synth. vs valid.): %6.4f" %(ks_max))

#- [3.3] comparison: training versus validation set

df = pd.read_csv('telecom_training_vg2.csv')
base_ks_max1, ecdf_real1, ecdf_synth1 = ks_delta(df, arr_location1, arr_value1)
base_ks_max2, ecdf_real2, ecdf_synth2 = ks_delta(df, arr_location2, arr_value2)
base_ks_max = max(base_ks_max1, base_ks_max2)
print("Base ECDF Kolmogorof-Smirnov dist. (train. vs valid.): %6.4f" %(base_ks_max))


#--- [4] visualizations

def vg_scatter(df, feature1, feature2, counter):

    # customized plots, subplot position based on counter

    label = feature1 + " vs " + feature2
    x = df[feature1].to_numpy()
    y = df[feature2].to_numpy()
    plt.subplot(3, 2, counter)
    plt.scatter(x, y, s = 0.1, c ="blue")
    plt.xlabel(label, fontsize = 7)
    plt.xticks([])
    plt.yticks([])
    #plt.ylim(0,70000)
    #plt.xlim(18,64)
    return()

def vg_histo(df, feature, counter):

    # customized plots, subplot position based on counter

    y = df[feature].to_numpy()
    plt.subplot(2, 3, counter)
    min = np.min(y)
    max = np.max(y)
    binBoundaries = np.linspace(min, max, 30)
    plt.hist(y, bins=binBoundaries, color='white', align='mid',edgecolor='red',
              linewidth = 0.3) 
    plt.xlabel(feature, fontsize = 7)
    plt.xticks([])
    plt.yticks([])
    return()

import matplotlib.pyplot as plt
import matplotlib as mpl
mpl.rcParams['axes.linewidth'] = 0.3

#- [4.1] scatterplots for Churn = 'No'

dfs = pd.read_csv('telecom_synth_vg2.csv')
dfs.drop(dfs[dfs['Churn'] == 0].index, inplace = True)
dfv = pd.read_csv('telecom_validation_vg2.csv')
dfv.drop(dfv[dfv['Churn'] == 0].index, inplace = True)

vg_scatter(dfs, 'tenure', 'MonthlyCharges', 1)
vg_scatter(dfv, 'tenure', 'MonthlyCharges', 2)
vg_scatter(dfs, 'tenure', 'TotalChargeResidues', 3)
vg_scatter(dfv, 'tenure', 'TotalChargeResidues', 4)
vg_scatter(dfs, 'MonthlyCharges', 'TotalChargeResidues', 5)
vg_scatter(dfv, 'MonthlyCharges', 'TotalChargeResidues', 6)
plt.show()

#- [4.2] scatterplots for Churn = 'Yes'

dfs = pd.read_csv('telecom_synth_vg2.csv')
dfs.drop(dfs[dfs['Churn'] == 1].index, inplace = True)
dfv = pd.read_csv('telecom_validation_vg2.csv')
dfv.drop(dfv[dfv['Churn'] == 1].index, inplace = True)

vg_scatter(dfs, 'tenure', 'MonthlyCharges', 1)
vg_scatter(dfv, 'tenure', 'MonthlyCharges', 2)
vg_scatter(dfs, 'tenure', 'TotalChargeResidues', 3)
vg_scatter(dfv, 'tenure', 'TotalChargeResidues', 4)
vg_scatter(dfs, 'MonthlyCharges', 'TotalChargeResidues', 5)
vg_scatter(dfv, 'MonthlyCharges', 'TotalChargeResidues', 6)
plt.show()

#- [4.3] ECDF scatterplot: validation set vs. synth data 

plt.xticks(fontsize=7)
plt.yticks(fontsize=7)
plt.scatter(ecdf_real1, ecdf_synth1, s = 0.1, c ="blue")
plt.scatter(ecdf_real2, ecdf_synth2, s = 0.1, c ="blue")
plt.show()

#- [4.4] histograms, Churn = 'No'

dfs = pd.read_csv('telecom_synth_vg2.csv')
dfs.drop(dfs[dfs['Churn'] == 0].index, inplace = True)
dfv = pd.read_csv('telecom_validation_vg2.csv')
dfv.drop(dfv[dfv['Churn'] == 0].index, inplace = True)
vg_histo(dfs, 'tenure', 1)
vg_histo(dfs, 'MonthlyCharges', 2)
vg_histo(dfs, 'TotalChargeResidues', 3)
vg_histo(dfv, 'tenure', 4)
vg_histo(dfv, 'MonthlyCharges', 5)
vg_histo(dfv, 'TotalChargeResidues', 6)
plt.show()

#- [4.5] histograms, Churn = 'Yes'

dfs = pd.read_csv('telecom_synth_vg2.csv')
dfs.drop(dfs[dfs['Churn'] == 1].index, inplace = True)
dfv = pd.read_csv('telecom_validation_vg2.csv')
dfv.drop(dfv[dfv['Churn'] == 1].index, inplace = True)
vg_histo(dfs, 'tenure', 1)
vg_histo(dfs, 'MonthlyCharges', 2)
vg_histo(dfs, 'TotalChargeResidues', 3)
vg_histo(dfv, 'tenure', 4)
vg_histo(dfv, 'MonthlyCharges', 5)
vg_histo(dfv, 'TotalChargeResidues', 6)
plt.show()
\end{lstlisting}


%-----------------------------------------------------------
\chapter{Fast Model-free Synthesizer with Hierarchical Bayesian Method}\label{chnogan2}

Deep learning models such as generative adversarial networks (GAN) require a lot of computing power, and are thus expensive. What if you could
 produce better data synthetizations, in a fraction of the time, with explainable AI and substantial cost savings?
 Very different from the tree-based NoGAN described in chapter~\ref{chnogan}, this new technology, abbreviated as NoGAN2, relies on resampling, an hierarchical sequence of runs, simulated annealing, and batch processing to boost
 performance, both in terms of output quality and time requirements. No neural network is involved. 

One of the strengths is the use of
 sophisticated output evaluation metrics for the loss function, and the ability to very efficiently update the loss function at each iteration, with a very small number of computations. In addition, default hyperparemeter values already provide good performance, making the method more stable
 than neural networks in the context of tabular data generation. It uses an auto-tuning algorithm,
 to automatically optimize hyperparameters via reinforcement learning. This capability helps you save a lot of time and money.

The purpose of this chapter is to show the spectacular performance of NoGAN2. One case study involves a dataset with 21 features, to predict student success based on college admission metrics. It includes categorical, ordinal and continuous features as well as missing values. Another case study is a telecom data set to predict customer attrition. Applications are not limited
 to data synthetization, but also include complex statistical inference problems. Finally, by contrast to most neural network methods, NoGAN2 leads to fully replicable results.

\section{Methodology}


 
The highly generic NoGAN2 technology described here is a powerful and better alternative to neural network synthetization methods for tabular data. It is very different from NoGAN described in chapter~\ref{chnogan}. Each offers its own benefits.
For reasons that will soon become obvious, I also call this new method \textcolor{index}{hierarchical deep resampling}\index{hierarchical deep resampling}, but I will use the word NoGAN2 for simplicity.
Again, it is designed to run much faster, compared to 
 training \textcolor{index}{generative adversarial networks}\index{generative adversarial networks} (GAN). Also, the quality of the generated data is far superior to almost all other products available on the market. 

Many evaluation metrics to measure faithfulness have critical flaws, sometimes rating synthetic data as excellent, when it is actually a failure, due to relying on low-dimensional indicators. 
This is especially noticeable on the circle dataset in~\cite{vgvendors}, where all GANs are evaluated as excellent, yet most
 fail to generate the correct distribution with points lying on two concentric circles.   As I did with NoGAN, here again I fix this problem using the full \textcolor{index}{multivariate empirical distribution}\index{empirical distribution!multivariate} (ECDF).  It produces much better evaluations. Performance is measured via  \textcolor{index}{cross-validation}\index{cross-validation} in all my examples. 

While the technique is not based on neural networks, it has several features that could also benefit GAN and other deep neural network architectures. 
 Indeed, NoGAN2 can be used as a sandbox to test various features before incorporating them into GAN and similar algorithms. For instance, testing 
 special loss functions, various hierarchical structures and batch processing, or auto-tuning hyperparemeters, is done a lot faster in my NoGAN2
 environment. NoGAN2 is also more intuitive and belongs to a set of methods referred to as \textcolor{index}{explainable AI}\index{explainable AI}.

\subsection{Base algorithm}\label{poireswds}

In a nutshell, the base algorithm is as follows, assuming you want to generate $n'$ synthetic observations, and $n$ is the number of observations in the training set:\vspace{1ex}

\begin{itemize}
\item[]{\bf Step 1}: Initial synthetization. For each feature separately, sample $n'$ values from the univariate \textcolor{index}{ECDF}\index{ECDF empirical distribution} (empirical distribution) computed on the training set, for the feature in question.
Put these values in a table, with one column for each feature, and $n$ rows total (the initial synthetic observations).
\vspace{1ex}
\item[]{\bf Step 2}: Deep resampling. Pick up one feature randomly, and pick up two rows randomly, from the table created in Step 1. Thus you have
 two values (possibly identical) for the feature in question. Swap these two values if doing so results in decreasing the 
\textcolor{index}{loss function}\index{loss function};  update the table accordingly. Repeat this step 
 until the loss function stops improving.
\end{itemize}\vspace{1ex}

\noindent Thanks to this design, the distribution attached to each feature is correctly replicated at all times during the synthetization process. This includes the means, variances, all statistical moments, percentiles, and all counts and proportions for each categorical variable.

The initial synthetization creates independent features with the correct univariate distributions, as observed in the training set. Then, the goal of Step 2 is to reconstruct
 the correct dependencies among the features via row shuffles within each feature separately. These shuffles, called \textcolor{index}{swaps}\index{swap}, preserve the separate empirical distributions generated in Step 1 at all times, without modifying them at all. In the end, Step 2 consists of keeping the correct univariate
 distributions, while reconstructing the full multivariate distribution attached to the training set. In the end, the algorithm performs massive permutations or recombinations to minimize the loss function. Thus, it is combinatorial in nature. Optimizing the loss function is done without \textcolor{index}{gradient descent}\index{gradient descent}. 

\subsection{Loss function}\label{ffgdfdsss}

The \textcolor{index}{loss function}\index{loss function} is a distance measuring the similarity between the synthesized data, and the training set. It is minimum and equal to zero when both sets have the same multivariate distribution. However, in the current version, the loss function does not directly compare the two multivariate ECDFs (synthetic data and training set). Instead, it uses proxy measurements to do this job indirectly, leading to extremely fast but approximate computations.

The proxy mechanism works as follows. Let's assume that you have $m$ features denoted as $X_1,\dots, X_m$ for the training set,
 and $X'_1,\dots, X'_m$ for the synthesized data. The functions $g_2(x), h_2(x)$ and so, in the algorithm below,  transform a vector (feature) into another vector with the same number of rows, element-wise. The star product is the element-wise product, also known as the 
\textcolor{index}{Hadamard product}\index{Hadamard product} [\href{https://en.wikipedia.org/wiki/Hadamard_product_(matrices)}{Wiki}].  Summations are over all the elements of the vector under the sum, not over $i,j$ or $k$. Now, here is how to define and compute the loss function:
\vspace{1ex}
\begin{itemize}
\item Compute 
  $Q_2[i,j] = \sum g_2(X_i) * h_2(X_j)$ for two-way interactions,  $Q_3[i,j,k] = \sum g_3(X_i)* h_3(X_j) * k_3(X_k)$ for three-way interactions, and so on, on the training set. Do it for all permutations of $1\leq i, j, k\leq m$.
\item Likewise, compute  $Q'_2[i,j] = g_2(X'_i) * h_2(X'_j)$ , $Q'_3[i,j,k] = g_3(X'_i) * h_3(X'_j) * k_3(X'_k)$, and so on, for the synthesized data under construction, at each iteration. 
\item Normalize the quantities so that they lie between $-1$ and $+1$. For instance, $Q_2[i,j]$ and $Q'_2[i,j]$ become
$$\rho_2[i, j] = \frac{\widetilde{Q}_2[i,j] - \text{E}[g_2(X_i)]\cdot \text{E}[h_2(X_j)]}{\sigma[g_2(X_i)] \cdot \sigma[h_2(X_i)]},\quad
\rho'_2[i, j] = \frac{\widetilde{Q}'_2[i, j] - \text{E}[g_2(X'_i)]\cdot \text{E}[h_2(X'_j)]}{\sigma[g_2(X'_i)] \cdot \sigma[h_2(X'_i)]},$$
where $$\widetilde{Q}_2[i, j] = \frac{Q_2[i, j]}{n}, \quad \widetilde{Q}'_2[i, j] = \frac{Q'_2[i, j]}{n'}.$$

\noindent Here $\sigma$ stands for the standard deviation. The means and standard deviations are computed respectively on the training set for $\rho_2[i,j]$, and on the synthesized data under construction for $\rho'_2[i,j]$. However they are computed only once: just after the initial synthetization obtained in Step 1. They will remain unchanged througout the Step 2 iterations. Also, $\rho_2[i,j]$ and $\rho'_2[i,j]$ are correlation coefficients, assuming categorical values are encoded as integers.
\item Typical functions are $g_2(x) = x$, denoted as $g_{21}(x)$, and $g_2(x) = x^2$, denoted as $g_{22}(x)$. Likewise for $h_2(x)$. In the current implementation, 
 there is no three-way interactions (the $g_3$ and $h_3$). 
From there, the partial distance functions for two-way interactions are defined as $\Delta_2[i,j] = |\,\rho_2[i, j] - \rho'_2[i, j]\,|$, for $1\leq i, j\leq m$.
 
If using four functions $g_{21}(x), g_{22}(x), h_{21}(x), h_{22}(x)$, then instead of $Q_2[i, j]$ we have %$Q_{21}[i, j]$ and $Q_{22}[i, j]$, defined as 
$$Q_{21}[i,j] = \sum g_{21}(X_i) * h_{21}(X_j), \quad Q_{22}[i,j] = \sum g_{22}(X_i) * h_{22}(X_j),$$
 where the sum is not over $i, j$ (assumed to be fixed), but over all the vector elements in each Hadamard product, with each element corresponding to a specific observation.
 Same for $Q'_2[i, j]$ and $\rho_2[i,j], \rho'_2[i, j]$, each broken down into two pieces. This leads to
$$\Delta_{21}[i, j] = |\,\rho_{21}[i, j] - \rho'_{21}[i, j]\,|, \quad \Delta_{22}[i, j]=|\,\rho_{22}[i, j] - \rho'_{22}[i, j]\,|.$$
\item Now let $\Delta_2[i, j] = \max(\alpha_1 \cdot\Delta_{21}[i, j], \, \alpha_2  \cdot\Delta_{21}[i, j])$ with $\alpha_1,\alpha_2\geq 0$ and 
 $\alpha_1 + \alpha_2 = 1$. If taking into account two-way interactions only, the \textcolor{index}{loss function}\index{loss function} $\Delta_2$ is the sum of 
 $\Delta_2[i, j]$ over all features $i, j$ with $i\neq j$. An alternative version $\widetilde{\Delta}_2$ consists in using the maximum rather than the sum.
\end{itemize}\vspace{1ex}

\noindent The goal was to design a loss function that maps one-to-one to the multivariate 
\textcolor{index}{Kolmogorov-Smirnov distance}\index{Kolmogorov-Smirnov distance} (KS) between synthesized and real data, as 
 KS is the perfect metric for evaluation purposes. We wanted the mapping to be continuous and order-preserving. By focusing on two-way interactions only, and breaking down $g_2, h_2$ into two components only
 ($g_{21}, g_{22}, h_{21}, h_{22})$, we get an approximate solution to this problem. But in practice, it works much better and faster than alternatives based on neural networks. In NoGAN3, I will use a different mapping, based on counts per bin, to incorporate the best from the NoGAN technique discussed in chapter~\ref{chnogan}. 

If $g_{21}(x) = h_{21}(x) = x$, then $\rho_{21}[i, j]$ is the correlation coefficient measured on the training set, between features $i$ and $j$.
 Here I assume that these features are continuous or 
\textcolor{index}{dummy  variables}\index{dummy variable} representing categories. Likewise, $\rho'_{21}[i, j]$ is the same quantity
 but measured on the synthesized data.
A future version will incorporate
\textcolor{index}{CramÃ©r's V}\index{CramÃ©r's V} coefficient: this is the standard metric to represent the association between arbitrary (non-binary) 
\textcolor{index}{categorical features}\index{categorical feature}.

Explaining the loss function is much easier to do in Python than English. So if the topic looks complicated, the Python code may clarify many points.
 The complexity is due to designing a very efficient architecture, so that tiny changes in the data require only tiny changes in the loss function.
 In doing so, I relied heavily on \textcolor{index}{tensors}\index{tensor}, yet without the need to define what it is, and in the code,
 without using \textcolor{index}{TensorFlow}\index{TensorFlow} or similar libraries.



\subsection{Hyperparemeters  and convergence}\label{oipvbc}

By contrast to GAN and related techniques, in the current implementation of NoGAN2, the loss function always decreases after swapping two values, albeit more and more slowly over time. It does not oscillate. Swaps become rarer and rarer over time, making progress towards the optimum extremely slow or even impossible after a while. Getting stuck is similar to the \textcolor{index}{vanishing gradient}\index{vanishing gradient} issue 
 [\href{https://en.wikipedia.org/wiki/Vanishing_gradient_problem}{Wiki}] in neural networks dealing with tabular data. However,
 the problem and side effects are typically less pronounced in NoGAN2. In section~\ref{daberikh}, I discuss how to reduce the risk of getting stuck too early. A future version will occasionally allow swaps even if they result in increasing the loss. It will be performed 
 using a \textcolor{index}{simulated annealing} schedule [\href{https://en.wikipedia.org/wiki/Simulated_annealing}{Wiki}], and result in an oscillating loss, with amplitudes decreasing over time, just like in a non-failing GAN. 

In practice, despite the combinatorial nature of the problem, a good solution is obtained once each value in the initial synthesized data has been proposed for a swap a couple of times. So the computational complexity is proportional to the number of observations to generate, multiplied by the
 number of features.  Generating small \textcolor{index}{batches} of observations separately (by splitting the initial synthesized data in a number of batches), can further improve performance. It is discussed
 in section~\ref{daberikh}.

The first version of NoGAN2 (now abandoned) was a step-wise procedure: you generate the second feature leaving the first feature unchanged, then the third feature leaving the first two features unchanged, and so on. Since optimizing the swaps one feature at a time is an 
 \textcolor{index}{assignment problem} [\href{https://en.wikipedia.org/wiki/Assignment_problem}{Wiki}], this approach allows you to use the \textcolor{index}{Hungarian algorithm} [\href{https://en.wikipedia.org/wiki/Hungarian_algorithm}{Wiki}]
to efficiently solve this combinatorial problem. However it resulted the inability to make improvements  after processing a few features. The final  synthetization had the first features very well replicated, and the other ones very poorly rendered.

In the current implementation, features are processed jointly rather than separately. Fine-tuning \textcolor{index}{hyperparameters}\index{hyperparameter} helps you accelerate the speed of convergence and avoid a local optima. I explain in section~\ref{daberikh} how the algorithm can \textcolor{index}{auto-tune}\index{auto-tuning} itself. Now I discuss the main hyperparameters: \vspace{1ex}

\begin{itemize}
\item The \textcolor{index}{seed}\index{seed (random number generator)} of the random number generator involved can have an impact on the final synthetization. You may try different seeds to improve the convergence or quality of the results.
\item The number of \textcolor{index}{batches}\index{batch} discussed earlier.
\item The weights $\alpha_1$ attached to $g_{21}, h_{21}$ and  $\alpha_2$ attached to $g_{22}, h_{22}$ in the loss function, see section~\ref{ffgdfdsss}.
\item The \textcolor{index}{loss function}\index{loss function} itself, especially the functions 
 $g_{21}, h_{21}, g_{22}, h_{22}$ and any additional functions that you may use. For instance, $g_{23}$ and $h_{23}$, then requiring the extra weight $\alpha_3$. 
\item When randomly picking up a feature to swap two values, by default, each feature has the same probability of being selected. You can change 
 these probabilities, selecting feature $i$ with probability $p_i$ instead. The probability vector $\text{P}=[p_1,\dots, p_m]$ is a core hyperparameter. 
 Here $m$ being the number of features.
\item If one of the probabilities is set to zero, then the feature in question will never be selected, and won't be updated. You can use
 this property as follows. Say you have three different groups of features, A B, and C.  For instance numerical features in A, ordinal features in B, and 
continuous features in C. Run NoGAN2 three times: first with non-zero probabilities in A only, then with non-zero probabilities in B only, then
 with non-zero probabilities in C only.
\end{itemize}\vspace{1ex}

\noindent In the last example, each subsequent run improves on the previous one, by adding features that haven't been processed yet. It is the reason
 why NoGAN2 is also called \textcolor{index}{hierarchical deep resampling}\index{hierarchical deep resampling}, by analogy to 
 \textcolor{index}{Bayesian hierarchical models}\index{Bayesian hierarchical models} [\href{https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling}{Wiki}]. In fact, in probability lingo, the three runs aimed at sampling from $P(A, B, C)$ by doing it
 first for $P(A)$, then $P(B|A)$, then $P(C|A, B)$, based on the fact that 
$P(A, B, C) = P(A) P(B|A) P(C | A, B)$.

The same idea can be used in traditional generative adversarial networks, leading to Hi-GAN, for \textcolor{index}{hierarchical GAN}\index{hierarchical GAN}. 
See for instance~\cite{Weifuge} and~\cite{nathieeewgan}. Finally if only one function $g_2(x) = h_2(x) = x$ is used in the loss function,  then NoGAN2
 is equivalent to the  \textcolor{index}{copula}\index{copula} technique discussed in section~\ref{piviiiurobvbc}.
In that case, starting with
 an initial synthetization with correct marginal distributions but zero cross-correlations, it reconstructs the correct correlation structure attached
 to the features.

\subsection{Acknowledgments}

I would like to thank 
 \href{https://www.linkedin.com/in/shakti-chaturvedi-49aab9106/}{Shakti Chaturvedi} for the numerous tests and research that he performed to compare 
the new technique proposed here,  with various generative adversarial networks. He brought the Telecom dataset to my attention, and tested improved versions of GAN and WGAN as well as vendor solutions and related methods. Earlier versions of the NoGAN2  code, along with WCGAN implementations,  are available as Jupyter notebooks on his GitHub repository,
 \href{https://github.com/shakti2594/Shared_Folder/blob/main/Main/DeepResampling_diabates.ipynb}{here}, illustrated on various datasets. 

I am also very grateful to \href{https://www.linkedin.com/in/rajivi/}{Rajiv Iyer} for turning the multivariate empirical distribution (ECDF) and related KS distance computations 
 into a production code Python library, available \href{https://pypi.org/project/genai-evaluation/}{here}. 
You can install it with \texttt{pip install genAI-evaluation}. See how I use it in section~\ref{pyityhohg}.
Rajiv also compared NoGAN2 with CTGAN on the student dataset. All comparisons are favorable to NoGAN2.


%-----------------------------------------------------------
\section{Case studies}\label{daberikh}

I synthesized two datasets. In the first one, observations consist of customer statistics from a Telecom company, to assess attrition rates by segment.
I used 4 features and 3500 observations. The feature ``total charges'' is highly correlated to ``tenure" (how many months the
 customer stayed with the company), and caused more problems when combined with ``monthly charges", visible only in higher dimensional plots. I first used GAN
 and Wasserstein GAN~\cite{ieeewgan}, with several data transforms and various tricks to improve the synthetization. In particular,
 I performed a \textcolor{index}{principal component analysis}\index{principal component analysis} (PCA) on the training set, then synthesized the transformed data, then applied the inverse PCA transform. The improvements, while substantial and discussed in chapter~\ref{failinggan}, were not
 good enough. It led to the creation of NoGAN discussed in chapter~\ref{chnogan}, and then NoGAN2 presented here. Both provide satisfactory results, in a short amount of time, and with little if any fine-tuning. 

The second dataset consists of student admission metrics. The outcome is the success rate at college. I used 21 features and 4400 observations, with a mix of categorical and numerical features, as well as missing data. The structure of the missing data is very simple. In this case, using a separate NoGAN2 for the 300 observations with missing values, is the easiest solution. For more scattered missing values, see how to do it in section~\ref{attrere}. 
 The goal here is not to impute missing values, but to replicate them correctly. For \textcolor{index}{imputation}\index{imputation (missing values)} done via synthetization, see the GAIN technique (a variant of GAN) in \cite{impugain}.  This dataset was first synthesized with CTGAN. Some
 improvement was obtained with NoGAN without  using the proper encoding for categorical variables. With proper encoding, the performance was increased by several orders of magnitudes. With NoGAN2, you get good performance (much better than GAN) without any encoding or data transform, in a fraction of the time required by neural networks methods, thus with considerable cost savings. 

All the tests involve splitting the real data into two parts: training and validation. The training set is used to generate the synthetic data, while the \textcolor{index}{validation set}\index{validation set} (also called \textcolor{index}{holdout}\index{holdout}) is used to assess its quality: how well it mimics the 
\textcolor{index}{joint empirical distribution}\index{empirical distribution!multivariate} (ECDF) observed in the validation set. To achieve this goal, I 
 implemented the powerful \textcolor{index}{Kolmgorov-Smirnov distance}\index{Kolmogorov-Smirnov distance} to compare the two multivariate ECDFs: training versus validation. Unlike most other distances in the marketplace, it does not miss high dimensional patterns; it will
 not mark a synthetization as good if it is a failure, undetected by classic evaluation metrics. See section~\ref{bulbdres}

Before diving into the actual synthetizations in sections~\ref{opewsd} and~\ref{opbg30}, I want to mention a few important points.
 First and obvious, do not check whether two identical values (in two different rows, for a specific feature) benefit from being swapped. Instead, look for different rows with distinct values. This will speed up the algorithm when dealing with numerous binary features such as dummy variables. Then,
 before fine tuning hyperparameters, read section~\ref{hphjkr}. Finally, to generate data outside the observation range, you need to stretch the Numpy
\textcolor{index}{quantile function}\index{quantile function} used in the algorithm, or write your own: it does not generate values below the observed minimum, or above the observed maximum. 
 See how to do it in~\cite{pcper43w}.

%-----------------------------------------------------------------------------------------------------------------------
\subsection{Synthesizing the student dataset}\label{opewsd}

The dataset is available on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/students.csv}{here}. I also use it in the Python code in section~\ref{pyityhohg}. In this section, I explain some techniques to accelerate performance, both in terms of speed and quality of the synthetization.
Figures~\ref{fig:nogan2ab} and~\ref{fig:nogan2abc} show the evolution of the loss function and cumulated number of swaps, respectively during the first and last 500k iterations.
 These iterations start after the initial synthetization, that is, after Step 1 in section~\ref{poireswds}.
An iteration consists of randomly selecting a feature, then randomly selecting two rows, and check whether or not the two values (one from each row) must be swapped. The swap takes place if it decreases the loss function. In that case, it increases the number of swaps by one.




\begin{figure}[H]
\centering
\includegraphics[width=0.86\textwidth]{nogan2A.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Number of swaps (left) and loss function (right) over time: first run}
\label{fig:nogan2ab}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[width=0.86\textwidth]{nogan2B.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Number of swaps (left) and loss function (right) over time: second run}
\label{fig:nogan2abc}
\end{figure}

The first mechanism consists in using \textcolor{index}{batches}\index{batch}. I first generate 2000 synthetic observations 
in the initial synthetization. Then I split the generated data into 8 subsets called batches. Resampling -- the swaps -- are performed one batch at a time, 
 swapping values internally from within a batch, without modifying the other batches. Because swaps tend to become more and more rare over time, 
 this technique accelerates convergence: this is noticeable in Figures~\ref{fig:nogan2ab} and~\ref{fig:nogan2abc}, where the loss function drops quickly at the beginning of each batch but then taper off. By using 8 batches rather than one, overall the loss function spends more time dropping sharply, than staying in a low entropy mode. It also keeps the number of swaps almost constant over time, rather than dropping to zero in late iterations. Bumps happen when moving from one batch to the next one. By contrast, Figure~\ref{fig:nofds2abcgff} (Telecom dataset) shows how slowly the loss function decreases after the initial drop, when using one batch only.

The initial synthetization (before starting deep resampling) correctly replicates each univariate distribution in the training set. Since deep resampling 
 does not modify at all the univariate distributions, all the histograms in Figure~\ref{fig:rt4nogan2abc} always look very good.
 Thus it is more important to focus on the scatterplots in Figure~\ref{fig:rt3nogan2abc}. They look good as well. Despite the 21 features, this data set is actually easy to synthesize, at least with NoGAN2. I used two runs, see part 8 in the code featured in section~\ref{pyityhohg}: the first run mostly for
 the numerical features, and the second run for to remaining features, conditionally to the first run. Thus we are not synthesizing the two sets of features
 separately, but jointly via a conditional (Bayesian) mechanism.
 Hyperparameter values are set to default, except for the weights. You may want to look at my choice for 
 the functions $g_{22}$ and $h_{22}$, respectively \texttt{g} and \texttt{h} in the code. 

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{std1.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Synthetization (left) vs validation set (right), student dataset}
\label{fig:rt3nogan2abc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{std2.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Synthetisation (top) vs validation set (bottom), student dataset}
\label{fig:rt4nogan2abc}
\end{figure}




Finally, a small number of observations have missing values, always for the same subset of features. I discarded these observations. In this case,
 it would be easy to run a separate NoGAN2 on these observations, after removing the features in question. Then putting back these features,
 where the value is always zero (missing) everywhere. The end result, after removing missing values, is  KS = 0.0651, and
 Base KS = 0.0405. This is within the range of what is considered a good synthetization. For a definition of KS and Base KS, see
 section~\ref{bulbdres}.




%Here is a little secret: I use

%Loss = Sum [ max(alpha1 * loss1, alpha2 * loss 2) ] . (A)

%The sum is over pairs of features. (actually with sum replaced by max for for circle data, making it a full-fledged min-max optimization problem). It works a lot better than

%Loss = Sum [ max(alpha1 * loss1 + alpha2 * loss 2) ]   (B)

%GAN uses something like (B) and I am wondering if using (A) in GAN could significantly improve performance.

%Then as I suggested using 3 terms in loss function, imagine a GAN with 3 terms: 1 generator, 2 discriminators. With alpha1, alpha2, alpha3, possibly automatically adjusted over epochs and batches via reinforcement learning.


% https://pypi.org/project/genai-evaluation/
% https://github.com/rajiviyer/genai_evaluation
% https://rajiviyer.github.io/genai_evaluation/api_reference/
% https://rajiviyer.github.io/genai_evaluation/user_guide/

%KS_symmetrical = (KS[real, synth.] + KS[synth., real] )/2

%---------------------------------------------------------------------------------------------------------------------------------------
\subsection{Synthesizing the Telecom dataset}\label{opbg30}

The Python code and dataset are on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/DeepResampling_telecom.py}{here}.
With 7000 observations and 4 features including a categorical one, the data is easy to synthesize with NoGAN2. However, 
 despite considerable efforts and testing various improvements, we were unable to produce synthetizations of the same quality with
 generative adversarial networks (GAN, WGAN and so on).
 Only NoGAN (chapter~\ref{chnogan}) matches the quality and speed of execution of NoGAN2. The details are in chapter~\ref{failinggan} entitled ``How to Fix a 
 Failing Adversarial Network''. In short, the problem arises when the  features ``TotalCharges'' and ``Tenure" are
 both present.
 The latter represents the number of months
 a customer has stayed with the company; it is highly correlated to the former. But even after decorrelating and scaling transforms, the GAN results, while significantly improved,
 are well below the performance of NoGAN2.

\begin{figure}[H]
\centering
\includegraphics[width=0.77\textwidth]{tc0.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Number of swaps (left) and loss function (right) over time}
\label{fig:nofds2abcgff}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[width=0.71\textwidth]{tc2.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Synthetisation with one term in loss function (left), vs real data (right)}
\label{fig:nofds2abchh}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.71\textwidth]{tc3.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Synthetisation with two terms in loss function (left), vs real data (right)}
\label{fig:nogrdf2abchh}
\end{figure}

With this dataset, only one run of deep resampling was needed, processing all features at once. However, unequal weights $\alpha_1 \neq \alpha_2$ in \texttt{weights}, combined with uneven values in the probability vector \texttt{hyperParam}, significantly improves the quality of
 the synthesized data. In particular, oversampling the problematic feature ``TotalCharges" really helps. This is done
 by setting \texttt{hyperParm[2]} to an unusually large value. Such fine-tuning is easy to automate, and intuitive. 

Finally, Figure~\ref{fig:nogrdf2abchh} shows why we need the two terms in the loss function, rather than just the first one. 
It significantly increases the quality, when compared to Figure~\ref{fig:nofds2abchh}. Interestingly, the functions
 used in the second term are $g_{22}(x) = h_{22}(x) = |\,x\,|$. It impacts correlations involving at least one feature with both positive and negative values:
 in this case, ``TotalChargeResidues", which is the decorrelated version of ``TotalCharges".
 Here, KS = 0.0379, and Base KS = 0.0176. As always, the univariate distributions are well rendered, see Figure~\ref{fig:nogrdf2abc}.



%----------
% xxxx update GitHub readme 
% xxx stretched percentiles: add to projects + Python library









\begin{figure}[H]
\centering
\includegraphics[width=0.71\textwidth]{tc_histo.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Synthetisation (top) vs validation set (bottom), Telecom dataset}
\label{fig:nogrdf2abc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.71\textwidth]{ci3.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Synthetization (left) vs validation set (right), circle dataset}
\label{fig:nohhys0oabc}
\end{figure}

%-------------------------------------------------------------------------------------------------------------------------------------
\subsection{Other case studies}

Besides the Telecom and education domains (the student data), I also explored datasets in the healthcare, insurance and cybersecurity industries. Finally,  I tested NoGAN2 on the challenging circle dataset, see Figure~\ref{fig:nohhys0oabc}. The Python code and datasets are on GitHub, 
 \href{https://github.com/VincentGranville/Main}{here}. Look for the documents starting with \texttt{DeepResampling} in the filename, for
 instance \texttt{DeepResampling\_circle.py}. These examples have both numerical and categorical features. Each one illustrates specific options 
 and hyperparameters. The peculiar cybesecurity case is discussed in the project textbook offered to participants in my 
Gen AI certification program, available \href{https://mltblog.com/3pWxvZK}{here}. The diabetes data also includes missing values, properly rendered by the synthetization.



The general conclusion is that NoGAN2 trains much faster than neural network equivalents, and consistently provide better results, when evaluated using the best distance in a cross-validation setting: the Kolmogorv-Smirnov distance (KS) based on the multivariate ECDF (joint empirical distribution). This evaluation metric is now available as an open-source Python library (\texttt{genAI-evaluation}). It captures all the interdependency patterns among the features. By contrast, distances currently used on the marketplace are not implemented in full multivariate mode. It frequently leads to false negatives: synthetizations rated as excellent, when they are actually very poor. This is magnified when synthesizing the circle dataset, as discussed in~\cite{vgvendors}.

 NoGAN2 requires less fine-tuning than GAN and other deep neural network techniques. Also, fine-tuning is straightforward, thanks to the explainable nature of the whole system. This allows for auto-tuning as discussed in see section~\ref{hphjkr}. In the end, for tabular data generation, the only real competitor to NoGAN2 is NoGAN explored in chapter~\ref{chnogan}, also developed in my laboratory. Both require very little bandwidth, significantly reducing costs while providing better results.

Finally, categorical features can benefit from being encoded as dummy variables, or from a loss function where standard correlations are replaced 
 by \textcolor{index}{CramÃ©r's V} [\href{https://mltblog.com/45VfElW}{Wiki}] or similar statistics~\cite{crame17}. Most examples also include a
 \textcolor{index}{label feature}\index{label feature} (the rightmost column in the dataset) to categorize each observation into various clusters. For instance, the label feature in the circle dataset indicates whether an observation belongs to the inner or outer circle in Figure~\ref{fig:nohhys0oabc}.
 Using a label feature significantly improves the performance. This is also true for GAN. The challenges with the circle data is the very small number of observations, combined with nearly duplicate features.


%https://towardsdatascience.com/create-your-own-python-package-and-publish-it-into-pypi-9306a29bc116

%do diabetes data include missing values
%xxx write mlt article / newsletter about PyPi





\subsection{Auto-tuning the hyperparameters}\label{hphjkr}

Before discussing \textcolor{index}{auto-tuning}\index{auto-tuning}, let's look at the correlation coefficients involved in  
the loss function, using the same notation as in section~\ref{ffgdfdsss}. For illustration purposes, see Table~\ref{tfgabtres} corresponding to the
 Telecom dataset~with 4 features. The table is organized as follows: \vspace{1ex}

\begin{itemize}
\item The first two columns are feature indices. The next three columns are associated to the first term in the loss function, and the rightmost three columns to the second term.
\item For each feature pair $\{i, j\}$, the correlations $\rho_{12},\rho_{21}$ are computed on the validation set, while
$\rho'_{12},\rho'_{21}$ are computed on the synthesized data. The metrics $\Delta_{12}, \Delta_{22}$ are the absolute difference between
 correlation coefficients. 
\item More specifically, the correlation coefficients are defined as
\begin{align}
\rho_{21}[i, j] & =\text{Corr}[\, g_{21}(X_i), h_{21}(X_j) \,], \nonumber \\
\rho'_{21}[i, j] & =\text{Corr}[\, g_{21}(X'_i), h_{21}(X'_j) \,], \nonumber\\
\rho_{22}[i, j] & =\text{Corr}[\, g_{22}(X_i), h_{22}(X_j) \,], \nonumber \\
\rho'_{22}[i, j] & =\text{Corr}[\, g_{22}(X'_i), h_{22}(X'_j) \,].\nonumber
\end{align}
Here $[X_1,\dots,X_m]$ and $[X'_1,\dots,X'_m]$ are respectively the validation and synthetic datasets. Each element represents a column. Also,  $g_{21}(x) = h_{21}(x) = x$ for the first term of the loss function. For the second~term,  I chose $g_{22}(x) = h_{22}(x) = | x |$ component-wise ($x$ is a vector).
\end{itemize}\vspace{1ex}


%\renewcommand{\arraystretch}{0.99999} %%%



The Python code for the Telecom data is available \href{https://github.com/VincentGranville/Main/blob/main/DeepResampling_telecom.py}{here}. The algorithm works best when the maximum values for $\Delta_{21}$ and $\Delta_{22}$ are similar. Here, the maximum is larger for
 $\Delta_{22}$. For optimization, increase $\alpha_2$ in the weight vector $\text{W} = [\alpha_1, \alpha_2]$, keeping $\alpha_1 + \alpha_2 = 1$. This will
 improve $\Delta_{22}$, but penalize $\Delta_{21}$.
Also, choose $g_{22}$ and $h_{22}$ so that the correlations $\rho_{21}, \rho_{22}$ in the first and second terms of the loss function are quite different, with $\rho_{22}$ not too close to zero. This is achieved by testing a few different functions, typically with $g_{22} = h_{22}$. To goal is to build a second term in the loss function, that
 brings significant improvements over using the first term alone.

The next hyperparameter is the probability vector $\text{P} = [p_1,\dots,p_m]$ with $p_1 + \cdots + p_m =1$, 
 also described in section~\ref{oipvbc}. It specifies how frequently 
 each feature is selected for a potential swap (swapping the values from two random rows, in the column corresponding to the feature in question). 
 In Table~\ref{tfgabtres}, the worst correlation discrepancies involve features $\{0, 2\}$ and $\{1, 2\}$. This suggests that feature 2 is
 more challenging. Increasing $p_2$ may reduce the error. However, it will penalize other features. To choose the optimum $p_2$, you can let the algorithm do the job, using an adaptive $p_2$ automatically fine-tuned over time to minimize the loss. The same applies to all the probabilities in P, as well as the
 two weight parameters in W.

Finally, the best improvements are obtained by running NoGAN2 twice. First, you split the set of features into two subsets A and B. In the first run, 
 you optimize the loss function for features in A. Then, in the second run, you optimize the full loss function: B conditionally to A, with synthetic values obtained for A in the first run, left unchanged. To implement this mechanism, an extra hyperparameter is needed, the flag vector
 $F = [q_1,\dots,q_m]$. In the first run, any $q_i$ set to zero forces feature $i$ to be ignored in the computation of the loss function. Also set $p_i$ to zero in that case. 
 In the code, P and F are represented respectively by \texttt{hyperParam} and \texttt{flagParam}. To decide which features to include in A, add one feature at a time until you hit a wall and the loss function gets stuck well above zero (convergence failure).  This process can be automated. 

\begin{table}[H]
\small
%\setlength\extrarowheight{-2pt}
\[
\begin{array}{|rr|rrr|rrr|}
\hline
i	&  j &  \rho_{21} & \rho'_{21}& \Delta_{21} & \rho_{22}& \rho'_{22} & \Delta_{22}\\ 
\hline
%\setlength\extrarowheight{2pt}
0&1&0.2486&0.2486&0.0000&0.2486&0.2486&0.0000\\
0&2&0.1225&0.1515&0.0290&0.7334&0.6708&0.0626\\
0&3&-0.3518&-0.3518&0.0000&-0.3518&-0.3518&0.0000\\
1&2&0.8186&0.7960&0.0226&-0.0137&-0.0091&0.0046\\
1&3&0.1815&0.1815&0.0000&0.1815&0.1815&0.0000\\
2&3&0.1229&0.1229&0.0000&-0.2830&-0.2830&0.0000\\
\hline
\end{array}
\]
\caption{\label{tfgabtres} Correlations in the loss function, Telecom dataset}
\end{table}
%\renewcommand{\arraystretch}{1.0} %%%



You can leverage the above mechanism to work with more than two terms in the loss function: to do this, use a different set of functions
 $g_{21}, h_{21}, g_{22}, h_{22}$ in the second run. If you implement this strategy, before the second run, you need to re-run
 \texttt{initialize\_cross\_products\_tables()}
 and \texttt{compute\_univariate\_stats()} in the Python code.
Increasing the number of terms in the loss function, may help. Because the loss function does not map one-to-one to the KS distance (it is only a proxy for KS),
 vastly different synthetizations may achieve zero loss, yet have a poor KS. In this case, the only way out is to use a better loss function, for instance with three terms rather than two. The NoGAN3 algorithm will address
 this issue by using bin counts rather than correlations, in the loss function.




\subsection{Evaluation with multivariate ECDF and KS distance}\label{bulbdres}

%what if alpha_1 = 1 us optimum


%-----------

The \textcolor{index}{multivariate empirical distribution}\index{empirical distribution!multivariate} (ECDF) and corresponding
 \textcolor{index}{Kolmogorov-Smirnov distance}\index{Kolmogorov-Smirnov distance} (KS) between the two ECDFs -- validation set vs synthesized data -- is described in details, along with the Python implementation, in chapter~\ref{chnogan}. It is now available as a Python library,
 named GenAI-Evalution, available \href{https://pypi.org/project/genai-evaluation/}{here}. In this section, I provide a brief overview.

The joint or {\em multivariate} ECDF has remained elusive to this day. It is a rather non-intuitive object, hard to visualize and handle even in two dimensions, let alone in higher dimensions with categorical features. For that reason, the 
 \textcolor{index}{empirical probability density function}\index{empirical density function} (EPDF) is more popular: it is associated to 
 \textcolor{index}{mixture models}\index{mixture model}, frequently embedded into neural networks,
  or the \textcolor{index}{Hellinger distance}\index{Hellinger distance} to evaluate the quality of synthesized data. However, the ECDF is more robust. Then, while the Hellinger distance also generalizes to multivariate EPDFs, in practice all the implementations are one-dimensional, with the distance computed for each feature separately.  The Hellinger equivalent based on ECDFs instead, is indeed the KS distance. It belongs to a class of measures known as 
 \textcolor{index}{integral probability metrics}\index{integral probability metrics}~\cite{vcxoi54, euclidf12}.

It is said that KS does not generalize to the multivariate case. Thus its 
 total absence in applications when the dimension is higher than too, despite the fact that it is the best metric to fully capture all 
the dependencies among features, especially the non-linear ones. 
 Asymptotics for the multivariate KS distance were investigated only recently~\cite{gtepouc21}, while an algorithm 
 for fast computation of the multivariate ECDF is discussed in~\cite{putrider22}. To my knowledge,
 the first practical implementation in high dimensions, tested on real datasets, is found in chapter~\ref{chnogan}. 


As a reminder, given a dataset and location $z = (z_1,\dots, z_m)$ in the feature space, the ECDF $F(z)$ evaluated at $z$ is defined as
 the proportion of observations $x = (x_1,\dots,x_m)$ 
 satisfying $x_i < z_i$ for $i=1,\dots,m$. Here $m$ is the number of features or columns, and $z_i$ is any observed value attached to
 feature $i$. The KS distance is then defined as
$$
\text{KS}(F_s, F_v) = \sup_{z} |F_s(z) - F_v(z)|,
$$
where $z$ is any location in the feature space, and $F_s, F_v$ are the ECDFs, respectively computed on the synthetic and validation set. 
In practice, $F_s,F_v$ are approximated using a number of locations called \textcolor{index}{interpolation nodes}\index{node (interpolation)}.
 These nodes are generated according to some stochastic process based on the quantiles of the original features. The goal is to guarantee
 that the nodes cover the sparse working area -- the tiny region of the feature space where the observations lie  -- with maximum efficiency.
 How small that area is, compared to the full potential feature space, depends to a large extent on the dimension: the number of features.



Try increasing \texttt{n\_nodes} (the number of nodes) from $10^3$ to $10^4$ and $10^5$, to see when KS stabilizes.  The scatter plot 
 in Figure~\ref{fig:nogan3h} is produced
 in part 10 in the code featured in section~\ref{pyityhohg}, while the value of \texttt{n\_nodes} is set in part 9.

Besides computing the KS distance between the synthetic and validation sets, it is useful to also compute KS between the training and validation sets. This distance is called ``Base KS" and named \texttt{KS\_base} in the code. Since the synthetization is based on training data only, if the training and validation sets are very different, you should expect that the synthesized data will not be great at mimicking the real data, outside the training set. 
 This occurs when not properly splitting the real data into training and validation sets. To summarize, the absolute difference between
 KS and Base KS may be the best indicator of faithfulness. When KS and Base KS are very similar, you may want to increase \texttt{n\_nodes} to
 get more accurate values for better discrimination.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{nogan3.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{ECDF scatterplot, synthetic vs validation set}
\label{fig:nogan3h}
\end{figure}

 
\section{Conclusion}

The context is tabular data generation. 
NoGAN2, by contrast to NoGAN and despite not being based on neural networks, shares a lot of properties with GAN. For instance, the two terms in the loss function fight against each other in a way similar to discriminator vs generator in GAN; I use weights to give each term a fair chance to win, as in \textcolor{index}{Wasserstein GAN}\index{Wasserstein GAN}, but with a min-max approach rather than averaging. \textcolor{index}{Batch processing}\index{batch} has its equivalent in neural networks, where it is called the same. The first layer in NN architecture corresponds to the first run in the resampling algorithm. Using a second run is similar to having a second (deep) layer in  \textcolor{index}{deep neural networks}\index{deep neural network}. The quality of the synthetization may be poor even if the \textcolor{index}{loss function}\index{loss function} reaches zero: this happens when the loss is not a good proxy to the full multivariate KS distance used to evaluate the quality; then it requires working with a different loss function. Or the algorithm may fail to reach a zero loss, getting stuck in a local minimum. In that case, you should change the loss function or the hyperparameters, or allowing the loss to go up and down with a general downward trend. This mechanism is sometimes referred to as 
\textcolor{index}{simulated annealing}\index{simulated annealing}~\cite{vgieee}. The fast computations, just like in GAN, are based on efficient use of tensors. Features causing problems in GAN (with high entropy or highly correlated to other features) cause similar problems in NoGAN2. It can be addressed using data transforms (called \textcolor{index}{transformers}\index{transformer} in LLM) such as feature decorrelation, scaling,  and PCA, followed by inverse transforms. All these properties make NoGAN2 a good sandbox to test and improve generative adversarial networks. 

Despite the similarities, there are major differences, besides the absence of \textcolor{index}{gradient descent}\index{gradient descent}. First, NoGAN2 is much faster, thought it can be tempting to use a large number of iterations, to increase the number of \textcolor{index}{swaps}\index{swap} (the analog of neuron \textcolor{index}{activation}\index{activation function}). It usually improves the results, although only marginally after a while. A better strategy is to use batches. Compared to GAN, the convergence issues are much less pronounced. Indeed, I managed to synthesize all my datasets rather quickly and with superior quality, consistently. NoGAN2 is also much more stable, and easier to fine-tune because of the intuitive nature of the hyperparameters. It illustrates \textcolor{index}{explainable AI}\index{explainable AI}. Because of this, it leads to 
\textcolor{index}{auto-tuning}\index{auto-tuning}, where fine-tuning is automated, possibly with \textcolor{index}{reinforcement learning}\index{reinforcement learning}, in little time. Ideally, you want to use the KS distance as your loss function, rather than a proxy mimicking 
 the approximation of a multivariate function by the first few terms of its Taylor series. The challenge is how to design an efficient architecture to
 achieve this goal. The upcoming NoGAN3 algorithm will solve this problem, using bin counts in the loss function (as in the NoGAN synthesizer 
 described in chapter~\ref{chnogan}), rather than correlations between transformed features.

Finally, NoGAN2 starts with an initial synthetization where all the features, taken separately, are perfectly replicated but lacking the cross-dependencies structure. It gives NoGAN2 a good head start, as all univariate statistical summaries are preserved throughout the algorithm. The deep resampling consists of reconstructing these interdependencies. If the loss function has one term only, then NoGAN2 is simply the 
\textcolor{index}{copula method}\index{copula}, implemented differently. The addition of a second term allows you to replicate not just feature correlations, but much more complex dependencies. A second or third run makes it an \textcolor{index}{hierarchical Bayesian model}\index{hierarchical Bayesian model}. 



\section{Python implementation}\label{pyityhohg}

The code in this section corresponds to \texttt{DeepResampling\_students.py} on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/DeepResampling_students.py}{here}. There are other examples in the same folder, all starting with \texttt{DeepResampling}, corresponding to
 other case studies, each featuring a different set of hyperparameters. There is also a Jupyter notebook, available 
 \href{https://github.com/VincentGranville/Notebooks/blob/main/DeepResampling.ipynb}{here}. The KS distance is computed using the
 GenAI-Evaluation library. You can install it like any other Python library.


\vspace{1ex}

\begin{lstlisting}
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import matplotlib as mpl
import genai_evaluation as ge
from matplotlib import pyplot
from statsmodels.distributions.empirical_distribution import ECDF
import warnings
warnings.simplefilter("ignore")


#--- [1] read data and only keep features and observations we want

def category_to_integer(category):
    if category == 'Enrolled':
        integer = 1
    elif category == 'Dropout':
        integer = 0
    else:
        integer = 2
    return(integer)

#- [1.1] read data

url = "https://raw.githubusercontent.com/VincentGranville/Main/main/students.csv"
# data = pd.read_csv('students_C2_full_nogan.csv')
data = pd.read_csv(url)
print(data)

# all features used here

features = [
    'Application_mode',                      #  0, categorical
    'Course',                                #  1, categorical
    'Previous_qualification_grade',          #  2, ordinal
    'Mother_qualification',                  #  3, categorical
    'Father_qualification',                  #  4, categorical
    'Mother_occupation',                     #  5, categorical
    'Father_occupation',                     #  6, categorical
    'Admission_grade',                       #  7, ordinal
    'Tuition_fees_up_to_date',               #  8, binary
    'Age_at_enrollment',                     #  9, ordinal [outliers]
    'Curricular_units_1st_sem_evaluations',  # 10, ordinal [0 = missing]
    'Curricular_units_1st_sem_approved',     # 11, ordinal [0 = missing]
    'Curricular_units_1st_sem_grade',        # 12, ordinal [0 = missing]
    'Curricular_units_2nd_sem_enrolled',     # 13, ordinal [0 = missing]
    'Curricular_units_2nd_sem_evaluations',  # 14, ordinal [0 = missing]
    'Curricular_units_2nd_sem_approved',     # 15, ordinal [0 = missing]
    'Curricular_units_2nd_sem_grade',        # 16, ordinal [0 = missing]
    'Unemployment_rate',                     # 17, ordinal
    'Inflation_rate',                        # 18, ordinal, can be < 0
    'GDP',                                   # 19, ordinal, can be < 0
    'Target'                                 # 20, categorical [outcome]
 ]

data['Target'] = data['Target'].map(category_to_integer) 

# remove rows with missing values
data = data[data['Curricular_units_1st_sem_evaluations'] != 0] 

print(data.head()) 
print (data.shape)
print (data.columns)

#- [1.2] set seed for replicability

pd.core.common.random_state(None)
seed = 106 ## 105
np.random.seed(seed)

#- [1.3] select features

data = data[features]
data = data.sample(frac = 1)  # shuffle rows to break artificial sorting

#- [1.4] split real dataset into training and validation sets

data_training = data.sample(frac = 0.5)
data_validation = data.drop(data_training.index)
data_training.to_csv('training_vg2.csv')
data_validation.to_csv('validation_vg2.csv')
data_train = pd.DataFrame.to_numpy(data_training) 

nobs = len(data_training)
n_features = len(features)


#--- [2] create initial synthetic data  

def create_initial_synth(nobs_synth):

    eps = 0.000000001
    n_features = len(features)
    data_synth = np.empty(shape=(nobs_synth,n_features))

    for i in range(nobs_synth):
        pc = np.random.uniform(0, 1 + eps, n_features)
        for k in range(n_features):
            label = features[k]
            data_synth[i, k] = np.quantile(data_training[label], pc[k], axis=0)
    return(data_synth)


#--- [3] loss functions Part 1

def compute_univariate_stats():

    # 'dt' for training data, 'ds' for synth. data

    # for first tem in loss function
    dt_mean  = np.mean(data_train, axis=0)
    dt_stdev = np.std(data_train, axis=0)
    ds_mean  = np.mean(data_synth, axis=0)
    ds_stdev = np.std(data_synth, axis=0)

    # for g(arr)
    dt_mean1  = np.mean(g(data_train), axis=0)
    dt_stdev1 = np.std(g(data_train), axis=0)
    ds_mean1  = np.mean(g(data_synth), axis=0)
    ds_stdev1 = np.std(g(data_synth), axis=0)

    # for f(arr)
    dt_mean2  = np.mean(h(data_train), axis=0)
    dt_stdev2 = np.std(h(data_train), axis=0)
    ds_mean2  = np.mean(h(data_synth), axis=0)
    ds_stdev2 = np.std(h(data_synth), axis=0)

    values = [dt_mean, dt_stdev, ds_mean, ds_stdev,
              dt_mean1, dt_stdev1, ds_mean1, ds_stdev1,
              dt_mean2, dt_stdev2, ds_mean2, ds_stdev2]
    return(values)

def initialize_cross_products_tables():

    # the core structure for fast computation when swapping 2 values
    # 'dt' for training data, 'ds' for synth. data
    # 'prod' is for 1st term in loss, 'prod12' for 2nd term

    dt_prod = np.empty(shape=(n_features,n_features))
    ds_prod = np.empty(shape=(n_features,n_features))
    dt_prod12 = np.empty(shape=(n_features,n_features))
    ds_prod12 = np.empty(shape=(n_features,n_features))

    for k in range(n_features):
        for l in range(n_features):
            dt_prod[l, k] = np.dot(data_train[:,l], data_train[:,k])  
            ds_prod[l, k] = np.dot(data_synth[:,l], data_synth[:,k])   
            dt_prod12[l, k] = np.dot(g(data_train[:,l]), h(data_train[:,k])) 
            ds_prod12[l, k] = np.dot(g(data_synth[:,l]), h(data_synth[:,k])) 
    products = [dt_prod, ds_prod, dt_prod12, ds_prod12]
    return(products)
    

#--- [4] loss function Part 2: managing loss function

# Weights hyperparameter:
#
#    1st value is for 1st term in loss function, 2nd value for 2nd term
#    each value should be between 0 and 1, all adding to 1
#    works best when loss contributions from each term are about the same

#- [4.1] loss function contribution from features (k, l) jointly

# before calling functions in sections [4.1], [4.2] and [4.3], first intialize
# by calling compute_univariate_stats() and compute_cross_products() before;
# this initialization needs to be done only once at the beginning

def get_distance(k, l, weights):

    dt_prodn = dt_prod[k, l] / nobs
    ds_prodn = ds_prod[k, l] / nobs_synth
    dt_r = (dt_prodn - dt_mean[k]*dt_mean[l]) / (dt_stdev[k]*dt_stdev[l])
    ds_r = (ds_prodn - ds_mean[k]*ds_mean[l]) / (ds_stdev[k]*ds_stdev[l])

    dt_prodn12 = dt_prod12[k, l] / nobs  
    ds_prodn12 = ds_prod12[k, l] / nobs_synth 
    dt_r12 = (dt_prodn12 - dt_mean1[k]*dt_mean2[l]) / (dt_stdev1[k]*dt_stdev2[l])
    ds_r12 = (ds_prodn12 - ds_mean1[k]*ds_mean2[l]) / (ds_stdev1[k]*ds_stdev2[l])

    # dist = weights[0]*abs(dt_r - ds_r) + weights[1]*abs(dt_r12 - ds_r12) 
    dist = max(weights[0]*abs(dt_r - ds_r), weights[1]*abs(dt_r12 - ds_r12)) 
    return(dist, dt_r, ds_r, dt_r12, ds_r12)
 
def total_distance(weights, flagParam): 

    eval = 0
    max_dist = 0
    super_max = 0
    lmax = n_features

    for k in range(n_features):
        if symmetric:
            lmax = k
        for l in range(lmax):
            if l != k and flagParam[k] > 0 and flagParam[l] >0: 
                values = get_distance(k, l, weights) 
                dist2 = max(abs(values[1] - values[2]), abs(values[3] - values[4]))
                eval += values[0]
                if values[0] > max_dist:
                    max_dist = values[0]
                if dist2 > super_max:
                    super_max = dist2
    return(eval, max_dist, super_max)

#- [4.2] updated loss function when swapping rows idx1 and idx2 in feature k
#        contribution from feature l jointly with k

def get_new_distance(k, l, idx1, idx2, weights):

    tmp1_k = data_synth[idx1, k]
    tmp2_k = data_synth[idx2, k]
    tmp1_l = data_synth[idx1, l]
    tmp2_l = data_synth[idx2, l]

    #-- first term of loss function

    remove1 = tmp1_k * tmp1_l
    remove2 = tmp2_k * tmp2_l
    add1 = tmp1_k * tmp2_l
    add2 = tmp2_k * tmp1_l
    new_ds_prod = ds_prod[l, k] - remove1 - remove2 + add1 + add2 

    dt_prodn = dt_prod[k, l] / nobs   
    ds_prodn = new_ds_prod / nobs_synth
    dt_r = (dt_prodn - dt_mean[k]*dt_mean[l]) / (dt_stdev[k]*dt_stdev[l])
    ds_r = (ds_prodn - ds_mean[k]*ds_mean[l]) / (ds_stdev[k]*ds_stdev[l])

    #-- second term of loss function

    remove1 = g(tmp1_k) * h(tmp1_l)
    remove2 = g(tmp2_k) * h(tmp2_l)
    add1 = g(tmp1_k) * h(tmp2_l)
    add2 = g(tmp2_k) * h(tmp1_l)
    new_ds_prod12 = ds_prod12[k, l] - remove1 - remove2 + add1 + add2       

    dt_prodn12 = dt_prod12[k, l] / nobs         
    ds_prodn12 = new_ds_prod12 / nobs_synth
    dt_r12 = (dt_prodn12 - dt_mean1[k]*dt_mean2[l]) / (dt_stdev1[k]*dt_stdev2[l])
    ds_r12 = (ds_prodn12 - ds_mean1[k]*ds_mean2[l]) / (ds_stdev1[k]*ds_stdev2[l])
 
    #--

    # new_dist = weights[0]*abs(dt_r - ds_r) + weights[1]*abs(dt_r12 - ds_r12)   
    new_dist = max(weights[0]*abs(dt_r - ds_r), weights[1]*abs(dt_r12 - ds_r12)) 
    return(new_dist, dt_r, ds_r, dt_r12, ds_r12)


#- [4.3] update prod tables after swapping rows idx1 and idx2 in feature k
#        update impacting feature l jointly with k

def update_product(k, l, idx1, idx2):  
    
    tmp1_k = data_synth[idx1, k]
    tmp2_k = data_synth[idx2, k]
    tmp1_l = data_synth[idx1, l]
    tmp2_l = data_synth[idx2, l]

    #-- first term of loss function

    remove1 = tmp1_k * tmp1_l
    remove2 = tmp2_k * tmp2_l
    add1 = tmp1_k * tmp2_l
    add2 = tmp2_k * tmp1_l
    ds_prod[k, l] = ds_prod[k, l] - remove1 - remove2 + add1 + add2
    ds_prod[l, k] = ds_prod[k, l] 

    #-- second term of loss function

    remove1 = g(tmp1_k) * h(tmp1_l)
    remove2 = g(tmp2_k) * h(tmp2_l)
    add1 = g(tmp1_k) * h(tmp2_l)
    add2 = g(tmp2_k) * h(tmp1_l)
    ds_prod12[k, l] = ds_prod12[k, l] - remove1 - remove2 + add1 + add2

    remove1 = h(tmp1_k) * g(tmp1_l)
    remove2 = h(tmp2_k) * g(tmp2_l)
    add1 = h(tmp1_k) * g(tmp2_l)
    add2 = h(tmp2_k) * g(tmp1_l)
    ds_prod12[l, k] = ds_prod12[l, k] - remove1 - remove2 + add1 + add2

    return()


#--- [5] feature sampling 

def sample_feature(mode, hyperParameter):
    
    # Randomly pick up one column (a feature) to swap 2 values from 2 random rows 
    # One feature is assumed to be in the right order, thus ignored

    if mode == 'Equalized': 
        u = np.random.uniform(0, 1)
        cutoff = hyperParam[0]
        feature = 0
        while cutoff < u:
            feature += 1
            cutoff += hyperParam[feature]
    else:
        feature = np.random.randint(1, n_features)  # ignore feature 0
    return(feature)


#--- [6] functions: deep synthetization, plot history, print stats 

#- [6.1] main function
 
def deep_resampling(hyperParameter, run, loss_type, n_batches, 
                    n_iter, nobs_synth, weights, flagParam, mode):
  
    # main function

    batch = 0
    batch_size = nobs_synth // n_batches  
    niter_per_batch = n_iter // n_batches
    lower_row = 0
    upper_row = batch_size
    nswaps = 0
    cgain = 0  # cumulative gain

    arr_swaps = []
    arr_history_quality = []
    arr_history_max_dist = []
    arr_time = []
    print()

    for iter in range(n_iter): 

        k = sample_feature(mode, hyperParameter)    
        batch = iter // niter_per_batch
        lower_row = batch * batch_size
        upper_row = lower_row + batch_size 
        idx1 = np.random.randint(lower_row, upper_row) % nobs_synth
        tmp1 = data_synth[idx1, k]
        tmp2 = tmp1
        counter = 0
        while tmp2 == tmp1 and counter < 20:  
            idx2 = np.random.randint(lower_row, upper_row) % nobs_synth
            tmp2 = data_synth[idx2, k]
            counter += 1

        g_param = 0.5
        h_param = g_param

        delta = 0
        delta2 = 0
        for l in range(n_features):  
            if l != k and flagParam[l] > 0: 
                values = get_distance(k, l, weights)
                delta += values[0] 
                if values[0] > delta2:
                    delta2 = values[0]
                if not symmetric:  # if functions g, h are different
                    values = get_distance(l, k, weights)
                    delta += values[0] 
                    if values[0] > delta2:
                        delta2 = values[0]

        new_delta = 0
        new_delta2 = 0
        for l in range(n_features): 
            if l != k  and flagParam[l] > 0: 
                values = get_new_distance(k, l, idx1, idx2, weights)
                new_delta += values[0] 
                if values[0] > new_delta2:
                    new_delta2 = values[0]
                if not symmetric:  # if functions g, h are different
                    values = get_new_distance(l, k, idx1, idx2, weights)
                    new_delta += values[0]
                    if values[0] > new_delta2:
                        new_delta2 = values[0]

        if loss_type == 'sum_loss':
            gain = delta - new_delta
        elif loss_type == 'max_loss':
            gain = delta2 - new_delta2
        if gain > 0: 
            cgain += gain
            for l in range(n_features):
                if l != k:
                    update_product(k, l, idx1, idx2) 
                    # update_product(l, k, idx1, idx2) 
            data_synth[idx1, k] = tmp2
            data_synth[idx2, k] = tmp1
            nswaps += 1

        if iter % 500 == 0: 
            quality, max_dist, super_max = total_distance(weights, flagParam)
            arr_swaps.append(nswaps)
            arr_history_quality.append(quality)
            arr_history_max_dist.append(max_dist)
            arr_time.append(iter)
            if iter % 5000 == 0:
                print("Iter: %6d    Distance: %8.4f    SupDist: %8.4f    Gain: %8.4f    Swaps: %6d"  
                        %(iter, quality, super_max, cgain, nswaps)) 

    return(nswaps, arr_swaps, arr_history_quality, arr_history_max_dist, arr_time)

#- [6.2] save synthetic data, show some stats

def evaluate_and_save(filename, weights, run, flagParam): 

    print("\nMetrics after deep resampling\n")
    quality, max_dist, super_max = total_distance(weights, flagParam)
    print("Distance: %8.4f" %(quality)) 
    print("Max Dist: %8.4f" %(max_dist)) 

    data_synthetic = pd.DataFrame(data_synth, columns = features)
    data_synthetic.to_csv(filename)
    print("\nSynthetic data, first 10 rows\n",data_synthetic.head(10))

    print("\nBivariate feature correlation:")
    print("....dt_xx for training set, ds_xx for synthetic data")
    print("....xx_r for correl[k, l], xx_r12 for correl[g(k), h(l)]\n")
    print("%2s %2s %8s %8s %8s %8s %8s" 
             % ('k', 'l', 'dist', 'dt_r', 'ds_r', 'dt_r12', 'ds_r12'))
    print("--------------------------------------------------")
    for k in range(n_features):
        for l in range(n_features):
            condition = (flagParam[k] >0 and flagParam[l] > 0) 
            if k != l and condition:
                values = get_distance(l, k, weights)
                dist = values[0]
                dt_r = values[1]    # training, 1st term of loss function
                ds_r = values[2]    # synth., 1st term of loss function
                dt_r12 = values[3]  # training, 2nd term of loss function
                ds_r12 = values[4]  # synth., 2nd term of loss function
                print("%2d %2d %8.4f %8.4f %8.4f %8.4f %8.4f" 
                       % (k, l, dist, dt_r, ds_r, dt_r12, ds_r12)) 
    return()

#- [6.3] plot history of loss function, and cumulated number of swaps

def plot_history(history):

    arr_swaps = history[1]
    arr_history_quality = history[2]
    arr_history_max_dist = history[3]
    arr_time = history[4]

    mpl.rcParams['axes.linewidth'] = 0.3
    plt.rc('xtick',labelsize=7)
    plt.rc('ytick',labelsize=7)
    plt.xticks(fontsize=7)
    plt.yticks(fontsize=7)
    plt.subplot(1, 2, 1)
    plt.plot(arr_time, arr_swaps, linewidth = 0.3)
    plt.legend(['cumulated swaps'], fontsize="7", 
        loc ="upper center", ncol=1)
    plt.subplot(1, 2, 2)
    plt.plot(arr_time, arr_history_quality, linewidth = 0.3)
    # plt.plot(arr_time, arr_history_max_dist, linewidth = 0.3)
    plt.legend(['distance'], fontsize="7", 
        loc ="upper center", ncol=1)
    plt.show()
    return()


#--- [7] initializations 

#- create intitial synthetization 

nobs_synth = 2000 
data_synth = create_initial_synth(nobs_synth)

#- specify 2nd part of loss function (argument is a number or array)

# do not use g(arr) = f(arr) = arr: this is pre-built already as 1st term in loss fct
# these two functions f, g are for the second term in the loss function

def g(arr):
    return(1/(0.01 + np.absolute(arr)))
def h(arr):
    return(1/(0.01 + np.absolute(arr)))  

symmetric = True # set to True if functions g and h are identical
# 'symmetric = True' twice as fast as 'symmetric = False'

#- initializations: product tables and univariate stats

products = initialize_cross_products_tables()
dt_prod   = products[0] 
ds_prod   = products[1] 
dt_prod12 = products[2] 
ds_prod12 = products[3]

values = compute_univariate_stats()
dt_mean   = values[0] 
dt_stdev  = values[1]
ds_mean   = values[2]
ds_stdev  = values[3]
dt_mean1  = values[4]
dt_stdev1 = values[5]
ds_mean1  = values[6] 
ds_stdev1 = values[7]
dt_mean2  = values[8] 
dt_stdev2 = values[9] 
ds_mean2  = values[10] 
ds_stdev2 = values[11]


#--- [8] deep resampling 

mode = 'Equalized'   # options: 'Standard', 'Equalized' 
eps2 = 0.0  

#- first run

run = 1
n_iter = 500001
n_batches = 8
loss_type = 'sum_loss'  # options: 'max_loss' or 'sum_loss' 
weights = [0.90, 0.10]  
hyperParam = np.zeros(len(features))
hyperParam[8]  = 1
hyperParam[10] = 1
hyperParam[11] = 1
hyperParam[12] = 1
hyperParam[13] = 1
hyperParam[14] = 1
hyperParam[15] = 1
hyperParam[16] = 1
hyperParam = hyperParam / np.sum(hyperParam)
flagParam = np.ones(len(features))  
history = deep_resampling(hyperParam, run, loss_type, n_batches, n_iter, 
                          nobs_synth, weights, flagParam, mode)
evaluate_and_save('synth_vg2.csv', weights, run, flagParam)
plot_history(history)

#- second run

run = 2
n_iter = 500001
n_batches = 8
loss_type = 'sum_loss'  # options: 'max_loss' or 'sum_loss' 
weights = [0.90, 0.10]  
hyperParam = np.ones(len(features))
hyperParam[8]  = 0
hyperParam[10] = 0
hyperParam[11] = 0
hyperParam[12] = 0
hyperParam[13] = 0
hyperParam[14] = 0
hyperParam[15] = 0
hyperParam[16] = 0
hyperParam = hyperParam / np.sum(hyperParam)
flagParam = np.ones(len(features))   
history = deep_resampling(hyperParam, run, loss_type, n_batches, n_iter, 
                          nobs_synth, weights, flagParam, mode)
evaluate_and_save('synth_vg2.csv', weights, run, flagParam)
plot_history(history)


#--- [9] Evaluation synthetization using joint ECDF & Kolmogorov-Smirnov distance

#        dataframes: df = synthetic; data = real data,
#        compute multivariate ecdf on validation set, sort it by value (from 0 to 1) 

print("\nMultivariate ECDF computations...\n")
n_nodes = 1000   # number of random locations in feature space, where ecdf is computed
seed = 50
np.random.seed(seed) 

df_validation = pd.DataFrame(data_validation, columns = features)
df_synthetic = pd.DataFrame(data_synth, columns = features)
df_training = pd.DataFrame(data_train, columns = features) 
query_lst, ecdf_val, ecdf_synth = ge.multivariate_ecdf(df_validation, df_synthetic, n_nodes, verbose = True) 
query_lst, ecdf_val, ecdf_train = ge.multivariate_ecdf(df_validation, df_training, n_nodes, verbose = True) 

ks = ge.ks_statistic(ecdf_val, ecdf_synth)
ks_base = ge.ks_statistic(ecdf_val, ecdf_train)
print("Test ECDF Kolmogorof-Smirnov dist. (synth. vs valid.): %6.4f" %(ks))
print("Base ECDF Kolmogorof-Smirnov dist. (train. vs valid.): %6.4f" %(ks_base))


#--- [10] visualizations (based on MatPlotLib version: 3.7.1)

def vg_scatter(df, feature1, feature2, counter):

    # customized plots, subplot position based on counter

    label = feature1 + " vs " + feature2
    x = df[feature1].to_numpy()
    y = df[feature2].to_numpy()
    plt.subplot(3, 2, counter)
    plt.scatter(x, y, s = 0.1, c ="blue")
    plt.xlabel(label, fontsize = 7)
    plt.xticks([])
    plt.yticks([])
    #plt.ylim(0,70000)
    #plt.xlim(18,64)
    return()

def vg_histo(df, feature, counter):

    # customized plots, subplot position based on counter

    y = df[feature].to_numpy()
    plt.subplot(2, 3, counter)
    min = np.min(y)
    max = np.max(y)
    binBoundaries = np.linspace(min, max, 30)
    plt.hist(y, bins=binBoundaries, color='white', align='mid',edgecolor='red',
              linewidth = 0.3) 
    plt.xlabel(feature, fontsize = 7)
    plt.xticks([])
    plt.yticks([])
    return()

mpl.rcParams['axes.linewidth'] = 0.3

#- [10.1] scatterplots 

dfs = pd.read_csv('synth_vg2.csv')
dfv = pd.read_csv('validation_vg2.csv')
vg_scatter(dfs, features[0], features[1], 1)
vg_scatter(dfv, features[0], features[1], 2)
vg_scatter(dfs, features[0], features[2], 3)
vg_scatter(dfv, features[0], features[2], 4)
vg_scatter(dfs, features[1], features[2], 5)
vg_scatter(dfv, features[1], features[2], 6)
plt.show()

#- [10.2] histograms

dfs = pd.read_csv('synth_vg2.csv')
dfv = pd.read_csv('validation_vg2.csv')
vg_histo(dfs, features[0], 1)
vg_histo(dfs, features[1], 2)
vg_histo(dfs, features[2], 3)
vg_histo(dfv, features[0], 4)
vg_histo(dfv, features[1], 5)
vg_histo(dfv, features[2], 6)
plt.show()
\end{lstlisting}



%----------------------------------------------------------

\chapter{Three Simple Optimization Techniques}\label{3opt}

You can use the techniques described in this chapter to speed up various machine learning algorithms. Reducing the sample size -- called data thinning here -- can produce better results in a fraction of the time.  Feature clustering allows you to split your columns into important versus non-predictive features. Ignoring non-predictive features or treating them separately is another way to boost performance. Finally, smart grid search allows you to  efficiently optimize hyperparameter values. These three techniques are particularly useful when dealing with slow algorithms, such as training geberative adversarial networks, and other neural networks in general. 

\section{Feature clustering}\label{fcvgfd}

I introduced \textcolor{index}{feature clustering}\index{feature clustering}, including the Python code, in section~\ref{dk6fb}. The goal here is
 to further investigate this method, focusing on the diabetes dataset.

Feature clustering is an unsupervised machine learning technique to separate the features of a dataset into homogeneous groups.
In short, it is a clustering procedure, but performed on the features rather than on the observations. Such techniques often
 rely on a similarity metric, measuring how close two features are to each other. In this section, I use the 
 absolute value of the correlation between two features. An immediate consequence is that the technique is scale-invariant: it does not
 depend on the units of measurement in your dataset. Of course, in some instances, it makes sense to transform the data using a logit or
 log transform prior to using the technique, to turn a multiplicative setting into an additive one.

The technique can also be used for traditional clustering performed on the observations. In that case, it is useful in the presence of 
wide data: when you have a large number of features but a small number of observations, sometimes smaller than the number of features
 as in clinical trials. When applied to features, it allows you to break down a high-dimensional problem (the dimension is the number of features), into a number of low-dimensional problems. It can accelerate many algorithms -- those with computing time growing exponentially  fast with the dimension -- and at the same time avoid issues related to the ``curse of dimensionality". In fact it can be used as a data reduction technique, where feature clusters with a low average correlation (in absolute value) are removed from the data set.





Applications are numerous. In my case I used it in the context of synthetic data generation, especially
 with generative adversarial networks (GAN). The idea is is to identify clusters of related features, and apply a separate GAN to each of them, 
 then put the synthetizations altogether back into one dataset. The benefits are faster processing with little to no loss in terms of capturing 
 the full correlation structure present in the data set. It also increases the robustness and explainability of the method, making it less volatile 
 during the successive epochs in the GAN model.
 For details, see section~\ref{dk6fb}.

 I summarize the feature clustering results in section~\ref{ioude}.
 I used the technique on a Kaggle dataset with 9 features, consisting of medical measurements. I offer two Python
 implementations: one based on hierarchical clustering, and one based on connected components (a 
 fundamental graph theory
 algorithm), see section~\ref{dk6fb}. In addition, the
 technique leads to a simple visualization of the 9-dimensional dataset, with one scatterplot and two colors: orange for diabetes 
 and blue for non-diabetes. Here diabetes is the binary response feature. 
%  one when the binary response is 1 (cancer
 % patients), and one when the response is 0 (non-cancer). 
This is because the largest feature cluster  contains only 3 
 features, and one of them is the response. In any well-designed experiment, you would expect the response to always be in
 a large feature cluster.

No linear algebra or calculus is required: the method is essentially math-free. This is in contrast to principal component analysis (PCA) which
 relies on eigenvalues, and turns your features into meaningless, arbitrary linear combinations that are hard to interpret.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{hcluster.png}  
\caption{Feature clustering using Scipy; Y-axis is $1-|\rho|$}
\label{fig:picbhggg2xspp}
\end{figure}

\subsection{Method and case study}\label{ioude}


The topic of \textcolor{index}{feature clustering}\index{feature clustering} is well covered in the literature, see for instance~\cite{fcnice}.  Here I provide simple methods. The first one  consists of finding the \textcolor{index}{connected components} 
\index{graph!connected components}\index{connected components} [\href{https://en.wikipedia.org/wiki/Component_(graph_theory)}{Wiki}] in the correlation matrix. The algorithm is a slight adaptation of a version used to detect
 connected components in nearest neighbor graphs. 
 Two features are connected if their correlation is above some parameter named \texttt{threshold} in the Python code.  A cluster of features is just a connected component of the 
 \textcolor{index}{undirected graph}\index{graph!undirected} in question. The second method uses 
\textcolor{index}{hierarchical clustering}\index{hierarchical clustering}. It requires only a few lines of code.  

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{fc4453.png}  
\caption{Correlation matrix for the medical dataset}
\label{fig:pihg}
\end{figure}

I applied the method to the medical dataset with \texttt{threshold=0.4}. See its  correlation matrix in Figure~\ref{fig:pihg}.
 Five feature clusters are
 detected: $\{0, 7\}, \{3, 5\}, \{1, 4, 8\}, \{2\}, \{6\}$ 
   in that order as seen in the dendogram in Figure~\ref{fig:picbhggg2xspp}.
For instance feature 0 corresponds to pregnancies, 1 to glucose, 2 to blood pressure, and so on,
 with feature 9 being the binary response: Outcome = 1 if patient has diabetes, or 0 otherwise. 



%It means, thanks to the low correlations between these 5 clusters, that a separate copula or GAN can be applied to each of them, thus splitting a 9D problem into a number a small-dimensional problems, each with a dimension no larger than 3. The Python code 
%\texttt{featureClustering.py} is also
 



The medical dataset can be found on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/diabetes.csv}{here}. I removed observations with missing values (encoded as 0 except for the response). A separate feature clustering step can be applied to these observations. Figure~\ref{fig:picbhggg2xs} shows the dendogram produced by the code in section~\ref{dk6fb}. The Y-axis 
 represents $1-|\rho|$, where $\rho$ is the correlation between 2 features. The 9 features are labeled 
 0 to 8, with the mapping between label and the feature name listed in Table~\ref{tabebbphuyfr}. The 5 clusters, with 2 of them consisting of a single feature, are listed in table~\ref{tabebbph12huy}. 



The scatterplot in Figure~\ref{fig:pihggf} illustrates the ``best" possible visualization of the 9-dimensional dataset, using just 3 features:
  the response or feature 8  (orange = diabetes, blue = no diabetes), the Glucose feature (X-axis) and the Insulin feature (Y-axis). 
 These 3 features belong to the largest feature cluster that also includes the response, as shown in Table~\ref{tabebbph12huy}.
 It provides a compact summary of the medical dataset. Each dot in the scatterplot represents an observation. 
 First, it is clear that diabetes and no-diabetes have very distinct locations. It makes discrimination relatively easy, given the reduction to a
 3-dimensional space. Of course, there is some overlapping, which would be less pronounced if you add more relevant features.
Then, the best predictors of diabetes are the two other features from the feature cluster in question: Glucose and Insulin.
In short, this feature cluster consists of highly inter-correlated features, with Glucose and Insulin also noticeably correlated. 

\begin{figure}[H]
\centering
\includegraphics[width=0.74\textwidth]{pic1.png}  
\caption{Scatterplot of feature 1 (horizontal axis) and 4 (vertical axis)}
\label{fig:pihggf}
\end{figure}

Finally, feature clustering via the correlation matrix is scale-invariant. You can also use this methodology for traditional clustering, by swapping features and observations. For instance, with 
\textcolor{index}{wide data}\index{wide data}, that is, a small number of observations (say less than $\num{10000}$) with a large number of features, as in clinical trials. 

\renewcommand{\arraystretch}{1.2} %%%
\renewcommand{\arraystretch}{1.2} %%%
\begin{table}[H]
%\small
\[
\begin{tabular}{cll}
\hline
 Code &  Feature name & Description  \\
\hline
\hline
 0 & Pregnancies & Number of pregnancies \\
 1 & Glucose & Plasma glucose concentration\\
 2 & BloodPressure &  Diastolic blood pressure in mm/Hg \\
 3 & SkinThickness &  Triceps skinfold thickness in mm\\
4 &  Insulin & Insulin in U/mL\\
5 &  BMI & Body mass index \\
6 &  DiabetesPedigreeFunction & Risk based on family history\\
7 & Age & Age of patient\\
8 & Outcome & Patient had diabetes  or not \\
\hline
\end{tabular}
\]
\caption{\label{tabebbphuyfr} Feature mapping table, medical dataset}
\end{table}

\renewcommand{\arraystretch}{1.2} %%%
\renewcommand{\arraystretch}{1.2} %%%
\begin{table}[H]
%\small
\[
\begin{tabular}{ccl}
\hline
 Cluster ID &  Features (codes) & Features (names)  \\
\hline
\hline
 1 & 0, 7 & Pregnancies, Age   \\
 2 & 3, 5 & SkinThickness, BMI  \\
 3 & 1, 4, 8 & Glucose, Insulin, Outcome  \\
 4 & 2  & BloodPressure  \\
 5 &  6 & DiabetesPedigreeFunction \\
\hline
\end{tabular}
\]
\caption{\label{tabebbph12huy} Feature clusters}
\end{table}

\section{Speed-up AI Training with Stochastic Thinning}\label{aithing}

Imagine a technique where you randomly delete as many as 80\% of your 
observations in the training set, without decreasing the predictive power (actually improving it in many cases), and reducing 
 computing time by an order of magnitude. In its simplest version, that's what stochastic thinning does. Here, performance improvement is measured outside the training set, on the validation set also called test data.
I illustrate this method on a real-life dataset, in the context of regression and neural networks. In the latter, 
 it speeds up the training stage by a noticeable factor. The thinning process applies to the training set,  and may involve multiple
 tiny random subsets called fractional training sets, representing less than 20\% of the training data when combined together. It can also be used
 for data compression, or to measure the strength of a machine learning algorithm.


I also show the potential limitations of the new technique, and introduce the concepts of {\em leading} or {\em influential} observations (those kept 
 for learning purposes) and {\em followers} (observations dropped from the training set). The word ``influential observations" should not be confused with its usage in statistics, although in both cases it leads to explainable AI.  The neural network used in this section
 offers replicable results by controlling all the sources of randomness, a property rarely satisfied in other implementations.



\subsection{Introduction}\label{derwas}

My interest in optimizing machine learning algorithms has been growing over time, both in terms of robustness, predictive power, simplicity and computing time. Section~\ref{smargsxxbv} on smart grid search (a method to accelerate grid search) 
 and section~\ref{fcvgfd} on feature clustering (to work with the most influential features), focus on this goal. My math-free gradient technique in chapter~\ref{gentil} is another example. 

More and more, my interest has shifted towards neural networks. This is also the case in the present article. But first, let me introduce the concept of \textcolor{index}{influential observations}\index{influential observation}. In some sense, it is the dual of 
 \textcolor{index}{influential features}\index{influential feature}, those obtained via 
 \textcolor{index}{feature clustering}\index{feature clustering} to reduce the number of features needed while achieving same performance. Besides reducing computing time, it helps better understand black box systems by focusing on a minimum set of features and observations that explain most of the output. Thus it is part of \textcolor{index}{explainable AI}\index{explainable AI}. 

In this section, a set of influential observations is a minimum set of observations that you can use, instead of the full training set, to
 achieve same performance in terms of predictive power, mean squared error or similar metrics measured on a 
\textcolor{index}{validation set}\index{validation set}, also called {\em test set}. In the case of multivariate regression with $m$ features,
 the set in question may contain as few of $m$ observations. Or even $m$ synthetic observations linearly independent and all located on the fitting hyperplane
 (fitting line in 2 dimensions). Finding a minimum set can be time consuming. Here I focus on methods that provide a good trade-off 
 between performance and computing time requirements. In practice, you can very quickly eliminate  80\% of you training set observations, and achieve even better predictions than with the full training set, with less overfitting.




\begin{table}[H]
%\[\
\begin{center}
\begin{tabular}{|c|l|c|l|}
\hline
 Label &  Name & Unit	& Description\\
\hline
\hline
0 & gender & ternary & male, female or child \\
1 &  length & mm & longest shell measurement \\
2 &  diameter & mm & perpendicular to length \\
3 &  height & mm & with meat in shell \\
4 &  whole weight & grams & whole abalone \\
5 &  shucked weight & grams &  weight of meat \\
6 & viscera weight &  grams & gut weight (after bleeding) \\
7 & shell weight &  grams &  after being dried \\
8 & rings count & integer & response (linked to age) \\
\hline
\end{tabular}
%\]
\caption{\label{choctres} Features attached to the abalone dataset}
\end{center}
\end{table}

I first introduce the abalone dataset used in my methodology, in section~\ref{treszx}. Then in section~\ref{linregd6}, 
 I illustrate how it works on regression.  Application to neural networks is discussed in section~\ref{nnflatulence}. Finally, Python
 implementations are offered in section~\ref{pygirleatmen}.

Stochastic thinning shows dramatic improvements in processing time especially for computationally intensive methods.  Reducing the size 
 of the training set in an $O(n^2)$ learning technique results in a reduction in computing time by a factor $5^2 = 25$. The examples discussed here are $O(n)$ where $n$ is the number of observations in the training set. The benefit is still substantial.  While the concept seems trivial, its implementation is not. Looking for ideal tiny subsets of the training set is easy to do if given enough time. The constraint is that all the time spent in identifying these tiny subsets must be very short, a lot shorter than the full, classic training stage. So not only all the
 tiny subsets combined must be very small compared to the full training set, but the time spent to identify them must also be very short.

The thinning process applies to the training set.  It involves working with multiple
 tiny random subsets called \textcolor{index}{fractional training sets}\index{fractional training set}, representing less than 20\% of the original training data when combined together. It can also be used
 for data compression, or to measure the strength of a machine learning algorithm. For instance, an algorithm that won't work well 
 no matter how  you reduce the training data by a factor 5, is more powerful than one that works well with just one tenth of the data in question. It means that the first agorithm is able to capture more patterns. It is thus superior to the second one, but at the same time more demanding in the amount of training data needed. Of course, the reduction factor is also a function of the complexity of the dataset itself. 

\subsection{The abalone dataset}\label{treszx}

Abalones are marine snails that can live 35 years or more. It is considered a culinary delicacy, but the species is at risk of extinction. 
 The goal is to predict the age of a specimen, based on 7 features. The dataset has 4177 observations, each representing an
 individual. The number of rings on the shell (feature 8) indicates the age: it is the response. The gender feature is ignored in this analysis. The remaining features (including the response) are described in Table~\ref{choctres}.



\noindent The dataset is split into two parts: training and test. The test (or validation set) represents one third of the observations. The same validation set is used everywhere, while only small portions of the training set are used, and compared to using the full training set. 
As one would expect, the features are cross-correlated and correlated with the response (age, determined by the number of rings), as shown in Table~\ref{choctreyur}. The rightmost column in the table is the response.


I uploaded the dataset on my GitHub repository, \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/thinning_abalone.csv}{here}. The original can be found \href{https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv}{here}, with the description
 \href{https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.names}{here}. See also Jason Brownlee's article
 ``Network Models for Combined Classification and Regression", \href{https://machinelearningmastery.com/neural-network-models-for-combined-classification-and-regression/}{here}. His methodology is based on the same dataset.

Given the strong predictive power of each feature, the question is whether combining them and applying a regression or neural network model can improve the predictive power of the best predictor alone, in this case feature 7 (shell weight). We shall see that the answer is positive. In this example, due
 to non-linear patterns, regression would also benefit from transforming the data first, in the same way that a logit transform sometimes provides better predictions. We shall see
 whether or not neural networks do a better job than regression, as they are usually better at dealing with non linear structures, present 
 in the abalone dataset. 

\begin{table}[H]
%%\[\
\begin{center}
\begin{tabular}{|c|ccccccc|c|}
\hline
  Label & 1 &         2 &         3 &         4 &         5 &         6 &         7 &         8 \\
\hline
\hline
1  &1.00  &0.98  &0.82  &0.92  &0.89  &0.90  &0.89  &0.55\\
2  &0.98  &1.00  &0.83  &0.92  &0.89  &0.89  &0.90  &0.57\\
3  &0.82  &0.83  &1.00  &0.81  &0.77  &0.79  &0.81  &0.55\\
4  &0.92  &0.92  &0.81  &1.00  &0.96  &0.96  &0.95  &0.54\\
5  &0.89  &0.89  &0.77  &0.96  &1.00  &0.93  &0.88  &0.42\\
6  &0.90  &0.89  &0.79  &0.96  &0.93  &1.00  &0.90  &0.50\\
7  &0.89  &0.90  &0.81  &0.95  &0.88  &0.90  &1.00  &0.62\\
8  &0.55  &0.57  &0.55  &0.54  &0.42  &0.50  &0.62  &1.00\\
\hline
\end{tabular}
%%\]
\caption{\label{choctreyur} Abalone dataset: feature correlation matrix}
\end{center}
\end{table}

\subsection{Stochastic thinning with fractional training sets}

The general principle is simple: use a proportion $p$ of your training set, instead of the full training set, to train your model. Typically,
  the sampling procedure depends on a parameter \texttt{seed} in your code. Different seeds yield different sub-samples called 
\textcolor{index}{fractional training sets}\index{fractional training set}. The seed is what initializes the random number generator. If you re-run the code twice with the same seed, you get the same fractional training sets, and the same results. Thus it also allows for replicability. 

The fractional training set eventually used for training is thus uniquely defined by the seed and the parameter $p$. In the following implementations, I try 10 different seeds for each $p$. For the parameter $p$, I tested 1\%, 2\%, 3\% and so on. When $p=100\%$, the fractional training set coincides with the original training set, regardless of the seed. The main loop is over $p$, with an inner loop 
 over the seed. At each iteration, two performance metrics are computed: the time required to train the algorithm given the size
 of the fractional training set (determined by $p$), and the quality of fit measured as the mean absolute error, mean squared error, or related metrics.  The quality of fit is measured on the validation (test) set, as in all cross-validation procedures.

\subsection{Linear regression}\label{linregd6}

I used the methodology first in a regression problem, on the abalone dataset. The full training set represents two thirds of the dataset, or about 2757 observations. The remaining third is used for validation purposes. Thus, with $p=1\%$, the size of the fractional training set is 
 about 28 observations. The absolute minimum necessary for regression with 8 features, is 8 observations. You would expect that with such a small set -- unless it was carefully built --  the quality of fit should be poor. Indeed it is.

For each $p$, I tried 10 different fractional training sets and computed the mean absolute error (MAE) for each of them. MAE is defined
  as the average difference between the predicted and observed value, in absolute value and computed on the test or validation set. I also
 computed MAE on the fractional training set. To distinguish between both, I use the notation 
 MAE$_\text{F}$ and MAE$_{\text{V,F}}$ respectively. The reason I use the notation MAE$_{\text{V,F}}$ for the validation set, instead 
 of MAE$_{\text{V}}$, is to emphasize the fact that it depends both on the validation set V and on the fractional training set F.

\subsubsection{Conclusions}

Results are displayed in Figure~\ref{fig:prqerw75}. For each $p$ between 1\% and 100\%, the average, minimum, and maximum   MAE$_{\text{V,F}}$ (respectively in blue, red, and grey) are 
 computed across 10 fractional training sets (one for each seed value).  The yellow curve represents the MAE$_{\text{V,F}}$ obtained
 by using the seed that provides the smallest MAE$_\text{F}$ among the 10 possible seed choices for a specific $p$. Thus, it 
 is the attainable minimum error in a blind experiment where you know nothing about the validation set, and all decisions are based solely 
 on results (quality of fit) obtained on the fractional training set in question. 

Obviously, as $p$ gets closer and closer to 1, the four curves merge. The striking  result is the fact that the best seed (measured using
  MAE$_\text{F}$ on the fractional training set for a specific $p$) always provides better predictions (measured on the validation set) than the full training set, for any $p$ between
 8\% and 55\% with a few exceptions. Beyond 55\%, fractional and full training sets yield about the same results. In practice, you 
generally need much fewer than 10 seeds to get satisfactory results, and pick up just one arbitrary $p$ around 10--15\%,  for a dataset like this one.

%--
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{mae_lr.png}  
\caption{MAE$_\text{V,F}$ on test set, using 10 different seeds for each $p$}
\label{fig:prqerw75}
\end{figure}

Now if you choose the fractional training set that yields the best MAE$_{\text{V,F}}$, any $p$ above 3\% will
 outperform using the full training set ($p=100\%$). Even $p=3\%$ is almost as good as $p=100\%$. However, I consider this approach as cheating: it is no longer a blind experiment as you use the validation set for further optimization. It may result in overfitting, even though this practice is widespread. 

One way to visualize the performance (predictive power) is to plot predicted versus observed values in the validation set, for a specific seed and $p$. The scatterplot in Figure~\ref{fig:prqerw76} does that for $p=100\%$, regardless of the seed. You can ignore the right plot for now, and focus on the left one. If the fit was perfect, all the red dots would be on the blue diagonal. Clearly, linear regression is not the best model here: predicted values tend to be too high when the observed value is small, and conversely. This can be fixed, but not with a linear adjustment: see section~\ref{impores} for details. Both fractional and full training sets exhibit the same issue. 

I did not include any scatterplot associated to good fractional training sets, because they are so similar to the one in Figure~\ref{fig:prqerw76} that the two would be indistinguishable to the naked eye. Except for one striking difference: the worst
 predicted value obtained with the full training set -- what looks like an outlier in the scatterplot -- is much closer to the blue diagonal when using a fractional training set with small $p$. This may explain why fractional training sets typically provide better MAE in this case. It is almost as if using the full training set results in overfitting.

Finally, since the features are strongly cross-correlated as shown in Table~\ref{choctreyur}, one would expect that
 many different sets of regression coefficients would yield the same near-optimum solution, making the regression
 coefficients meaningless. This is illustrated in Table~\ref{cho98treyur}, where the regression coefficients $\beta_1,\dots,\beta_7$
 match the corresponding features in Table~\ref{choctreyur}. The minimum error corresponds to the smallest fractional training set.

\begin{table}[H]
%\[\
\begin{center}
\begin{tabular}{c|c|rrrrrrr}
\hline
  $p$ & MAE$_{\text{V,F}}$ & $\beta_1$ &         $\beta_2$ &         $\beta_3$ &         $\beta_4$ &        $\beta_5$ &         $\beta_6$ &     $\beta_7$   \\
\hline
\hline
0.04	& 1.59601 & -8.55	& 23.91 & 0.23 &6.75 &-17.11&	-4.58	&9.82\\
0.06	& 1.60308&	-3.80	&9.68	&22.27	&5.51	&-16.56	&-2.79	&11.15 \\
0.09	&1.60748&	-2.20	&8.49	&16.37	&13.34&	-22.60	&-13.31&	2.55 \\
1.00	&1.61335&	-3.09	&12.85	&24.92&	9.33	&-20.52&	-10.10	&7.23 \\
\hline
\end{tabular}
%\]
\caption{\label{cho98treyur} Regression coefficients for selected fractional training sets}
\end{center}
\end{table}

\subsubsection{Potential improvement}\label{impores}

The left scatterplot in Figure~\ref{fig:prqerw76} shows some imbalance between predicted and observed values. Predicted values tend to be too high when the observed value is small, and conversely. I corrected this by re-calibrating the predicted values. This is equivalent to using a
 post-processing non-linear transform or mapping to the response. It is accomplished with the functions
\texttt{create\_pred\_correction\_table} and \texttt{adjusted\_prediction} in the Python code in section~\ref{pytrevea}. The result can be seen on the right plot in Figure~\ref{fig:prqerw76}. Now, the predicted values are properly balanced after the remapping, but the spread has somewhat increased. In the end, MAE is
 better, but barely.

\begin{figure}[H]
\centering
\includegraphics[width=0.79\textwidth]{scat3.png}  
\caption{Regression: observed vs predicted response (calibrated, right plot); diagonal = perfect fit} 
\label{fig:prqerw76}
\end{figure}

The correct solution is to apply the same mapping as a preprocessing step instead, before the regression. However, you don't know how to ``correct" the situation until after you have performed the regression: this is a catch-22. A workaround consists of computing the proper
 mapping after the regression, then apply it back to the response, then re-do the regression. Then update the mapping after the second regression, apply it to the previously mapped response, and do the regression one more time. You may repeat this process iteratively a few times assuming it converges.  

This is a work in progress, with big potential: it could be used in different contexts such as logistic regression. Instead of using a logit transform, the transform is dictated by the data itself, leading to a customized mapping that fits better to the data.

Finally, the correlation between predicted and observed values on the validation set is about 0.69 when using the full training set, and 0.71 
 when using one of the tiny fractional training sets. By contrast, the correlation between the response (age) and the best
 predictor (shell weight) is 0.62. I expect the method to face more challenges when the number of features is large, possibly requiring feature selection via my feature clustering algorithm. Likewise, if the dataset has some unusual small buckets or abnormal observations, it may be wise
 to over-sample these observations: in short, applying a different $p$ depending on the data segment.





\subsection{Time series}

The methodology also applies to stationary time series $\{X_t\}$ that are modeled using auto-regressive processes such as ARMA. When trend and seasonality have been removed, it is possible to work with a fraction of the training set. If your predicted value for $X_t$ depends on $X_{t-1},\dots,X_{t-q}$, then your fractional training set consists of a small number of blocks, each block containing
 $q+1$ consecutive observations. The blocks can be randomly spread over time, but ideally you want them to not overlap.

Another example is interpolation. For instance, I looked at some ocean tide dataset (Dublin, Ireland), with 5-min increments. It turned out that using 80-min observations was enough to reconstruct the whole time series with almost no error. It also removed a few irregularities such as chaotic spikes, that look like noise. The details are in chapter~\ref{chapterInterpol}. In this case, using only 6.25\% of the training set was more than enough. A generalization to two-dimensional spatial data is also discussed in the same chapter, with a real-world example:  the Chicago temperatures dataset.

\subsection{Neural networks}\label{nnflatulence}

It is a lot more difficult to provide a general solution that works in all cases for neural networks. However, I provide a few general guidelines.
 Clearly, my neural network implementation outperforms linear regression, no matter how you look at it. Also, fractional training sets
 work just as well as the full training set, assuming $p$ is not too small. While $p=20\%$ did well with linear regression, here you need either to increase to $p=40\%$, or work with $p=30\%$ and increase the number of \textcolor{index}{epochs}\index{epoch (neural networks)} from (say) 150 to above 300, for the abalone dataset. This in turns increases the amount of computing time required.

An epoch is a full run of the fractional training set F. The number of epochs -- also called iterations -- ranges from a few hundreds to several thousands in most neural networks. Let $\tau$ be the number of epochs, divided by~150. The time it takes to train the model on my laptop is well approximated by the quadratic formula
$$t = a + b \tau + c  p + d  \tau p,$$
where $a=0.2, b = 1.1, c = 5, d = 3$
and  $t$ measured in seconds. Remember that $p$ is the proportion of the original training set used to train the model. 
 The reduced training set is called the fractional training set and denoted as F. A smaller $p$ typically requires a larger $\tau$. 

\begin{table}[H]
%\[\
\begin{center}
\scalebox{0.9}{
\begin{tabular}{|c|cccr|cccr|cccr|}
\hline
    \multirow{2}{*}{} &
      %\multicolumn{1}{c|}{ } &
      \multicolumn{4}{c|}{ epochs = 150} &
      \multicolumn{4}{c|}{ epochs = 300} &
      \multicolumn{4}{c|}{ epochs = 450} \\
%\hline
  $p$ & seed & MAE$_{\text{V,F}}$ & MAE$_\text{F}$ & $t$ & 
 seed & MAE$_{\text{V,F}}$ & MAE$_\text{F}$ & $t$ & 
 seed & MAE$_{\text{V,F}}$ & MAE$_\text{F}$ & $t$ \\
\hline
\hline
0.1  & 105 & 1.7504 & 1.5207 & 2.37 & 	105&1.6589 & 1.4457 & 4.10 & 		105&1.6063 & 1.4192 & 5.78\\			
0.2  & 104 & 1.6432 & 1.5854 & 3.53 & 	104&1.6126 & 1.5092 & 6.47 & 		104&1.6016 & 1.5115 & 9.37\\			
0.3  & 105 & 1.6359 & 1.5424 & 4.89 & 	105&1.5753 & 1.5081 & 8.87 & 		105&1.5544 & 1.4740 & 13.00\\			
0.4  & 105 & 1.5819 & 1.5338 & 5.87 & 	107&1.5290 & 1.5073 & 11.00 & 	105&1.5636 & 1.5010 & 16.28\\			
0.5  & 105 & 1.5741 & 1.5210 & 6.89 & 	105&1.5529 & 1.4752 & 13.79 & 	105&1.5107 & 1.4710 & 30.47\\			
0.6  & 107 & 1.5663 & 1.5372 & 13.76 & 	105&1.5161 & 1.4964 & 18.99 & 	107&1.5168 & 1.4796 & 25.32\\			
0.7  & 103 & 1.5269 & 1.5502 & 10.46 & 	107&1.5160 & 1.5238 & 20.13 & 	107&1.4969 & 1.4992 & 27.94\\			
0.8  & 107 & 1.5553 & 1.5369 & 11.84 & 	107&1.5004 & 1.5132 & 22.39 & 	107&1.4913 & 1.4922 & 36.85\\			
0.9  & 107 & 1.5194 & 1.5488 & 13.43 & 	107&1.5260 & 1.5199 & 25.87 & 	107&1.4887 & 1.5175 & 48.89\\			
1.0  & 103 & 1.5439 & 1.5323 & 14.62 & 	107&1.4949 & 1.5192 & 26.35 & 	107&1.5011 & 1.5032 & 40.20\\	
\hline		
\end{tabular}
}
%\]
\caption{\label{ct121eur} Performance of neural network with best seed ($t$ is computing time)}
\end{center}
\end{table}

The results are very sensitive to the choice of $p,\tau$ and the seed that initializes all random 
number generators involved in the method. Each combination leads to a different local minimum of the MAE$_\text{F}$ function. Convergence is stochastic: after a few hundred epochs, MAE$_\text{F}$  oscillates within a stable range of low values. So you can stop as soon as MAE$_\text{F}$  has stabilized, given a specific seed and $p$. Whether working with the full training set ($p=100\%$) or a fractional part ($p<100\%$), it is thus a good idea to test 5 different seeds, and for each seed, stop as soon as MAE$_\text{F}$  stops improving.  

To summarize, you may use $p=35\%$, test 5 seeds and for each one, 
 test $\tau = 2$ or $3$ to find the minimum MAE$_\text{F}$. This sounds to be the fastest method, 
 leading to results very similar to those obtained with more epochs or a larger $p$. 
Again, MAE$_\text{F}$ stands for the mean absolute error measured on the fractional training set F. It is denoted as \texttt{error\_train} in the code
 in section~\ref{crottalbourrique}. It is a function of $p$ (which governs F), the seed, and the number of epochs, everything else (like the learning rate) being constant. 

Finally, given the redundancy among the features as shown in Table~\ref{choctreyur}, reducing the number of predictive features
 from 7 to 4 or 3 using \textcolor{index}{feature clustering}\index{feature clustering}, is another way to slash the computing time by a factor 2, probably with minimum loss in predictive
 power. 

\subsubsection{Conclusions}

Table~\ref{ct121eur} shows MAE$_{\text{V,F}}$ (measured on the validation set) for various $p$, when using 150, 300, or 450 epochs. In each case 5 seeds were tested, and only the one with the minimum MAE$_\text{F}$ (measured on the training set) is shown in the table. They are clearly better (lower) than the best MAE$_{\text{V,F}}$ obtained via regression and shown in Figure~\ref{fig:prqerw75}. The correlation between predictive and observed values on the validation set is also better, topping $0.74$, up from $0.71$ for the regression method with the best fractional training set. 






Perhaps the best result for neural networks, when using fractional training sets, is obtained with 300 epochs and $p=40\%$. Similar quality is obtained with 150 epochs and $p=70\%$, or 450 epochs and $p=30\%$. The improvements using neural networks over regression are real and not minuscule, but they are not
 very significant either. 

When comparing the left scatterplot in Figure~\ref{fig:prqerw76} with Figure~\ref{fig:pry998rw76}, we are still facing the same problem we had with regression:
  low values of the response get a predicted value that is too high, and conversely.  As discussed in section~\ref{impores}, it can be addressed by first transforming 
 the data to recalibrate the values, before running any model, be it regression or neural networks. However, with neural networks, the point cloud is more concentrated around the blue diagonal which represents perfect fit. The outlier dot at the top in Figure~\ref{fig:prqerw76} is now much closer 
to the diagonal in Figure~\ref{fig:pry998rw76}, but the same improvement was also achieved with regression using a fractional training set with $p<20\%$.


Figure~\ref{fig:lo09bre76} illustrates the convergence of the neural network over successive epochs (up to 450) starting with three different seeds and $p=40\%$.
The left plot shows apparent convergence to a same minimum of MAE$_{\text{F}}$  in all cases. Remember that the neural network is just a 
\textcolor{index}{gradient descent}\index{gradient descent} algorithm [\href{https://en.wikipedia.org/wiki/Gradient_descent}{Wiki}] to minimize a function -- in this case MAE$_{\text{F}}$ -- that depends on a very large number of variables: the weights attached to the connections between neurons. 



\begin{figure}[H]
\centering
\includegraphics[width=0.57\textwidth]{nn_max2.png}  
\caption{Neural network: observed vs predicted response when MAE$_{\text{V,F}}\approx 1.52$} 
\label{fig:pry998rw76}
\end{figure}

\noindent However, if you look more closely by zooming in (right plot in Figure~\ref{fig:lo09bre76}), then the following become apparent: \vspace{1ex}
\begin{itemize}
\item Depending on the seed, you may end up getting stuck circling around some local minimum or even the global minimum if there is one, but in a sub-optimum position. 
Of course there are many other parameters that you can fine-tune to improve the situation, for instance
 changing the \textcolor{index}{learning rate}\index{learning rate} [\href{https://en.wikipedia.org/wiki/Learning_rate}{Wiki}] over time. I did not explore these options; instead I used different starting points (the seeds).
\item Once an equilibrium is reached (or even before), there are still significant variations in MAE$_{\text{F}}$ over successive epochs. It makes sense to stop as soon as a solid low -- a minimum that seems unbreakable -- is reached. This considerably reduces the number of epochs needed, and thus the computing time. 
\item For the abalone dataset, seed 107 has consistently been a winner in my experiment with neural networks, and it shows in the plot. It is as if there is an easier path between this
 initial configuration, and a strong minimum. Indeed, you may go beyond 450 epochs and probably continue to reduce MAE$_{\text{F}}$. However the price to pay is more computing time. When first analyzed with neural networks, the original author set the number of epochs to 150 and used only one seed: 
see \href{https://machinelearningmastery.com/neural-network-models-for-combined-classification-and-regression/}{here}.
\end{itemize}\vspace{1ex}
There are few results about the theoretical convergence of neural network implementations. I proved convergence for a related algorithm in computer vision in 1996~\cite{vgieee}, in a similar context: simulated annealing. This obscure article was cited hundreds of times in the last few years, probably due to its applicability 
 to neural networks. The proof is based on Markov chains with millions of states, transitioning from one state to another during successive epochs until reaching an equilibrium distribution.
 Thus the convergence is stochastic. The same stochastic convergence can be observed in my fuzzy clustering algorithm based on image filtering techniques.
 You can see it in action, in a video posted \href{https://www.youtube.com/shorts/MKpQimCFfHA}{here}. The details are in my book on 
stochastic processes~\cite{vgsimulnew}. Again, it goes through a warm-up stage before exhibiting stable oscillations in equilibrium mode. 



\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{history.png}  
\caption{Neural network: MAE$_{\text{F}}$ over successive epochs (3 different seeds, $p=40\%$)} 
\label{fig:lo09bre76}
\end{figure}

There are still three important points that I want to mention. First, optimizing MAE$_{\text{F}}$ on the training set or fractional training set F does not always lead to the best MAE$_{\text{V,F}}$ when computed on the test or validation set V. Sometimes, relatively modest performance on the training set leads to excellent results in the test set.
 Then, trying to optimize as much as possible should be avoided as it leads to overfitting. Thus the decision to work only with 5 seeds, and stop at epoch 300 even if significantly better results are obtained at epoch 287 or 305. Finally, in case of pure convergence (as opposed to stochastic convergence), you should not expect to reduce the error by a factor 2 if you double the number of epochs.
 The stochastic nature of the processes involved means that the error is (on average) proportional to the square root of the number of epochs.



\subsubsection{Jump-starting neural networks with regression} 

Neural networks work better than regression on the full training set. But do they also work better on the best fractional training set used for regression? If this was the case,
  you would expect to outperform regression with the tiny training set in question, and perhaps get results comparable to the best obtained so far, while working
 with a much lower $p$. It turns out that performance is not as good on such small fractional training sets, and even worse than regression. But I only did one test. My guess is that you won't obtain great results with neural networks if $p<30\%$ unless you carefully choose your fractional training set, or test a larger number of seeds. 

Given the amount of time needed to train a neural network, it could make sense to explore how to reduce the training set as much as possible, using techniques
 more sophisticated than randomly discarding observations. This may be the topic of future research. 



\subsection{Python code}\label{pygirleatmen}

Section~\ref{pytrevea} deals with regression, while section~\ref{crottalbourrique} deals with neural networks.  The core of the method is similar in both cases:
  a main loop over $p$, with an inner loop over selected seeds. Neural networks have an extra loop over the number of epochs.  The goal,
  given $p$, is to test different seeds to find which one provides the best mean absolute error MAE$_\text{F}$ on the fractional training set F determined by $p$. 
  Then compute the corresponding MAE$_\text{V,F}$ on the validation or test set V. The set V remains fixed in all cases, and identical both for regression and
 neural networks.

The fractional training set corresponds to \texttt{X\_train\_small} and \texttt{y\_train\_small} in the code: the latter represents the response, the former the independent features.
 The full, original training set is \texttt{X\_train} and \texttt{y\_train}, respectively for the independent features and the response.
Likewise, the fixed validation set corresponds to \texttt{X\_test} and \texttt{y\_test}. Predicted values are denoted as 
\texttt{y\_pred\_test} and \texttt{y\_pred\_train}, depending on whether the predictions are for the fractional (training) or validation set. The mean square error for the training set is \texttt{error\_train}, and for the test set it is \texttt{error\_test}. 


\subsubsection{Stochastic thinning for regression}\label{pytrevea}

The code is also on GitHub, \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/thinned_regression.py}{here}. In addition to the
 tasks listed at the beginning of section~\ref{pygirleatmen}, the program also computes some summary statistics, generates scatter plots, and recalibrates the
 observations if desired. These different parts are easy to identify thanks to the documentation inside the code.\vspace{1ex}

\begin{lstlisting}
import time
import numpy as np
import pandas as pd
from pandas import read_csv
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn import preprocessing, svm, metrics
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# initialize plot parameters for nicer plots
mpl.rcParams['axes.linewidth'] = 0.5
fig = plt.figure() 
axes = plt.axes()
axes.tick_params(axis='both', which='major', labelsize=8)
axes.tick_params(axis='both', which='minor', labelsize=8)

# load dataset
url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'
dataframe = read_csv(url, header=None)
dataset = dataframe.values

# print correlation matrix
print(dataframe.corr()) 

# split into input (X) and output (y) variables
X, y = dataset[:, 1:-1], dataset[:, -1]
X, y = X.astype('float'), y.astype('float')
n_features = X.shape[1]

# split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)

#--- main algorithm

print("\np\tseed\terror\tdtime\tsize")
stats = {}

for p in np.arange(0.01, 1.0001, 0.01):
    for seed in range(203,213):

            # use seed to check volatility between identical runs
            np.random.seed(seed)     

            # keep proportion p of the original training set 
            X_train_small = X_train  
            y_train_small = y_train
            max_size = len(X_train_small)
            while len(X_train_small) >= p * max_size:
                index = np.random.randint(len(X_train_small))
                X_train_small = np.delete(X_train_small, (index), axis=0)
                y_train_small = np.delete(y_train_small, (index), axis=0)

            # run model on small training set, save history
            model = LinearRegression(fit_intercept = True)
            time1 = time.time()         
            model.fit(X_train_small, y_train_small)
            time2 = time.time()
            dtime = time2 - time1
            train_size = len(X_train_small)/len(X_train)
            coeff = model.coef_

            # evaluate on training set
            y_pred_train = model.predict(X_train_small)
            error_train = metrics.mean_absolute_error(y_train_small, y_pred_train)

            # evaluate on test set
            y_pred_test = model.predict(X_test)
            error_test = metrics.mean_absolute_error(y_test, y_pred_test)
            stats[(p,seed)] = (error_test, error_train, dtime, model.coef_)
            print("%5.4f\t%4d\t%6.4f\t%8.6f" % (p, seed, error_test, dtime))

#--- improve (recalibrate) predictions obtained via last regression

def create_pred_correction_table(observed, predicted):
    delta = {}
    count = {}
    for k in range(len(predicted)):
        pred_value = int(0.5 + predicted[k])
        if pred_value in delta:
            delta[pred_value] += pred_value - observed[k]
            count[pred_value] += 1
        else:
            delta[pred_value] = pred_value - observed[k]
            count[pred_value] = 1
    for pred_value in delta:
        delta[pred_value] /= count[pred_value]
    return(delta)

def adjusted_prediction(pred, delta):
    pred_int = int(0.5 + pred)
    if pred_int in delta: 
        pred_adjusted = pred + delta[pred_int]
    else:
        pred_adjusted = pred
    return(pred_adjusted)

delta = create_pred_correction_table(y_pred_train, y_train_small)

# recalibrate predictions
y_pred_test_adj = []
for k in range(len(y_pred_test)):
    pred = y_pred_test[k]
    adj_pred = adjusted_prediction(pred, delta)
    y_pred_test_adj.append(adj_pred)

#--- plot last regression obtained on full training set (p = 1), and correls

plt.xlim(0,25)
plt.ylim(-5,35)
plt.plot([-5, 35],[-5, 35], c='blue', linewidth = 0.5)
plt.scatter(y_test,y_pred_test_adj, s=28, c='r', edgecolors='r', \
    linewidth=0, alpha = 0.2) 
plt.show()
c1 = np.corrcoef(y_train_small,y_pred_train)
c2 = np.corrcoef(y_test,y_pred_test)
c3 = np.corrcoef(y_test,y_pred_test_adj)
print("\nCorrels:\non training set: %5.4f\non test set: %5.4f\non test set adj: %5.4f" 
     % (c1[0,1],c2[0,1],c3[0,1]) )        

#--- compute aggregated statistics (one set for each p, computed across seeds)

avg_error_test = {}     # avg X_test error given p
min_error_test = {}     # min X_test error given p
max_error_test = {}     # max X_test error given p
min_error_train = {}    # min X_train_small error given p
min_error3 = {}    # X_test error for best_seed2 (minimizing X_train_small error given p)
best_seed ={}      # seed that minimizes X_test error given p
best_seed2 = {}    # seed that minimizes X_train_small error given p
count = {}         # number of seeds used for specific p

for key in stats:
    p    = key[0]
    seed = key[1]
    statistics = stats[key]
    error_test = statistics[0]    # computed on test (validation) set
    error_train = statistics[1]   # computed on small training set
    arr_coeff = statistics[3]
    if p in avg_error_test:
       avg_error_test[p] += error_test
       if error_test < min_error_test[p]:
           min_error_test[p] = error_test
           best_seed[p] = seed
       if error_test > max_error_test[p]:
           max_error_test[p] = error_test
       if error_train < min_error_train[p]:
           min_error_train[p] = error_train
           best_seed2[p] = seed
           min_error3[p] = error_test
       count[p] +=1
    else:
       avg_error_test[p]  = error_test
       min_error_test[p]  = error_test
       max_error_test[p]  = error_test
       min_error_train[p] = error_train
       min_error3[p] = error_test
       best_seed[p]  = seed
       best_seed2[p] = seed
       count[p] = 1

# print summary stats
print("\nAggregated stats\np\tcount\tavg err test\tmin err test \
           \tmax err test\tseed1\tmin err train\tmin err3\tseed2")
for p in count:
    avg_error_test[p] /= count[p]
    print("%5.4f\t%3d\t%7.5f\t%7.5f\t%7.5f\t%4d\t%7.5f\t%7.5f\t%4d"
        % (p, count[p], avg_error_test[p], min_error_test[p], max_error_test[p], 
           best_seed[p], min_error_train[p], min_error3[p], best_seed2[p]))

# for each p, print min_error[p] and the corresponding regression coefficients 
print("\nRegression coefficients, best fit given p")
for p in count: 
    seed = best_seed[p]
    key = (p, seed)
    statistics = stats[(p, seed)]
    error_test = statistics[0] 
    print("%5.4f\t%7.5f\t" % (p, error_test), end=" ")
    regr_coeff = statistics[3]   
    for var in range(len(regr_coeff)):
        print(" %6.2f" % (regr_coeff[var]), end=" ")
    print(" ")
\end{lstlisting}

\subsubsection{Stochastic thinning for neural networks}\label{crottalbourrique}

Also available on GitHub, \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/thinned_neuralNets.py}{here}. 
The results are fully replicable.\vspace{1ex}

\begin{lstlisting}
import time
import numpy as np
import pandas as pd
from pandas import read_csv
import matplotlib as mpl
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from keras import metrics
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
import random as python_random
from tensorflow import random

# initialize plot parameters for nicer plots
mpl.rcParams['axes.linewidth'] = 0.5
fig = plt.figure() 
axes = plt.axes()
axes.tick_params(axis='both', which='major', labelsize=8)
axes.tick_params(axis='both', which='minor', labelsize=8)

# load dataset
url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/abalone.csv'
dataframe = read_csv(url, header=None)
dataset = dataframe.values
X_train, y_train = dataset[:, 1:-1], dataset[:, -1]

# split into input (X) and output (y) variables
X, y = dataset[:, 1:-1], dataset[:, -1]
X, y = X.astype('float'), y.astype('float')
n_features = X.shape[1]

# split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)


header1 = "p\tn_epochs\tseed\terror\tfinal_mae\tdtime\tsize"
print(header1)
OUT = open("history.txt","w")

for p in np.arange(0.10, 1.01, 0.10):
    for n_epochs in range(150, 500, 150):
        for seed in range(103,108):

            # use seed to check volatility between identical runs
            np.random.seed(seed)     # for numpy
            random.set_seed(seed)    # for tensorflow/keras
            python_random.seed(seed) # for python

            # define the keras model
            model = Sequential()
            model.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))
            model.add(Dense(10, activation='relu', kernel_initializer='he_normal'))
            model.add(Dense(1, activation='linear'))

            # compile the keras model
            model.compile(loss='mse', optimizer='adam', metrics=[metrics.mean_squared_error,
                  metrics.mean_absolute_error, metrics.mean_absolute_percentage_error])

            # keep proportion p of the original training set 
            X_train_small = X_train  
            y_train_small = y_train
            max_size = len(X_train_small)
            while len(X_train_small) >= p * max_size:
                index = np.random.randint(len(X_train_small))
                X_train_small = np.delete(X_train_small, (index), axis=0)
                y_train_small = np.delete(y_train_small, (index), axis=0)

            # run model on small training set, save history
            time1 = time.time()                   
            hist = model.fit(X_train_small, y_train_small, epochs=n_epochs, \
               batch_size=32, verbose=0)
            time2 = time.time()
            dtime = time2 - time1
            mape = hist.history['mean_absolute_percentage_error'] 
            mse  = hist.history['mean_squared_error']
            mae  = hist.history['mean_absolute_error']
            for epoch in range(n_epochs): 
                OUT.write("%4.3f\t%4d\t%4d\t%7.4f\t%7.4f\t%7.4f\t\n" 
                      %(p, seed, epoch, mse[epoch], mae[epoch], mape[epoch]))
            error_train = mae[n_epochs-1]

            # evaluate on test set
            y_pred_test = model.predict(X_test, verbose = 0)
            error_test = mean_absolute_error(y_test, y_pred_test)

            print("%4.3f\t%4d\t%4d\t%6.4f\t%6.4f\t%5.2f\t%4d" % 
                (p, n_epochs, seed, error_test, error_train, dtime, len(X_train_small)))

OUT.close()

#--- plot last predictions obtained, along with associated correls

plt.xlim(0,25)
plt.ylim(-5,35)
plt.plot([-5, 35],[-5, 35], c='blue', linewidth = 0.5)
plt.scatter(y_test,y_pred_test, s=28, c='r', edgecolors='r', linewidth=0, alpha = 0.2) 
plt.show()
y_pred_test = np.transpose(y_pred_test)
c = np.corrcoef(y_test,y_pred_test)
print("\nCorrels:\non test set: %5.4f" % (c[0,1]) )        
\end{lstlisting}





%----------------------------------

\section{Smart Grid Search for Faster Hyperparameter Tuning}\label{smargsxxbv}

%xxx
%=======

The objective is two-fold. First, I introduce a 2-parameter generalization of the discrete geometric and zeta distributions. 
Indeed, a combination of both. It allows you to simultaneously match the variance and mean in observed data, thanks to the two
 parameters $p$ and $\alpha$. To the contrary, each distribution taken separately only has one parameter, and can not achieve this goal. The zeta-geometric distribution offers more flexibility, especially when dealing with 
 unusual tails in your data. I illustrate the concept when synthesizing real-life tabular data with parametric copulas, for one of the features 
 in the dataset: the number of children per policyholder.  

Then, I show how to significantly improve grid search, and make it a viable alternative
 to gradient methods to estimate the two parameters $p$ and $\alpha$. The cost function -- that is, the error to minimize -- is the 
 combined distance between the mean and variance computed on the real data, and the mean and variance of the target
 zeta-geometric distribution. Thus the mean and variance are used as proxy estimators for $p$ and $\alpha$. This technique
 is known as minimum contrast estimation, or moment-based estimation in statistical circles. The ``smart'' grid search consists
 of narrowing down on smaller and smaller regions of the parameter space over successive iterations.

The zeta-geometric distribution is just one example of an hybrid distribution. I explain how to design such hybrid models in general, using a very simple technique. They are useful to combine multiple distributions into a single one, leading to model generalizations with an increased number of parameters. The goal is to design distributions that are a good fit when some in-between solutions are needed to better represent the reality.




\subsection{Introduction to hybrid distributions}\label{dk6fb}

The approach in this section is very much typical of machine learning. There is no statistical theory involved. Yet in the end, 
 I sample data with the desired distribution, in order to fit the observed data. The 
\textcolor{index}{zeta-geometric distribution}\index{zeta-geometric distribution} is defined as follows:
\begin{equation}
P(X = k) = C\cdot \frac{p^k}{(k+1)^\alpha},\quad k=0,1,2,\dots \label{treces}
\end{equation}
The constant $C$ depends on $p$ and $\alpha$; it is chosen so that the the probabilities add up to 1. It makes sense 
 to require $0< p\leq 1$. It also makes sense to require $\alpha\geq 0$, and even $\alpha > 1$ if $p=1$. However, in my example,
  a negative value for $\alpha$ combined with $p<1$ works well: it tends to increase the probabilities attached to small values of $k$, that is, families with a small
 number of children.  Two particular cases are:\vspace{1ex}
\begin{itemize}
\item \textcolor{index}{Geometric distribution}\index{geometric distribution} [\href{https://en.wikipedia.org/wiki/Geometric_distribution}{Wiki}]: $\alpha = 0$. In that case, we have $C = 1-p$. The expectation and variance are respectively
 $$\text{E}[X]=\frac{p}{1-p}, \quad \text{Var}[X]=\frac{p}{(1-p)^2}.$$
\item \textcolor{index}{Zeta distribution}\index{zeta distribution} [\href{https://en.wikipedia.org/wiki/Zeta_distribution}{Wiki}]: $p=1,\alpha>1$. In that case, $C=\zeta(\alpha)$ is the 
\textcolor{index}{Riemann zeta function}\index{Riemann zeta function} [\href{https://en.wikipedia.org/wiki/Riemann_zeta_function}{Wiki}]. The expectation and variance are respectively
 $$
\text{E}[X] =\frac{\zeta(\alpha-1)}{\zeta(\alpha)} \text{ for } \alpha>2, \quad
 \text{Var}[X] =\frac{\zeta(\alpha)\zeta(\alpha-2) - \zeta^2(\alpha-1)}{\zeta^2(\alpha)}  \text{ for } \alpha>3.
$$
\end{itemize}
Here I use the zeta-geometric distribution to model the number of children per family. A geometric distribution may provide the perfect mean, but it tends to produce
 a few outliers with too many kids per family (above 10) even on samples with fewer than 2000 observations. As a result, the variance is too large, compared to that on the real data. A truncated geometric distribution can fix this issue, but it comes with an
 arbitrary threshold. The zeta-geometric distribution addresses these two issues. For other generalizations of the geometric distribution  with a 5 parameters family, see~\cite{gammageo17}.


The general idea of \textcolor{index}{hybrid distributions}\index{hybrid distribution} is as follows: given two parametric distributions with probability density functions (PDF) $f_1$ and $f_2$, respectively with parameters $p$ and $\alpha$ (these parameters can be multidimensional), you want to create a new one with the following PDF:
$$
f(x) = C\cdot f_1^{\beta_1} (x) f_2^{\beta_2}(x)
$$
where $\beta_1, \beta_2\geq 0$ are two additional parameters and $C$ is a normalization constant depending on all the parameters, chosen so that the new density $f$ integrates to one. In our case, the densities $f_1, f_2$ are discrete, thus I use $k$ instead of $x$, and summations rather than integrals. More
 precisely, in my example, $f_1, f_2$ are respectively a geometric and zeta density. The original distributions correspond 
 respectively to $\beta_1 = 1, \beta_2=0$ and $\beta_1 = 0, \beta_2=1$. 

Note that in my zeta-geometric example, you don't need $\beta_1, \beta_2$ as you can particularize the generalized density by playing with $p$ and $\alpha$ alone. Of course, you can combine more than two distributions into a single one.  When $\beta_1=\beta_2=1$, it has a Bayesian interpretation, with (say) $f_2$ playing the role of a prior, and $f=f_1 f_2$ being the posterior. 

%--
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{pdf3.png}  
\caption{Three discrete PDFs with same expectation, different variances}
\label{fig:prqerw75}
\end{figure}


Figure~\ref{fig:prqerw75} shows three discrete probability density functions with the same expectation: zeta-geometric, zeta, and geometric.
 The parameters were chosen so that the expectation matches the average value observed in the dataset. Here, the feature 
of interest is the number of children per family. 
Because the geometric and zeta distributions only have one parameter, it was possible to match the average number of children, but not the standard deviation. The zeta-geometric distribution, with its two 
parameters $p$ and $\alpha$, is able to match both. The tail of the geometric distribution is too thick to fit nicely to the data. The zeta 
 distribution is even worse. 

So how can we combine both to get a distribution with a much thinner tail? The answer is in the fact that 
 I use a negative value for $\alpha$. A zeta distribution with a negative $\alpha$ can not exist. But when multiplied by a geometric density,
 the resulting distribution is well defined and has a much thinner tail. It fits much better to the data. In particular, the zeta-geometric PDF
 in Figure~\ref{fig:prqerw75} (the green bars) is the only one with the correct variance, identical to that in the dataset.

Another generalization of the geometric distribution is as follows. Let $X$ be the standard geometric distribution defined by~(\ref{treces}). Then the \textcolor{index}{power-geometric distribution}\index{power-geometric distribution} of parameters $p,\gamma$ is defined by  
$$P(X_\gamma=k) = \frac{1-p}{\mu_\gamma}\cdot k^\gamma p^k, \quad k=0,1,2,\dots$$ 
Here $\mu_\gamma = \text{E}[X^\gamma]$ and $\gamma\geq 0$. 
The standard
 geometric distribution corresponds to $\gamma=0$.
 Also, the moments are related to those of the geometric distribution via the formula

$$\text{E}[X_\gamma^\lambda] = \frac{\text{E}[X^{\gamma+\lambda}]}{\text{E}[X^\gamma]}=\frac{\mu_{\gamma+\lambda}}{\mu_{\gamma}}, \quad \gamma+\lambda \geq 0.$$ 
More generally, multiplying a density $P(X =k)$ by a power $k^\gamma$ with $\gamma\geq 0$ results in an hybrid distribution that satisfies similar properties. This is also true for positive continuous densities, where the integer $k$ is replaced by a real number $x\geq 0$.
If $\gamma$ is a positive integer, the restriction to $x\geq 0$ is not needed.


\subsection{Case study: parametric copulas to synthesize data}

I used the methodology discussed in this section, in the context of data synthetization, to improve the quality of  
\textcolor{index}{synthetic data}\index{synthetic data} produced by the copula technique. It relies on \textcolor{index}{empirical quantiles}\index{empirical quantiles} [\href{https://en.wikipedia.org/wiki/Quantile_function}{Wiki}] measured on the real data, to replicate the correlation structure and the distribution attached to each feature, in the original dataset. Thus the technique is parameter-free. But one of the drawbacks is its inability to sample outside
 the range observed in the real data. 
One way to address this issue is to replace the empirical quantiles by those from a parametric family of distributions, and choose the
 parameters to fit to the real data. 

I was particularly interested in one of the features in the data: the number of children per policyholder. The insurance dataset consisted of 7 features: age, gender, smoking status, number of children, location, and charges for 1400
 policyholders. The goal was to predict the cost (charges) to the insurance company, based on the other features. The company was interested in synthetic data to enrich the training set, and for compliance with regulations
 pertaining to privacy and security.

To summarize, the maximum number of children was 5 in the dataset. This feature was well modeled by the one-parameter geometric distribution except that it produced a maximum of 10 to 11 children per family, depending on the generated sample. It proved that a parametric distribution 
 could definitely sample outside the observation range (if enough observations are generated), but I wanted a better model. One that would replicate not only the average number of children per family (close to one, with many having no child), but also the standard deviation. The zeta-geometric distribution accomplished this goal, thanks to its two parameters. It lowered the maximum number of 
 children to 7, closer to the observed value in the real data.

In the remaining of this section, I quickly describe the 
\textcolor{index}{copula method}\index{copula} [\href{https://en.wikipedia.org/wiki/Copula_(probability_theory)}{Wiki}] and show where the zeta-geometric distribution fits in. Then
 I show how to sample from this distribution to get the best possible fit to the real data (for the number of children), and thus, good quality synthetic data. The estimation of the two parameters $p,\alpha$ attached to the zeta-geometric is based on an enhanced version of the \textcolor{index}{grid search algorithm}\index{grid search} and discussed in section~\ref{butare}. There is no maximum likelihood function or statistical theory involved.
 Instead, I use the parameter vector $(p, \alpha)$ of the zeta-geometric as a proxy for the mean and variance. The technique is known
 as \textcolor{index}{maximum contrast estimation}\index{maximum contrast estimation} in operations research, or  
\textcolor{index}{moment estimation method}\index{moment estimation method} [\href{https://en.wikipedia.org/wiki/Generalized_method_of_moments}{Wiki}]
 in statistics.

\subsubsection{Brief overview of the copula method}\label{chaleur}

The goal is to produce  synthetic data that mimics the correlation structure and individual distributions attached to each feature in a real dataset. It is a multivariate version of  
\textcolor{index}{inverse transform sampling}\index{inverse transform sampling} [\href{https://en.wikipedia.org/wiki/Inverse_transform_sampling}{Wiki}], and works well with a blend of ordinal and continuous features.
 The method, applied to the insurance data mentioned here, is discussed in detail in section~\ref{piviiiurobvbc}.
 It is broken down into 4 steps:\vspace{1ex}

\begin{itemize}
\item Step 1: Compute the $m\times m$ correlation matrix $W$ associated to your real data, where $m$ is the number of features.
 This symmetric matrix contains the cross-correlations for each pair of features. Each element in the diagonal is equal to 1. 
\item Step 2: Generate $n$ deviates from a multivariate Gaussian distribution with zero mean and covariance matrix $W$. Each deviate is a 
 row vector $Z_i$ ($i=1,\dots,n$), with the components matching the features in the real data set.
\item Step 3: For each generated $Z_{ij}$ (the $j$-th feature in your $i$-th vector, with
 $1\leq i\leq n$ and $1\leq j\leq m$) compute $U_{ij}=\Phi(Z_{ij})$, where $\Phi$ is
 the CDF (cumulative distribution function) of a univariate standard normal distribution. Thus $0\leq U_{ij}\leq 1$. These
 are uniform deviates on $[0, 1]$, with the correct feature cross-correlation structure.
\item Step 4: Compute $S_{ij}=Q_j(U_{ij})$ where $Q_j$ is the univariate
\textcolor{index}{empirical quantile distribution}\index{quantile!empirical} (the inverse of the 
\textcolor{index}{empirical distribution}\index{empirical distribution}) attached to the $j$-th feature, and computed on the real data.
\end{itemize}\vspace{1ex}

\noindent To get a better understanding of what this algorithm accomplishes, look at what happens when $m=1$. What I described here 
 is the Gaussian copula method, one of the most popular. The last step is 
\textcolor{index}{inverse transform sampling}\index{inverse transform sampling} applied to each feature separately, to reconstruct the individual distributions.

\begin{table}[H]
\[
\begin{array}{lrrrrr}
\hline
\text{Distribution}	& p & \alpha	& \mu&	\sigma & \text{max}\\
\hline
\hline
 \text{Zeta-geometric} & 0.27 & -1.65 &  1.09 &  1.20 & 10\\
\text{Zeta} & 1.00 & 2.33 &   1.10 & 4.27 & 99 \\
\text{Geometric}  & 0.52 & 0.00 &  1.10 & 1.53 & 17\\
%Sample stats: mean: 1.094 std: 1.202 max: 10.000
%Sample stats Zeta: mean: 1.096 std: 4.266 max: 99.000
%Sample stats Geom: mean: 1.099 std: 1.527 max: 17.000
\hline
\end{array}
\]
\caption{\label{chocolat} $\mu$, $\sigma$, and max. children; sample size: $n=\num{50000}$}
\end{table}

In the dataset, the number of children is the fourth feature ($j=4$). In the last step, instead of using the empirical quantile distribution $Q_4$, 
 I replace it with the quantile distribution of a zeta-geometric distribution of parameters $p,\alpha$, with $p,\alpha$ estimated on the real data (the fourth feature), as described in section~\ref{butare}. More specifically, I generate $n$ deviates of the
 target zeta-geometric distribution. Then I compute its empirical quantiles based on that sample, and replace $Q_4$ by the quantiles in question. The quantiles are also called percentiles.

The same method can be applied to any feature, not just the number of children. Or even to combinations of features rather than individual features, using multivariate rather than univariate hybrid distributions. Finally, Table~\ref{chocolat} shows the dramatic improvement when using
 a zeta-geometric (ZG) instead of a zeta or geometric distribution. In each case, the generated sample has $n =\num{50000}$ observations.
 The 3 distributions produce the same mean $\mu\approx 1.09$ thanks to the choice of parameters, but only ZG can produce 
the target standard deviation $\sigma\approx 1.20$. The maximum number of children is 7 on a test sample with $n=1400$, the same number 
 of observations as in the real data. The case $n=1400$ is featured in Figure~\ref{fig:prqerw75}. Confidence intervals are obtained by generating a large number of samples of same size. The theoretical standard deviation of the zeta distribution with $\alpha = 2.33$, is infinite.  


\subsubsection{Sampling from a zeta-geometric distribution}

I use the standard method to sample deviates from an arbitrary discrete distribution. The process consists of the following steps: \vspace{1ex}
\begin{itemize}
\item Compute the constant $C$ in~(\ref{treces}), given specific values of $p$ and $\alpha$. 
\item Recursively compute $P(X\leq k)= P(X=k)+P(X\leq k-1)$ using~(\ref{treces}).
\item Generate a uniform deviate $U$ on $[0, 1]$.
\item Find the largest $k$ such that $U\leq P(X\leq k)$, starting at $k=0$. 
\end{itemize}\vspace{1ex}
The value of $k$ obtained in the last step is a deviate from the zeta-geometric distribution of parameters $p,\alpha$. Repeat the whole
 procedure a thousand times, and you get a thousand deviates. This is implemented
 in the \texttt{sample\_from\_CDF} function in the Python code in section~\ref{mangeputy}.


\subsection{Smart grid search: viable alternative to gradient descent}\label{butare}

This algorithm is simple yet quite efficient. It combines features of 
\textcolor{index}{grid search}\index{grid search} [\href{https://en.wikipedia.org/wiki/Hyperparameter_optimization}{Wiki}]
 and 
\textcolor{index}{random search}\index{random search} [\href{https://en.wikipedia.org/wiki/Random_search}{Wiki}], and does not require the target function to have a gradient. In fact it works with pure data, in the absence of any
 mathematical function. The algorithm is a multivariate version of the 
\textcolor{index}{bisection method}\index{bisection method} [\href{https://en.wikipedia.org/wiki/Bisection_method}{Wiki}]. See also~\cite{manulo17}.

\begin{figure}%[H]
\centering
\includegraphics[width=0.78\textwidth]{ZipfGeom.png}  
\caption{Error $\varphi(p,\alpha)$ to minimize, with $p$ on the X-axis, $\alpha$ on the Y-axis}
\label{fig:pytz238675}
\end{figure}

I use it to estimate the parameters $p,\alpha$ of a zeta-geometric distribution, to provide a good fit to real data, by matching the
 average and standard deviation -- in this case for the number of children per family -- in the insurance dataset. Once the parameters are estimated, I generate a sample from the zeta-geometric distribution in question, and use its quantiles in the copula method described
 in section~\ref{chaleur}, as a substitute to the empirical quantiles. The end-goal is to get better synthetic data, by sampling outside of the observed range, yet fitting nicely to to the real data. 
See the \texttt{grid\_search} function in the Python code, and how I use this function iteratively.



\noindent The cost function to minimize is 
$\varphi(p,\alpha) =(\mu_{p,\alpha}-\mu_0)^2 + (\sigma_{p,\alpha}-\sigma_0)^2$
where \vspace{1ex}

\begin{itemize}
\item $\mu_0$ and $\sigma_0$ are the mean and standard deviation measured on the observed data,
\item $\mu_{p,\alpha}$ and $\sigma_{p,\alpha}$ are the mean and standard deviation of a ZG($p,\alpha$) distribution.
\end{itemize}\vspace{1ex}
Here ZG stands for zeta-geometric. The vector $(p,\alpha)$ achieving the minimum (if unique) is
 denoted as $(p^\star,\alpha^\star)$. Here we have $\mu_0 = 1.09$ and $\sigma_0 = 1.20$,
 resulting in $p^\star\approx 0.27$ and $\alpha^\star\approx -1.65$. Figure~\ref{fig:pytz238675} shows a contour plot of $\varphi$, 
 with $p$ on the X-axis and $\alpha$ on the Y-axis. The minimum is indeed unique and global, at least in the
 pictured region. 

The long narrow basin makes a 
\textcolor{index}{gradient descent}\index{gradient descent} approach [\href{https://en.wikipedia.org/wiki/Gradient_descent}{Wiki}]    
  difficult
 to solve this problem. It also means that many values of $(p,\alpha)$ are nearly optimum: we could choose one 
 that provides not only a good match for the first two moments (they all do in the basin), but also a good match for the third moment or some other 
 statistic such as the maximum number of children. 

\begin{table}[H]
\[
\begin{array}{lrrrrr}
\hline
n	& p_n& \alpha_n	& \mu_n&	\sigma_n & \varphi(p_n,\alpha_n)\\
\hline
\hline
0 & 0.5000 & 0.0000 & -  & - & - \\
 1 & 0.3000 & -1.5000 &  1.1612 &  1.2698 & 0.0926\\
2 & 0.3000 & -1.3500 &   1.0640 & 1.2219 & 0.0353\\
3  & 0.2700 & -1. 6500 &  1.0962 & 1.2020 & 0.0032\\
%delta: 0.0926 mu: 1.1612 std: 1.2698 p: 0.3000 a: -1.5000
%delta: 0.0353 mu: 1.0640 std: 1.2219 p: 0.3000 a: -1.3500
%delta: 0.0032 mu: 1.0962 std: 1.2020 p: 0.2700 a: -1.6500
\hline
\end{array}
\]
\caption{\label{tablerone} Convergence of smart grid search; $\varphi(p_n,\alpha_n)$ is the error}
\end{table}

\noindent Now I explain how smart grid search works to solve the aforementioned optimization problem. The concept is very intuitive. Here $\epsilon_1, \delta_1$ are strictly positive real numbers, set respectively to $0.4$ and $3.0$ in this example. \vspace{1ex}
\begin{itemize}
 \item Start with initial values
 $p_0,\alpha_0$ and a range $R_0 = [p_0 - \epsilon_0, p_0+\epsilon_0] \times [\alpha_0 - \delta_0, \alpha_0+\delta_0]$.
\item Test 
100 values of $(p,\alpha)$ evenly spread in $R_0$. 
Let $(p_1,\alpha_1)$ be the vector minimizing $\varphi(p,\alpha)$, among the
 100 tests. 
With 10 different values for $p$, and 10 different values for $\alpha$, the total is $10\times 10 = 100$ tests.
\item Let $\epsilon_1 = \rho \epsilon_0$ and $\delta_1 = \rho \delta_0$, with $0<\rho<1$.
\item Go back to the first step, with $p_0,\alpha_0, R_0$ replaced by $p_1,\alpha_1, R_1$
 with $R_1 = [p_1 - \epsilon_1, p_1+\epsilon_1] \times [\alpha_1 - \delta_1, \alpha_1+\delta_1]$.
\end{itemize}\vspace{1ex}
This procedure is repeated a few times, generating a sequence $(p_n,\alpha_n)\in R_n$, with $\epsilon_n, \delta_n \rightarrow 0$.
 Thus $R_n$ gets smaller and smaller at each iteration. In my test, an excellent approximation to the optimum was
 obtained in 3 iterations ($n=3$), with $\rho=\frac{1}{2}$. See Table~\ref{tablerone}. In the table, $\mu_n,\sigma_n$ are the
 expectation and standard deviation of a GC($p_n,\alpha_n$) distribution. Again, the target
 values observed in the real dataset are $\mu=1.09$ and $\sigma = 1.20$. The error $\varphi(p_n,\alpha_n)$ measures
 the distance between the target values, and those obtained at iteration $n$.




  
%xxx99   https://arxiv.org/ftp/arxiv/papers/2010/2010.04507.pdf    https://core.ac.uk/download/pdf/232720617.pdf
   

\subsection{Python code}\label{mangeputy}

The code is also available on GitHub, 
 \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/ZetaGeom.py}{here}.
Look for \texttt{ZetaGeom.py}.
\vspace{1ex}




 

\begin{lstlisting}
import numpy as np

#--- compute mean and stdev of ZetaGeom[p, a]

def ZetaGeom(p, a):
    C = 0
    for k in range(200):
        C += p**k/(k+1)**a
    mu = 0
    m2 = 0
    for k in range(200):
        mu += k*p**k/(k+1)**a
        m2 += k*k*p**k/(k+1)**a
    mu /= C
    m2 /= C
    var = m2 - mu*mu
    stdev = var**(1/2)
    return(mu, stdev)

#--- smart grid search to find optimal p and a

def grid_search(grid_range):
    p_min = grid_range[0][0]
    p_max = grid_range[0][1]
    a_min = grid_range[1][0]
    a_max = grid_range[1][1]
    p_step = (p_max - p_min)/10
    a_step = (a_max - a_min)/10
    min_delta = 999999999.9
    for p in np.arange(p_min, p_max, p_step):
        for a in np.arange(a_min, a_max, a_step):
            (mu, std) = ZetaGeom(p, a)
            delta = np.sqrt((mu - target_mu)**2 + (std - target_std)**2)
            if delta < min_delta:
                p_best = p
                a_best = a
                mu_best = mu
                std_best = std
                min_delta = delta
    return(p_best, a_best, mu_best, std_best, min_delta)

#--- estimating p and a based on observed mean and standard deviation

target_mu    = 1.095  # mean
target_std   = 1.205  # standard deviation

p = 0.5
a = 0.0
step_p = 0.4
step_a = 3.0

for level in range(3):
    step_p /= 2
    step_a /= 2
    p_min = max(0, p - step_p)
    p_max = p + step_p
    a_min = a - step_a
    a_max = a + step_a
    grid_range = [(p_min, p_max),(a_min, a_max)]
    (p, a, mu, std, min_delta) = grid_search(grid_range)
    print("delta: %6.4f mu: %6.4f std: %6.4f p: %6.4f a: %6.4f" 
         % (min_delta, mu, std, p, a))

# now (p_fit, a_fit) is such that (mean, std) = (target_mu, target_std)
p_fit = p  
a_fit = a

#--- sampling from ZetaGeom[p, a]

def CDF(p, a):
    C = 0
    for k in range(100):
        C += p**k/(k+1)**a
    arr_CDF = []
    CDF = 0
    for k in range(100):
        CDF += (p**k/(k+1)**a)/C
        arr_CDF.append(CDF)
    return(arr_CDF)

def sample_from_CDF(p, a):
    u = np.random.uniform(0,1)
    k = 0
    arr_CDF = CDF(p, a)
    while u > arr_CDF[k]:
        k = k+1
    return(k)

#--- sample using estimated p, a to match target mean and stdev

nobs = 50000  # number of deviates to produce
seed = 500
np.random.seed(seed)
sample1 = np.empty(nobs)
for n in range(nobs):
    k = sample_from_CDF(p_fit, a_fit)
    sample1[n] = k

mean = np.mean(sample1)
std  = np.std(sample1)
maxx = max(sample1)
print("\nSample stats: mean: %5.3f std: %5.3f max: %5.3f" 
   % (mean, std, maxx))

#--- optional: plotting approximation error for p, a 

from mpl_toolkits import mplot3d
import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib import cm # color maps

xa = np.arange(0.0, 0.6, 0.005)
ya = np.arange(-3.0, 0.0, 0.025)
xa, ya = np.meshgrid(xa, ya)
za = np.empty(shape=(len(xa),len(ya)))

kk = 0
for p in np.arange(0.0, 0.6, 0.005):
    hh = 0
    for a in np.arange(-3.0, 0.0, 0.025):
        (mu, std) = ZetaGeom(p, a)
        delta = np.sqrt((mu - target_mu)**2 + (std - target_std)**2)
        za[hh, kk] = delta
        hh += 1
    kk += 1

mpl.rcParams['axes.linewidth'] = 0.5
fig = plt.figure() 
axes = plt.axes()
axes.tick_params(axis='both', which='major', labelsize=8)
axes.tick_params(axis='both', which='minor', labelsize=8)
CS = axes.contour(xa, ya, za, levels=150, cmap=cm.coolwarm, linewidths=0.35)
cbar = fig.colorbar(CS, ax = axes, shrink = 0.8, aspect = 5)
cbar.ax.tick_params(labelsize=8)
plt.show()

#--- compare with zeta with same mean mu = 1.095

p = 1.0
a = 2.33  # a < 3 thus var is infinite
sample2 = np.empty(nobs)
for n in range(nobs):
    k = sample_from_CDF(p, a)
    sample2[n] = k

mean = np.mean(sample2)
std  = np.std(sample2)
maxx = max(sample2)
print("Sample stats Zeta: mean: %5.3f std: %5.3f max: %5.3f" 
   % (mean, std, maxx))

#--- compare with geom with same mean mu = 1.095

p = target_mu/(1 + target_mu)
a = 0.0
sample3 = np.empty(nobs)
for n in range(nobs):
    k = sample_from_CDF(p, a)
    sample3[n] = k

mean = np.mean(sample3)
std  = np.std(sample3)
maxx = max(sample3)
print("Sample stats Geom: mean: %5.3f std: %5.3f max: %5.3f" 
   % (mean, std, maxx))

#--- plot probability density functions

axes.tick_params(axis='both', which='major', labelsize=4)
axes.tick_params(axis='both', which='minor', labelsize=4)
mpl.rc('xtick', labelsize=8) 
mpl.rc('ytick', labelsize=8) 
plt.xlim(-0.5,9.5)
plt.ylim(0,0.8)

cdf1 = CDF(p_fit, a_fit) 
cdf2 = CDF(1.0, 2.33) 
cdf3 = CDF(target_mu/(1+target_mu), 0.0) 

for k in range(10):
    if k == 0:
        pdf1 = cdf1[0]
        pdf2 = cdf2[0]
        pdf3 = cdf3[0]
    else:
        pdf1 = cdf1[k] - cdf1[k-1]
        pdf2 = cdf2[k] - cdf2[k-1]
        pdf3 = cdf3[k] - cdf3[k-1]
    plt.xticks(np.linspace(0,9,num=10))
    plt.plot([k+0.2,k+0.2],[0,pdf1],linewidth=5, c='tab:green', label='Zeta-geom')
    plt.plot([k-0.2,k-0.2],[0,pdf2],linewidth=5, c='tab:orange',label='Zeta')
    plt.plot([k,k],[0,pdf3],linewidth=5, c='tab:gray',label='Geom')

plt.legend(['Zeta-geom','Zeta','Geom'],fontsize = 7)
plt.show()
\end{lstlisting}


%--------------------------------------------------------------------------------------------------------------
\chapter{How to Fix a Failing Generative Adversarial Network}\label{failinggan}

The NoGAN technology (chapter~\ref{chnogan}) and subsequent deep resampling synthesizer (chapter~\ref{chnogan2}) were created to address 
 the poor performance of generative adversarial networks (GAN) on some datasets. However, it is possible to significantly improve GAN performance
 when the standard algorithm fails. The purpose of this chapter is to explore several solutions. Some of them also apply to NoGAN. Also, deep
 resampling have many similarities to GAN. It can be used as a sandbox to quickly test potential enhancements, before implementing them in
  deep neural networks such as GAN.


\section{Context}\label{context}

Shakti Chaturvedi was working on a Telecom project (public data)
 while participating in the \href{https://mltblog.com/3pWxvZK}{GenAI training program} offered by my AI/ML research lab, to evaluate the
 quality of the data generated by some home-made \textcolor{index}{generative adversarial networks}\index{generative adversarial networks} (GAN).
 The goal was to predict churn. In particular, we wanted to synthesize observations for the minority group -- the users who left and switched to
 a different provider -- in order to augment and rebalance the dataset, to obtain an enhanced training set and get more robust insights and predictions.  
It quickly became clear that the synthetization was very poor. 

We first narrowed down on the three numerical features -- tenure (in months), monthly charges, and total charges -- ignoring the
 numerous categorical features. The first 2D GAN based on tenure and monthly charges only, worked as expected. 
However, when adding total charges (the charges accumulated by a user over the time period studied), the results were bad. This was a surprise since
 enriching the dataset, for instance by adding labels, usually enhance GAN performance. We then tried many variations of the
 hyperparameters attached to the three deep neural networks used in GAN: the discriminator, the generator, and the full model. In particular,  
 we tested: \vspace{1ex}
\begin{itemize}
\item The \textcolor{index}{learning rates}\index{learning rate} controlling the speed at which the three \textcolor{index}{gradient descent}\index{gradient descent} algorithms progress to 
 minimize the \textcolor{index}{loss functions}\index{loss function}. Too fast leads you to miss the minimum; too slow gets you stuck in local minima.
\item The type of gradient descent, \textcolor{index}{Adam}\index{Adam} being quite popular. You can use a different one for each of the three GAN components: generator, discriminator, full model.
\item The loss functions to be minimized. Ideally, we wanted the loss function attached to the generator to be the evaluation metric measuring the
 quality of the synthesized data, that is, the distance between the real and the synthesized data. 
\item The \textcolor{index}{activation functions}\index{activation function} that turn the output of the final layer into synthetic data. In particular, 
we tried \textcolor{index}{softmax}\index{softmax (activation function)} for integer-valued feature, as recommended. In the end, we used
 a different initialization for each feature, using Keras branches, see \href{https://github.com/VincentGranville/Notebooks/blob/main/GAN_Telecom_test2.ipynb}{here}. 
\item The \textcolor{index}{seeds}\index{seed (random number generator)} that initialize the various random number generators. GAN is very sensitive to the seed, and starting with a good seed means starting with a configuration close to an optimum, thus increasing the chance of convergence to a good solution. Our implementation is the only one that uses seeds, allowing for full replicapibily.
\item When to stop, that is, how many \textcolor{index}{epochs}\index{epoch (neural networks)} to use.  In the GAN algorithm, we created a synthetization after each epoch, evaluating its quality with a metric called \texttt{g\_dist} in the Python code, instead of focusing on the values
 of the loss functions \texttt{d\_hist} and \texttt{g\_hist} respectively for the discriminator and generator. In a typical example, the best synthetization was obtained after $\num{12749}$ epochs, while the final one after $\num{20000}$ epochs was considerably inferior.
\end{itemize} \vspace{1ex}

\noindent There are many other options to play with, for instance the 
\textcolor{index}{batch size}\index{batch size (neural networks)}, or the kind of neural network architecture. In particular,
 \textcolor{index}{Wasserstein GAN}\index{Wasserstein GAN} (WGAN) seems promising~\cite{ieeewgan}. %, however we haven't tested it yet. 
It consists of using a particular loss function. 
 Despite all~our efforts, we were able to obtain modest improvements only. So we worked on a different approach, knowing that
 adding the feature ``total charges" was the source of all the problems.

The issue is that ``tenure" and ``total charges" are highly correlated, thus hiding the extra information buried in the latter, and confusing GAN.
 Indeed, tenure (the number of months you stayed with the company), is an excellent predictor of total charges. By decorreleting the two,
 replacing total charges by total charges residues -- the residues in a linear regression -- we were able to significantly boost the performance,
 especially when combined with all the tests previously mentioned. Section~\ref{potxs} describes how we did it, and how we went from GAN to NoGAN.

One important aspect is how to evaluate the quality of synthetic data. Many metrics widely used in many applications have critical flaws, failing to capture non-linear inderdepencies among features. Indeed, even the well-know SDV library, failing just as dramatically as we did with our home-made GAN, rates its
 own generated  data as excellent. We also discuss how to detect these false negatives. 

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{telco.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Real data (leftmost) vs synthetic:  each row shows a specific feature}
\label{fig:nogvvc}
\end{figure}

\section{GAN enhancements techniques}\label{potxs}

Before describing the various methods outlined in section~\ref{context} and their impact on the results, I discuss the strategy that produced
 the first substantial improvement. It consists of transforming the real data, then applying GAN to the transformed data to produce synthetic observations, and finally applying the inverse transform to the synthetic data. In essence, it is similar to using 
\textcolor{index}{transformers}
 in large language models.

\subsection{Linear transform to decorrelate features}\label{lin12qdj}

The transformation must be invertible.  For the telecom dataset, due to the high correlation 0.95 between tenure and total charges, linear decorrelation of these two features is the obvious choice. This partial \textcolor{index}{principal component analysis}\index{partial principal component analysis} (PCA) worked well. When more features are present, a full PCA is a good choice. It is indeed a linear transformation with invertible matrix. How to
 inverse PCA is described \href{https://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-com}{here}. In our case, we simply replaced ``total charges" by
\begin{equation}
\text{total charge residues} = \text{total charges} - \beta \times \text{tenure}, \label{eq12kvr}
\end{equation}
where $\beta = 85.20$ is chosen so that residues average zero on the real data. It reduced the correlation from 0.95 down to 0.24.  This gives the impression that GAN may underperform in the presence of strong
 multicollinearity, and that feature orthogonalization to eliminate this problem also improves GAN, in the same way that it improves
 linear regression. A stronger decorrelation consists in using 
$$
\beta = \frac{\text{Covar}[\,\text{tenure}, \text{total charges}\,]}{\text{Var}[\,\text{tenure}\,]}
$$ 
 in~(\ref{eq12kvr}), computed on the real data. It leads to zero correlation between tenure and the residues, but with reduced
 interpretability and $\beta=92.31$. Scaling the residues (multiplication by a factor) to get its variance similar to that of the other features, may further improve GAN. The quoted values for $\beta$ and the correlations are based on the real data in 
 \href{https://github.com/VincentGranville/Main/blob/main/telecom.xlsx}{this spreadsheet}.



\subsection{WGAN with PCA transform and standardization}

Wasserstein GANs are said to avoid some of the drawbacks of traditional GANs, see 
\href{https://developers.google.com/machine-learning/gan/problems}{here}: \textcolor{index}{mode collapse}\index{mode collapse}, 
vanishing gradients, failure to converge and so on. They are based on a different loss function, the 
\textcolor{index}{Wasserstein loss}\index{Wasserstein loss} [\href{https://en.wikipedia.org/wiki/Wasserstein_GAN}{Wiki}].
 See simple example \href{https://machinelearningmastery.com/how-to-code-a-wasserstein-generative-adversarial-network-wgan-from-scratch/}{here}.

A successful implementation is discussed in~\cite{ieeewgan}. It also involves an artificial binary feature called ``labels", 
to discriminate between real and synthetic observations. However, many of the evaluation metrics fail to detect poor performance. This is illustrated on 
 the circle dataset~\cite{vgvendors}, where marginal distributions and lack of correlations is correctly replicated, but more complex multivariate patterns such
 as circular distribution, involving two or more features jointly, are entirely missed. Better evaluation metrics for 
\textcolor{index}{deep comparison}\index{deep comparison} of real and synthetic data, are discussed in section~\ref{evrd7hg}.
 See also~\cite{ieeeaccess22}.

Some of the issues related to GAN or WGAN applied to tabular data are: synthetizations strongly depend on the seed, quality significantly varies from 
 epoch to epoch; it may perform poorly on small datasets, requires data standardization, works well on some datasets but not at all on other datasets, and relies heavily on the choice of hyperparameters. This is the reason why I developed NoGAN alternatives, discussed in chapter~\ref{chnogan}. Our version of WGAN 
 is implemented \href{https://github.com/VincentGranville/Main/blob/main/WGAN.py}{here}. Thus far, we are still experimenting with it. However you might want to look at the code, as it illustrates several improvements such as the use of labels and various data transforms including PCA. 

Transforming the data via PCA prior to synthesizing, followed by the inverse PCA transform applied to the generated data, has the following benefit.
 It creates non-correlated features, thus GAN or WGAN does not need to generate the correct correlation structure, as long as it is able to
 preserve the absence of correlations in the transformed features. The inverse PCA does the job of putting the correct correlations back into the generated data. However, it significantly decreases the value brought in by neural networks. Indeed, it is very easy to sample from the
 \textcolor{index}{empirical distribution function}\index{empirical distribution} separately for each feature, without neural networks. Combined with restoring the proper correlation structure,
 this is identical to using \textcolor{index}{copulas}\index{copula} for synthetization. See section~\ref{piviiiurobvbc}. 

There are few examples on how to implement the full inverse PCA transform, even though it is a simple linear transformation. See
 \href{https://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-com}{here} for
 an illustration.  Since the goal is to work with uncorrelated features, there are simpler alternatives to PCA and inverse PCA. In section
 7.2.1 in~\cite{vgelsevier}, I describe a simple method to decorrelate features: it requires the computation of a square root of the
 covariance matrix. In the end, we haven't succeeded yet with WGAN, which is a back-end modification of GAN. However, in section~\ref{ou6xkj},
 I explain how we fixed many of the issues using front-end modifications. Yet so far, NoGAN outperforms everything else by a long shot,
 both in computing time and quality of the synthetizations.

\section{Front-end enhancements to GAN technology}\label{ou6xkj}

Eventually, the most significant improvements over standard GAN, were obtained by the following front-end mechanisms: \vspace{1ex}

\begin{itemize}
\item Using the simple linear transform described in section~\ref{lin12qdj}, rather than a full PCA.
\item Testing with different seeds, choosing the seed that results in the best synthetization.
\item Creating a full synthetization and evaluating the quality of the generated data at each epoch.
 The final synthetization corresponds to the epoch yielding the best result. 
\end{itemize}\vspace{1ex}

\noindent The last item in the above list amounts to using the evaluation metric as the loss function, outside the neural network / 
\textcolor{index}{gradient descent}\index{gradient descent} framework. In the Python code,
 the evaluation is performed by the  \texttt{gan\_distance} function. The corresponding value (one per epoch) is denoted
 as \texttt{g\_dist}. Finally, to generate a full synthetization and evaluation at each epoch, we use \texttt{n\_eval=1}. The evaluation
 metric measures the distance between the generated and real data, with 0 being best, and 1 being worst. It is based 
 on a combination of the \textcolor{index}{correlation matrix distance}\index{correlation matrix distance} (real vs synthetic) and the 
\textcolor{index}{Kolmogorov-Smirnov distances}\index{Kolmogorov-Smirnov distance} for each feature separately, comparing 
 the \textcolor{index}{empirical distributions}\index{empirical distribution}, synthetic vs real. It is further described in~\cite{vgvendors}. 
 Better evaluation metrics are described in section~\ref{evrd7hg}, in particular using the distance between the multivariate empirical distributions(ECDFs). 
This metric is similar to the \textcolor{index}{total variation distance}\index{total variation distance} [\href{https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures}{Wiki}], and works well both with categorical and numerical features.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{gan1.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Typical behavior of working GAN, here using two features only}
\label{fig:nog29j}
\end{figure}



\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{gan2.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Failed GAN with very high and volatile loss functions}
\label{fig:nog29jfg}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{gan3.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Failed GAN, same as Figure~\ref{fig:nog29jfg} but using scale of Figure~\ref{fig:nog29j}}
\label{fig:nog29jq8}
\end{figure}

\section{Evaluating synthetizations}

In this section, I compare the results obtained initially using only two numerical features from the telecom dataset, how adding a new feature made GAN to fail, and the improvements obtained by fixing it. Comparison is based on summary statistics, the \texttt{g\_dist} distance described in
 section~\ref{ou6xkj}, and plots of the loss functions over the successive epochs. These plots feature the back-end losses
 \texttt{d\_hist} and \texttt{g\_hist} associated respectively to the discriminator and generator models internal to GAN, as well as the 
front-end \texttt{g\_dist} evaluation metric.

\subsection{Loss function history log}

Figure~\ref{fig:nog29j} shows the history log when using only two features in the dataset: tenure (the number of months the customer has been
 with the company), and average monthly charges per customer. Everything is working as expected, with losses becoming smaller and smaller over 
 successive epochs, and stabilizing in less than 3000 epochs. By contrast, Figure~\ref{fig:nog29jfg} shows the history log after adding a third feature: total charges per customer, accumulated over the entire tenure period.  
This new feature is highly correlated to tenure, causing GAN not to converge. In particular, losses are highly volatile and much higher, showing no sign of stabilization even after $\num{10000}$ epochs. Figure~\ref{fig:nog29jq8} is identical to Figure~\ref{fig:nog29jfg} except that the Y-axis (loss values)
 has been truncated to make it comparable to Figure~\ref{fig:nog29j}. It truly shows how bad the situation is.



Finally, Figure~\ref{fig:nog29jcz} show the loss functions after fixing the issue, using the techniques discussed in 
 section~\ref{ou6xkj}. While losses are higher than in Figure~\ref{fig:nog29j} (partly because the loss function depends on the number of features), the
 behavior is back to normal, with convergence at least to a local minimum. Other metrics discussed later in this section confirms the
 significant improvement.

\subsection{Histograms: real versus synthetic data}

In Figure~\ref{fig:nogvvc}, we  show how the empirical distributions attached to the real data (leftmost plots)
 compare  to that observed in the synthetizations. The top, middle and bottom plots correspond respectively to the following features:
 tenure, monthly charges, and total charge residues obtained via the transform discussed in section~\ref{lin12qdj}.
From left to right, the histograms correspond to the real data, the 2D GAN based on two features (thus the bottom plot is absent), 
 the failed GAN after the introduction of total monthly charges, the fixed GAN obtained as discussed in section~\ref{ou6xkj}, 
 and finally (rightmost plots), the NoGAN synthesizer discussed in chapter~\ref{chnogan} and significantly outperforming all other methods both in term of
 speed and quality.


\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{gan4.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Fixed GAN, with behavior similar to that in Figure~\ref{fig:nog29j}}
\label{fig:nog29jcz}
\end{figure}

\subsection{Summary statistics: real versus synthetic data}

The summary statistics presented here are an extract from a spreadsheet with detailed results, available 
\href{https://github.com/VincentGranville/Main/blob/main/telecom.xlsx}{here} on GitHub. 
 I first compare the impact of choosing the best epoch and best seed, on the quality of the generated data. Then I show how statistics such as correlations, means, and standard deviations are replicated in the generated data, depending on the synthesizer.

Table~\ref{ccpufd2po} shows how sensitive to the seed GANs are (see parameter \texttt{seed} in the code).
 It also illustrates that stopping at the last epoch in a run of (say) $\num{10000}$ epochs is not only time consuming, but
 also quite inefficient, due to the volatility of \texttt{g\_dist} between successive epochs. The best synthetizations depend
 strongly on the seed and the \textcolor{index}{stopping rule}\index{stopping rule}: at which epoch you decide to stop the training. Clearly the fixed GAN does a lot better than the failed one, when the distance between the real and synthetic data
 is measured with \texttt{g\_dist}.

\begin{table}[H]
\small
%\[\
\begin{center}
\scalebox{1.0}{
\begin{tabular}{|l|cc|cc|cc|cc|}
\hline
    % \multirow{2}{*}{} &

    \multicolumn{1}{|c|}{} &
      \multicolumn{2}{c|}{2D GAN} &
      \multicolumn{2}{c|}{Failed GAN} &
      \multicolumn{2}{c|}{Fixed, seed 104} &
      \multicolumn{2}{c|}{Fixed, seed 108}   \\
%\hline
\vspace{-4.5ex} \\
   & \texttt{g\_dist}  & \texttt{epoch}  &  \texttt{g\_dist}  & \texttt{epoch} & \texttt{g\_dis}t  & \texttt{epoch} &  \texttt{g\_dist}  & \texttt{epoch}  \\
%$K_\text{avg}$ & $K_\text{max}$ & 
%  $\Delta_{\text{avg}}$ & $\Delta_{\text{max}}$ & $K_\text{avg}$ & $K_\text{max}$  \\ 
\hline
\hline
Best epoch  & 0.1739& $\num{2971}$ &0.6458& $\num{2172}$ &0.4960& $\num{1670}$ &0.1793& $\num{12749}$ \\		
Last epoch  &0.2152& $\num{3000}$ &1.6765& $\num{10000}$ &0.5778& $\num{3000}$  &0.3360& $\num{20000}$\\		
\hline		
\end{tabular}
}
%\]
\caption{\label{ccpufd2po} Quality \texttt{g\_dist} of synthetization, depending on model and epoch number}
\end{center}
\end{table}

%---------------------------------------------------


In the remaining of this section, ``Fixed GAN" is the fixed version based on the best tested seed (seed 108). Also, features 1, 2, and 3
 are respectively tenure, monthly charges, and total monthly charges (reconstructed if applicable, rather than the residues). This applies to Tables~\ref{trabtresg1a} and~\ref{trcazi8esg1a}.


%\renewcommand{\arraystretch}{0.99999} %%%

\begin{table}[H]
\small
%\setlength\extrarowheight{-2pt}
\[
\begin{array}{lccc}
\hline
\text{Correlations}	&  1/2 & 1/3 & 2/3\\ 
\hline
 \text{Real data} &  0.40 &  0.95 & 0.55 \\
\text{Failed GAN} &  0.50 &  0.81 & 0.91 \\
\text{Fixed GAN} &  0.36 &  0.99 & 0.43 \\
\text{NoGAN} &  0.38 &  0.95 & 0.53 \\
\hline
\end{array}
\]
\caption{\label{trabtresg1a} Feature pairwise correlations}
\end{table}
%\renewcommand{\arraystretch}{1.0} %%%


\begin{table}[H]
\small
%\setlength\extrarowheight{-2pt}
\[
\begin{array}{lcrrrrrr}
\hline
\text{Synthetization}	&  \text{Feature} & \text{Mean} & \text{Stdev} & \text{P$_{.25}$} & \text{P$_{.75}$} & \text{Min} & \text{Max}\\ 
\hline
\hline
 \text{Real data} &  1  &  17.98 &	19.53	&2.00	&29.00&	1.00	&72.00 \\
\text{Failed GAN} &  1 &11.99&	2.80&	9.96&	13.82&	4.68&	22.29\\
\text{Fixed GAN} &  1  &  14.10&	15.98&	1.84&	21.63&	-0.22&	106.66 \\
\text{NoGAN} &  1      &  17.57 &	19.10&	2.00&	28.00&	1.00&	71.00 \\
\hline
\text{Real data} &  2  &  74.44&	24.67&	56.15&	94.20&	18.85&	118.35 \\
\text{Failed GAN} &  2 &107.21&	23.39&	90.73&	121.40&	46.02&	207.97\\
\text{Fixed GAN} &  2  & 77.84&	28.43&	55.56&	95.44&	13.22&	194.45\\
\text{NoGAN} &  2      &  74.61&	24.75&	59.56&	94.42&	18.97&	117.79 \\
\hline
\text{Real data} &  3& 1531.80&	1890.82&	134.50&	2331.30&	18.85&	8684.80 \\
\text{Failed GAN} &  3 &851.73&	166.74&	733.97&	965.02&	426.87&	1485.58\\
\text{Fixed GAN} &  3      &  1213.48&	1464.58&	118.47&	1893.85&	-58.08&	9392.70 \\
\text{NoGAN} &  3  & 1487.23&	1841.66&	139.39&	2211.96&	16.91&	8378.25\\
\hline
\end{array}
\]
\caption{\label{trcazi8esg1a} Summary statistics, comparing real data with various synthetizations}
\end{table}
%\renewcommand{\arraystretch}{1.0} %%%

Table~\ref{trcazi8esg1a} shows the substantial improvements obtained with the fixed GAN. Yet NoGAN brings the quality to
 a whole new level, in just 5 seconds rather than 15 minutes to train GAN. In addition, it generates integer numbers for ordinal or binary features, avoiding truncation issues. This is visible if you look at feature 1 (tenure, measured in months). Note that GAN can be accelerated without quality loss by randomly eliminating
 a large number of observations from the training set, see section~\ref{aithing}. NoGAN is discussed in details in chapter~\ref{chnogan}. 


\section{Python code for home-made GAN}

This version corresponds to the fixed GAN with three features, the linear transform discussed in section~\ref{lin12qdj} (thus with total charges replaced by residues), and \texttt{seed=108}. A full synthetization is produced and evaluated with
 \texttt{g\_dist} at each epoch, 
 thanks to \texttt{n\_eval=1}. It allows you to identify which epoch produces the best synthetization. 
 The Python code is also on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/GAN_telecom.py}{here}. \vspace{1ex}



%plots  crossvalidate

%xxx

%nogan
%wgan
%how to know gan is failing / gan false negative

%xxx
%middle column red is shifted

\begin{lstlisting}
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import random as python_random
from tensorflow import random
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam    # type of gradient descent optimizer
from numpy.random import randn
from matplotlib import pyplot
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
import scipy
from scipy.stats import ks_2samp
from statsmodels.distributions.empirical_distribution import ECDF


#--- read data and only keep features and observations we want

url = "https://raw.githubusercontent.com/VincentGranville/Main/main/Telecom.csv"
data = pd.read_csv(url)
# data.dropna(how="any",inplace = True) 

# keep minority group only (Churn = 'Yes')
data.drop(data[(data['Churn'] == 'No')].index, inplace=True)

# use numerical features only
features = ['tenure', 'MonthlyCharges', 'TotalCharges']    
X = data[features]

# transforming TotalCharges to TotalChargeResidues, add to dataframe
arr1 = data['tenure'].to_numpy()
arr2 = data['TotalCharges'].to_numpy() 
arr2 = [eval(i) for i in arr2]                        # turn strings to floats
residues = arr2 - arr1 * np.sum(arr2) / np.sum(arr1)  # also try arr2/arr1
data['TotalChargeResidues'] = residues

# use numerical features only
# features = ['tenure', 'MonthlyCharges', 'TotalCharges']    
features = ['tenure', 'MonthlyCharges', 'TotalChargeResidues']   
X = data[features]

# without this, Tensorflow fails
X.to_csv('telecom_temp.csv') 
data = pd.read_csv('telecom_temp.csv')

print(data.head())
print (data.shape)
print (data.columns)

nobs = len(X)
n_features = len(features)


#--- some initializations

seed = 108  #104   # to make results replicable (much better than 102, 103)
np.random.seed(seed)     # for numpy
random.set_seed(seed)    # for tensorflow/keras
python_random.seed(seed) # for python

g_adam = Adam(learning_rate=0.05)  # gradient descent for generator
d_adam = Adam(learning_rate=0.001) # gradient descent for discriminator 
adam = Adam(learning_rate=0.001)   # gradient descent for full GAN
latent_dim = 20 ## 
batch_size = 128
n_inputs   = n_features 
n_outputs  = n_features 
mode = 'Enhanced'  # options: 'Standard' or 'Enhanced'


#--- define models and components (latent data)

def generate_latent_points(latent_dim, n_samples):
    x_input = randn(latent_dim * n_samples) 
    x_input = x_input.reshape(n_samples, latent_dim)
    return x_input

def generate_fake_samples(generator, latent_dim, n_samples):
    x_input = generate_latent_points(latent_dim, n_samples) # random N(0,1) data
    X = generator.predict(x_input,verbose=0) 
    y = np.zeros((n_samples, 1))  # class label = 0 for fake data
    return X, y

def generate_real_samples(n):
    data_real = pd.DataFrame(data=data, columns=features) 
    X = data_real.sample(n)   # sample from real data
    y = np.ones((n, 1))  # class label = 1 for real data
    return X, y

def define_generator(latent_dim, n_outputs): 
    model = Sequential()
    model.add(Dense(15, activation='relu',  kernel_initializer='he_uniform', input_dim=latent_dim))
    model.add(Dense(30, activation='relu'))
    model.add(Dense(n_outputs, activation='linear'))
    model.compile(loss='mean_absolute_error', optimizer=g_adam, metrics=['mean_absolute_error']) # 
    return model

def define_discriminator(n_inputs):
    model = Sequential()
    model.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))
    model.add(Dense(50, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer=d_adam, metrics=['accuracy']) 
    return model

def define_gan(generator, discriminator):
    discriminator.trainable = False # weights must be set to not trainable
    model = Sequential()
    model.add(generator) 
    model.add(discriminator) 
    model.compile(loss='binary_crossentropy', optimizer=adam)  
    return model


#--- model evaluation, also generate the synthetic data

def gan_distance(model, latent_dim, nobs_synth): 

    # generate nobs_synth synthetic rows as X, and return it as data_fake
    # also return correlation distance between data_fake and real data


    latent_points = generate_latent_points(latent_dim, nobs_synth)  
    X = model.predict(latent_points, verbose=0)  
    data_fake = pd.DataFrame(data=X, columns=features) 
    data_real = pd.DataFrame(data=data, columns=features) 

    # convert Outcome field to binary 0/1
    #outcome_mean = data_fake.Outcome.mean()
    #data_fake['Outcome'] = data_fake['Outcome'] > outcome_mean
    #data_fake["Outcome"] = data_fake["Outcome"].astype(int)

    # compute correlation distance
    
    R_data      = np.corrcoef(data_real.T) # T for transpose
    R_data_fake = np.corrcoef(data_fake.T)
    max_R = np.max(abs(R_data-R_data_fake)) # 

    # compute Kolmogorov-Smirnov (ks) distance

    max_ks = 0
    for col in features:
        # loop over each numerical feature
        dr = data_real[col]
        dt = data_fake[col]
        stats = ks_2samp(dr, dt)
        ks = stats.statistic
        if ks > max_ks:
            max_ks = ks 
    return(data_fake, max_R, max_ks) 


#--- main function: train the model

def train(g_model, d_model, gan_model, latent_dim, mode, n_epochs=20000, n_batch=batch_size, n_eval=1):   
    
    # determine half the size of one batch, for updating the  discriminator
    half_batch = int(n_batch / 2)
    d_history = [] 
    g_history = [] 
    g_dist_history = []
    if mode == 'Enhanced':
        g_dist_min = 999999999.0  

    for epoch in range(0,n_epochs+1): 
                 
        # update discriminator
        x_real, y_real = generate_real_samples(half_batch)  # sample from real data
        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)
        d_loss_real, d_real_acc = d_model.train_on_batch(x_real, y_real) 
        d_loss_fake, d_fake_acc = d_model.train_on_batch(x_fake, y_fake)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # update generator via the discriminator error
        x_gan = generate_latent_points(latent_dim, n_batch)  # random input for generator
        y_gan = np.ones((n_batch, 1))                        # label = 1 for fake samples
        g_loss_fake = gan_model.train_on_batch(x_gan, y_gan) 
        d_history.append(d_loss)
        g_history.append(g_loss_fake)

        if mode == 'Enhanced': 
            (data_fake, max_R, max_ks) = gan_distance(g_model, latent_dim, nobs_synth=1869) 
            g_dist = 0.5 * max_R + max_ks 
            if g_dist < g_dist_min and epoch > int(0.4*n_epochs): 
               g_dist_min = g_dist
               best_data_fake = data_fake
               best_epoch = epoch
               print("  --> Best epoch %6d: max_R = %8.5f | max_ks = %8.5f" %(epoch, max_R, max_ks))
        else: 
            g_dist = -1.0
        g_dist_history.append(g_dist)
                
        if epoch % n_eval == 0: # evaluate the model every n_eval epochs
            print('>%d, max_R=%5.3f, max_ks=%5.3f d=%5.3f g=%5.3f g_dist=%5.3f g_dist_min=%5.3f'  
                % (epoch, max_R, max_ks, d_loss,  g_loss_fake, g_dist, g_dist_min))       
            plt.subplot(1, 1, 1)
            plt.plot(d_history, label='d')
            plt.plot(g_history, label='gen')
            # plt.show() # un-comment to see the plots
            plt.close()
       
    OUT=open("history.txt","w")
    for k in range(len(d_history)):
        OUT.write("%6.4f\t%6.4f\t%6.4f\n" %(d_history[k],g_history[k],g_dist_history[k]))
    OUT.close()
    
    if mode == 'Standard':
        # best synth data is assumed to be the one produced at last epoch
        best_epoch = epoch
        (best_data_fake, max_R, max_ks) = gan_distance(g_model, latent_dim, nobs_synth=1869)  
        g_dist_min = 0.5 * max_R + max_ks 
       
    return(g_model, best_data_fake, g_dist_min, best_epoch) 


#--- main part for building & training model

discriminator = define_discriminator(n_inputs)
discriminator.summary()
generator = define_generator(latent_dim, n_outputs)
generator.summary()
gan_model = define_gan(generator, discriminator)

model, data_fake, g_dist, best_epoch = train(generator, discriminator, gan_model, latent_dim, mode)

data_fake.to_csv('telecom_gan.csv') 
print(data_fake.head(10))
print("Distance between real/synthetic: %5.3f" % (g_dist))
print("Based on epoch number: %5d" % (best_epoch))
\end{lstlisting}




%================
\chapter{Miscellaneous Topics}\label{aasdatk3q}

This chapter covers additional topics complementing the material discussed earlier. In particular, section~\ref{cfd43wpo}
 explores an additional generative AI technique, namely \textcolor{index}{agent-based modeling}\index{agent-based modeling}.
 It is an alternative framework to GAN, NoGAN, deep resampling, copulas and interpolation methods discussed in previous chapters. Popular
 in operations research circles, its goal is to simulate complex systems.  

Section~\ref{g0n2fd} focuses on balancing datasets, one of the many usages of synthetic data. It is particular useful when dealing 
 with imbalanced data, in this case in the context of fraud detection: because fraudulent transactions are rare, a solution consists in generating
 artificial ones to augment your training set. The goal is to obtain more robust predictions. The cybersecurity data set explored here is akward and appears challenging at first glance.
 However, it can easily be compacted for easy processing, using SQL queries to perform aggregation.

The remainder of the chapter focuses on original topics presented in my earlier book on statistical science. Only the most interesting ones
 are discussed, with my selection favoring material still very important in modern AI developments.  

\section{Agent-based Modeling:  Simulating Aggregative Processes}\label{cfd43wpo}

In this section, I explain how to efficiently simulate the evolution of agglomerative processes, and visualize their behavior
 with data animations. I use a generic, simple model for illustration purposes: atoms, initially consisting of one electron, collide and merge over time, with a pre-specified maximum number of electrons per atom: the maximum limit. Given enough time, small atoms eventually disappear, leading to a universe with heavy atoms only.

The focus is on the distribution of atom sizes over time and the number of collisions, becoming rarer and rarer as atoms get bigger and the density of atoms per unit volume decreases.  You can use the method and Python implementation in various contexts by updating the algorithm accordingly, and changing the terminology: replacing atoms by particles, or small celestial bodies in the birth of a  start system with planets, or molecules, or even the soap bubbles merging together. A potential simple improvement is to allow the atoms to not only merge and grow in size, but also to split or lose electrons.

Perhaps the most interesting feature is that you can simulate the evolution and interactions of the $10^{80}$ atoms in our universe  without working with individual atoms. Indeed, I use very fast and efficiently simulations based on arrays with fewer than 100 cells, to study the macro behavior. The implementation allows you to simulate either one or hundreds of evolution paths in parallel. The second option leads to the theoretical evolving distribution of atom sizes over time.
 It can be customized to a variety of agglomerative processes.

\subsection{Introduction}\label{dk6fb}\label{twql98}

I use the keywords ``atom" and ``electron" to help people materialize what I mean by agglomerative process. In short, a system
 that starts with elementary particles -- atoms with one electron -- and evolves over time as the result of mergers or
 collisions. In this case, atoms with more and more electrons as they combine together.  But the idea is not restricted to atoms only, and even in the case of atoms, the agglomeration mechanism described here may not correspond to the reality. Think of it as a synthetic universe where atoms combine according to laws that you can customize and not necessarily compatible
 with standard physical laws.

The size of an atom is defined as its number of electrons. I start with $N$ atoms all with one electron, at time $t_0=0$. A collision occurs at time $t_1,t_2$ and so on, randomly between two atoms of any size, resulting in the loss of the two atoms in question, and the creation of a new larger atom.
 The size of the new atom is the sum of the sizes of the two atoms involved in the collision. The maximum size allowed in a collision is set to 50 in the Python implementation. This upper limit is customizable and denoted as \texttt{limit} in the code. 

The probability of a collision does not depend on the size of the atoms involved. However, it would be easy to change this rule, or even to allow the atoms to break apart into smaller atoms. The timing of the collisions ($t_1,t_2$ and so on) depends on the atom density in the system at any given time. Here, it is inversely proportional to the number of atoms 
 in the system. Thus collisions become rarer and and rarer over time. In other words, the time between successive collisions increases over time.  Eventually, after a very long time period, the system gets stuck in a final configuration where no more collisions can take place. The final configuration is random and will be different in each simulation, even if starting with the same number of atoms. 

The purpose here is to study the trajectory (also called path or orbit) of the system: the atom size distribution at any given time, the timing of the collisions, the mean, maximum and minimum atom size at any given time, the time to equilibrium, the speed of the transitions and so on. Both for a single simulation, as well as averages across a very large number of simulations  all starting with the same number of atoms.

\subsection{Atom size distribution} 

You can infer the atom size distribution from the model described in section~\ref{twql98}. I first describe how to perform the computations based on a single simulation. Then I show how you can approximate the expected or theoretical distribution when blending a large number of simulations, all starting with $N$ atoms, each with one electron. Figure~\ref{twql9809} shows the observed distribution based on one simulation, at four different times: soon after the beginning (top left), after some time (top right), after even more time (bottom left) and the final configuration or attractor (bottom right). No more collision occur once reaching the final configuration. This configuration is reached in a finite but very large number of steps.

In Figure~\ref{twql9809}, the X-axis represents the size of the atoms, while the Y-axis represents the frequencies, given the size. For instance, the proportion of atoms of size 1 is 100\% at the beginning. At the end this proportion is 0\%, and the proportion of atoms of size 50 (the maximum size allowed) is about 9\%. However the 9\% may slightly vary depending on the simulation. A data animation featuring how the distribution evolves over time can be seen in  
\href{https://youtu.be/MUvoO9YmfgY}{this video}. I produced it by averaging the distribution across 100 simulations. 
In the video, the time is rescaled so that successive video frames correspond to visible changes in the distribution. Otherwise, the video would look static for a very long time, and show dramatic changes over very short time periods. Time rescaling evens out these changes. 

In this example, $N=8000$, and the number of iterations is a lot larger. This is because in the implementation, when approaching the final state, collisions become very rare as most of them (if allowed) would result in atoms with a size larger than 50. 

\begin{figure}[H]
\centering
\includegraphics[width=0.54\textwidth]{histo2.png}  
\caption{Atom sizes over time, ranging from 1 to 50 electrons}
\label{twql9809}
\end{figure}


\subsubsection{Core algorithm: single simulation path}

For a comprehensive study of particle aggregation, collision rate, speed of particles and \textcolor{index}{collision theory}\index{collision theory} [\href{https://en.wikipedia.org/wiki/Collision_theory}{Wiki}] in the context of statistical mechanics, see the books 
``A Kinetic View of Statistical Physics"~\cite{knie}, and ``Statistical Mechanics"~\cite{sp496}. 
The goal here is considerably more modest but also very different: to provide
 a practical implementation, easy to customize depending on the context, leading to useful visualizations, and videos in particular. Also, the focus is on the macro-behavior, not the interactions of individual particles. Finally, the context may be very different from statistical physics and could even include mergers and acquisitions 
 in the business world, celestial mechanics, or applications in chemistry.


At the core of the algorithm is the sampling procedure: randomly selecting two atoms of arbitrary size, to generate a collision. Not all sizes may be available. At the beginning, we only have small atoms. Towards the end, small atoms are gone. The average and maximum size of an atom is pictured on the left plot in Figure~\ref{fig:pif65lkh}, for a
 typical evolutionary path, where the X-axis represents the time. The right plot shows the cumulative number of collisions over time. If an attempted collision results in an atom with more than 50 electrons (the \texttt{limit} parameter in the Python code), it is rejected. This explains the convex shape and plateauing of the curve on the right plot. However, in case of failure, the Python code, via the parameter \texttt{max\_trials}, allows you to try multiple times at any given step until a collision is accepted. This parameter is set to 1 in Figure~\ref{fig:pif65lkh}.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{size_time.png}  
\caption{Mean vs max atom size (left), collisions over time (right); X-axis is time}
\label{fig:pif65lkh}
\end{figure}

To sample candidate atoms for a collision, you first sample the two atom sizes, denoted as 
\texttt{k} and \texttt{l} in the code (see section ``core of the algorithm"). The frequency table for atom sizes is stored
 in the array \texttt{pvals}. To perform one single simulation, set \texttt{N\_simul=1}. Then \texttt{pvals} is a 
 one dimensional array. Otherwise there is a separate entry for each simulation, and \texttt{pvals} is thus a 2D array.
 Both cases are treated the same way: there is no distinction between one or multiple simulations.

Sampling \texttt{k} and \texttt{l} is performed the classic way, using the empirical CDF (cumulative distribution function) attached to the histogram 
 \texttt{pvals}. To sample deviates from the empirical CDF, call the
 function \texttt{sample\_size}. If some size is missing (no atom with the size in question), it will not cause any problem: 
 missing sizes are implicitly ignored by the sampling procedure. After sampling the first size \texttt{k}, one atom of size 
\texttt{k} must be removed from \texttt{pvals} before sampling \texttt{l}. Note that \texttt{k} and \texttt{l} may be identical, especially at the beginning.   

Now that \texttt{k} and \texttt{l} have been selected (and the sum is smaller or equal to 50, possibly after a few trials), you need to update the atom counts in the main array \texttt{atom\_sizes}. This array controls the evolution of the system. In particular \texttt{atom\_sizes[simul,k]} is the number of atoms of size \texttt{k}, for a particular simulation 
\texttt{simul} (here there is just one simulation),
 at any given iteration. We now need to update this array after the collision, as follows:\vspace{1ex}
\begin{itemize}
\item Decrease the number of atoms of size \texttt{k} by 1,
\item Decrease the number of atoms of size \texttt{l} by 1,
\item Increase the number of atoms of size \texttt{k+l} by 1,
\item Update the time.
\end{itemize}
Again, note that we might have \texttt{k=l}. Thus a collision of two atoms of the same size requires that we have at least two atoms of that size prior to the collision. Also, prior to the collision, it is possible that the number of 
atoms of size \texttt{k+l} is zero. I now discuss how the time is updated, in section~\ref{timul}.

\subsubsection{Time scale}\label{timul}

To mimic some natural processes, the collision times $t_1,t_2$ and so on are updated so that the difference between two 
 subsequent events is proportional to the inverse of the total number of atoms in the system prior to the collision. 
Also, not all attempts result in an actual collision, due to the constraints. The time may or may not be updated when
 an attempted collision is rejected or impossible. To increase the chance of an actual collision at each step, you
 can increase the parameter \texttt{max\_trials}. However, towards the end, once you entered into a final configuration 
 (the equivalent of an absorbing state in a Markov chain), no matter the number of trials, it is not possible to change the system. 

To give a sense of timing, the top left plot in Figure~\ref{twql9809} corresponding to ``early times"  is supposed to represent the universe (atom distribution) after dozens of billions of years. That is, a lot older than the current universe.
 Hydrogen (atoms of size 1) still dominate, but they represent less than 30\% of all atoms. Today, hydrogen represents 90\% of all atoms. In a few trillion years, the distribution will look like the bottom right plot, where heavy elements now dominate, and hydrogen, helium and all the light elements are gone. Of course, this is science fiction and not a prediction, as the actual laws of evolution are not the same as those used in the Python code. 
I set the upper limit to 50 for the size of an atom, as the size can not grow indefinitely, heavy elements eventually decay over time, and elements heavier than iron (size 26) are harder to produce. A more realistic model is to assign a non-uniform  probability $p_{kl}$ for two elements of size $k$ and $l$, to merge. And allow atoms to disintegrate at various speeds depending on size.  

The evolution mechanism implemented here is more appropriate in contexts other than astronomy, especially regarding the long term. But you can customize it to your needs. 
In the Python code, the array \texttt{arr\_t} contains the timing of all attempted collisions, for each simulation. It is updated after any attempted collision. The index \texttt{iter} in the main loop counts attempted collisions. 


\subsubsection{Averaging across multiple simulations}

Figure~\ref{twql9809} shows results obtained with one simulation. It is also easier to understand the implementation when there is only one simulation. In the video posted \href{https://www.youtube.com/watch?v=MUvoO9YmfgY}{here}, the distribution is averaged across 100 simulations. It is a lot smoother, and a good approximation of the theoretical distribution. By contrast, my first video involved one simulation, and
 can be viewed \href{https://www.youtube.com/watch?v=g3tc2ARw7B0}{here}. 

In the implementation, the number of simulations is determined by \texttt{N\_simul}. The main loop is over \texttt{iter} (also
 called step or iteration), with each increment corresponding to an attempted collision. The inner loop is over the simulations, indexed by \texttt{simul} in the code. Thus, at each iteration, a collision is attempted separately for each of the \texttt{N\_simul}
 simulations.  The parallel implementation works as follows: all the important statistics are stored in arrays with one extra dimension: the simulation number. This includes historical data, attempted collision times, cumulated number of collisions, the atom counts for each atom size, and the total number of atoms at the current iteration, with one set of values attached to each simulation. These arrays are respectively denoted as
 \texttt{history\_min}, \texttt{history\_max}, \texttt{history\_mean},
 \texttt{history\_time},
 \texttt{history\_collisions},
 \texttt{atom\_sizes}, and
 \texttt{arr\_N}
 in the code.  

\subsubsection{Collision graphs}

In order to study collision graphs in details, you need to look at individual atoms and their interactions. This is easy
 when you have only a few thousand atoms. In this section, the goal is to investigate global properties rather than local
 behavior, so I did not look at collision graphs. However, I studied the local behavior in my book on synthetic data~\cite{vgelsevier}. The goal was to produce
 synthetic collision graphs, in the context of star collisions. Figure~\ref{fig:pi1238675} shows an example of such graph.
 The number in each circle represents the star involved, and the number attached to each arrow represents the time when the collision occurred. 


\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{graph.png}  
\caption{Example of collision graph}
\label{fig:pi1238675}
\end{figure}

For instance, the arrow with weight 237 between stars 756 and 47 means that stars 756 and 47 collided and merged at time 237. The newly formed star kept the label 47, because it was the largest of the two in the collision, having absorbed more stars in the past. In this example,
 stars were moving according to some physical laws in a synthetic universe, and a collision occurred each time two stars were passing too close to each other. 


\subsection{Python implementation}\label{ioudert}

The code, also available on GitHub \href{https://github.com/VincentGranville/Visualizations/blob/main/Source-Code/atoms.py}{here}, 
 is much simpler  than it looks at first glance. The core of the algorithm is in the small
 section entitled ``core of the algorithm". It is easy to understand what it does by first assuming that the 
number \texttt{N\_simul} of simulations is 1, thus eliminating the inner loop. If you also limit the number of trials to 1 when
 attempting a collision, it eliminates the deepest loop, reducing it to one step. 

The code is somewhat long because of the numerous options. A good chunk focuses on producing high-quality videos and images,
 including the functions \texttt{my\_plot\_init} and \texttt{save\_image}. The latter resizes the images so that they all have the same size and an even 
 number of pixels, a requirement to produce the video. 
 Quite a bit of code is devoted to collecting historical data: this also includes the function \texttt{get\_summary\_stats}.
 
Figure~\ref{fig:pi12o8uy75} is a screenshot of the output generated. Frame indicates the number of video frames produced so far:
  you can choose the schedule for frame generation, to not save an image at each iteration. Otherwise, the code would produce a large number of images, including many subsequent images that are almost identical. The time increases a lot faster than the iteration counter after a while, due to many attempted collisions being rejected as they result in atoms larger than allowed. Also collisions become rarer over time, as the likelihood of a collision is determined by the number of atoms present in the system, decreasing by one after each collision.
 In short, the smaller the number of atoms, the longer the time elapsed between collisions: this is handled by updating the time accordingly. 
 

\begin{figure}[H]
\centering
\includegraphics[width=0.55\textwidth]{pyput2.png}  
\caption{Summary stats starting at video frame 362}
\label{fig:pi12o8uy75}
\end{figure}

Finally, the mean size represents the average atom size at a given iteration (averaged over all simulations). If the maximum size allowed is 50 and the current mean is above 40, you know that you are close to the final configuration: the absorbing state, after which no more collision can take place. In this example, the initial number of atoms was 8000. The mean size in the final configuration is around 42.

To set the frame generation schedule, and thus determine how the final video will look like, you have three options, offered
 by the parameter \texttt{mode}:\vspace{1ex}
\begin{itemize}
\item \texttt{mode='time'} allows you to create a new frame after a fixed time increment. It also allows you to skip the early times when very little action is visible.
\item \texttt{mode='n\_collisions'} allows you to create a new frame after a fixed number of attempted collisions. However, this
  number should be large at the beginning and small at the end, otherwise the video will have some sections moving very slowly, and some sections moving very fast. You can also skip the first few hundred collisions, as action is not very visible during this time period (in other words, things are moving slowly initially).
\item \texttt{mode='atom\_size'} allows you to create a new frame each time the mean atom size has increased by a pre-specified threshold.
 It creates the best videos visually speaking, but also the least realistic as time gets considerably distorted. The benefit is that
 everything seems to be moving at the same speed throughout the evolution.
\end{itemize}\vspace{1ex}
  

\noindent The parameter \texttt{yaxis} allows you to choose between a fixed Y-axis with values between 0 and 1, or an Y-axis that self-adjusts over time. With a fixed Y-axis, changes are not slow initially, but quite rapidly, the distribution of sizes considerably spread
 between 1 and the maximum authorized, making the bar chart looks smaller and smaller, eventually barely visible.  Choosing a variable Y-axis is the best solution in my opinion, but it makes the beginning of the evolution process seem rather static. This effect can be countered
 by setting \texttt{mode='atom\_size'}.  \vspace{1ex}


\begin{lstlisting}
# atoms.py
# Generating and videolizing agglomerative processes

import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import moviepy.video.io.ImageSequenceClip

N       = 8000    # initial number of particles (hydrogen atoms) 
n_iter  = 9000    # number of iterations (time)
limit   = 50      # max number of electrons/atom allowed
N_simul = 100     # number of systems (atom configurations) running in parallel
max_trials = 1    # trials allowed until 1 collision happens
seed = 87
np.random.seed(seed)

atom_sizes = np.zeros((N_simul,limit+1))
collisions = np.zeros(N_simul)
arr_N = []
arr_t = []
N_0 = N
for simul in range(N_simul):
    atom_sizes[(simul,1)] = N
    arr_N.append(N)
    arr_t.append(0.0)

def sample_size(pvals):
    u = np.random.uniform()
    k = 0
    cdf_val = pvals[0]
    while cdf_val < u:
        k = k + 1
        cdf_val = cdf_val + pvals[k] 
    return(k)

def my_plot_init(plt):
    # produces professional-looking plots
    axes = plt.axes()
    [axx.set_linewidth(0.2) for axx in axes.spines.values()]
    axes.margins(x=0)
    axes.margins(y=0)
    axes.tick_params(axis='both', which='major', labelsize=12)
    axes.tick_params(axis='both', which='minor', labelsize=12)
    return()

def save_image(fname,frame):
    global fixedSize
    plt.savefig(fname, bbox_inches='tight')    
    # make sure each image has same size and size is multiple of 2
    # required to produce a viewable video   
    im = Image.open(fname)
    if frame == 0:  
        # fixedSize determined once for all in the first frame
        width, height = im.size
        width=2*int(width/2)
        height=2*int(height/2)
        fixedSize=(width,height)
    im = im.resize(fixedSize) 
    im.save(fname,"PNG")
    return()

def get_summary_stats(pvals):
    min  = -1
    mean = 0
    for k in range(limit+1):    # k is the atom size
        if pvals[k] > 0: 
            if min == -1:
                min = k   # minimum size
            max = k       # maximum size
            mean += k * pvals[k]
    return(min, max, mean)


#---
mode   = 'atom_size'  # options: 'n_collisions', 'time' or 'atom_size'
yaxis  = 'variable'      # options: 'fixed' or 'variable'
show_last_frame = False  # options: True or False
n_frames = -1
t_last = 0.0
n_collisions_last = 0
mean_last = 1
my_dpi = 300          # dots per each for images and videos 
width  = 2400         # image width
height = 1800         # image height
flist = []            # list image filenames for video
history_time = []
history_min  = []     # min atom size over time
history_max  = []     # max atom size over time
history_mean = []     # mean atom size over time
history_collisions = [] 

for iter in range(n_iter+1):

    #-- core of the algorithm (agglomeration)
    for simul in range(N_simul):
        old_N = arr_N[simul]
        trial = 0
        while old_N == arr_N[simul] and trial < max_trials:  
            arr_t[simul] += N_0/arr_N[simul]
            pvals = np.copy(atom_sizes[simul,:])
            pvals = pvals / old_N 
            k = sample_size(pvals)
            aux = np.copy(atom_sizes[simul,:])
            aux[k] = aux[k] - 1      # must be >= 0
            pvals = aux /(old_N - 1)
            l = sample_size(pvals)
            if k + l <= limit and N > 1:
                atom_sizes[(simul,k)] = atom_sizes[(simul,k)] - 1   # must be >= 0
                atom_sizes[(simul,l)] = atom_sizes[(simul,l)] - 1   # must be >= 0
                atom_sizes[(simul,k+l)] = atom_sizes[(simul,k+l)] + 1
                arr_N[simul] = old_N - 1
                collisions[simul] += 1
            trial +=1 

    #-- compute summary stats across the N_simul simulations and over time        
    pvals = np.zeros(limit+1)
    for k in range(limit+1):
        for simul in range(N_simul):
            pvals[k] += atom_sizes[simul,k]/arr_N[simul]
        pvals[k] /= N_simul  # proba a particle (atom) is of size k
    mean_time = np.mean(arr_t)
    mean_collisions = np.mean(collisions)
    (min, max, mean) = get_summary_stats(pvals)  
    print("Frame:%4d Iteration:%6d Time:%6.0f Mean size:%6.2f" 
         %  (n_frames, iter, mean_time, mean))
    history_time.append(mean_time)
    history_min.append(min)
    history_max.append(max)
    history_mean.append(mean)
    history_collisions.append(mean_collisions) 

    #-- visualizations 
    show_image = False
    if mode == 'time':
        if mean_time - t_last > 20 and mean_time > 50:
            show_image = True
            t_last = mean_time
    elif mode == 'n_collisions':
        if mean_collisions - n_collisions_last > 30 and mean_collisions > 500:  
            show_image = True
            n_collisions_last = mean_collisions
    elif mode == 'atom_size':
        if mean - mean_last > 0.1: 
            show_image = True
            mean_last = mean
    if show_last_frame and iter == n_iter - 1:
       show_image = True   
    if show_image:
        if n_frames == -1:
            n_frames = 0
        plt.figure(figsize=(width/my_dpi, height/my_dpi), dpi=my_dpi)
        my_plot_init(plt)
        if yaxis == 'fixed':
            if n_frames == 0:
                y_axis_lim = max(pvals)
            plt.ylim(0, y_axis_lim)
        x_axis = np.linspace(0,limit,limit+1)
        plt.bar(x_axis[1:], pvals[1:])
        fname='histo_frame'+str(n_frames)+'.png'
        flist.append(fname)
        save_image(fname,n_frames)
        n_frames += 1 
        plt.close()
    
clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(flist, fps=10)
clip.write_videofile('histo.mp4')
    
plt.close()
my_plot_init(plt)
plt.ylim(0,max+2)
plt.plot(history_time,history_max, linewidth=0.4)
plt.plot(history_time,history_mean, linewidth=0.4)
plt.show()
plt.close()

my_plot_init(plt)
plt.plot(history_time,history_collisions, linewidth=0.4)
plt.show()
plt.close()
\end{lstlisting}



\section{Fraud Detection and Cybersecurity: Balancing Datasets}\label{g0n2fd}

This section is an extract from my book ``Practical AI \& Machine Learning: Projects and Datasets", offered to participants in my GenAI
 certification program. The presentation style is different: I first discuss the problem, split it into simple steps, then I offer my own solution. 
Details about the certification program (how to enroll, benefits, costs, sample projects, and so on) are available \href{https://mltblog.com/3pWxvZK}{here}.

While the end goal is to synthesize a very challenging dataset, highly imbalanced, the scope of this project is not data synthetization.
 Rather, it consists of splitting the dataset into a small number of rather homogeneous parts, each part having specific characteristics. 
 By implementing an  algorithm separately to each part, one can expect much better results, whether for data synthetization or any other purpose. At the end, the different outputs are recombined together.

What makes this dataset so special? It has about 117k observations, and 16 features There are two big clusters of duplicate observations, representing 
about 95\% of the dataset: each contains a single observation vector, repeated time and over.  Then a smaller number of clusters, each consisting of 2 to 5 duplicate observations. 
Most features are categorical, and some are a mixture of categorical and numerical values. Then there are some obvious outliers.
This is not an error, it is the way the data is. The case study is about cybersecurity, looking at server data to identify fraud. The amount of fraud is also very small. In this project, most of the heavy work consists of identifying and separating the different parts.
 It is done using SQL-like statements in \textcolor{index}{Pandas}\index{Pandas} (Python library). See code in section~\ref{gfrk65zs},
 after completing the project.

\subsection{Project description}

The dataset is located \href{https://raw.githubusercontent.com/VincentGranville/Main/main/iot_security.csv}{here}.
 First, you need to do some exploratory analysis to find the peculiarities. For each feature, find the distinct values, their type (category or continuous) and count the multiplicity of each value. Also count the multiplicity of each observation vector when all features
 are combined. We will split the dataset into three subsets named \texttt{A}, \texttt{C1} and \texttt{C2}, 
and remove some outliers in \texttt{C1} (or at least treat them separately). Simulating observations distributed as in 
\texttt{A} or \texttt{C1} is straighforward.
 For \texttt{C2}, we will use the NoGAN synthesizer. In the remaining, by observation or observation vector, I mean a full row in the dataset. 

\noindent The project consists of the following steps: \vspace{1ex}
\begin{itemize}
\item[] {\bf Step 1}:  Split the data into two subsets \texttt{A} and \texttt{B}. Here \texttt{A} consists of the two big clusters discussed earlier, each containing one 
observation vector duplicated thousands of times. Also add to \texttt{A} any observation duplicated at least 4 times. Keep one copy of each unique observation in \texttt{A}, and add one new feature: the observation count, named \texttt{size}. The set \texttt{B} contains all the unique observations except those that are now in \texttt{A}, with a count (\texttt{size}) for each observation.\vspace{1ex}
\item[] {\bf Step 2}:  Create set \texttt{C} as follows. Remove the columns \texttt{scr\_port} and \texttt{size} from set \texttt{B}.
Then keep only one copy of each duplicated observation, and add an extra feature to count the multiplicity attached to each observation. 
Finally, split \texttt{C} into \texttt{C1} and \texttt{C2}, with \texttt{C1} consisting of observation vectors with multiplicity larger than one, and \texttt{C2} for the other ones (observation vectors that do not have duplicates). Find outliers in \texttt{C1} and remove them.
\vspace{1ex}
\item[] {\bf Step 3}: The feature \texttt{scr\_port} is absent in sets \texttt{C1} and \texttt{C2}, after step 2. 
 Reconstruct the list of values for \texttt{scr\_port} with the correct count, separately for \texttt{C1} and \texttt{C2}, as we will need it in step 4. These 
 satellite tables
 are named \texttt{map\_C1} and \texttt{map\_C2} in the Python code. Double check that all the computations, splitting, mapping, counts, uniques, and aggregation, are correct. \vspace{1ex}

\item[] {\bf Step 4}: Generate synthetic observations for \texttt{C2}, using the NoGAN algorithm described in project~\ref{genaiyert}.
Use the following features only: \vspace{1ex}\\
\textcolor{white}{MMMMM} \texttt{bidirectional\_syn\_packets}	\\
\textcolor{white}{MMMMM}  \texttt{src2dst\_syn\_packets}	\\
\textcolor{white}{MMMMM}  \texttt{application\_category\_name}	\\
\textcolor{white}{MMMMM}  \texttt{application\_confidence}\\
\textcolor{white}{MMMMM}  \texttt{src2dst\_mean\_ps}\\
\textcolor{white}{MMMMM}  \texttt{src2dst\_psh\_packets}\\
\textcolor{white}{MMMMM}  \texttt{bidirectional\_mean\_ps}\\	
\textcolor{white}{MMMMM}  \texttt{label}\vspace{1ex}

\noindent The feature ``label" indicates fraud when the value is not 0. Few observations 
 are labeled as non-fraud in \texttt{C2}. How would you proceed to substantially increase the proportion 
 of non-fraud in \texttt{C2}, in the generated data? How about generating values for the \texttt{src\_port} feature, based on the \texttt{map\_C2} distribution obtained 
 in step 3? Is the distribution in question uniform within its range, between $\num{40000}$ 
and $\num{60000}$? If yes, you could generate  uniform
 values for this feature, possibly different from those actually observed.
\vspace{1ex}

\item[] {\bf Step 5}: Think of a general strategy to synthesize observations not just for \texttt{C2}, but for the full
 original data. Say you want $N = 10^5$ synthetic observations. You need to generate $n_1, n_2, n_3$ observations respectively for 
\texttt{A}, \texttt{C1}, \texttt{C2}, with $N = n_1 + n_2 + n_3$, using a 
\textcolor{index}{multinomial distribution}\index{multinomial distribution} of parameter $[N; p_1, p_2, p_3]$ where
  $p_1, p_2, p_3$ are the the proportions of observations falling respectively in \texttt{A}, \texttt{C1}, and \texttt{C2}.
 
What are the values of $p_1, p_2, p_3$? Finally, describe how you would proceed to synthesize observations 
 separately for \texttt{A}, \texttt{C1}, and \texttt{C2}, once the random observation counts $n_1, n_2, n_3$ have been generated.
 
\end{itemize}

\subsection{Solution}

The code to produce \texttt{A}, \texttt{C1}, and \texttt{C2} is in section~\ref{gfrk65zs}. It also 
produces \texttt{C1\_full} and \texttt{C2\_full}, identical to \texttt{C1} and \texttt{C2} except that duplicate observations are now kept as duplicates, rather
 than aggregated. 
This completes \textcolor{red}{Step 1} and \textcolor{red}{Step 2}. The same code produces
 \texttt{map\_C1} and \texttt{map\_C2}. All these tables are saved as separate tabs
 in a spreadsheet \texttt{iot\_security.xls}, available on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/iot_security.xlsx}{here}.

To check that all the counts are correct, compute the full number of observations in each subset, and verify that the sum matches
  the number of observations in the original data. For instance, \texttt{C1} has 70 unique (distinct) observations or rows, with multiplicity  stored
 in the column \texttt{size}. The sum of this column is $\num{7158}$, representing the actual number of observations.
 Likewise, \texttt{C2} has 87 rows and 110 observations when counting the duplicates. And \texttt{A} has 
 23 rows and $\num{110467}$ observations. Finally, $110 + \num{7158} +\num{110467} = \num{117735}$, matching the number of rows in the original dataset.
This completes \textcolor{red}{Step 3}.

To synthesize \texttt{C2}, I used a minimalist version of the NoGAN code in project~\ref{genaiyert}. This updated version is very generic,
 with all the selected features declared in section [1.2] in the code, along with the sublist of those that are categorical. 
  The code is on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/NoGAN_iot.py}{here}.
 The results -- synthetic data, validation and training sets, along with some evaluation metrics -- are
 in the \texttt{C2\_Synth\_NoGAN} tab 
 in the \texttt{iot\_security.xlsx} spreadsheet,
 \href{https://github.com/VincentGranville/Main/blob/main/iot_security.xlsx}{here}. 
I used the \textcolor{index}{hyperparameter}\index{hyperparameter} 
\texttt{[80,80,80,80,80,80,80,80]} where each component represents the number of bins used for the corresponding feature,
 whether continuous or categorical.

The NoGAN synthesizer works as follows. It splits the real data, in this case 
\texttt{C2\_full}, into two subsets: the \textcolor{index}{training set}\index{training set} to generate
 new observations, and the \textcolor{index}{validation set}\index{validation set} to check how good the generated observations are, by comparing the \textcolor{index}{joint empirical distribution function}\index{empirical distribution!multivariate} (ECDF) of the synthetic data, with that of the validation set. This
 \textcolor{index}{cross-validation}\index{cross-validation} technique is known as the 
\textcolor{index}{holdout method}\index{holdout method}: observations in the validation set are held out (that is, not used to train the model),
 to assess performance outside the training set. The distance between the two multivariate ECDFs, denoted as KS, 
 is the \textcolor{index}{Kolmogorov-Smirnov distance}\index{Kolmogorov-Smirnov distance}. In addition, the KS distance between
 the training and validation sets is also computed, and referred to as ``Base KS". All the KS distances range from 0 (very good) to 1 (very bad).
 A synthetization is good when both the KS and Base KS distances are very similar.

The \texttt{C2\_full} dataset only has 110 observations, including duplicates and outliers. This makes it hard to synthesize, especially since only 50\% of these observations are used for training. The Base KS distance between the training and validation sets is rather large: 0.1455. It means that the validation set is quite different from the training set. But the KS distance between the synthesized and validation sets is barely any larger: 0.1727. Thus, NoGAN did a pretty good job. The KS distance between the synthesized data and the training set is a lot smaller. To balance the dataset (increasing the proportion of observations with label equal to 0), create a large enough sample and discard generated observations with non-zero label. This completes the main part of
 \textcolor{red}{Step 4}.

To answer \textcolor{red}{Step 5}, based on earlier computations,  the proportions $p_1, p_2, p_3$ are respectively
$$
p_1 = \frac{\num{110467}}{\num{117735}}, \quad p_2 =\frac{ 7158}{\num{117735}}, \quad p_3 = \frac{110}{\num{117735}}.
$$
Since the set \text{A} only has 23 distinct observations repeated time and over (totaling $\num{110467}$ when not deduped), to synthesize \texttt{A} we can again use a multinomial distribution with the correct probabilities, this time to generate 23 random counts
 adding to $n_1$. Each count 
 tells you how many times the corresponding unique observation must be repeated. The probability attached to a unique observation is its
 frequency measured in the training data.
 The synthetization of \texttt{C1} is left as an exercise.


\subsection{Python code with SQL queries}\label{gfrk65zs}

The code, also available on GitHub  \href{https://github.com/VincentGranville/Main/blob/main/iot_security.py}{here},
 splits the original dataset into \texttt{A}, \texttt{C1}, and \texttt{C2}. 
It also produces \texttt{map\_C1} and \texttt{map\_C2}. I did not include
 the adapted NoGAN code, but you can find it on GitHub,
 \href{https://github.com/VincentGranville/Main/blob/main/NoGAN_iot.py}{here}. It also contains quite a bit of SQL
 to compute the KS distance. I want to thank \href{https://www.linkedin.com/in/willie-waters-79451350/}{Willie Waters} for bringing this dataset to my attention. \vspace{1ex}


\begin{lstlisting}
import numpy as np
import pandas as pd

url = "https://raw.githubusercontent.com/VincentGranville/Main/main/iot_security.csv"
data = pd.read_csv(url) 
# data = pd.read_csv('iot.csv')
features = list(data.columns)
print(features)
data_uniques = data.groupby(data.columns.tolist(), as_index=False).size()
data_B = data_uniques[data_uniques['size'] <= 3] #
data_A = data_uniques[data_uniques['size'] > 3]
data_A.to_csv('iot_A.csv')
print(data_A)

data_C = data_B.drop(['src_port','size'], axis=1) 
data_C = data_C.groupby(data_C.columns.tolist(), as_index=False).size()
data_C1 = data_C[(data_C['bidirectional_mean_ps'] == 60) | 
                 (data_C['bidirectional_mean_ps'] == 1078) |
                 (data_C['size'] > 1)]
data_C2 = data_C[(data_C['bidirectional_mean_ps'] != 60) & 
                 (data_C['bidirectional_mean_ps'] != 1078) &
                 (data_C['size'] == 1)]
print(data_C)
data_C1.to_csv('iot_C1.csv')
data_C2.to_csv('iot_C2.csv')

data_B_full = data_B.join(data.set_index(features), on=features, how='inner') 
features.remove('src_port')
data_C1_full = data_C1.merge(data_B_full, how='left', on=features) 
data_C2_full = data_C2.merge(data_B_full, how='left', on=features) 
data_C1_full.to_csv('iot_C1_full.csv')
data_C2_full.to_csv('iot_C2_full.csv')

map_C1 = data_C1_full.groupby('src_port')['src_port'].count()
map_C2 = data_C2_full.groupby('src_port')['src_port'].count()
map_C1.to_csv('iot_C1_map.csv')
map_C2.to_csv('iot_C2_map.csv')

data_C1 = data_C1_full.drop(['src_port','size_x', 'size_y'], axis=1)
data_C1 = data_C1.groupby(data_C1.columns.tolist(), as_index=False).size()
data_C2 = data_C2_full.drop(['src_port','size_x', 'size_y'], axis=1)
data_C2 = data_C2.groupby(data_C2.columns.tolist(), as_index=False).size()
data_C1.to_csv('iot_C1.csv')
data_C2.to_csv('iot_C2.csv')
\end{lstlisting}


%================

\section{Advances in Applied Statistical Engineering}

The deep resampling synthesizer discussed in chapter~\ref{chnogan2} is based on a much better version of 
 \textcolor{index}{bootstrapping}\index{bootstrapping} [\href{https://en.wikipedia.org/wiki/Bootstrapping_(statistics)}{Wiki}], compared to the traditional method. The generated data
 can be used to produce non-parametric \textcolor{index}{confidence intervals}\index{confidence interval} (CI). In this section, I focus instead on another
 technique to compute distribution-free, data-driven CIs more efficiently, that is, with smaller samples than expected.  In short, designing CI
 of length proportional to $n^{-\beta}$, with $\beta>\frac{1}{2}$, where $n$ is the sample size. Standard CIs have $\beta = \frac{1}{2}$ and are thus wider and less accurate.



\subsection{Traditional statistics versus new machine learning approach}\label{excetp09}

The standard symmetric confidence interval of level $0<p<1$, for a statistic or parameter $\tau$ (mean, proportion, correlation and so on) has the following form:
\begin{equation}
\text{CI}(\tau, p, n) = [\tau_n - \lambda_p \gamma n^{-\beta},  \tau_n + \lambda_p \gamma n^{-\beta}].\label{oregon}
\end{equation}
Here $\tau_n$ is the value of $\tau$ estimated on your dataset, $n$ is the number of observations (sample size),
$\beta = 1/2$, and $\lambda_p$ is some quantile of the Gaussian distribution. For instance, for a $95\%$ confidence level ($p=0.95$),
 we have $\lambda_p \approx 1.96$. Formula~(\ref{oregon}) is a first-order 
\textcolor{index}{asymptotic approximation}\index{asymptotic approximation} resulting from the
 \textcolor{index}{central limit theorem}\index{central limit theorem}. The larger $n$, the better the approximation.
 Examples are provided in Table~\ref{mse112dsdcx12}, with an explicit value for $\gamma$ in each case.

Let $\alpha n^{-\beta}$ be the length of the interval. 
When the CI is symmetric, $\alpha = 2\lambda_p \gamma$.
 The value of $\gamma$ in 
 Table~\ref{mse112dsdcx12} when $\tau$ is the
 $q^{th}$ quantile, is well-known, with explanations \href{https://stats.stackexchange.com/questions/99829/how-to-obtain-a-confidence-interval-for-a-percentile}{here}.  
In this case, $F$ is the 
\textcolor{index}{cumulative distribution function}\index{distribution function} abbreviated as CDF, the inverse $F^{-1}$ is the \textcolor{index}{quantile function}\index{quantile function}, and the derivative $F'$ is the \textcolor{index}{probability density function}\index{probability density function} abbreviated as PDF.
 Since in practice $F$ is not known, you replace it by the \textcolor{index}{empirical distribution}\index{empirical distribution}, abbreviated as ECDF and denoted as $F_n$ in Table~\ref{mse112dsdcx12}. Same for $F^{-1}$ and $F'$.

Likewise, for the correlation between two features, the value of $\gamma$ in Table~\ref{mse112dsdcx12}
 is an approximation based on the \textcolor{index}{Fisher transform}\index{Fisher transform} [\href{https://en.wikipedia.org/wiki/Fisher_transformation}{Wiki}].
 For details, see~\cite{b18psy}. Note that $\sigma_n, F_n, \rho_n$ are estimates respectively of $\sigma, F, \rho$ based on $n$ observations.
 The conclusion is that accuracy is proportional to the square root of the number of observations, because $\beta = 1/2$. This fact is also well-known in the context of Monte-Carlo simulations. Thus convergence is slow. Any method resulting in a higher $\beta$ would reduce the required sample size to achieve a specific
 accuracy. Thus, it would reduce the cost of data acquisition accordingly.



\renewcommand{\arraystretch}{1.0} %%%
\renewcommand{\arraystretch}{1.2} %%%
\begin{table}[H]
\[
\begin{array}{lccl}
\hline
 \text{Statistic } \tau &  \gamma  & \beta & \text{Comment}   \\
\hline
\text{mean}	&	\sigma & 1/2 &  \text{use observed } \sigma_n \\[0.5ex]
%\text{Variance}	& \sqrt{2}\cdot\sigma^2 & 1/2 &  \text{observed } \sigma^2\\[0.5ex]
q^{th} \text{quantile}	&  \mathlarger{ \frac{\sqrt{q(1-q)}}{|F' (F^{-1}(q))|}    } & 1/2 & \text{use observed $F_n$} \\[2ex]
\text{correlation} &    1-\rho^2  & 1/2 & \text{use observed } \rho_n \\
\hline
\end{array}
\]
\caption{\label{mse112dsdcx12} $\gamma,\beta$ for classic confidence intervals}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%

The purpose of section~\ref{trwsd} is to discuss an alternative to standard confidence intervals. Assuming the dataset has
 $N$ observations, the idea is to compute distribution-free CIs for various subsamples of size $n\leq N$, and then extrapolate for 
 $n > N$. So, you fit the interval width $W_n$ to the curve $W_n = \alpha n^{-\beta}$, choosing $\alpha,\beta$ that produce the best fit.
 In many cases, $\beta > 1/2$.

Before discussing the technique, it is important to know that even in classical statistics, the rule $\beta=1/2$ has exceptions. 
Let us consider the range $R_n$, that is, the difference between maximum and
 minimum, assuming you observe $n$ values. In this case, $\beta$ depends on how the values are distributed. As 
$n\rightarrow \infty$, we have: \vspace{1ex}
\begin{itemize}
\item Uniform on $[0, 1]$: $n^\beta \cdot \text{Stdev}[R_n] \rightarrow \sqrt{2}$, with $\beta = 1$.
\item Triangular symmetric on $[0, 2]$: $n^\beta\cdot\text{Stdev}[R_n] \rightarrow \xi$, with $\beta = 1/2$.
\item Exponential: $n^\beta\cdot\text{Stdev}[R_n]\rightarrow \pi/(\theta\sqrt{6})$, with $\beta = 0$.
\end{itemize}\vspace{1ex}
Here $\xi\approx 0.93$. For the exponential distribution with density $f(x) = \theta \exp(-\theta x)$ with $\theta>0$ and $x\geq 0$, we have 
$$ \text{E}[R_n] =  \frac{1}{\theta}\sum_{k=1}^{n-1}\frac{1}{k} \sim \frac{\log n}{\theta}, 
 \quad \text{Var}[R_n] = \frac{1}{\theta^2}\sum_{k=1}^{n-1}\frac{1}{k^2} \rightarrow \frac{\pi^2}{6\theta^2}.
$$
The result for the exponential distribution is a consequence of 
\textcolor{index}{RÃ©nyi's representation}\index{RÃ©nyi's representation} \cite{renini53}, see \href{https://math.stackexchange.com/questions/3231505/variance-of-the-range-for-the-exponential-distribution}{here}.
%For the standard normal distribution, $E[R_n]\sim \sqrt{2\log n}$, see \href{https://math.stackexchange.com/questions/89030/expectation-of-the-maximum-of-gaussian-random-variables}{here}. 
%For the uniform distribution, $R_n$ has a \textcolor{index}{beta distribution}\index{beta distribution}, thus the result: see \href{https://math.stackexchange.com questions/2830378/distribution-of-range-of-uniform-0-1-distribution}{here}. 
Since $R_n$ is unbounded in this case, it makes sense
 to compute a confidence interval for $R^*_n = R_n - \text{E}[R_n]$, instead of $R_n$. Both $R_n$ and $R^*_n$ have the same standard deviation, and $\beta$ 
applies to the CI for $R^*_n$. The conclusion is that the CI for $R^*_n$ converges very fast to zero for the uniform distribution ($\beta=1$), at standard speed for the triangular distribution ($\beta=1/2$), and not at all for the exponential distribution ($\beta=0$), as $n\rightarrow\infty$. For the Gaussian distribution, convergence seems slower than $n^\beta$ regardless of $\beta>0$.

Another simple case is when the $n$ values are distributed as the square root of a uniform random variable on $[0, 1]$. In that case, 
 asymptotically, the range $R_n$ and the minimum $T_n$ have the same variance. In particular:
\begin{align}
\text{E}[T_n] & = 2n\int_0^1 x^2(1-x^2)^{n-1} dx = \frac{\sqrt{\pi}\Gamma(n+1)}{2\Gamma(n + 3/2)} \sim \frac{\sqrt{\pi}}{\sqrt{4n}},
 \nonumber\\
\text{E}[T_n^2] & = 2n\int_0^1 x^3(1-x^2)^{n-1} dx = \frac{1}{n+1}. \nonumber
\end{align}
From there, it is easy to verify that $n^{\beta}\cdot\text{Stdev}[T_n] \rightarrow \frac{1}{2}\sqrt{4 - \pi}$, with $\beta = 1/2$. The same is true
 for $R_n$. All these results have been confirmed by simulation. Finally, for the Gaussian distribution,
 $\text{E}[R_n]$ is of the order $\sqrt{\log n}$. It is an order of magnitude smaller than for the exponential distribution, the latter having a fatter tail. For details, see \href{https://stats.stackexchange.com/questions/9001/approximate-order-statistics-for-normal-random-variables}{here}. For the variance of $R_n$ for the exponential distribution, see proof of the formula, \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/exporange.pdf}{here}. The Python code for the simulations is on GitHub, 
 \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/Range_simulations.py}{here}, and also listed below.
\vspace{1ex}

\begin{lstlisting}
import numpy as np

n = 5000
m = 10000
nu = 0.5
beta = 0.5

prange = np.zeros(m)
pmin = np.zeros(m)

for k in range(m):
    if k % 1000 == 0:
        print(k)
    x = np.random.uniform(0, 1, n)
    x = x**nu
    # x = np.random.triangular(0, 1, 2, n)
    # x = np.random.poisson(1, n)
    min = np.min(x)
    max = np.max(x)
    prange[k] = max - min
    pmin[k] = min

mean = np.mean(prange)
stdev = np.std(prange)   
mean2 = np.mean(pmin)
stdev2 = np.std(pmin)

print("\nRange expectation:    %6.5f\nMinimum expectation: %8.5f" %(mean, mean2))
print("Range stdev:          %6.5f\nMinimum stdev:       %8.5f" %
          ((n**beta)*stdev, (n**beta)*stdev2))  


\end{lstlisting}



\subsection{How to build small samples that outperform big data}\label{trwsd}

The goal here is to build confidence intervals (CI) with smaller samples than usually required, thus saving costs when data is expensive to acquire. My  concept of CI is different from what is discussed in the literature, and even from my own distribution-free, Bayesian-like dual confidence intervals discussed in my book on synthetic data~\cite{vgelsevier}.  In particular, 
 \textcolor{index}{confidence levels}\index{confidence level} have a different interpretation, not based on any probability distribution.

My CIs are built as follows. Assume that your dataset has $N$ observations.   The first step is to create $M$ subsamples of size $n$, 
  for several values of $n$ lower or equal to $n$. Here $M$ is fixed, preferably not too small. I used $M=20$ and $N=5000$ in many examples. 

Now, let $\tau$ be the statistic you want to build a CI for. In the following, I illustrate the method with $\tau$ being the correlation coefficient between two features. For each value of $n$, say $n=100, 200$ and so on up to $n=N$, you compute $M$ different values of $\tau$: one for each of the $M$ subsamples of size $n$.  Your CI of level $p$ for $\tau$, based on $n$ observations,  
is denoted as $\text{CI}(\tau, p, n)$.
 Its lower and upper bounds are respectively $Q(p')$ and $Q(1-p')$ with $p' = (1-p)/2$. The
 central point is $Q(\frac{1}{2})$. Typically, $p = 95\%$, and $Q$ is the 
 \textcolor{index}{empirical quantile function}\index{quantile function} based on the $M$ values in question, each one being an estimate of $\tau$ based on $n$ observations. Thus $Q(\frac{1}{2})$ is the median value.
The quantile function is the inverse of the \textcolor{index}{empirical distribution}\index{empirical distribution}.
 In Numpy (Python library), the function is called \texttt{quantile}. Its two parameters are $p'$, and the one dimensional array with the $M$ values in question.

I now discuss two key features of the new confidence intervals. Both aim at reducing the amount of data required, to save costs.

\subsubsection{Extrapolating confidence intervals beyond the observed data}\label{hgboutght}

I discuss the case for symmetric confidence intervals. The generalization to asymmetric CIs is straightforward and left as an exercise. 
Look at Figure~\ref{fig:nogrdf2abcgt43}. The left plot shows CIs (the upper and lower bounds) for various values of $n\leq N$
 (with $N = 5000$), for the
 coefficient correlation $\rho$. Here the confidence level is $p=95\%$. 
The right plot shows the curve $W = \alpha n^{-\beta}$. It is the best fit 
 for the width $W$ of the CI, as a function of $n$. In this case, $\alpha = 32.88$ and $\beta = 0.9205$. The estimate
 $\tau$ based on $N$ observation is $\tau_0 = 0.0500$. Thus, the interpolated symmetric CI, for any $n\leq N$, is
\begin{equation}
 \text{CI}(\tau, p, n) = \Big[\tau_0 - \frac{1}{2}\alpha n^{-\beta}, \tau_0 + \frac{1}{2}\alpha n^{-\beta}\Big].\label{babeurretupues}
\end{equation}

It becomes very interesting when you plug in a value of $n$ much larger than $N$ in Formula~(\ref{babeurretupues}). Then,
 we are not doing interpolation anymore, but extrapolation instead: computing a CI for a sample size (say) twice that of our dataset!
 Clearly, the curve fitting in the left plot of Figure~\ref{fig:nogrdf2abcgt43} is good enough to allow for safe extrapolation
 well beyond $n = N$.


\begin{figure}[H]
\centering
\includegraphics[width=0.84\textwidth]{CI_converge.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Confidence interval bounds and width, as a function of sample size}
\label{fig:nogrdf2abcgt43}
\end{figure}


\subsubsection{Beating the laws of randomness to reduce costs by factor 10} 

Another benefit of the method is the large value of $\beta$. For standard confidence intervals, the width is $\alpha n^{-\beta}$
 with $\beta = 1/2$: this is the norm, with exceptions discussed in section~\ref{excetp09}. To the contrary, the new approach yields
 larger $\beta$, for instance $\beta = 0.9205$ in the example discussed in section~\ref{hgboutght}. Thus you get more accurate
 CIs with a smaller sample size. However, in practice, $\alpha$ is larger in the new method. So the gain materializes only when the sample size is large enough.

To illustrate this concept, look at the Table~\ref{widthpol}. It is based on $\alpha = 2, \beta = 1/2$ for the standard confidence interval,   and $\alpha = 32.88, \beta=0.9205$ for the new method. This is for the
 dataset and CI discussed in section~\ref{resa3oblo}.  These typical values may vary slightly based on the dataset and what you want to estimate. When $n>780$, the new CI is shorter, compared to the standard one.

\renewcommand{\arraystretch}{1.0} %%%
\renewcommand{\arraystretch}{1.2} %%%
\begin{table}[H]
\small
\[
\begin{array}{lcc}
\hline
 n  &  \text{New width} & \text{Standard width} \\
\hline
100	& 0.4742	&0.2000\\
500	& 0.1078	&0.0894\\
780	&0.0716	&0.0716\\
1000	&0.0569	&0.0632\\
2000	&0.0301	&0.0447\\
5000	&0.0129	&0.0283\\
\hline
\end{array}
\]
\caption{\label{widthpol} Width of CI, standard versus new}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%
%-----------------------

If your sample size is $5000$ and you extrapolate to $n=\num{10000}$ using the method described in section~\ref{hgboutght},
 the width of the new CI is $0.0068$. You would need a sample size of $\num{90000}$ to reach that level of accuracy with the standard CI. That is, a dataset 18 times bigger. Somehow, the new method defeats the fundamental law of randomness, which states that convergence speed in random experiments is 
 proportional to the square root of the sample size ($\beta = 1/2$). Finally since the new method is more computer-intensive, the cost savings are in terms of data acquisition only, allowing you to work with much smaller datasets than typically required by classical
  techniques.

In the example discussed here and further investigated in section~\ref{resa3oblo}, I created a CI for the correlation $\rho$ between two features. Due to the nature of the experiment, the theoretical value is known, and equal to $\rho = 0.05$. Your estimated correlation  would  be just that if your dataset was infinite. This case study was chosen precisely because $\rho$ is very close to zero, and the goal is
 to check how much data you need to gather to come to the conclusion that $\rho\neq 0$. In addition, the correlation coefficient is not the easiest parameter to handle when using the standard method for CI.


\subsection{The power of smart resampling: case study}\label{resa3oblo}

The dataset has two features and $N$ observations. Observation $k$ is denoted as
 $(x_k, y_k)$, with $x_k = ka - \lfloor ka\rfloor$, and $y_k = kb - \lfloor kb \rfloor$. Here $a =-1 +\sqrt{5}/2$  and $b = 10\sqrt{5}/5$.
 The brackets represent the integer part function, also called floor function. The two sequences $\{x_k\}, \{y_k\}$ are
 time series known as \textcolor{index}{Beatty sequences}\index{Beatty sequences} [\href{https://en.wikipedia.org/wiki/Beatty_sequence}{Wiki}]. 
Each has strong long-range autocorrelations, and both are cross-correlated. The theoretical value of the empirical correlation $\rho$ between the two infinite sequences depends on $a$ and $b$. In this case, it is equal to $1/20$. This type of sequence is studied in my book
 ``Gentle Introduction To Chaotic Dynamical Systems"~\cite{vgchaos}.

You can choose $a$ and $b$ so that $\rho$ gets as close to zero as you want, or even exactly zero. This allows you to test
  whether your method is able to detect tiny deviations from zero, and the minimum sample size needed to do so. Thus, Beatty sequences are a great tool to benchmark various algorithms. Here, I start by reshuffling the bivariate observations to eliminate 
 the autocorrelations. Yet, this step preserves the cross-correlation $\rho$. The reason for this is to illustrate the importance of reshuffling: many datasets contain artificial autocorrelations due to the way the data is organized. A technique that works 
 under the assumption of zero autocorrelation, may fail if that assumption is violated. Figure~\ref{fig:not6resd8},
 featuring the \textcolor{index}{autocorrelation function}\index{autocorrelation function} [\href{https://en.wikipedia.org/wiki/Autocorrelation}{Wiki}] for the two sequences $\{x_k\}, \{y_k\}$, 
 shows the effect of the reshuffling step. 

\begin{figure}[H]
\centering
\includegraphics[width=0.77\textwidth]{CI_ts.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Autocorrelation functions before and after reshuffling}
\label{fig:not6resd8}
\end{figure}

%-----------
\subsection{Best Practices}

The next step (after reshuffling) consists of building the confidence intervals. I describe it in section~\ref{hgboutght}, 
 with Python code in section~\ref{joyofpython}. Here I focus on best practices. Let $N$ be the number of observations in your dataset.
 For each $n=100, 200$ and so on, up to $n = N$, you create $M$ subsamples of size $n$ to build a CI based on $n$ observations. 
When $nM \leq N$, the Python implementation performs sampling without replacement. However, when $nM > N$, replacement is taking place as in traditional \textcolor{index}{bootstrapping}\index{bootstrapping}. As a rule of thumb, you should choose $M\geq 20$.
 Larger $M$ work better but require more computing time and involve more sampling with replacement. A theoretical question that needs to be addressed is whether or not increasing $M$ leads to a stabilization of the results, that is, convergence to an optimal solution. 
 Preliminary investigations suggest that this is the case, however it may depend on a number of conditions.

Finally, the power curve $\alpha n^{-\beta}$ to fit the CI width depends on values computed for the various $n$ in question. For some datasets, the fit may not be as good as the right plot in Figure~\ref{fig:nogrdf2abcgt43}. I recommend to ignore small and large values of $n$ to fit this curve, focusing on values of $n$ between $N/3$ and $2N/3$ instead. In practice, high values of $\beta$ 
 are associated to information-poor datasets, easy to compress. Lower values (still above $1/2$) typically correspond to rich data
 with more complex patterns, and less easy to predict or compress. Thus, $\beta$ is an indicator of the complexity of your dataset.

%xxx
%xxx add new book to estore, bulkbuffer and to MLT newsletter page
%xxx nogan2
%xxx make it a separate paper: Novel machine learning approach to CI , with fast convergence


\subsection{Python code}\label{joyofpython}

%xxxxx xxxyyy %-----

The code is also available on GitHub, \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/confidenceIntervals.py}{here}. It covers all the steps discussed in section~\ref{trwsd}, including reshuffling, creation of the dataset (as well as a different one for comparison purposes), 
 curve fitting to determine $\alpha$ and $\beta$, the visualizations including a scatterplot of the 
 bivariate data (not shown here). The latter reveals the very curious distribution of the observations in 2D, lying on 8 parallel line segments in the unit square, equally spaced. \vspace{1ex}

\begin{lstlisting}
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import statsmodels.api as sm

M = 20      # number of subsamples
p = 0.025   # confidence level for CI (0 < p < 0.50)
data = []
data_raw = []
CI_nobs = []
CI_lower = []
CI_upper = []
CI_mean = [] 
CI_width = []

#--- create dataset with 2 features; theoretical correl = 0.05 

b1 = -1 + np.sqrt(5) / 2
b2 = 2 / np.sqrt(5)
seed = 67
np.random.seed(seed)
[x1, x2] = [0, 0]

mode = 'Beatty'   # options: 'Beatty' or 'Random'
if mode == 'Beatty':
    N = 5000   # size of full dataset
else:
    N = 50000  # size of full dataset

for k in range(N): 
    if mode == 'Beatty':
        x1 = x1 + b1 - int(x1 + b1)
        x2 = x2 + b2 - int(x2 + b2)
    else:
        x1 = np.random.uniform(0, 1) 
        x2 = np.random.uniform(0, 1) 
    data.append([x1, x2])
data_raw = np.array(data)
data = np.copy(data_raw)
np.random.shuffle(data)
print(data)
print()

#--- compute correl CI on M subsamples of size n, for various n

mu_x = np.zeros(M)
mu_y = np.zeros(M)
mu_xx = np.zeros(M)
mu_yy = np.zeros(M)
mu_xy = np.zeros(M)
correl = np.zeros(M)

for n in range(1, N): 

    for sample in range(M):

        idx = int(n + sample*N/M) % N
        obs = data[idx]
        mu_x[sample] += obs[0]
        mu_y[sample] += obs[1]
        mu_xx[sample] += obs[0]*obs[0]  
        mu_yy[sample] += obs[1]*obs[1]  
        mu_xy[sample] += obs[0]*obs[1]  

        if n > 1:  # otherwise variance is zero
            s_mu_x = mu_x[sample]/n  
            s_mu_y = mu_y[sample]/n  
            s_mu_xy = mu_xy[sample]/n 
            var_x = (mu_xx[sample]/n) - (s_mu_x*s_mu_x)
            var_y = (mu_yy[sample]/n) - (s_mu_y*s_mu_y)
            correl[sample] = (s_mu_xy - s_mu_x*s_mu_y) / np.sqrt(var_x * var_y)
  
    if n % 100 == 0:  
        print("Building CI for sample size", n)
        lower_bound = np.quantile(correl, p)
        upper_bound = np.quantile(correl, 1-p)
        CI_nobs.append(n)
        CI_mean.append(np.mean(correl)) 
        CI_lower.append(lower_bound)
        CI_upper.append(upper_bound)
        CI_width.append(upper_bound - lower_bound)

#--- Plot confidence intervals as a function of sample size

mpl.rcParams['axes.linewidth'] = 0.3
mpl.rcParams['legend.fontsize'] = 0.1
plt.rc('xtick',labelsize=7)
plt.rc('ytick',labelsize=7)
plt.xticks(fontsize=7)
plt.yticks(fontsize=7)
plt.subplot(1, 2, 1)
plt.plot(CI_nobs, CI_mean, linewidth = 0.3)
plt.plot(CI_nobs, CI_lower, linewidth = 0.3)
plt.plot(CI_nobs, CI_upper, linewidth = 0.3)
plt.legend(['CI Center','CI Lower Bound','CI Upper Bound'],fontsize = 7)
plt.xlabel('Sample size for confidence interval (CI)', fontsize=7)
plt.ylabel('Estimated value for rho',fontsize=7)

#--- Width of confidence interval based on sample size, and fitted curve

plt.subplot(1, 2, 2)
plt.plot(CI_nobs, CI_width, linewidth = 0.3)
plt.xlabel('Sample size for confidence interval (CI)', fontsize=7)
plt.ylabel('Width of confidence interval',fontsize=7)

CI_nobs_log = np.log(CI_nobs)
CI_width_log = np.log(CI_width)
table = np.stack((CI_nobs_log, CI_width_log), axis = 0)
cov = np.cov(table)
beta = cov[0, 1] / cov[0, 0]
log_alpha = np.mean(CI_width_log) - beta * np.mean(CI_nobs_log)
alpha = np.exp(log_alpha)
Fitted_width = alpha * (CI_nobs**beta)
print("alpha = ", alpha)
print("beta = ", beta)

plt.plot(CI_nobs, Fitted_width, linewidth = 0.3)
plt.ylim([0.00, 0.35])
plt.legend(['CI Width','Fitted curve'],fontsize = 7)
plt.show()

#--- Plot autocorrelations

nlags = 100
lags = np.arange(nlags)
acf_x_raw = sm.tsa.acf(data_raw[:,0], nlags = nlags)
acf_y_raw = sm.tsa.acf(data_raw[:,1], nlags = nlags)
acf_x = sm.tsa.acf(data[:,0], nlags = nlags)
acf_y = sm.tsa.acf(data[:,1], nlags = nlags)

plt.subplot(2,1,1)
plt.plot(lags[1:nlags], acf_x_raw[1:nlags], linewidth = 0.3)
plt.plot(lags[1:nlags], acf_y_raw[1:nlags], linewidth = 0.3)
plt.legend(['1st feature','2nd feature'],fontsize = 7, loc='upper right')
plt.xlabel('Autocorrelation function before reshuffling', fontsize=7)
plt.subplot(2,1,2)
plt.plot(lags[1:nlags], acf_x[1:nlags], linewidth = 0.3)
plt.plot(lags[1:nlags], acf_y[1:nlags], linewidth = 0.3)
plt.legend(['1st feature','2nd feature'],fontsize = 7, loc='upper right')
plt.xlabel('Autocorrelation function after reshuffling', fontsize=7)
plt.show()

plt.scatter(data[:,0], data[:,1], s = 0.1)
plt.show()
\end{lstlisting}



\section{Generating data outside the observation range}

All of the GenAI apps that I tested, including my own, have the same problem. They cannot easily generate data outside the observation range. 
As an example, let's focus on the insurance dataset discussed in section~\ref{sdsvc}. I use it to generate synthetic data
 with \textcolor{index}{GAN}\index{GAN}\index{generative adversarial networks} (generative adversarial networks) and 
 the NoGAN models discussed in chapters~\ref{chnogan} and~\ref{chnogan2}. In the 
 training set, one of the features
 is ``charges", that is, the medical expenses incurred by the policy holder, in a given year.  The range is from $\$1121$ to  $\$\num{63770}$. In the synthesized data, the amount always stays within these two bounds. Worst, most models are 
 unable to produce a synthetic maximum above $\$\num{60000}$. The issue is undetected due to poor evaluation metrics,
 and compounded by the small size of the training set. The same is true for all the other features.  The problem shows up
 in all the tested datasets, no matter how many observations you generate.

The consequences are persistent algorithm bias, and the inability to generate enriched or unusual data. The solution currently adopted is to work
 with gigantic training sets, further increasing costs linked to training, cloud and GPU time usage. What I propose here goes in the opposite direction: cost reduction, smaller training sets, high quality output based on the best evaluation metrics (see section~\ref{evrd7hg}), and the ability
 to generate more diversified data, including meaningful outliers. All this with a fast, simple algorithm based on a clever idea. 

\subsection{Quantile convolution} 

I now discuss the concept in layman's terms, and then briefly explain the underlying theory. By comparison  to
 \textcolor{index}{diffusion models}\index{diffusion model} [\href{https://en.wikipedia.org/wiki/Diffusion_model}{Wiki}] used in computer vision to adress the issue, the technique is a lot simpler. Here I  focus on the one-dimensional case here, but it generalizes to higher dimensions. 

The training set consists of 
 $n$ observations $x_1,\dots, x_n$. I then create a \textcolor{index}{Gaussian mixture model}\index{Gaussian mixture model} (GMM)
 with $n$ components, all having the same weight $1/n$ and same variance $\sigma^2_n$. 
The $k$-th component is a Gaussian centered at $x_k$, with variance $\sigma^2_n$. I then sample $N$ deviates from the GMM to compute its \textcolor{index}{quantile function}\index{quantile function} (the inverse of the CDF), typically with $N$ much larger
 than $n$. At this point, we have three distributions:\vspace{1ex}

\begin{itemize}
\item The empirical distribution $H_n$ attached to the Gaussian mixture.
\item The empirical distribution $F_n$ attached to the training set observations.
\item A generic normal distribution $G_n$, called \textcolor{index}{kernel}\index{kernel}, with zero mean and variance  $\sigma_n^2$. 
\end{itemize}
\vspace{1ex}
\noindent The setting is identical to 
\textcolor{index}{kernel density estimation}\index{kernel density estimation} [\href{https://en.wikipedia.org/wiki/Kernel_density_estimation}{Wiki}]. The derivative of $H_n$ plays the role of the smooth density function estimate. I denote the corresponding 
\textcolor{index}{probability density functions}\index{probability density function} (PDF) as
 $h_n, f_n$ and $g_n$. We have:
\begin{equation}
H_n( z) = \frac{1}{n}\sum_{k=1}^n G_n(z-x) I(x = x_k), \label{qeq1}
\end{equation}
where $I$ is the indicator function, equal to 1 if $x=x_k$, and 0 otherwise. When $n$ is large and
 the discrete PDF $f_n$ is well approximated by a continuous density $f$, we have
$H_n(z) \sim \int G_n(z-x) f(x) dx$. Thus, taking the derivative with respect to $z$, we obtain:
\begin{equation}
h_n(z) \sim \int g_n(z-x)f(x) dx = (g_n * f)(z). \label{qeq2}
\end{equation}
The $*$ symbol denotes the \textcolor{index}{convolution product}\index{convolution product}, in this case
the convolution of probability distributions [\href{https://en.wikipedia.org/wiki/Convolution_of_probability_distributions}{Wiki}]. If $\sigma_n\rightarrow 0$ as $n\rightarrow\infty$,
 then $h_n\rightarrow f$. Also, if $\sigma_n = 0$, then $H_n = F_n$ corresponds to the empirical
 distribution (ECDF) computed on the training set. The ECDF $F_n$ is known to converge to the true
 continuous underlying CDF $F$. This is another way to look at the asymptotic behavior:
 $\sigma_n \rightarrow 0 \Rightarrow H_n \sim F_n \rightarrow F$. 

So, by choosing $\sigma_n > 0$ yet small enough, especially if $n$ is large, we achieve the 
 following comprise: $h_n$ is some intermediate PDF between the discrete, chaotic $f_n$ and the smooth, continuous theoretical but unknown limit, $f$. In practice, for $\sigma_n$, you can choose the standard deviation computed on the training set, multiplied by a small positive factor denoted as $v_n$.

%If you replace the mixture weights $w_n(x_k) = 1/n$ for $k=1,\dots,n$ by an arbitrary positive function $w_n$, then formulas~(\ref{qeq1}) and~(\ref{qeq2}) become
%\begin{align}
%H_n( z) & = \sum_{k=1}^n G_n(z-x) w(x_k), \nonumber \\
%h_n(z) & \sim \int g_n(z-x)w(x)f(x) dx = (g * wf)(x). \nonumber \\ \nonumber
%\end{align}
%Here it is assume that $w_n(x_1) + \dots w_n(x_n)=1$ and the limiting function integrates to 1.

With this framework, it is fast and easy to sample (say) $N=10^6$ deviates from the $H_n$ distribution and sort them to compute (say) $10^3$ quantiles, from $0.0005$ to $0.9995$ by increments of $10^{-3}$, and store them in an array. 
More granular quantiles are obtained by interpolating the pre-computed values. 
If $\sigma_n$ is not too small, it will generate values outside the observation range in the training set.
 Unlike the standard method described 
\href{http://www.awebb.info/probability/2017/05/12/quantiles-of-mixture-distributions.html}{here} and 
\href{https://www.jamesatkins.net/posts/quantile-function-of-mixture-distributions-in-python/}{here} 
to sample from a mixture, it does not involve inverting the CDF $H_n$, resulting
 in a much faster implementation.  The NoGAN 
techniques to generate \textcolor{index}{synthetic data}\index{synthetic data} (see  chapters~\ref{chnogan} and~\ref{chnogan2}) heavily rely on quantiles in high dimensions, and the convoluted quantiles discussed here can easily be
 integrated into these algorithms.

In the end, my technique is a fully automated, data-driven version of
 \textcolor{index}{quantile extrapolation}\index{quantile!extrapolated}. For more on this topic,
 see ``Nonparametric Extrapolation of Extreme Quantiles: a Comparison Study",
 published in 2022~\cite{pcper43w}. Besides smoothing empirical quantiles and outside-the-range
 synthetizations, another application is the generation of meaningful outliers: those not
 resulting from some error, but naturally occurring, albeit rarely, in the real world. This could
 be useful in contexts such as fraud detection.

\begin{figure}[H]
\centering
\includegraphics[width=0.73\textwidth]{equant_charges.png}  
\caption{Histograms for extrapolated ``charges'' ($v=0$ is the training set)}
\label{figttyd38675}
\end{figure}


\subsection{Truncated Gaussian mixtures and bias detection}

 One may use kernels other than Gaussian. Discrete kernels are the best solution when dealing with a 
 categorical or discrete
 feature, such as ``number of children" in the insurance dataset. In this section, I discuss a different type
 of kernel: \textcolor{index}{truncated Gaussian}\index{truncated Gaussian}. You use it if your data is constrained to stay within a specific domain $D$.
 For instance, in the insurance dataset, ``charges" must be positive, and possibly above $\$1000$ due to
 business rules. They may be capped at $\$\num{100000}$. For business reasons, ``age" must be
 between 18 and 64 inclusive. 

In the case studies in section~\ref{cscsder}, I use \textcolor{index}{rejection sampling}\index{rejection sampling} [\href{https://en.wikipedia.org/wiki/Rejection_sampling}{Wiki}] to meet these needs. The principle is simple: 
 if a generated deviate is outside the domain $D$, reject it and continue sampling until you get one that lies inside $D$. This may result in noticeable bias in the synthetic data if $\sigma_n$ is not small enough.
 However, one might say that the bias is in the real data,  not in the synthetic data, especially if $n$ is small.
 The synthetization may be a better representation of the reality. This is true especially when the truncation
 is one-sided, with a hard minimum (values must be above zero) but no hard maximum.

To assess whether the bias is in the real or in the synthetic data, proceed as follows. Create a training set using 
 Monte-carlo simulations and known distribution, with $2n$ values. Then:\vspace{1ex}
\begin{itemize}
\item Use $n$ values (half of the training set) to compute the quantile table $Q_n$ 
 based on $H_n$.
\item Compute the mean both on the half training set $S_n$, and on the quantile table $Q_n$. They  are denoted respectively 
 as $\mu(S_n)$ and $\mu(Q_n)$.
\item Compute the mean $\mu(S_{2n})$ on the full training set.
\end{itemize}
\vspace{1ex}
If $|\mu(Q_n) - \mu(S_{2n})| < |\mu(S_n) - \mu(S_{2n})|$, then the bias is likely more pronounced in the real data (the half training set), rather than in the synthetic data! Alternatively, you can transform the generated quantiles so that the mean and variance match those measured on the training set. This
 makes sense if you use an unusually large $\sigma_n$ with 
 one-sided truncation, resulting in generated values far beyond the maximum or minimum observed in the training set.
 

%--------

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{equant_test.png}  
\caption{Histograms for extrapolated mixture ($v=0$ is the training set)}
\label{tssaqws09}
\end{figure}


\renewcommand{\arraystretch}{1.0} %%%
\renewcommand{\arraystretch}{1.2} %%%
\begin{table}[H]
\small
\[
\begin{array}{lcrrrr}
\hline
\text{Feature}  &v  & P._{0005} & P._{9995} & \text{Median} & \text{Stdev} \\
\hline
\hline
\text{charges} & 0.0 & 1122 & \num{63770} &  \num{9374} & \num{12103}\\
\text{charges} & 0.1 & 1076 & \num{63709} &  \num{9393}  & \num{12079} \\
\text{charges} & 0.2 & 1076 & \num{64538} &  \num{9485}  &  \num{12090}\\
\text{charges} & 0. 4& 1077 & \num{66520} & \num{10392}  & \num{12113}\\
\hline
\text{bmi} & 0.0  &  15.96 &53.13 &30.40 & 6.09 \\
\text{bmi} & 0.1 & 15.89 & 53.10 & 30.39 & 6.13 \\
\text{bmi} & 0.2  & 15.09 &53.44 &30.42 & 6.21 \\
\text{bmi} & 0.4  & 12.77 & 54.31&30.42&6.57\\
\hline
\text{simulated} & 0.0  &  -3.87& 6.66& 0.05&2.87\\
\text{simulated} & 0.1  & -4.35& 7.20& 0.13&2.89\\
\text{simulated} & 0.2  &  -4.87&7.85& 0.21&2.93\\
\text{simulated} & 0.4  & -5.99& 9.21& 0.43&3.09\\
\hline
\end{array}
\]
\caption{\label{widthprtz09} Extreme values as a function of $v$ (training set: $v=0$)}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%




\subsection{Case studies} \label{cscsder}

Figures \ref{figttyd38675}, \ref{tssaqws09} and \ref{fig:pif65lkh} show histograms associated to $H_n$, each with 50 bins, with 4 plots in each picture.
 For $\sigma_n$, I chose the standard deviation computed on the training set, multiplied
 by a small factor $v$, ranging from $v=0.0$ (top left plot) to $v=0.4$ (bottom right). 
So, the top left plot corresponds to $\sigma_n=0$. It represents the frequency distribution in the training set. The Y-axis features bin counts, totaling $n=1000$ across all 50 bins in each plot. The X-axis represents  the
 observed values: the extended range, after extrapolation based on quantile convolution.








Figures \ref{figttyd38675} and  \ref{fig:pif65lkh} correspond to two of the features in the insurance dataset: ``charges" (in dollar amount), and ``bmi" (body mass index). I used a truncated Gaussian for the kernel. 
Figure~\ref{tssaqws09} pictures an artificial dataset: the data was created using a mixture with three components, with $n=100$ observations. Quantile convolution ($v>0$) with a Gaussian kernel clearly generates values  outside the observation range. The Python code is in section~\ref{pylgore}.




\begin{figure}[H]
\centering
\includegraphics[width=0.73\textwidth]{equant_bmi.png}  
\caption{Histograms for extrapolated ``bmi" ($v=0$ is the training set)}
\label{fig:pif65lkh}
\end{figure}

In my examples, $v=0.1$ seems to be the best value, preserving the patterns in the distribution attached to the training set, while generating extreme values that are not too far from the minimum and maximum in 
 the real data. Table~\ref{widthprtz09} summarizes the findings. In particular, the ``simulated" feature
 was created as a mixture with 3 components, also called clusters. With $v=0.1$ or $v=0.2$, the 3 components are still visible: see Figure~\ref{tssaqws09}. 
The technique could also be used to detect the optimum number of clusters, and generalizes to higher dimensions. Note that with $v=0.4$, generated values extend far beyond the observation range, allowing
 you to create meaningful outliers. In Table~\ref{widthprtz09}, $P._{0005}$ and
 $P._{9995}$ are extreme quantiles (convoluted if $v>0$).

Finally, it would be interesting to see what happens when you iterate the method: starting 
with $H_{0,n} =F_n$ to produce $H_{1,n}=H_n$, then using $H_{1,n}$ to produce
  $H_{2,n}$ and so on. In short, synthesizing the synthetic data and so on. Most data synthesizers are unable to sample outside the observation range, resulting 
 in successive iterations generating data within a shrinking range. Conversely, with a large $\sigma_n$, my method will generate data in an expanding range, over several iterations.  The best solution is to choose $\sigma_n$ that keeps the range stable over many iterations. 




\subsection{Conclusion}

The quantile convolution technique helps you generate data outside the observation range, thus 
creating truly enriched datasets, contrarily to all the tools that I tried in the context of synthetic data, whether based on deep neural networks or not, whether open-source or vendor platforms. 
Generalizing quantiles to higher dimensions may not seem trivial, but it has been done with NoGAN and sister methods discussed in chapters~\ref{chnogan} and~\ref{chnogan2}. The new method, akin to
 quantile extrapolation, blends easily with NoGAN to enhance its performance.

Current techniques to evaluate the quality of synthetic fail to capture complex feature dependencies, resulting in false negatives: generated data scored as excellent, when it is actually very poor. Deep neural networks can be very slow and volatile, requiring ad-hoc tuning for each new dataset. In this book, the focus was on new algorithms -- not necessarily neural networks -- that are fast and easy to train, lead to 
 explainable AI and auto-tuning, and require less rather than more data to address the traditional challenges.  
 For instance, in section~\ref{aithing}, I illustrate how you can get better results, in addition to saving time,
 by randomly deleting 50\% of the data in the training set. All of this using sound evaluation metrics
 and cross-validation.

The main goal of all this framework is cost savings while delivering better results: using less training, GPU and cloud time. It goes
 against the modern trend of using bigger and bigger datasets. The popularity of oversized datasets stems from the fact that it seems to be the easy solution. Yet my algorithms are simpler. Then, large companies offering
 cloud and GPU services have strong incentives to favor big data: the bigger, the more revenue for them, the
 higher the costs for the client. Since I offer free solutions, thus bearing the cost of computations, I have strong incentives to optimize for speed while maintaining high quality output. In the end, my goals are thus aligned
 with those of the client, not with those of cloud companies or vendor charging a premium for cloud usage, based on the volume of data. 


\subsection{Python code}\label{pylgore}

The function \texttt{get\_test\_data} creates the simulated training set used in Figure~\ref{tssaqws09},
 referenced as ``simulated" in Table~\ref{widthprtz09}. The insurance data set is accessed
 from GitHib via the URL in the code, in the \texttt{get\_real\_data} function. Truncation
 is determined by the parameters \texttt{minz} and \texttt{maxz}, with no truncation
 if \texttt{minz>maxz}. I do not list $v$ as a variable, but it is
 implicitly used in instructions such as \texttt{sigma3=0.2*np.std(data)}, where $v=0.2$.
 Convoluted quantiles are stored in arrays, e.g. \texttt{e1quant1}, while standard
 quantiles are in \texttt{pquant}.
The code is also on GitHub, 
 \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/equantile.py}{here}.
\vspace{1ex}

%xxx update book on estore
%get it as openai web api
%... github


\begin{lstlisting}
# equantile.py: extrapolated quantiles

import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import pandas as pd

seed = 76
np.random.seed(seed)

def get_test_data(n=100):
    data = []
    for k in range(n):
        u = np.random.uniform(0, 1)
        if u < 0.2:
            x = np.random.normal(-1, 1)
        elif u < 0.7:
            x = np.random.normal(0, 2)
        else: 
            x = np.random.normal(5.5, 0.8)
        data.append(x)
    data = np.array(data)
    return(data)

def get_real_data():
    url = "https://raw.githubusercontent.com/VincentGranville/Main/main/insurance.csv"
    data = pd.read_csv(url)
    # features = ['age', 'sex', 'bmi', 'children', 'smoker', 'region', 'charges'] 
    data = data['bmi']    # choose 'bmi' or 'charges'
    data = np.array(data)
    return(data)

#--

def truncated_norm(mu, sigma, minz, maxz):
    z = np.random.normal(mu, sigma)
    if minz < maxz:
        while z < minz or z > maxz:
            z = np.random.normal(mu, sigma)
    return(z)

#- sample from mixture

def mixture_deviate(N, data, f, sigma, minz, maxz, verbose=False):
    sample = []
    point_idx = np.random.randint(0, len(data), N) 
    mu = data[point_idx]
    for k in range(N):
        z = truncated_norm(mu[k], sigma, minz, maxz)
        sample.append(z)
        if verbose and k%10 == 0:
            print("sampling %6d / %6d" %(k, N))
    sample = np.array(sample)
    sample = np.sort(sample)
    return(sample)

#--- Main part

# data = get_test_data(100)
data = get_real_data()
N = 1000000
truncate = False

# minz > maxz is the same as (minz = -infinity, maxz = +infinity)
if truncate == True:
    minz = 0.50 * np.min(data)  # use 0.95 for 'charges', 0.50 for 'bmi'
    maxz = 1.50 * np.max(data)  # use 1.50 for 'charges', 1.50 for 'bmi'
else:
    minz = 1.00
    maxz = 0.00

sigma1 = 0.0 * np.std(data) 
sample1 = mixture_deviate(N, data, truncated_norm, sigma1, minz, maxz)

sigma2 = 0.1 * np.std(data) 
sample2 = mixture_deviate(N, data, truncated_norm, sigma2, minz, maxz)

sigma3 = 0.2 * np.std(data) 
sample3 = mixture_deviate(N, data, truncated_norm, sigma3, minz, maxz)

sigma4 = 0.4 * np.std(data) 
sample4 = mixture_deviate(N, data, truncated_norm, sigma4, minz, maxz)

arrq = []
equant1 = []
equant2 = []
equant3 = []
equant4 = []
pquant = []

pbins = 1000
step = N / pbins    # N must be a multiple of pbins
for k in range(pbins):
    p = (k + 0.5) / pbins
    arrq.append(p)
    eq_index = int(step * (k + 0.5))
    equant1.append(sample1[eq_index])
    equant2.append(sample2[eq_index])
    equant3.append(sample3[eq_index])
    equant4.append(sample4[eq_index])
    pquant.append(np.quantile(data, p))

mpl.rcParams['axes.linewidth'] = 0.3
plt.rcParams['xtick.labelsize'] = 7
plt.rcParams['ytick.labelsize'] = 7

#--- Plot results

bins=np.linspace(np.min(equant4), np.max(equant4), num=100)

plt.subplot(2,2,1)
plt.hist(equant1,color='orange',edgecolor='red',bins=bins,linewidth=0.3,label='v=0.0')
plt.legend(loc='upper right', prop={'size': 6}, )
plt.ylim(0,35)
plt.subplot(2,2,2)
plt.hist(equant2,color='orange',edgecolor='red',bins=bins,linewidth=0.3,label='v=0.1')
plt.legend(loc='upper right', prop={'size': 6}, )
plt.ylim(0,35)
plt.subplot(2,2,3)
plt.hist(equant3,color='orange',edgecolor='red',bins=bins,linewidth=0.3,label='v=0.2')
plt.legend(loc='upper right', prop={'size': 6}, )
plt.ylim(0,35)
plt.subplot(2,2,4)
plt.hist(equant4,color='orange',edgecolor='red',bins=bins,linewidth=0.3,label='v=0.4')
plt.legend(loc='upper right', prop={'size': 6}, )
plt.ylim(0,35)
plt.show()

#--- Output some summary stats

print()
print("Observation range, min: %8.2f" %(np.min(data)))
print("Observation range, max: %8.2f" %(np.max(data)))
pmin = np.quantile(data, 0.5/pbins)
pmax = np.quantile(data, 1 - 0.5/pbins)
print("Python quantile %6.4f: %8.2f" % (0.5/pbins, pmin))
print("Python quantile %6.4f: %8.2f" % (1-0.5/pbins, pmax))
print("Python quantile %6.4f: %8.2f" % (0.5, np.quantile(data,0.5)))
print("Dataset stdev         : %8.2f" %(np.std(data)))

print()
print("sigma1: %6.2f" %(sigma1))
print("Equant quantile %6.4f: %8.2f" %(0.5/pbins, equant1[0]))
print("Equant quantile %6.4f: %8.2f" %(1-0.5/pbins, equant1[999]))
print("Equant quantile %6.4f: %8.2f" %(0.5, np.median(equant1)))
print("Equant-based stdev    : %8.2f" %(np.std(equant1)))

print()
print("sigma2: %6.2f" %(sigma2))
print("Equant quantile %6.4f: %8.2f" %(0.5/pbins, equant2[0]))
print("Equant quantile %6.4f: %8.2f" %(1-0.5/pbins, equant2[999]))
print("Equant quantile %6.4f: %8.2f" %(0.5, np.median(equant2)))
print("Equant-based stdev    : %8.2f" %(np.std(equant2)))

print()
print("sigma3: %6.2f" %(sigma3))
print("Equant quantile %6.4f: %8.2f" %(0.5/pbins, equant3[0]))
print("Equant quantile %6.4f: %8.2f" %(1-0.5/pbins, equant3[999]))
print("Equant quantile %6.4f: %8.2f" %(0.5, np.median(equant3)))
print("Equant-based stdev    : %8.2f" %(np.std(equant3)))

print()
print("sigma4: %6.2f" %(sigma4))
print("Equant quantile %6.4f: %8.2f" %(0.5/pbins, equant4[0]))
print("Equant quantile %6.4f: %8.2f" %(1-0.5/pbins, equant4[999]))
print("Equant quantile %6.4f: %8.2f" %(0.5, np.median(equant4)))
print("Equant-based stdev    : %8.2f" %(np.std(equant4)))


\end{lstlisting}

%-------------------------------------------------------------

\appendix

\chapter{Open Source Python Libraries}\label{aasdattt}

Python libraries are now available both for the NoGAN algorithm (chapter~\ref{chnogan}) 
 and the KS distance (section~\ref{evrd7hg}). 
 The latter is used to compare the {\em multivariate} distributions (ECDFs) attached to two datasets: in this case the synthetic data, with the real data
  it is generated from. The code in this section illustrates how to use these libraries, applied to the students dataset discussed in section~\ref{opewsd}. 
 Install these libraries with \texttt{pip install GenAI\_evaluation}, and \texttt{pip install NoGan\_Synthesizer}. Figure~\ref{fig:pi3qawty2xsuuvc} shows
 scatterplots comparing the validation with the synthetic data. This dataset has 21 features.



\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{noganlibtelco.png}  
\caption{NoGAN, students dataset: synthetic data (left), validation set (right)}
\label{fig:pi3qawty2xsuuvc}
\end{figure}

The generic KS distance returns a value between 0 and 1, with 0 being best and 1 being worst. It works both with categorical and numerical features.
See official description on PyPi, \href{https://pypi.org/project/genai-evaluation/}{here}. 
 Documentation is available on GitHub, \href{https://rajiviyer.github.io/genai_evaluation/api_reference/}{here}.  NoGAN is 
 also on PyPi, \href{https://pypi.org/project/nogan-synthesizer/}{here}. The documentation is located \href{https://rajiviyer.github.io/nogan_synthesizer/api_reference/}{here}. For a Jupyter notebook example, see \href{https://github.com/VincentGranville/Notebooks/blob/main/NoGAN_library_students.ipynb}{here}. The code below is also on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/NoGAN_library_sudents.py}{here}. An high-level PowerPoint presentation describing NoGAN, is available \href{https://mltblog.com/453YS2N}{here}.\vspace{1ex}

%############ https://rajiviyer.github.io/nogan_synthesizer/api_reference/
%# https://pypi.org/project/nogan-synthesizer/
%############ https://rajiviyer.github.io/genai_evaluation/api_reference/
%# https://pypi.org/project/genai-evaluation/
%# https://github.com/VincentGranville/Notebooks/blob/main/NoGAN_library_students.ipynb
%https://github.com/VincentGranville/Main/blob/main/NoGAN_library_sudents.py

\begin{lstlisting}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl
import genai_evaluation as ge
from genai_evaluation import multivariate_ecdf, ks_statistic
import nogan_synthesizer as ns
from nogan_synthesizer import NoGANSynth
from nogan_synthesizer.preprocessing import wrap_category_columns, unwrap_category_columns

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

pd.core.common.random_state(None)
seed = 1003
np.random.seed(seed)

#--- [1] read data, feature selection, remove missing values

url = "https://raw.githubusercontent.com/VincentGranville/Main/main/students.csv"
data = pd.read_csv(url)

data = data.drop(data[data["Curricular_units_2nd_sem_grade"] == 0].index)

features = [
  'Curricular_units_2nd_sem_approved',
  'Curricular_units_2nd_sem_grade',
  'Curricular_units_1st_sem_approved',
  'Curricular_units_1st_sem_grade',
  'Admission_grade',
  'Tuition_fees_up_to_date',
  'Curricular_units_2nd_sem_evaluations',
  'Age_at_enrollment',
  'Previous_qualification_grade',
  'Curricular_units_1st_sem_evaluations',
  'Course',
  'Father_occupation',
  'Mother_occupation',
  'Unemployment_rate',
  'GDP', 
  'Application_mode',
  'Father_qualification',
  'Curricular_units_2nd_sem_enrolled',
  'Mother_qualification',
  'Inflation_rate',
  'Target']

target_column = 'Target'

cat_cols = ['Course', 'Father_occupation', 'Mother_occupation', 
            'Unemployment_rate', 'GDP', 'Inflation_rate',
            'Application_mode', 'Father_qualification',
            'Mother_qualification', 'Tuition_fees_up_to_date', 'Target'
            ]

num_cols = [f for f in features if f not in cat_cols]

data = data[features]

#--- [2] Split real data into training and validation sets 

training_data = data.sample(frac = 0.5)
validation_data = data.drop(training_data.index)

#--- [3] Preprocess Categorical Columns

wrapped_train_data, idx_to_key_train, key_to_idx_train = \
                    wrap_category_columns(training_data,cat_cols)
                    
wrapped_val_data, idx_to_key_val, key_to_idx_val = \
                    wrap_category_columns(validation_data,cat_cols)     

#--- [4] Train model

bin_values = [100] * len(wrapped_train_data.columns) 
nogan = NoGANSynth(wrapped_train_data,random_seed=seed)
nogan.fit(bins = bin_values)

#--- [5] Generate 3 synth. data: original NoGAN, Gaussian, Uniform

stretch_type_gaussian  = (["Gaussian"] * (len(wrapped_train_data.columns)-1)) + ["Uniform"]
stretch_nogan_orig  = [-1] * len(wrapped_train_data.columns)

print("Synth. NoGAN Original:")
wrapped_nogan_orig_synth_data = nogan.generate_synthetic_data(len(wrapped_train_data),
                                                         debug = True ,
                                                         stretch = stretch_nogan_orig
                                                         )
print("\nSynth. NoGAN Gaussian:")
wrapped_nogan_gauss_synth_data = nogan.generate_synthetic_data(len(wrapped_train_data),
                                                         debug = True ,
                                                         stretch_type= stretch_type_gaussian
                                                         )
print("\nSynth. NoGAN Uniform:")
wrapped_nogan_uniform_synth_data = nogan.generate_synthetic_data(len(wrapped_train_data),
                                                         debug = True ,
                                                         )
print()

#--- [6] Evaluate using ECDF & KS Stat

_, ecdf_val1, ecdf_nogan_orig_synth = \
            multivariate_ecdf(wrapped_val_data, 
                              wrapped_nogan_orig_synth_data, 
                              n_nodes = 1000, 
                              verbose = True,
                              random_seed=seed)

_, ecdf_val2, ecdf_nogan_gauss_synth = \
            multivariate_ecdf(wrapped_val_data, 
                              wrapped_nogan_gauss_synth_data, 
                              n_nodes = 1000, 
                              verbose = True,
                              random_seed=seed)            

_, ecdf_val3, ecdf_nogan_uniform_synth = \
            multivariate_ecdf(wrapped_val_data, 
                              wrapped_nogan_uniform_synth_data, 
                              n_nodes = 1000, 
                              verbose = True,
                              random_seed=seed)
            
_, ecdf_val4, ecdf_train = \
            multivariate_ecdf(wrapped_val_data, 
                              wrapped_train_data, 
                              n_nodes = 1000, 
                              verbose = True,
                              random_seed=seed)            
print()            
ks_stat_nogan_orig = ks_statistic(ecdf_val1, ecdf_nogan_orig_synth)
ks_stat_nogan_gauss = ks_statistic(ecdf_val2, ecdf_nogan_gauss_synth)
ks_stat_nogan_uniform = ks_statistic(ecdf_val3, ecdf_nogan_uniform_synth)
ks_stat_train = ks_statistic(ecdf_val4, ecdf_train)

print(f"KS Stat NoGAN Original(Synth vs Validation): {ks_stat_nogan_orig:.5f}")
print(f"KS Stat NoGAN Gauss (Synth vs Validation): {ks_stat_nogan_gauss:.5f}")
print(f"KS Stat NoGAN Uniform (Synth vs Validation): {ks_stat_nogan_uniform:.5f}")
print(f"Base KS Stat (Synth vs Train): {ks_stat_train:.5f}")

#--- [7] Expand Categorical Columns in Synth Data

nogan_orig_synth_data = unwrap_category_columns(data=wrapped_nogan_orig_synth_data,
                                                 idx_to_key=idx_to_key_train, cat_cols=cat_cols)
nogan_orig_synth_data = nogan_orig_synth_data[features]

nogan_gauss_synth_data = unwrap_category_columns(data=wrapped_nogan_gauss_synth_data,
                                                 idx_to_key=idx_to_key_train, cat_cols=cat_cols)
nogan_gauss_synth_data = nogan_gauss_synth_data[features]

nogan_uniform_synth_data = unwrap_category_columns(data=wrapped_nogan_uniform_synth_data,
                                                 idx_to_key=idx_to_key_train, cat_cols=cat_cols)
nogan_uniform_synth_data = nogan_uniform_synth_data[features]

#--- [8] Evaluation scatterplots

dfs_orig = nogan_orig_synth_data
dfs_gauss = nogan_gauss_synth_data
dfs_uniform = nogan_uniform_synth_data
dfv = validation_data

def vg_scatter(df, feature1, feature2, counter, xlim, ylim):

    # customized plots, subplot position based on counter

    label = feature1[0:20] + " vs " + feature2[0:20]    
    df = df[(df[feature1] >= xlim[0]) & 
            (df[feature1] <= xlim[1]) &
            (df[feature2] >= ylim[0]) &
            (df[feature2] <= ylim[1])
           ]
    x = df[feature1].to_numpy()
    y = df[feature2].to_numpy()
    plt.subplot(3, 2, counter)
    plt.scatter(x, y, s = 0.1, c ="blue")
    plt.xlabel(label, fontsize = 7)
    plt.xticks([])
    plt.yticks([])
    return()

mpl.rcParams['axes.linewidth'] = 0.3

[col1, col2] = [features[4], features[7]]
xlim = [min(dfv[col1]), max(dfv[col1])]
ylim = [min(dfv[col2]), max(dfv[col2])]
vg_scatter(dfs_uniform, col1, col2, 1, xlim, ylim)
vg_scatter(dfv, col1, col2, 2, xlim, ylim)

[col1, col2] = [features[4], features[3]]
xlim = [min(dfv[col1]), max(dfv[col1])]
ylim = [10, max(dfv[features[3]])]
vg_scatter(dfs_uniform, features[4], features[3], 3, xlim, ylim)
vg_scatter(dfv, features[4], features[3], 4, xlim, ylim)

[col1, col2] = [features[2], features[0]]
xlim = [min(dfv[features[2]]), max(dfv[features[0]])]
ylim = [min(dfv[features[2]]), max(dfv[features[0]])]
vg_scatter(dfs_uniform, features[2], features[0], 5, xlim, ylim)
vg_scatter(dfv, features[2], features[0], 6, xlim, ylim)

plt.show()
\end{lstlisting}

%---



%----------------------------------------------------------------------------------------------------------------------

\chapter{Glossary: GAN and Tabular Data Synthetization}\label{aasdaaqw}

The following list features the most important concepts related to tabular data synthetization and evaluation methods, with a focus on generative adversarial networks. %\vspace{1ex} \\

% MLT article: Generative AI Glossary: Part 1

% I started to create a glossary featuring well-known concepts, keywords and abbreviations. This part with 51 entries focuses on generative adversarial networks and tabular data synthetization. It was initially included in <this article | project textbook>.

%gen ai glossary: tabular data synthetization / transform
%add to project textbook (appendix) and stats optim book

\DefTblrTemplate{caption}{default}{}
\DefTblrTemplate{middlehead,lasthead}{default}{}
\DefTblrTemplate{firstfoot,middlefoot}{default}{}

\begin{center}
\begin{longtblr}{p{\dimexpr3.5cm-2\tabcolsep}p{\dimexpr12cm-2\tabcolsep}}
\hline activation function & Function transforming values from the last layer of a deep neural network such as GAN, into actual output. For dummy variables, it is customary to use softmax.  \\
\hline algorithm bias & Algorithm are designed by architects with their own biases, train on data reflecting these biases (for instance, pictures of mostly white people) and decision from blackbox systems (who gets a loan) impacted by these biases. Synthetic data can help address this issue.\\
\hline base distance & When evaluating generated data, you compare your synthetic data with the validation data, a subset of the real data not use for training. The base distance is the distance between the part of the real data not used for training (the validation set), and the part of the real data actually used for training. \\
\hline batch & In GAN implementations, during each epoch (a full run of the dataset), you synthetize small batches of data and evaluate these batches separately one at a time, as it is a lot faster than doing it on the whole data at once.\\
\hline binning & Many algorithms such as XGboost work on binned data, where feature values - either jointly or separately - are aggregated into buckets called bins of flag vectors. Bin counts also work well with categorical data.\\
\hline categorical feature & A non numerical feature sometimes represented by dummy variables, one per category value, such as disease type or keyword. It can lead to a large number of features, artificially increasing the dimension of the problem. Grouping and aggregation techniques can reduce
 the dimensionality. \\
\hline copula & Data synthetization technique based on empirical quantiles and the feature correlation matrix, generalizing the inverse transform  sampling method to multivariate data. \\
\hline correlation matrix & The distance between two correlation matrices, one computed on the real data and the other one on the synthetic data, is a fundamental evaluation metric to measure the quality of the generated data.\\

\hline Cramer's V & A generalization of the correlation coefficient to measure the association between categorical features, or between a categorical and numerical feature. The value is between 0 (no association) and 1 (strong association).\\
\hline data augmentation & The method consists of adding synthetic observations to your training set, to produce more robust predictions or classifications. By enriching the training set, your algorithm will be better trained to deal with future real data not in the training set.\\
\hline data cleaning & Required step before using any modeling technique, to detect outliers, missing values, duplicates,
 wrong formatting, and so on. Can be automated to a large extent.\\
\hline dummy variable & Binary feature with two values (0 and 1) to represent categorical information, for instance California = 0/1 to indicate
  whether the location is in California or not. In this case, you may have 50 dummy variables, one for each state. It allows you to use numerical algorithms on categorical data.\\
\hline EDA & Exploratory data analysis.  Used to detect outliers, unique values with count and frequency (for each feature), percentiles, duplicated and missing values,
 correlation between features, and empirical distributions. Also used to bin the data.\\
\hline ECDF &  Empirical cumulative distribution function uniquely characterizing the underlying distribution in a dataset. Works with numerical and categorical features. The one-dimensional version is computed for each feature separately. \\
\hline EPDF & Empirical probability density function. The discrete derivative of the ECDF, and more difficult to handle than ECDF. For discrete variables, there is a one-to-one mapping between ECDF and EPDF.\\
\hline epoch & Also called iteration. One full run of your real data when training a GAN model. The loss functions (generator and discriminator) are
 computed at each epoch and should stabilize to low values after thousands of epochs, depending on the hyperparameters. \\
\hline explainable AI & Set of methods leading to easy interpretation, with simple explanations whenever the 
 blackbox system makes a decision. Explainability can be increased using feature importance scores.
 Some algorithms such as NoGAN are fully explainable by design. \\
\hline faithfulness & One of the goals of synthetization is to correctly mimic the statistical distributions and patterns found in the real data. 
 Faithfulness metrics
 such as KS distance measure how well this is accomplished. Metrics measuring the quality of predictions (via training set augmentation and cross-validation), are called utility metrics. Security metrics measure how well personal information has been transformed.\\
\hline
GAN & Generative adversarial network. Data synthetization technique based on 3 deep neural networks: the generator to generate synthetic observations, the discriminator to distinguish between fake and real data (competing with the generator), and the full model. \\
\hline gradient descent & Most machine learning algorithms including GAN, aim to minimize a loss function, or equivalently, 
maximize model fitting to data. Gradient descent performs this task. It may or may not succeed depending on parameters such as learning rate. Stochastic gradient descent is popular.
 Discretized versions are available. \\
\hline Hellinger distance & Metric to evaluate the quality of synthetizations. Based on the probability density functions (EPDF), computed on the real and synthetic data, then compared. Typically performed on each feature separately.\\
\hline hexagonal bin & Hexagonal bin plots are scatterplots where each dot is replaced by a fixed-size bin containing a variable number of observations. The color intensity represents the number of observations in each bin. Each bin is hexagonal: this is the optimum shape. The hexagons  are arranged in an  tessellation with hexagonal underlying lattice. \\
\hline holdout & The holdout method consists of using a portion of your real data (called training set) to train a synthesizer, and the remaining (called validation set) to evaluate the quality of the generated data. \\
\hline hyperparameter & In neural networks, parameters are the weights attached to the synapses connecting the neurons. Hyperparameters
 control the behavior of the whole system, and specify its architecture. For instance: number of epochs, batch size, loss functions, activation functions, 
 learning rate, type of gradient descent, number and type of layers (dense or sparse), and so on.\\
\hline imbalance & In a dataset, segments with few observations (for instance, fraudulent transactions) cause imbalance. Synthetization allows you to generate more observations for these segments, to rebalance the dataset to improve the performance of some algorithms.\\
\hline KS distance & Kolmogorov-Smirnov distance. To evaluate the quality of synthesized data. While the Hellinger distance is based
 on the density (EPDF) and averaged deviations, KS is based on the maximum deviation between the two ECDFs: real versus synthetic. It is
 more robust than Hellinger.\\
\hline latent variable & In GANs, feature values that we cannot interpret directly, but which encode a meaningful internal representation of externally observed events.\\
\hline learning rate & Parameter that governs increments in gradient descent algorithms. Small values means slow convergence and possibly getting stuck around a local minimum. Large values
 may lead to missing the optimum or lack of convergence.\\
\hline loss function & The function to minimize in a gradient descent algorithm. For instance, the maximum KS distance between the generated and real data, in a synthetization problem.\\
\hline
metadata & Information attached to a tabular dataset, specifying the type of data for each column: categorical, ordinal (integer), text, timestamp, 
 continuous feature, and so on.  \\
\hline missing values & Can be encoded as NaN, a blank cell, the empty string `', a large integer, or zero. NoGAN easily handles them. Techniques to retrieve missing values are called imputation methods.\\
\hline mode collapse & In GANs, mode collapse happens when the generator can only produce a single type of output or a small set of outputs. This may happen due to problems in training, such as the generator finding a type of data that can easily fools the discriminator and thus keeps generating that one type.\\
\hline multivariate ECDF & Same as ECDF but in this case computed jointly for multiple features, rather than separately for each feature. 
 The computation is not straightforward.\\
\hline NoGAN & Synthesizer not based on GAN or neural networks. A very efficient one, both in terms of speed and quality of the output, sharing some similarities with XGboost, is described 
in chapter~\ref{chnogan}. The copula and interpolation methods also fall in that category.\\
\hline overfitting & Synthetic data that looks too good to be true, could be the result of overfitting. This can happen when fine-tuning the hyperparameters to work on one particular dataset. To reduce overfitting, evaluate the quality of a synthetization on a validation set using the holdout method. Or assess performance of predictions based on augmented data, using cross-validation.\\
\hline oversampling & Consists of producing a larger proportion of synthetic observations for underrepresented segments in the real data (for instance fraudulent transactions),  
 to fix the imbalance problem.  \\
\hline PCA & Principal component analysis. Used as a transform to decorrelate the features in the real data, prior to training GAN, as this can improve synthetizations. The correct correlation structure is then put back into the synthetization, using the inverse PCA transform, after running GAN.\\
\hline quantile & The empirical quantile function is the inverse of the ECDF. It generalizes percentiles.\\
\hline reinforcement learning & Machine learning classification technique where correct allocation of future observations (outside the training set) is rewarded,
enabling the system to self-learn via trial and error.\\
\hline replicability & A replicable neural network is one that can produce the exact same results when run multiple times on the same data, regardless of the platform. Usually controlled by a seed parameter: using the same seed leads to the same results. \\
\hline scaling & A transformation that keeps the values of each feature within the same range, or with the same variance in the real data, before using GAN. 
  A measurement, whether in yards or miles, will be scale-free after the transformation. It can dramatically improve the
 quality of the generated data. Inverse scaling is then applied to the generated data, after the GAN synthetization.\\
\hline seed & Parameter used to initialize the various random number generators involved in the GAN architecture, typically one for each Python library 
 that generates random numbers. It produces replicable results, at least with CPU implementations. In GPU, the problem is different. \\
\hline stopping rule & A criterion to decide when to stop training a GAN, typically when an epoch produces an unusually good synthetization, 
 based on quality evaluation metrics such as the KS distance. It produces much better results than stopping after a fixed number of epochs.\\
\hline synthetization & Production of generated observations, also called synthetic data, with statistical properties  
 mimicking those computed on a pre-specified real data set.\\
\hline tabular data & Data arranged in tables, where columns represent features, and rows represent observations. Typically used
 for transactional data. Time series are treated with specific algorithms. \\
\hline training set & The portion of your real data used to train your synthesizer. The other part is called the validation set, and used to evaluate the quality of the synthetic data (how well it mimics real data). This setting known as holdout allows you to test you synthetisizer on future data and avoid overfitting.\\
\hline transform & Similar to transformers in large language models. Consists of using an invertible transform on your real data prior to GAN
 processing, to improve GAN performance. You need to apply the inverse transform on the generated data, after GAN. Example of transforms: scaling, PCA, standardization (transformed features having the same variance and zero mean), and normalization (to eliminate skewness).\\
\hline validation set & See training set. \\
\hline vanishing gradient & When the gradient gets close to zero in a gradient descent algorithm, it can prevent further progress towards locating the optimum. In the worst case, this may completely stop the neural network from further training. \\
\hline Wasserstein loss & The GAN Wasserstein loss function seeks to increase the gap between the scores for real and generated data. It is one of the many loss functions to improve the gradient descent algorithm, avoiding mode collapse and similar problems in some synthetizations.\\
\hline WGAN & Wasserstein GAN, based on the Wasserstein loss function.\\
\hline
\end{longtblr}
\end{center}



%----------------------------------------------------------------------------------------------------------------------

\setlength{\glsdescwidth}{0.75\hsize}
\pagebreak
%\printnoidxglossary[type=gloss,style=long,title={Glossary},sort=def] %%%%
\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{refstats} % Entries are in the refs.bib file in same directory as the tex file

%\pagebreak

\printindex

\hypersetup{linkcolor=red} % red %
\hypersetup{linkcolor=red}



\end{document}