\documentclass[oneside,10pt]{book}
\usepackage{amsmath} % for "\cfrac" macro

\usepackage{tabularray}
\usepackage{makecell}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{geometry}

\usepackage[export]{adjustbox}
\usepackage{relsize}
\usepackage{array}
\usepackage{enumitem}
\usepackage{relsize}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
\pagestyle{plain}
\usepackage{comment}
\newcommand\Chapter[2]{
  %\chapter[#1: {\itshape#2}]{#1\\[2ex]\Large\itshape#2}
  \chapter[#1]{#1\\[2ex]\Large\itshape#2}
}

%\usepackage{fontspec}
%\setmainfont{Times New Roman} %Times New Roman
%\setmonofont{Consolas}

%\usepackage{selinput}
%\SelectInputMappings{Euro={€}}

%\usepackage[utf8]{inputenc}
\usepackage[titles]{tocloft}
\setlength{\cftbeforechapskip}{7pt} %%%%%%%%%% 6pt
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{amsmath}    % need for subequations
\usepackage{amsfonts}
\usepackage{amssymb}  % needed for mathbb  OK
\usepackage{bigints}
\usepackage{graphicx}   % need for figures
\usepackage{subfig}
\usepackage{verbatim}   % useful for program listings
\usepackage{color}      % use if color is used in text
%\usepackage{subfigure}  % use for side-by-side figures
\usepackage{parskip}
\usepackage{float}
\usepackage{courier}
%\usepackage{artemisia} %%%
\usepackage{exercise}%[title=Project] %%%%%
%%%\usepackage{exercises}%[title=Project] %%%%%%%%%
\renewcommand\ExerciseName{Project}
\usepackage{sistyle}
\usepackage{textcomp} 
%

%%%\usepackage[utf8]{luainputenc}
%\usepackage{luatextra}
%
%%\usepackage[utf8]{inputenc}
%%\usepackage[T1]{fontenc}
%%\usepackage{textcomp,upgreek}
%\usepackage{fontspec} %,xltxtra}
%\usepackage{unicode}
\usepackage[euler]{textgreek}
%%\DeclareUnicodeCharacter{3B8}{\ensuremath{\uptheta}}
%

\SIthousandsep{,}
%\usepackage{numprint}
\setlength\parindent{0pt}

\newtheorem{prop}{Proposition}

\renewcommand{\DifficultyMarker}{}
\newcommand{\AtBeginExerciseHeader}{\hspace{-21pt}}  %-0.2pt
\renewcommand{\ExerciseHeader}{\AtBeginExerciseHeader\textbf{\ExerciseName~\ExerciseHeaderNB} \ExerciseTitle}
\renewcommand{\AnswerHeader}{\large\textbf{\AnswerName~\ExerciseHeaderNB}\smallskip\newline}
\setlength\AnswerSkipBefore{1em}

\usepackage{xspace}
\usepackage{imakeidx}
\makeindex

\usepackage[nottoc]{tocbibind}

\usepackage[colorlinks = true,
          linktocpage=true,
            pagebackref=true, % add back references to bibliography
            linkcolor = red,
            urlcolor  = blue,
            citecolor = red,
 %           refcolor  =red,
            anchorcolor = blue]{hyperref}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{gray2}{rgb}{0.35,0.35,0.35}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{index}{rgb}{0.88,0.32,0}

%------- source code settings
\usepackage{listings} 
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

%-----------------------------------------------------------------

\usepackage{blindtext}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }


\setlength{\baselineskip}{0.0pt} 
\setlength{\parskip}{3pt plus 2pt}
\setlength{\parindent}{20pt}
\setlength{\marginparsep}{0.0cm}
\setlength{\marginparwidth}{0.0cm}
\setlength{\marginparpush}{0.0cm}
\setlength{\tabcolsep}{4pt}
\setcounter{tocdepth}{2}
\renewcommand{\arraystretch}{1.4} %%%
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

%\renewcommand{\thechapter}{\arabic{chapter}.}
%\renewcommand{\thesection}{\thechapter\arabic{section}.}

\usepackage[symbols,nogroupskip,acronym]{glossaries-extra}
%\usepackage[xindy,symbols,nogroupskip,sort=def,acronym]{glossaries}
\makenoidxglossaries %%%%%%%%%%%%%%%
%\setlength{\glsdescwidth}{1.3\hsize}

\begin{document}

\hypersetup{linkcolor=blue}
%inserting a glossary entry in gloss: \gls{gls:keyword1} \\

\baselineskip=2\baselineskip
\thispagestyle{empty}
\hspace{0pt}
\vfill
%\hrulefill
\begin{center}
\rule{0.90\textwidth}{.4pt}
\end{center}

\begin{center}
{\Huge \bf{State-of-the-Art GenAI and LLMs\\\vspace{0.5ex}Creative Projects, with Solutions} }  
\end{center}


\baselineskip=0.5\baselineskip
\addvspace{2cm}
\begin{center}
%\includegraphics[width=0.7\textwidth]{linear.png}  \\
%\addvspace{1cm}
%\includegraphics[width=0.6\textwidth]{imgpyRiemannFinalOrbits-v2-small2.jpg} 
\includegraphics[width=0.67\textwidth]{GAN_history_nice.png}  
\end{center}
\addvspace{2cm}
\begin{center}
\rule{0.90\textwidth}{.4pt}
\end{center}
\begin{center}
Vincent Granville, Ph.D. $|$ \href{https://mltechniques.com/}{www.MLTechniques.com} $|$ Version 4.3, May 2024 
\end{center}
%\hrulefill

\hypersetup{linkcolor=red} % red %

\vfill
\hspace{0pt}
\pagebreak

\chapter*{Preface} %completed

With tutorials, enterprise-grade projects with solutions, and real-world case studies, this coursebook covers state-of-the-art material on GenAI, generative adversarial networks (GAN), data synthetization, and much more, in a compact format with many recent references. It includes deep dives into innovative, ground-breaking AI technologies such a xLLMs (extreme LLMs), invented by the author. 

The focus is on explainable AI with faster, simpler, high-performance, automated algorithms. For instance: NoGAN, new evaluation metrics, xLLM (self-tuned multi-LLM based on taxonomies with application to clustering and predictive analytics), variable-length embeddings, generating observations outside the training set range, fast probabilistic vector search, or Python-generated SQL queries. I also discuss alternatives to traditional methods, for instance to synthesize geo-spatial data or music. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\fbox{\includegraphics[width=0.95\textwidth]{snap.png}}   
\caption{The first three steps in the main xLLM project}
\label{fig:scgh4aslo}
\end{figure}

%-------------------------

This textbook is an invaluable resource to instructors and professors teaching AI, or for corporate training. Also, it is useful to prepare for job interviews or to build a robust portfolio. And for hiring managers, there are plenty of original interview questions. The amount of Python code accompanying the solutions is considerable, using a vast array of libraries as well as home-made implementations showing the inner workings and improving existing black-box algorithms. By itself, this book constitutes a solid introduction to Python and scientific programming. The code is also on my GitHub repository.

%---


\section*{About the author}

Vincent Granville is a pioneering data scientist and machine learning expert, co-founder of Data Science Central  (acquired by TechTarget), founder of \href{https://mltechniques.com/}{MLTechniques.com}, former VC-funded executive, author and patent owner.
\begin{wrapfigure}{l}{0.14\textwidth}
\vspace{-1ex}
\includegraphics[width=0.98\linewidth]{vgr3.png} 
%\caption{Caption1}
%\label{fig:wrapfig}
\end{wrapfigure}
\vspace{-2ex}\quad \\
\noindent Vincent’s past corporate experience includes Visa, Wells Fargo, eBay, NBC, Microsoft, and CNET. 
Vincent is also a former post-doc at Cambridge University, and the National Institute of Statistical Sciences (NISS).  
He  published in {\em Journal of Number Theory}, {\em Journal of the Royal Statistical Society} (Series B), and {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}. He is also the author of multiple books, available \href{https://mltechniques.com/resources/}{here}. He lives  in Washington state, and enjoys doing research on stochastic processes, dynamical systems, experimental math and probabilistic number theory.

%\renewcommand{\baselinestretch}{0.97}\normalsize
%%% \listoffigures
%\renewcommand{\baselinestretch}{1.00}\normalsize
%\listoftables


\hypersetup{linkcolor=red}
\renewcommand{\baselinestretch}{0.97}\normalsize
\tableofcontents 
\renewcommand{\baselinestretch}{1.00}\normalsize


%---------------------------------------

\chapter{Getting Started} %completed

If you are familiar with Python, you can skip this chapter. It explains different ways to install and work with Python, start with
 Jupyter Notebook if you want to, and post your projects or notebooks on GitHub. Along the way, you will learn how to produce a video in Python with sample code based on  Plotly -- a more advanced version of Matplotlib for scientific programming. You will also learn what Google Colab is about: a virtual server where you can run Python remotely in a Jupyter notebook, and sync with GitHub.


%----------
\section{Python, Jupyter Notebook, and Google Colab}

In this section, I provide guidance to machine learning beginners. After finishing the reading and the accompanying classes (including successfully completing the student projects), you should have a strong exposure to the most important topics, many covered in detail in my books. At this point, you should be able to pursue the learning on your own -- a never ending process even for top experts.

%\subsection{Getting started and learning how to learn}\label{staerqlk}

The first step is to install Python on your laptop. While it is possible to use \textcolor{index}{Jupyter Notebook}\index{Jupyter notebook} instead [\href{https://jupyter.org/}{Wiki}], this option is limited and won't give you the full experience of writing and testing serious code as in a professional, business environment. Once Python is installed, you can install Notebook from within Python, on the  
\textcolor{index}{command prompt}\index{command prompt} [\href{https://en.wikipedia.org/wiki/Cmd.exe}{Wiki}] 
with the instruction 
 \texttt{python -m pip install jupyter}, 
see \href{https://www.geeksforgeeks.org/how-to-install-jupyter-notebook-in-windows/}{here}. Or you may want to have it installed on a 
\textcolor{index}{virtual machine}\index{virtual machine}  \href{https://en.wikipedia.org/wiki/Virtual_machine}{[Wiki]} on your laptop: see VMware virtual machine installation \href{https://www.vmware.com/products/workstation-player.html}{here}.
Or you can access Notebook remotely via
 \textcolor{index}{Google Colab}\index{Colab}, see \href{https://colab.research.google.com/notebooks/}{here} and \href{https://colab.research.google.com/}{here}. In my case, Jupyter automatically starts a virtual machine on my laptop: it does not interfere with or modify my standard Python environment, with few exceptions (when installing a third-party library that forces reinstallation of older versions of libraries
 already in my environment).



Installing Python depends on your system. See the official Python.org website \href{https://www.python.org/downloads/}{here} to get started and download Python. On my Windows laptop, I first installed the Cygwin environment (see \href{https://www.cygwin.com/}{here} how to install it) to emulate a Unix environment. That way, I can use Cygwin windows instead of the Windows command prompt. The benefits is that it recognizes Unix commands. An alternative to Cygwin is \textcolor{index}{Ubuntu}\index{Ubuntu}
\href{https://ubuntu.com/download}{[Wiki]}. 
You could also use the \textcolor{index}{Anaconda}\index{Anaconda}  environment \href{https://www.anaconda.com/products/distribution}{[Wiki]}. 

Either way, you want to save your first Python program as a text file, say \texttt{test.py}. To run it, type in \texttt{Python test.py} in the command window. You need to be familiar with basic file management systems, to create folders and sub-folders as needed. Shortly, you will need to install Python libraries on your machine. Some of the  most common ones are Pandas, ScyPy, Seaborn, Numpy, Statsmodels, Scikit-learn, rRndom and Matplotlib. You can create your own too: see my \texttt{GD\_util} library on GitHub, \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source%20Code/GD_util.py}{here} 
and the Python script \texttt{PB\_NN.py} that uses some of its functions, 
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Source%20Code/PB_NN.py}{here}.
In your Python script, only use the needed libraries. Typically, they are listed at the beginning of your code, as in the following example:\vspace{1ex}



\begin{lstlisting}{frame=none}
import numpy as np
import matplotlib.pyplot as plt
import moviepy.video.io.ImageSequenceClip  # to produce mp4 video
from PIL import Image  # for some basic image processing 
\end{lstlisting}\vspace{1ex}

\noindent To install (say) the Numpy library, type in \texttt{pip install numpy} in the Windowns command prompt. In notebook, the command needs to be preceded by the exclamation point. You can also run Unix commands in a notebook cell: again, it needs to be preceded by the exclamation point, for instance \texttt{!pwd} or \texttt{!pip install tensorflow} or \texttt{!ls -l}. To include plots from Matplotlib to your notebook, add
 \texttt{\%matplotlib inline} in a cell before producing the images. See example \href{https://github.com/VincentGranville/Notebooks/blob/main/copula_insurance_nogroup.ipynb}{here}.  You can also add HTML and LaTeX formula to make your document look like a webpage, both with internal and external links. Use the \textcolor{index}{Markdown language}\index{Markdown}
 \href{https://www.markdownguide.org/getting-started/}{[Wiki]} to create notebook cells that include HTML to document your code (as opposed to Python cells consisting of code). In Markdown, \textcolor{index}{LaTex}\index{LaTeX} (for math formulas) start and end with a dollar sign. 

If running your notebook on 
\textcolor{index}{Google Colab}\index{Colab}, you can automatically save it on GitHub. Or upload it on Colab, from GitHub. Even if you don't use Notebook, I strongly encourage you to create a \textcolor{index}{GitHub}\index{GitHub} account. It will help you with 
\textcolor{index}{versioning}\index{versioning}  \href{https://en.wikipedia.org/wiki/Software_versioning}{[Wiki]} and sharing your code. 
 To copy and paste a piece of you code stored locally, into notebook, use \texttt{Ctrl-V}. 

You can directly run a Notebook found on GitHub, cell by cell, in your own Colab environment. Try \href{https://colab.research.google.com/github/VincentGranville/Notebooks/blob/main/copula_insurance_nogroup.ipynb}{this one}. Note that the original Notebook is in 
 my GitHub repository, \href{https://github.com/VincentGranville/Notebooks/blob/main/copula_insurance_nogroup.ipynb}{here}. 
To avoid problems with local files, the dataset used in this Python program is fetched directly from where it is located on the web, in this case, also on
 my GitHub repository. This is accomplished as follows, using the 
\textcolor{index}{Pandas}\index{Pandas}\index{Python library!Pandas} library. \vspace{1ex}

\begin{lstlisting}
import pandas as pd

url="https://raw.githubusercontent.com/VincentGranville/Main/main/insurance.csv"
data = pd.read_csv(url)
print(data.head(10))
\end{lstlisting}\vspace{1ex}

Finally, over time, after installing more and more Python libraries, you are going to face incompatibilities. For instance,
 \textcolor{index}{TensorFlow}\index{TensorFlow (Python library)}\index{Python library!TensorFlow} automatically installs a number of libraries or relies on existed ones  in your environment. Depending on which library versions are installed or exist in your environment, installing
 TensorFlow may or may not succeed. 
In many cases, reinstalling specific versions of some other libraries, or an older version of the library that you try to install, may fix the issue. In case of failed installation, check out the error message to see which libraries are causing problems, detect
 the versions currently on your system with \texttt{pip show pandas} (for the Pandas library in this example, assuming it is the cause of the failure) and install a different, compatible version  with the command \texttt{pip install -I pandas==1.5.3}. In this example,
version 1.5.3 and Pandas were mentioned in the error message to help me detect and fix the issue. In my current system (before the fix),
 I had Pandas 2.0 which was not compatible. 
The \texttt{-I} option forces the new installation and overwrites any other version you have in your system. 

In some cases, making the recommended change results in some other library -- needed with a specific version for a particular application -- 
 not to work anymore. For instance it requires Pandas 2.0 but you had to change it to Pandas 1.5.3 due to another compatibility issue with a different application. It creates a circular error loop impossible to fix. In this case, having two Python implementations with one on a virtual machine, may help.

%xxx xxx pip install -I pandas // pip install pandas==1.5.3  xx pip show
%xxxxxxxxxxxxxxx  %pip install --upgrade pip
%% pip3 install numpy==1.18
% ctrl-C to copy from Jupyter notebook


\subsection{Online Resources and Discussion Forums} 

One of the easiest ways to learn more and solve new problems using Python or any programming language is to use the Internet. For instance, when I designed my 
 sound generation algorithm in Python (see my article ``The Sound that Data Makes", \href{https://mltechniques.com/2022/08/29/the-sound-that-data-makes/}{here}), I googled keywords such as ``Python sound processing". I quickly discovered a number of libraries and tutorials, ranging from simple to advanced.  Over time, you discover websites that consistently offer solutions suited to your needs, and you tend to stick with them, until you ``graduate" to the next level of expertise and use new resources. 

It is important to look at the qualifications of people posting their code online, and how recent these posts are. You have to discriminate between multiple sources, and identify those that are not good or outdated. Usually, the best advice comes from little comments posted in discussion forums, as a response to solutions offered by some users. Of course, you can also post your own questions. Two valuable sources here to stay are GitHub and StackExchange. There are also numerous Python groups on LinkedIn and Reddit. In the end, after spending some time searching for sound libraries in Python, I've found solutions that do not require any special library: Numpy can process sound files. It took me a few hours to discover all I needed on the Internet. 

Finally, the official documentation that comes with Python libraries can be useful, especially if you want to use special parameters and understand the inner workings (and limitations) rather than using them as black-boxes with the default settings. For instance, when looking at model-free parameter estimation for time series (using optimization techniques), I quickly discovered the 
  \texttt{curve\_fit} function from the Scipy library. It did not work well on my unusual datasets. I discovered, in the official online documentation, several settings 
 to improve the performance. Still unsatisfied with the results (due to numerical instability in my case), I searched for alternatives and discovered that the swarm optimization technique (an alternative to \texttt{curve\_fit}) is available in the Pyswarms library. In the end, testing these libraries on rich synthetic data allows you to find what works best for your data.  
 
See also how I solve a new problem step by step and find the relevant code, in section~\ref{tipsdfesd}. Below is a list of resources that I
 regularly use. They have been around for many years
 Each of them has its own search box, which is useful to identify specific topics in the vast amount of knowledge that they cover.
 \vspace{1ex}

\begin{itemize}
\item \href{https://stackoverflow.com/questions/tagged/python}{StackExchange forum discussions} about Python 
\item \href{https://www.reddit.com/r/MachineLearning/}{Reddit Machine Learning forum} 
\item \href{https://www.analyticsvidhya.com/}{AnalyticsVidhya} originally based in India
\item \href{https://towardsdatascience.com/}{Towards Data Science} owned by Medium 
\item \href{https://machinelearningmastery.com/}{Machine Learning Mastery} (popular blog, all in Python)  
\item \href{https://datasetsearch.research.google.com/}{Google data sets}, \href{https://www.kaggle.com/datasets/}{Kaggle data sets} 
\item \href{https://openai.com/}{OpenAI} and other GPT apps for Python code
\end{itemize}

%-----

\subsection{Beyond Python}

Python has become the standard language for machine learning. Getting familiar with the R programming language will make you more competitive on the job market. In the last chapter in my book on synthetic data, 
 I show how to create videos and better-looking charts in R. Finally, machine learning professionals should know at least the basics of SQL, since many jobs still involve working with traditional databases. 

In particular, in one of the companies I was working for, I wrote a script that would accept SQL code as input (in text file format) to run queries against the Oracle databases, and trained analysts on how to use it in place of the dashboard they were familiar with. They were still using the dashboard (Toad in this case) to generate the SQL code, but run the actual queries with my script. The queries were now running $10$ times faster: the productivity gain was tremendous.  

To summarize, Python is the language of choice for machine learning, R is the language of statisticians, and SQL is the language of business and data analysts.

\section{Automated data cleaning and exploratory analysis}

It is said that data scientists spend $80\%$ of their time on data cleaning and \textcolor{index}{exploratory analysis}\index{exploratory analysis} [\href{https://en.wikipedia.org/wiki/Exploratory_data_analysis}{Wiki}]. This should not be the case. To the beginner, it looks like each new dataset comes with a new set of challenges.
 Over time, you realize that there are only so many potential issues. Automating the data cleaning step can save you a lot of time, and eliminate boring, repetitive tasks. A good Python script allows you to automatically take care of most problems. Here, I review the most common ones. 

First, you need to create a summary table for all features taken separately: the type (numerical, categorical data, text, or mixed). For each feature, get the top $5$ values, with their frequencies. It could reveal a wrong or unassigned zip-code such as 99999. Look for other special values such as NaN (not a number), N/A, an incorrect date format, missing values (blank) or special characters. For instance,  accented characters, commas, dollar, percentage signs and so on can cause issues with text parsing  and \textcolor{index}{regular expression}\index{regular expression} [\href{https://en.wikipedia.org/wiki/Regular_expression}{Wiki}]. Compute the minimum, maximum, median and other percentiles for numerical features. Check for values that are out-of-range: if possible, get the expected range from your client before starting your analysis. Use 
\textcolor{index}{checksums}\index{checksum} [\href{https://en.wikipedia.org/wiki/Checksum}{Wiki}] 
if possible, with encrypted fields such as credit card numbers or ID fields. 
A few Python libraries can take care of this. In particular: Pandas-Profiling, Sweetviz and D-Tale. See \href{https://medium.com/@karteekmenda93/exploratory-data-analysis-tools-83ef538c879f}{here} for details. 

The next step is to look at interactions between features. Compute all cross-correlations, and check for redundant or duplicate features that can be ignored. Look for IDs or keys that are duplicate or almost identical. Also two different IDs might have the same individual attached to them. This could reveal typos in your data. Working with a table of common typos can help. Also, collect data using 
pre-populated fields in web forms whenever possible, as opposed to users manually typing in their information such as state, city, zip-code, or date. Finally, check for misaligned fields. This happens frequently in NLP problems, where data such as URLs are parsed and stored in CSV files before being uploaded in databases. Now you can standardize your data. 

Sometimes, the data has issues beyond your control. When I was working at Wells Fargo, internet session IDs generated by the Tealeaf software were broken down into multiple small sessions, resulting in 
 wrong userIDs and very short Internet sessions. Manually simulating such sessions and looking how they were tracked in the database, helped solve this mystery, leading to correct analyses. Sometimes, the largest population segment is entirely missing in the database. For instance, in Covid data, people never tested who recovered on their own (the vast majority of the population in the early days) did not show up in any database, giving a lethality rate of $6\%$ rather than the more correct $1\%$, with costly public policy implications. Use common sense and out-of-the-box thinking to detect such issues, and let stakeholders known about it. Alternate data sources should always be used whenever possible. In this case, sewage data -- a proxy dataset -- provides the answer.

Finally, in many cases, transforming or standardizing your data may be necessary to get meaninful, consistent results. 
 For instance, a log transform for stock prices makes sense. Or you may want all the continuous features to have zero mean and unit variance, possibly even decorrelate them. Is your algorithm invariant under change of scale? If not, using different units (for instance, day instead of hour) may result in different clusters or predictions. Metrics such as \textcolor{index}{Mean Squared Error}\index{mean squared error}  may be measured in ``squared days", and you want to avoid that. As long as the transformation is reversible, you can apply your technique to the transformed data, than map back to the original using the inverse transform. 

\section{Tips to quickly solve new problems}\label{tipsdfesd}

%xxxx include image 3d contour levels  plotly runs faster # scientific computing ...

The goal here is to describe the whole process of starting a Python project, facing roadblocks as always, and how to overcome them. It shows how to leverage external resources to quickly get satisfactory results. This is typically what professionals writing code do on a daily basis. You will learn how to create a video in Python, and get exposure to the \textcolor{index}{Plotly}\index{Plotly}\index{Python library!Plotly} library, a more advanced version of Matplotlib, typically used in the context of scientific programming.

\subsection{Original solution to visualization problem}\label{vesurine}


Assume you are asked to produce good quality, 3D contour plots in Python -- not surface plots -- and you have no idea how to start. Yet, you are familiar with Matplotlib, but rather new to Python. You have 48 hours to complete this project (two days of work at your office, minus the regular chores eating your time every day). How would you proceed? Here I explain how I did it, as I was in the same situation (self-imposed onto myself). I focus on the 3D contour plot only, as I knew beforehand how to quickly turn it into a video, as long as I was able to produce a decent single plot. My strategy is broken down into the following steps. \vspace{1ex}

\begin{itemize}
\item I quickly realized it would be very easy to produce 2D contour or surface plots, but not 3D contour plots. I googled ``contour map 3d matplotib". Not satisfied with the results, I searched images, rather than the web. This led me to a Stackoverflow forum question \href{https://stackoverflow.com/questions/35445424/surface-and-3d-contour-in-matplotlib}{here}, which in turn led me to some page on the Matplotlib website, \href{https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contour.html}{here}.

\item After tweaking some parameters, I was able to produce Figure~\ref{fig:screentr4xmplcv}. Unsatisfied with the result and spending quite a bit of time trying to fix the glitch, I asked for a fix on Stackoverflow. You can see my question, and the answers that were posted, \href{https://stackoverflow.com/questions/74166875/problem-with-3d-contour-plots-in-matplotlib}{here}. One participant suggested to use color transparency, but this was useless, as I had tried it before without success. The second answer came with a piece of code, and the author suggested that I used Plotly instead of Matplotlib. I trusted his advice, and got his code to work after installing Plotly (I got an error message asking me to install Kaleido, but that was easy to fix). Quite exciting, but that was far from the end.

\item I googled ``Matplotlib vs Plotly", to make sure it made sense using Plotly. I was convinced, especially given my interest in scientific computing. Quickly though, I realized my plots were arbitrarily truncated. I googled ``plotly 3d contour map" and ``plotly truncated 3d contour", which led me to various websites including a detailed description of the \texttt{layout} and \texttt{scene} parameters. This \href{https://stackoverflow.com/questions/73187799/truncated-figure-with-plotly}{webpage} was particularly useful, as it offered a solution to my problem.

\item I spent a bit of time to figure out how to remove the axes and labels, as I feared they could cause problems in the video, changing from one frame to the next one, based on past experience. It took me 30 minutes to find the solution by trial and error. But then I realized that there was one problem left: in the PNG output image, the plot occupied only a small portion, even though it looked fine within the Python environment. Googling ``plotly write\_image" did not help. I tried to ask for a fix on Stackoverflow, but was not allowed to ask a question for another 24 hours. I asked my question in the Reddit Python forum instead.

\item Eventually, shrinking the Z axis, modifying the orientation of the plot, the margins, and the dimensions of the images, I got a substantial improvement. By the time I checked for a potential answer to my Reddit question, my post had been deleted by an admin. But I had finally solved it. Well, almost.

\item My concern at this point was using the correct DPI (dots per inch) and FPS (frames per second) for the video, and make sure the size of the video was manageable. Luckily, all the 300 frames (the PNG images, one per plot), now automatically generated, had the exact same physical dimension. Otherwise I would have had to resize them (which can be done automatically). Also, the rendering was good, not pixelized. So I did not have to apply anti-aliasing techniques. And here we are, I produced the video and was happy!

\item So I thought. I realized, when producing the animated gif, that there was still a large portion of the images unused (blank). Not as bad as earlier, but still not good enough for me. Now I know how to crop hundreds of images automatically in Python, but instead I opted to load my video on \href{https://ezgif.com/crop-video}{Ezgif}, and use the crop option. The final version posted in this chapter is this cropped version. I then produced another video, with 4 mountains, rising up, merging or shrinking according to various schedules. This might be the topic of a future article, as it is going into a new direction: video games.
\end{itemize}\vspace{1ex}

\noindent 
The first version of my code, using \textcolor{index}{Matplotlib}\index{Matplotlib}\index{Python library!Matplotlib}, is available
 on GitHub \href{https://github.com/VincentGranville/Visualizations/blob/main/Source-Code/contourMatplotlib.py}{here}. It is also included in this section, and was used to produce Figure~\ref{fig:screentr4xmplcv}.

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{contourmpl.png}   
\caption{Same as Figure~\ref{fig:screentr4x2998cv}, produced with Matplotlib}
\label{fig:screentr4xmplcv}
\end{figure}
%imgpy9979_2and3.PNG screen2e.png  
%-------------------------


\begin{lstlisting}
import numpy as np
import matplotlib.pyplot as plt

plt.rcParams['lines.linewidth']= 0.5
plt.rcParams['axes.linewidth'] = 0.5
plt.rcParams['axes.linewidth'] = 0.5
 
SMALL_SIZE  = 6
MEDIUM_SIZE = 8
BIGGER_SIZE = 10

plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title
plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels
plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize
plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

fig = plt.figure()
ax = fig.add_subplot(111, projection="3d")
X, Y = np.mgrid[-3:3:30j, -3:3:30j]
Z= np.exp(-(abs(X)**2 + abs(Y)**2)) + 0.8*np.exp(-4*((abs(X-1.5))**4.2 + (abs(Y-1.4))**4.2))

ax.plot_surface(X, Y, Z, cmap="coolwarm", rstride=1, cstride=1, alpha=0.2)
# ax.contourf(X, Y, Z, levels=60, colors="k", linestyles="solid", alpha=0.9, antialiased=True) 
ax.contour(X, Y, Z, levels=60, linestyles="solid", alpha=0.9, antialiased=True) 

plt.savefig('contour3D.png', dpi=300)
plt.show()
\end{lstlisting}

\subsection{New solution, after doing some research}\label{porousbutane}

\noindent You can see the final result in Figure~\ref{fig:screentr4x2998cv}, and watch the corresponding 
video \href{https://www.youtube.com/watch?v=Jpjw8wtoZrM}{here}. 
The code in this section corresponds to the Plotly version, including the production of the video. The choice of the colors is determined by the parameter \texttt{colorscale}, set to ``Peach" here. The list of available palettes is posted \href{https://plotly.com/python/builtin-colorscales/}{here}. You can easily add axes and labels, change font sizes and so on. The parameters to handle this are present in the source code, but turned off in the present version. The code is also available on GitHub, \href{https://github.com/VincentGranville/Visualizations/blob/main/Source-Code/contourvideoplotly.py}{here}.


%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{contour299.png}   
\caption{Contour plot, 3D mixture model, produced with Plotly}
\label{fig:screentr4x2998cv}
\end{figure}
%imgpy9979_2and3.PNG screen2e.png  
%-------------------------




\begin{lstlisting}
import numpy as np
import plotly.graph_objects as go

def create_3Dplot(frame):

    param1=-0.15 + 0.65*(1-np.exp(-3*frame/Nframes)) # height of small hill
    param2=-1+2*frame/(Nframes-1)     # rotation, x
    param3=0.75+(1-frame/(Nframes-1))  # rotation, y
    param4=1-0.7*frame/(Nframes-1)   # rotation z 

    X, Y = np.mgrid[-3:2:100j, -3:3:100j]
    Z= 0.5*np.exp(-(abs(X)**2 + abs(Y)**2)) \
        + param1*np.exp(-4*((abs(X+1.5))**4.2 + (abs(Y-1.4))**4.2))

    fig = go.Figure(data=[
        go.Surface(
            x=X, y=Y, z=Z,
            opacity=1.0,
            contours={
                "z": {"show": True, "start": 0, "end": 1, "size": 1/60,   
                      "width": 1, "color": 'black'} # add <"usecolormap": True>
            }, 
            showscale=False,  # try <showscale=True>
            colorscale='Peach')],
    )

    fig.update_layout(
        margin=dict(l=0,r=0,t=0,b=160),
        font=dict(color='blue'),
        scene = dict(xaxis_title='', yaxis_title='',zaxis_title='',
            xaxis_visible=False, yaxis_visible=False, zaxis_visible=False,
            aspectratio=dict(x=1, y=1, z=0.6)),                       # resize by shrinkink z
        scene_camera = dict(eye=dict(x=param2, y=param3, z=param4)))  # change vantage point  

    return(fig)

#-- main

import moviepy.video.io.ImageSequenceClip  # to produce mp4 video
from PIL import Image  # for some basic image processing

Nframes=300 # must be > 50
flist=[]    # list of image filenames for the video
w, h, dpi = 4, 3, 300 # width and heigh in inches
fps=10   # frames per second

for frame in range(0,Nframes): 
    image='contour'+str(frame)+'.png' # filename of image in current frame
    print("Creating image",image) # show progress on the screen
    fig=create_3Dplot(frame)
    fig.write_image(file=image, width=w*dpi, height=h*dpi, scale=1)
    #  fig.show()
    flist.append(image)

# output video / fps is number of frames per second
clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(flist, fps=fps) 
clip.write_videofile('contourvideo.mp4')
\end{lstlisting}





%---------------------------
\chapter{Machine Learning Optimization}

%xxx

%sponsor conferences
% interested in becoming a sponsor? Contact me for details, or visit this page

\begin{comment}
xxxx put the following in previous chapter:
%My own collab: https://colab.research.google.com/github/VincentGranville/Notebooks/blob/main/copula_insurance_nogroup.ipynb

xxx

projects
   feature clustering
   generalized geometric distrib
   data thinning 
   xgboost nlp
   curve fitting 
   fourier regression, better than polynomial regression [data transform]
   time series in cloud regression / numerical instability
   smart search grid
   curve smoothing
   mpmath, gmpy2
   stochastic integral equation

xxxx data transforms
xxxx check for xxxx and ?? when finished
xxxxx faster code with no loop; see my gmpy2 app
xxxx unix command in notebook . showing image. pip install ctrl-V to paste
xxx time series: when curve fitting won't work: in ML certif [add 250 simuls curve fitting]

cloud regression / curve fitting
math-free gradient 
feature clustering
data thinning
smart search grid
\end{comment}

This chapter covers several fundamental techniques to solve various machine learning problems. The focus is on
 optimization, either for speed or quality of the results (ideally both simulatneously), featuring state-of-the-art methods
 that outperform many neural network blackbox systems. As far as possible, the methods in question lead to
 intuitive and explainable AI. Project~\ref{genaiyert} is actually about generative AI. However, I decided to include it in this chapter
 as it does not use neural networks.

\section{Fast, high-quality NoGAN synthesizer for tabular data}\label{genaiyert}

This project  features a very fast synthesizer for tabular data, consistently leading to better synthetizations 
than those produced by 
\textcolor{index}{generative adversarial networks}\index{GAN (generative adversarial network)} (GAN). It has similarities to \textcolor{index}{XGboost}\index{XGboost},
  and does not require fine-tuning. Also, it epitomizes intuitive and \textcolor{index}{explainable AI}\index{explainable AI}.
For the full description of the technology, see~\cite{vgnogan}. The purpose of this project is to further optimize this
 new method.  Evaluation of the generated data
 is based on the \textcolor{index}{multivariate empirical distribution}\index{ECDF (empirical distribution)}\index{empirical distribution!multivariate} (ECDF), capturing all the patterns found in the real data, spanning across multiple features, categorical and numerical. The
 full joint ECDF has never been implemented in production mode due to computational complexity. Here it is, for the
 first time. To avoid \textcolor{index}{overfitting}\index{overfitting}, the real data is split into two parts: the training set to build the synthesizer, and the \textcolor{index}{validation set}\index{validation set} to check how it performs outside the training set. We test it on a well-known telecom dataset.

\subsection{Project description}

The project is based on the material \href{https://mltblog.com/3DBfjsi}{in this paper}. The password to download the article
 is \texttt{MLT12289058}. I included the original source code in section~\ref{hs32jnv}. It is also on GitHub,
 \href{https://github.com/VincentGranville/Main/blob/main/NoGAN.py}{here}. The goal here is to improve the methodology. The first two steps focus on the multivariate empirical distributions (ECDF), at the core of the \textcolor{index}{Kolmogorov-Smirnov distance}\index{Kolmogorov-Smirnov distance} (KS) that evaluates the quality of the synthetization. See also the glossary
 in appendix~\ref{aasdaaqw} for additional help.

\noindent The project consists of the following steps: \vspace{1ex}

\begin{itemize}
\item[] {\bf Step 1:  ECDF scatterplot}. Create a Jupyter notebook for the code in section~\ref{hs32jnv}, and run it ``as is". The code automatically loads the telecom dataset. Section [4.3] in the code produces the scatterplot for the ECDF values (empirical distribution functions):  the X-axis and Y-axis represent ECDF values respectively for the real (validation set) and synthetic component: each dot corresponds to a
 computation of the two ECDFs  at a specific random argument, called ``location" in the feature space.
 In particular \texttt{(ecdf\_real1,ecdf\_synth1)} is based on transformed locations, so that the scatterplot
 is better spread out along the diagonal: see left plot in Figure~\ref{fig:scatxntr4x2998cv}, compared to the middle plot without the transformation. Apply the same transformation to the ECDF values, instead of the arguments, and plot the result. You should obtain
 something similar to the right plot in the  Figure~\ref{fig:scatxntr4x2998cv}.

 \vspace{1ex}
\item[] {\bf Step 2: Convergence of the KS distance}.  The KS distance is equal to $\max |F_{\text{v}}(z) -F_{\text{s}}(z)|$ over all locations $z$ in the feature space, where $F_{\text{v}}(z)$ and $F_{\text{s}}(z)$ are the multivariate ECDFs, computed respectively on the validation set and the synthetic data. Here, the real data is split into two parts: the training set to produce the synthetic data, and the validation set to
evaluate its quality. In practice, we sample \texttt{n\_nodes} locations $z$ to compute the ECDFs. Check out if \texttt{n\_nodes=1000} (the value used in the code) is large enough: see if the KS distance stays
 roughly the same by increasing \texttt{n\_nodes} from $10^3$ to $10^4$ and $10^5$.  See also how the KS distance (denoted as \texttt{KS\_max} in the code) depends on the number of observations, both in the validation set and the synthetic data.
\vspace{1ex}

\item[]{\bf Step 3: From uniform to Gaussian sampling}. The core of the \textcolor{index}{NoGAN}\index{NoGAN} architecture consists of fixed-size multivariate bins
 covering all the points in the training set, whether the features are categorical or numerical. For synthetization, a random number of points is generated in each bin: these numbers follow a very specific \textcolor{index}{multinomial distribution}\index{multinomial distribution}. In each bin, 
 synthetic observations are uniformly and independently generated. Bins are \textcolor{index}{hyperrectangles}\index{hyperrectangles} in the feature space, with sides either parallel or perpendicular to the axes. 

For any specific bin, the multivariate median computed
 on the training set is stored in the array \texttt{median}; the list of training set points lying in the bin is 
 stored in \texttt{obs\_list}, an array where each entry  is a multidimensional observation from the training set.
 The upper and lower bounds of the bin (one per feature), are stored respectively in the arrays 
\texttt{L\_bounds} and \texttt{U\_bounds}, while \texttt{count} represents the number of points to generate, in the bin in question.
 All of this is located towards the bottom of section [2.3] in the code.

The generation of one synthetic observation vector uniformly distributed in the bin is performed separately for each component   
\texttt{k} (also called feature or dimension) by the instruction 
$$
\texttt{new\_obs[k] = np.random.uniform(L\_bounds[k], U\_bounds[k])}.
$$
 In this step, you are asked to 
 replace the uniform distribution by a Gaussian one, with the mean coinciding with the above \texttt{median}. The 
covariance matrix of the Gaussian may be diagonal for simplicity. About 95\% of the generated Gaussian observations should lie
 within the bin. Those that don't are rejected (try later without \textcolor{index}{rejection sampling}). In order to
 produce the required \texttt{count} observations within the bin, you need to oversample to be able to meet that \texttt{count} after rejection. In addition, do it without as few loops as possible, using \textcolor{index}{vector operations}\index{vectorization} instead. You may also replace the nested loops to compute \texttt{new\_obs[k]}, by vector operations.
\vspace{1ex}

\item[]{\bf Step 4: Speeding up the computations}. To find the first value larger or equal to a pre-specified value 
 \texttt{arr[idx]} in a sorted list \texttt{arr}, I use brute force, sequentially browsing the list until finding the value in question is found, with the following instruction:
$$
\texttt{while obs[k] >= arr[idx] and idx < bins\_per\_feature[k]},
$$
incrementing \texttt{idx} after each iteration.  Replace the \texttt{while} loop by a dichotomic search. Measure the gain in computing time, after the change. In short, it improves \textcolor{index}{time complexity}\index{time complexity} from linear to logarithmic.
\vspace{1ex}

\item[]{\bf Step 5: Fine-tuning the hyperparameter vector}. The main parameter in NoGAN
 is the vector $[n_1,\dots,n_d]$ named \texttt{bins\_per\_feature} in the code. Here $d$ is the number of features
 or dimension of the problem, and $n_k$ the desired number of intervals when binning feature $k$. For each feature, intervals are chosen so that they contain about the same number of observed values: wherever the density is high, intervals are short, and conversely.
 In the code, the \textcolor{index}{hyperparameter}\index{hyperparameter} is set to \texttt{[50,40,40,4]} in section [1.5]. The last value is attached to a
 binary feature called ``Churn". If you change 4 to 3, there will be no observation with Churn equal to 1 in the synthetic data. Why, and how 
 do you automatically determine the optimum value for this feature?

In the Python code, I only use 4 features, including the three numerical ones. But the telecom dataset contains many more. 
 Add a few more features, and adjust the hyperparameter vector accordingly. For numerical features, small values in the hyperparameter  result in artificial linear boundaries in the scatterplots in Figure~\ref{fig:scxcby5rtg} (produced in section [4.1] and [4.2] in the code). Illustrate this fact by
 reproducing Figure~\ref{fig:scxcby5rtg} but with a different hyperparameter. Can Gaussian sampling, discussed in \textcolor{red}{step 3}, fix this issue?
Very large values in the hyperparameter fix this problem. But too large is not good. Why? Time permitting, you may
 want to optimize the hyperparameter vector using the smart grid search technique explained in~\cite{vgsmart}.

Finally, for each feature, rather than using intervals based on constant \textcolor{index}{quantile}\index{quantile} increments as in section [2.1] in the code,
 use arbitrary intervals. In order words, allow the user to provide his own \texttt{pc\_table2}, rather than the default one
 based on the hyperparameter. Note that \texttt{pc\_table2[k]} corresponds to feature $k$; it is itself a sub-array 
with $n_k +1$ elements, specifying the bounds of the binning intervals for the feature in question.
\vspace{1ex}

\item[]{\bf Step 6: Confidence intervals}. This step is optional, and consists of four separate sub-projects. 
\vspace{1ex}
\begin{itemize}
\item[$\bullet$] Find great hyperparameter vectors using for instance 
 the \textcolor{index}{smart grid search}\index{grid search} technique described in~\cite{vgsmart}. How do you define and measure ``great" in this context?
\item[$\bullet$] Using subsets of the training set, assess the impact 
 of training set size on the KS distance. Reducing the training set while preserving the quality of the output, is a technique
 frequently used to speed up AI algorithms, see~\cite{vgthin}.  A more sophisticated version
 is called \textcolor{index}{data distillation}\index{data distillation}.
\item[$\bullet$] Try 100 different seeds (the parameter \texttt{seed} in section [1.4] in the code) to generate 100 different synthetizations. Use the generated data to compute confidence intervals of various levels for various statistics (for instance, correlation  between tenure and residues), based on the size of the training set.
\item[$\bullet$] Test NoGAN on different datasets, or with much more than four features.
\end{itemize}

\vspace{1ex}


\end{itemize}\vspace{1ex}

\noindent Note that the ECDFs take values between 0 and 1, as it estimates probabilities. Thus the KS distance -- the maximum distance between the two ECDFs, synthetic vs validation -- also takes values between 0 (best possible synthetization) and 1 (worst case). The dots in the scatterplots in Figure~\ref{fig:scatxntr4x2998cv}
 should always be close to the main diagonal. When the two ECDFs are identical, the dots lie exactly on the main diagonal.


\subsection{Solution}

The solution to \textcolor{red}{step 1} consists of elevating the ECDFs \texttt{ecdf\_real2} and \texttt{ecdf\_synth2}
 (taking values between 0 and 1) at the power $1/d$
in section [4.3] in the code. Here $d$ is the number of features, also called dimension, and denoted
 as \texttt{n\_features}. The updated version of section [4.3] is on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/NoGAN_ecdf_scatter.py}{here}. It produces the 3 plots in Figure~\ref{fig:scatxntr4x2998cv}, with the new one on the right-hand side. In case of perfect synthetization,
 all the dots are on the main diagonal.


%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{scatx.png}   
\caption{ECDF scatterplots (validation vs synthtetic) computed three ways}
\label{fig:scatxntr4x2998cv}
\end{figure}
%imgpy9979_2and3.PNG screen2e.png  
%-------------------------

The NoGAN tab in \texttt{telecom.xlsx} features sample synthetic data. This spreadsheet is  in 
 the same folder, \href{https://github.com/VincentGranville/Main/blob/main/telecom.xlsx}{here}.
 The other tabs in this spreadsheet feature synthetizations obtained via 
\textcolor{index}{generative adversarial networks}\index{generative adversarial network} (GAN), for comparison 
 purposes. For more details, see my article ``How to Fix a Failing Generative Adversarial Network"~\cite{failedgan}.



%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{nogan2.png}   
\caption{Feature scatterplots, synthetic (left) and validation dataset (right)}
\label{fig:scxcby5rtg}
\end{figure}
%imgpy9979_2and3.PNG screen2e.png  
%-------------------------

As for \textcolor{red}{Step 3}, if you use Gaussian instead of uniform sampling within each multivariate bin, 
 it will reduce edge effects in the synthesized data,
especially if using non-truncated Gaussian deviates, with sampled points spilling into neighboring bins. To some extent, this is similar
 to using \textcolor{index}{diffusion}\index{diffusion} [\href{https://en.wikipedia.org/wiki/Diffusion_model}{Wiki}] in neural network models.
As an illustration of the edge effect, look at Figure~\ref{fig:scxcby5rtg}: you can (barely) see some linear
 borders between different areas of the plot, in the left middle scatterplot. In particular, on the lower boundary of the 
cloud point. This happens when the values in the hyperparameter vector, for the features in question, are too low.
 Here the hyperparameter is \texttt{[50,40,40,4]}, with 50 for ``tenure", and 40 for ``residues" (the two features
 in the scatterplot in question). If you decrease these two values to (say) 15, the edge effect will be more pronounced. To the contrary, if you increase it to (say) 80, it won't be noticeable. High values can lead to overfitting and should be avoided if possible. 
An implementation of Gaussian NoGAN can be found \href{https://github.com/VincentGranville/Main/blob/main/NoGAN_gaussian.py}{here}. Look at lines 142--150 and 192--200 in the code in question.

I now jump to one of the most important parts: \textcolor{red}{Step 5}. I provided answers to some of the questions in the
 previous paragraph. To choose the hyperparameter vector, the basic rule is this: higher values leads to better synthetizations up to some extent; too high leads to overfitting. If one feature has several categories, and the proportion of observations in the smallest category  is $p$, then the corresponding hyperparameter value must be an integer larger than $1/p$. Otherwise, 
 the smallest category may not be generated in the synthesized data. In practice, for important data segments with very few observations in the training set (such as fraud), 
 you may want to run a separate NoGAN. This is illustrated in project~\ref{balbel}. 

Now answering \textcolor{red}{Step 6}. First, a great hyperarameter vector is one resulting in a small KS distance. The smaller~KS, the more faithful your synthetic data is. Then, regarding \textcolor{index}{confidence intervals}\index{confidence intervals!model-free} (CI), the solution is as follows. To obtain a 90\% CI for the 
correlation $\rho$ between ``tenure" and ``residues" (the latter named \texttt{TotalChargeResidues} in the Python code),
 compute $\rho$ on each of the 100 synthetizations (one per seed). The 5 and 95 percentiles computed on these $\rho$'s, 
 with the Numpy \texttt{quantile} function, are respectively the lower and upper bound of your CI.
Finally, to test NoGAN on other datasets, try the circle, insurance, and diabetes datasets featured in my article comparing vendor products,
 available \href{https://mltechniques.com/2023/06/16/generative-ai-synthetic-data-vendor-comparison-and-benchmarking-best-practices/}{here}.


%glossary: grid search , diffusion
%xxx
%mention spreadsheet. step xxx: inverse transform, see spreadsheet; project yyy: test your own data


\subsection{Python implementation}\label{hs32jnv}

Explanations about the different steps, including a description of the main variables and tables, can be found in~\cite{vgnogan}.
Compared to GAN, this implementation requires very few library functions and only three imports: Numpy, Statsmodels, and Pandas. This significantly reduces
 incompatibilities between library versions, and increases the chance that you can run it ``as is" on any platform, without
 impacting your own environment. The code \texttt{NoGAN.py} is also available on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/NoGAN.py}{here}. \vspace{1ex}

\begin{lstlisting}[numbers=left]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import pyplot
from statsmodels.distributions.empirical_distribution import ECDF

#--- [1] read data and only keep features and observations we want

#- [1.1] utility functions

def string_to_numbers(string):

    string = string.replace("[", "")
    string = string.replace("]", "")
    string = string.replace(" ", "")
    arr = string.split(',')
    arr = [eval(i) for i in arr]
    return(arr)

def category_to_integer(category):
    if category == 'Yes':
        integer = 1
    elif category == 'No':
        integer = 0
    else:
        integer = 2
    return(integer)

#- [1.2] read data

url = "https://raw.githubusercontent.com/VincentGranville/Main/main/Telecom.csv"
data = pd.read_csv(url)
features = ['tenure', 'MonthlyCharges', 'TotalCharges','Churn'] 
data['Churn'] = data['Churn'].map(category_to_integer) 
data['TotalCharges'].replace(' ', np.nan, inplace=True)
data.dropna(subset=['TotalCharges'], inplace=True)  # remove missing data
print(data.head()) 
print (data.shape)
print (data.columns)

#- [1.3] transforming TotalCharges to TotalChargeResidues, add to dataframe

arr1 = data['tenure'].to_numpy()
arr2 = data['TotalCharges'].to_numpy() 
arr2 = arr2.astype(float)
residues = arr2 - arr1 * np.sum(arr2) / np.sum(arr1)  # also try arr2/arr1
data['TotalChargeResidues'] = residues

#- [1.4] set seed for replicability

pd.core.common.random_state(None)
seed = 105
np.random.seed(seed)

#- [1.5] initialize hyperparameters (bins_per_feature), select features

features = ['tenure','MonthlyCharges','TotalChargeResidues','Churn'] 
bins_per_feature = [50, 40, 40, 4]   

bins_per_feature = np.array(bins_per_feature).astype(int)
data = data[features]
print(data.head())
print (data.shape)
print (data.columns)

#- [1.6] split real dataset into training and validation sets

data_training = data.sample(frac = 0.5)
data_validation = data.drop(data_training.index)
data_training.to_csv('telecom_training_vg2.csv')
data_validation.to_csv('telecom_validation_vg2.csv')

nobs = len(data_training)
n_features = len(features)
eps = 0.0000000001 


#--- [2] create synthetic data  

#- [2.1] create quantile table pc_table2, one row for each feature

pc_table2 = []
for k in range(n_features):
    label = features[k]
    incr = 1 / bins_per_feature[k]   
    pc = np.arange(0, 1 + eps, incr)
    arr = np.quantile(data_training[label], pc, axis=0)
    pc_table2.append(arr)

#- [2.2] create/update bin for each obs [layer 1]
#        Faster implementation: replace 'while' loop by dichotomic search

npdata = pd.DataFrame.to_numpy(data_training[features])
bin_count = {}   # number of obs per bin
bin_obs = {}     # list of obs in each bin, separated by "~", stored as a string
for obs in npdata:
    key = [] 
    for k in range(n_features):
        idx = 0
        arr = pc_table2[k]    # percentiles for feature k
        while obs[k] >= arr[idx] and idx < bins_per_feature[k]: 
            idx = idx + 1
        idx = idx - 1  # lower bound for feature k in bin[key] attached to obs
        key.append(idx)
        skey = str(key)
    if skey in bin_count:
        bin_count[skey] += 1
        bin_obs[skey] += "~" + str(obs)
    else:
        bin_count[skey] = 1
        bin_obs[skey] = str(obs)

#- [2.3] generate nobs_synth observations (if mode = FixedCounts, nobs_synth = nobs)

def random_bin_counts(n, bin_count):
    # generate multinomial bin counts with same expectation as real counts
    pvals = []
    for skey in bin_count:
        pvals.append(bin_count[skey]/nobs)
    return(np.random.multinomial(n, pvals))

def get_obs_in_bin(bin_obs, skey): 
    # get list of observations (real data) in bin skey, also return median
    arr_obs = []
    arr_obs_aux = (bin_obs[skey]).split('~')
    for obs in arr_obs_aux:
        obs = ' '.join(obs.split())
        obs = obs.replace("[ ", "")
        obs = obs.replace("[", "")
        obs = obs.replace(" ]", "")
        obs = obs.replace("]", "")
        obs = obs.split(' ')
        obs = (np.array(obs)).astype(float)
        arr_obs.append(obs)
    arr_obs = np.array(arr_obs)
    median = np.median(arr_obs, axis = 0)
    return(arr_obs, median) 

    
mode = 'RandomCounts'  # (options: 'FixedCounts' or 'RandomCounts')
if mode == 'RandomCounts':
    nobs_synth = nobs 
    bin_count_random = random_bin_counts(nobs_synth, bin_count)
    ikey = 0

data_synth = []
bin_counter = 0

for skey in bin_count:

    if mode == 'FixedCounts':
        count = bin_count[skey]
    elif mode == 'RandomCounts': 
        count = bin_count_random[ikey]
        ikey += 1
    key = string_to_numbers(skey)
    L_bounds = []
    U_bounds = []
    bin_counter += 1

    for k in range(n_features):
        arr = pc_table2[k] 
        L_bounds.append(arr[key[k]])
        U_bounds.append(arr[1 + key[k]])

    # sample new synth obs (new_obs) in rectangular bin skey, uniformily
    # try other distrib, like multivariate Gaussian around bin median 
    # the list of real observations in bin[skey] is stored in obs_list (numpy array)
    # median is the vector of medians for all obs in bin skey

    obs_list, median = get_obs_in_bin(bin_obs, skey) # not used in this version
    
    for i in range(count):        
        new_obs = np.empty(n_features) # synthesized obs
        for k in range(n_features):
            new_obs[k] = np.random.uniform(L_bounds[k],U_bounds[k])
        data_synth.append(new_obs)

    str_median = str(["%8.2f" % number for number in median])
    str_median = str_median.replace("'","")
    print("bin ID = %5d | count = %5d | median = %s | bin key = %s" 
              %(bin_counter, bin_count[skey], str_median, skey))

data_synth = pd.DataFrame(data_synth, columns = features)

# apply floor function (not round) to categorical/ordinal features
data_synth['Churn'] = data_synth['Churn'].astype('int') 
data_synth['tenure'] = data_synth['tenure'].astype('int')

print(data_synth)    
data_synth.to_csv('telecom_synth_vg2.csv')


#--- [3] Evaluation synthetization using joint ECDF & Kolmogorov-Smirnov distance

# dataframes: df = synthetic; data = real data,
# compute multivariate ecdf on validation set, sort it by value (from 0 to 1) 

#- [3.1] compute ecdf on validation set (to later compare with that on synth data)

def compute_ecdf(dataframe, n_nodes, adjusted):

    # Monte-Carlo: sampling n_nodes locations (combos) for ecdf
    #    - adjusted correct for sparsity in high ecdf, but is sparse in low ecdf  
    #    - non-adjusted is the other way around
    # for faster computation: pre-compute percentiles for each feature
    # for faster computation: optimize the computation of n_nodes SQL-like queries

    ecdf = {} 

    for point in range(n_nodes):

        if point % 100 == 0:
            print("sampling ecdf, location = %4d (adjusted = %s):" % (point, adjusted))
        combo = np.random.uniform(0, 1, n_features)
        if adjusted:
            combo = combo**(1/n_features)
        z = []   # multivariate quantile
        query_string = ""
        for k in range(n_features):
            label = features[k]
            dr = data_validation[label]
            percentile = combo[k] 
            z.append(eps + np.quantile(dr, percentile))
            if k == 0:
                query_string += "{} <= {}".format(label, z[k])
            else: 
                query_string += " and {} <= {}".format(label, z[k])

        countifs = len(data_validation.query(query_string))
        if countifs > 0: 
            ecdf[str(z)] = countifs / len(data_validation)
  
    ecdf = dict(sorted(ecdf.items(), key=lambda item: item[1]))

    # extract table with locations (ecdf argument) and ecdf values:
    #     - cosmetic change to return output easier to handle than ecdf 

    idx = 0
    arr_location = []
    arr_value = []
    for location in ecdf:
        value = ecdf[location]
        location = string_to_numbers(location)
        arr_location.append(location)
        arr_value.append(value)
        idx += 1

    print("\n")
    return(arr_location, arr_value)


n_nodes = 1000   # number of random locations in feature space, where ecdf is computed
reseed = False
if reseed:
   seed = 555
   np.random.seed(seed) 
arr_location1, arr_value1 = compute_ecdf(data_validation, n_nodes, adjusted = True)
arr_location2, arr_value2 = compute_ecdf(data_validation, n_nodes, adjusted = False)

#- [3.2] comparison: synthetic (based on training set) vs real (validation set)

def ks_delta(SyntheticData, locations, ecdf_ValidationSet):

    # SyntheticData is a dataframe
    # locations are the points in the feature space where ecdf is computed
    # for the validation set, ecdf values are stored in ecdf_ValidationSet
    # here we compute ecdf for the synthetic data, at the specified locations
    # output ks_max in [0, 1] with 0 = best, 1 = worst

    ks_max = 0
    ecdf_real = []
    ecdf_synth = []
    for idx in range(len(locations)):
        location = locations[idx]
        value = ecdf_ValidationSet[idx]
        query_string = ""
        for k in range(n_features):
            label = features[k]
            if k == 0:
                query_string += "{} <= {}".format(label, location[k])
            else: 
                query_string += " and {} <= {}".format(label, location[k])
        countifs = len(SyntheticData.query(query_string))
        synth_value = countifs / len(SyntheticData)
        ks = abs(value - synth_value)
        ecdf_real.append(value)
        ecdf_synth.append(synth_value)
        if ks > ks_max:
            ks_max = ks
        # print("location ID: %6d | ecdf_real: %6.4f | ecdf_synth: %6.4f"
        #             %(idx, value, synth_value))
    return(ks_max, ecdf_real, ecdf_synth)

df = pd.read_csv('telecom_synth_vg2.csv')
ks_max1, ecdf_real1, ecdf_synth1 = ks_delta(df, arr_location1, arr_value1)
ks_max2, ecdf_real2, ecdf_synth2 = ks_delta(df, arr_location2, arr_value2)
ks_max = max(ks_max1, ks_max2)
print("Test ECDF Kolmogorof-Smirnov dist. (synth. vs valid.): %6.4f" %(ks_max))

#- [3.3] comparison: training versus validation set

df = pd.read_csv('telecom_training_vg2.csv')
base_ks_max1, ecdf_real1, ecdf_synth1 = ks_delta(df, arr_location1, arr_value1)
base_ks_max2, ecdf_real2, ecdf_synth2 = ks_delta(df, arr_location2, arr_value2)
base_ks_max = max(base_ks_max1, base_ks_max2)
print("Base ECDF Kolmogorof-Smirnov dist. (train. vs valid.): %6.4f" %(base_ks_max))


#--- [4] visualizations

def vg_scatter(df, feature1, feature2, counter):

    # customized plots, subplot position based on counter

    label = feature1 + " vs " + feature2
    x = df[feature1].to_numpy()
    y = df[feature2].to_numpy()
    plt.subplot(3, 2, counter)
    plt.scatter(x, y, s = 0.1, c ="blue")
    plt.xlabel(label, fontsize = 7)
    plt.xticks([])
    plt.yticks([])
    #plt.ylim(0,70000)
    #plt.xlim(18,64)
    return()

def vg_histo(df, feature, counter):

    # customized plots, subplot position based on counter

    y = df[feature].to_numpy()
    plt.subplot(2, 3, counter)
    min = np.min(y)
    max = np.max(y)
    binBoundaries = np.linspace(min, max, 30)
    plt.hist(y, bins=binBoundaries, color='white', align='mid',edgecolor='red',
              linewidth = 0.3) 
    plt.xlabel(feature, fontsize = 7)
    plt.xticks([])
    plt.yticks([])
    return()

import matplotlib.pyplot as plt
import matplotlib as mpl
mpl.rcParams['axes.linewidth'] = 0.3

#- [4.1] scatterplots for Churn = 'No'

dfs = pd.read_csv('telecom_synth_vg2.csv')
dfs.drop(dfs[dfs['Churn'] == 0].index, inplace = True)
dfv = pd.read_csv('telecom_validation_vg2.csv')
dfv.drop(dfv[dfv['Churn'] == 0].index, inplace = True)

vg_scatter(dfs, 'tenure', 'MonthlyCharges', 1)
vg_scatter(dfv, 'tenure', 'MonthlyCharges', 2)
vg_scatter(dfs, 'tenure', 'TotalChargeResidues', 3)
vg_scatter(dfv, 'tenure', 'TotalChargeResidues', 4)
vg_scatter(dfs, 'MonthlyCharges', 'TotalChargeResidues', 5)
vg_scatter(dfv, 'MonthlyCharges', 'TotalChargeResidues', 6)
plt.show()

#- [4.2] scatterplots for Churn = 'Yes'

dfs = pd.read_csv('telecom_synth_vg2.csv')
dfs.drop(dfs[dfs['Churn'] == 1].index, inplace = True)
dfv = pd.read_csv('telecom_validation_vg2.csv')
dfv.drop(dfv[dfv['Churn'] == 1].index, inplace = True)

vg_scatter(dfs, 'tenure', 'MonthlyCharges', 1)
vg_scatter(dfv, 'tenure', 'MonthlyCharges', 2)
vg_scatter(dfs, 'tenure', 'TotalChargeResidues', 3)
vg_scatter(dfv, 'tenure', 'TotalChargeResidues', 4)
vg_scatter(dfs, 'MonthlyCharges', 'TotalChargeResidues', 5)
vg_scatter(dfv, 'MonthlyCharges', 'TotalChargeResidues', 6)
plt.show()

#- [4.3] ECDF scatterplot: validation set vs. synth data 

plt.xticks(fontsize=7)
plt.yticks(fontsize=7)
plt.scatter(ecdf_real1, ecdf_synth1, s = 0.1, c ="blue")
plt.scatter(ecdf_real2, ecdf_synth2, s = 0.1, c ="blue")
plt.show()

#- [4.4] histograms, Churn = 'No'

dfs = pd.read_csv('telecom_synth_vg2.csv')
dfs.drop(dfs[dfs['Churn'] == 0].index, inplace = True)
dfv = pd.read_csv('telecom_validation_vg2.csv')
dfv.drop(dfv[dfv['Churn'] == 0].index, inplace = True)
vg_histo(dfs, 'tenure', 1)
vg_histo(dfs, 'MonthlyCharges', 2)
vg_histo(dfs, 'TotalChargeResidues', 3)
vg_histo(dfv, 'tenure', 4)
vg_histo(dfv, 'MonthlyCharges', 5)
vg_histo(dfv, 'TotalChargeResidues', 6)
plt.show()

#- [4.5] histograms, Churn = 'Yes'

dfs = pd.read_csv('telecom_synth_vg2.csv')
dfs.drop(dfs[dfs['Churn'] == 1].index, inplace = True)
dfv = pd.read_csv('telecom_validation_vg2.csv')
dfv.drop(dfv[dfv['Churn'] == 1].index, inplace = True)
vg_histo(dfs, 'tenure', 1)
vg_histo(dfs, 'MonthlyCharges', 2)
vg_histo(dfs, 'TotalChargeResidues', 3)
vg_histo(dfv, 'tenure', 4)
vg_histo(dfv, 'MonthlyCharges', 5)
vg_histo(dfv, 'TotalChargeResidues', 6)
plt.show()
\end{lstlisting}

\section{Cybersecurity: balancing data with automated SQL queries}\label{balbel}

While the end goal is to synthesize a very challenging dataset, highly imbalanced, the scope of this project is not data synthetization.
 Rather, it consists of splitting the dataset into a small number of rather homogeneous parts, each part having specific characteristics. 
 By implementing an  algorithm separately to each part, one can expect much better results, whether for data synthetization or any other purpose. At the end, the different outputs are recombined together.

What makes this dataset so special? It has about 117k observations, and 16 features There are two big clusters of duplicate observations, representing 
about 95\% of the dataset: each contains a single observation vector, repeated time and over.  Then a smaller number of clusters, each consisting of 2 to 5 duplicate observations. 
Most features are categorical, and some are a mixture of categorical and numerical values. Then there are some obvious outliers.
This is not an error, it is the way the data is. The case study is about cybersecurity, looking at server data to identify fraud. The amount of fraud is also very small. In this project, most of the heavy work consists of identifying and separating the different parts.
 It is done using SQL-like statements in \textcolor{index}{Pandas}\index{Pandas} (Python library). See code in section~\ref{gfrk65zs},
 after completing the project.

\subsection{Project description}

The dataset is located \href{https://raw.githubusercontent.com/VincentGranville/Main/main/iot_security.csv}{here}.
 First, you need to do some exploratory analysis to find the peculiarities. For each feature, find the distinct values, their type (category or continuous) and count the multiplicity of each value. Also count the multiplicity of each observation vector when all features
 are combined. We will split the dataset into three subsets named \texttt{A}, \texttt{C1} and \texttt{C2}, 
and remove some outliers in \texttt{C1} (or at least treat them separately). Simulating observations distributed as in 
\texttt{A} or \texttt{C1} is straighforward.
 For \texttt{C2}, we will use the NoGAN synthesizer. In the remaining, by observation or observation vector, I mean a full row in the dataset. 

\noindent The project consists of the following steps: \vspace{1ex}
\begin{itemize}
\item[] {\bf Step 1}:  Split the data into two subsets \texttt{A} and \texttt{B}. Here \texttt{A} consists of the two big clusters discussed earlier, each containing one 
observation vector duplicated thousands of times. Also add to \texttt{A} any observation duplicated at least 4 times. Keep one copy of each unique observation in \texttt{A}, and add one new feature: the observation count, named \texttt{size}. The set \texttt{B} contains all the unique observations except those that are now in \texttt{A}, with a count (\texttt{size}) for each observation.\vspace{1ex}
\item[] {\bf Step 2}:  Create set \texttt{C} as follows. Remove the columns \texttt{scr\_port} and \texttt{size} from set \texttt{B}.
Then keep only one copy of each duplicated observation, and add an extra feature to count the multiplicity attached to each observation. 
Finally, split \texttt{C} into \texttt{C1} and \texttt{C2}, with \texttt{C1} consisting of observation vectors with multiplicity larger than one, and \texttt{C2} for the other ones (observation vectors that do not have duplicates). Find outliers in \texttt{C1} and remove them.
\vspace{1ex}
\item[] {\bf Step 3}: The feature \texttt{scr\_port} is absent in sets \texttt{C1} and \texttt{C2}, after step 2. 
 Reconstruct the list of values for \texttt{scr\_port} with the correct count, separately for \texttt{C1} and \texttt{C2}, as we will need it in step 4. These 
 satellite tables
 are named \texttt{map\_C1} and \texttt{map\_C2} in the Python code. Double check that all the computations, splitting, mapping, counts, uniques, and aggregation, are correct. \vspace{1ex}

\item[] {\bf Step 4}: Generate synthetic observations for \texttt{C2}, using the NoGAN algorithm described in project~\ref{genaiyert}.
Use the following features only: \vspace{1ex}\\
\textcolor{white}{MMMMM} \texttt{bidirectional\_syn\_packets}	\\
\textcolor{white}{MMMMM}  \texttt{src2dst\_syn\_packets}	\\
\textcolor{white}{MMMMM}  \texttt{application\_category\_name}	\\
\textcolor{white}{MMMMM}  \texttt{application\_confidence}\\
\textcolor{white}{MMMMM}  \texttt{src2dst\_mean\_ps}\\
\textcolor{white}{MMMMM}  \texttt{src2dst\_psh\_packets}\\
\textcolor{white}{MMMMM}  \texttt{bidirectional\_mean\_ps}\\	
\textcolor{white}{MMMMM}  \texttt{label}\vspace{1ex}

\noindent The feature ``label" indicates fraud when the value is not 0. Few observations 
 are labeled as non-fraud in \texttt{C2}. How would you proceed to substantially increase the proportion 
 of non-fraud in \texttt{C2}, in the generated data? How about generating values for the \texttt{src\_port} feature, based on the \texttt{map\_C2} distribution obtained 
 in step 3? Is the distribution in question uniform within its range, between $\num{40000}$ 
and $\num{60000}$? If yes, you could generate  uniform
 values for this feature, possibly different from those actually observed.
\vspace{1ex}

\item[] {\bf Step 5}: Think of a general strategy to synthesize observations not just for \texttt{C2}, but for the full
 original data. Say you want $N = 10^5$ synthetic observations. You need to generate $n_1, n_2, n_3$ observations respectively for 
\texttt{A}, \texttt{C1}, \texttt{C2}, with $N = n_1 + n_2 + n_3$, using a 
\textcolor{index}{multinomial distribution}\index{multinomial distribution} of parameter $[N; p_1, p_2, p_3]$ where
  $p_1, p_2, p_3$ are the the proportions of observations falling respectively in \texttt{A}, \texttt{C1}, and \texttt{C2}.
 
What are the values of $p_1, p_2, p_3$? Finally, describe how you would proceed to synthesize observations 
 separately for \texttt{A}, \texttt{C1}, and \texttt{C2}, once the random observation counts $n_1, n_2, n_3$ have been generated.
 
\end{itemize}

\subsection{Solution}

The code to produce \texttt{A}, \texttt{C1}, and \texttt{C2} is in section~\ref{gfrk65zs}. It also 
produces \texttt{C1\_full} and \texttt{C2\_full}, identical to \texttt{C1} and \texttt{C2} except that duplicate observations are now kept as duplicates, rather
 than aggregated. 
This completes \textcolor{red}{Step 1} and \textcolor{red}{Step 2}. The same code produces
 \texttt{map\_C1} and \texttt{map\_C2}. All these tables are saved as separate tabs
 in a spreadsheet \texttt{iot\_security.xls}, available on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/iot_security.xlsx}{here}.

To check that all the counts are correct, compute the full number of observations in each subset, and verify that the sum matches
  the number of observations in the original data. For instance, \texttt{C1} has 70 unique (distinct) observations or rows, with multiplicity  stored
 in the column \texttt{size}. The sum of this column is $\num{7158}$, representing the actual number of observations.
 Likewise, \texttt{C2} has 87 rows and 110 observations when counting the duplicates. And \texttt{A} has 
 23 rows and $\num{110467}$ observations. Finally, $110 + \num{7158} +\num{110467} = \num{117735}$, matching the number of rows in the original dataset.
This completes \textcolor{red}{Step 3}.

To synthesize \texttt{C2}, I used a minimalist version of the NoGAN code in project~\ref{genaiyert}. This updated version is very generic,
 with all the selected features declared in section [1.2] in the code, along with the sublist of those that are categorical. 
  The code is on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/NoGAN_iot.py}{here}.
 The results -- synthetic data, validation and training sets, along with some evaluation metrics -- are
 in the \texttt{C2\_Synth\_NoGAN} tab 
 in the \texttt{iot\_security.xlsx} spreadsheet,
 \href{https://github.com/VincentGranville/Main/blob/main/iot_security.xlsx}{here}. 
I used the \textcolor{index}{hyperparameter}\index{hyperparameter} 
\texttt{[80,80,80,80,80,80,80,80]} where each component represents the number of bins used for the corresponding feature,
 whether continuous or categorical.

The NoGAN synthesizer works as follows. It splits the real data, in this case 
\texttt{C2\_full}, into two subsets: the \textcolor{index}{training set}\index{training set} to generate
 new observations, and the \textcolor{index}{validation set}\index{validation set} to check how good the generated observations are, by comparing the \textcolor{index}{joint empirical distribution function}\index{empirical distribution!multivariate} (ECDF) of the synthetic data, with that of the validation set. This
 \textcolor{index}{cross-validation}\index{cross-validation} technique is known as the 
\textcolor{index}{holdout method}\index{holdout method}: observations in the validation set are held out (that is, not used to train the model),
 to assess performance outside the training set. The distance between the two multivariate ECDFs, denoted as KS, 
 is the \textcolor{index}{Kolmogorov-Smirnov distance}\index{Kolmogorov-Smirnov distance}. In addition, the KS distance between
 the training and validation sets is also computed, and referred to as ``Base KS". All the KS distances range from 0 (very good) to 1 (very bad).
 A synthetization is good when both the KS and Base KS distances are very similar.

The \texttt{C2\_full} dataset only has 110 observations, including duplicates and outliers. This makes it hard to synthesize, especially since only 50\% of these observations are used for training. The Base KS distance between the training and validation sets is rather large: 0.1455. It means that the validation set is quite different from the training set. But the KS distance between the synthesized and validation sets is barely any larger: 0.1727. Thus, NoGAN did a pretty good job. The KS distance between the synthesized data and the training set is a lot smaller. To balance the dataset (increasing the proportion of observations with label equal to 0), create a large enough sample and discard generated observations with non-zero label. This completes the main part of
 \textcolor{red}{Step 4}.

To answer \textcolor{red}{Step 5}, based on earlier computations,  the proportions $p_1, p_2, p_3$ are respectively
$$
p_1 = \frac{\num{110467}}{\num{117735}}, \quad p_2 =\frac{ 7158}{\num{117735}}, \quad p_3 = \frac{110}{\num{117735}}.
$$
Since the set \text{A} only has 23 distinct observations repeated time and over (totaling $\num{110467}$ when not deduped), to synthesize \texttt{A} we can again use a multinomial distribution with the correct probabilities, this time to generate 23 random counts
 adding to $n_1$. Each count 
 tells you how many times the corresponding unique observation must be repeated. The probability attached to a unique observation is its
 frequency measured in the training data.
 The synthetization of \texttt{C1} is left as an exercise.


\subsection{Python code with SQL queries}\label{gfrk65zs}

The code, also available on GitHub  \href{https://github.com/VincentGranville/Main/blob/main/iot_security.py}{here},
 splits the original dataset into \texttt{A}, \texttt{C1}, and \texttt{C2}. 
It also produces \texttt{map\_C1} and \texttt{map\_C2}. I did not include
 the adapted NoGAN code, but you can find it on GitHub,
 \href{https://github.com/VincentGranville/Main/blob/main/NoGAN_iot.py}{here}. It also contains quite a bit of SQL
 to compute the KS distance. I want to thank \href{https://www.linkedin.com/in/willie-waters-79451350/}{Willie Waters} for bringing this dataset to my attention. \vspace{1ex}


\begin{lstlisting}
import numpy as np
import pandas as pd

url = "https://raw.githubusercontent.com/VincentGranville/Main/main/iot_security.csv"
data = pd.read_csv(url) 
# data = pd.read_csv('iot.csv')
features = list(data.columns)
print(features)
data_uniques = data.groupby(data.columns.tolist(), as_index=False).size()
data_B = data_uniques[data_uniques['size'] <= 3] #
data_A = data_uniques[data_uniques['size'] > 3]
data_A.to_csv('iot_A.csv')
print(data_A)

data_C = data_B.drop(['src_port','size'], axis=1) 
data_C = data_C.groupby(data_C.columns.tolist(), as_index=False).size()
data_C1 = data_C[(data_C['bidirectional_mean_ps'] == 60) | 
                 (data_C['bidirectional_mean_ps'] == 1078) |
                 (data_C['size'] > 1)]
data_C2 = data_C[(data_C['bidirectional_mean_ps'] != 60) & 
                 (data_C['bidirectional_mean_ps'] != 1078) &
                 (data_C['size'] == 1)]
print(data_C)
data_C1.to_csv('iot_C1.csv')
data_C2.to_csv('iot_C2.csv')

data_B_full = data_B.join(data.set_index(features), on=features, how='inner') 
features.remove('src_port')
data_C1_full = data_C1.merge(data_B_full, how='left', on=features) 
data_C2_full = data_C2.merge(data_B_full, how='left', on=features) 
data_C1_full.to_csv('iot_C1_full.csv')
data_C2_full.to_csv('iot_C2_full.csv')

map_C1 = data_C1_full.groupby('src_port')['src_port'].count()
map_C2 = data_C2_full.groupby('src_port')['src_port'].count()
map_C1.to_csv('iot_C1_map.csv')
map_C2.to_csv('iot_C2_map.csv')

data_C1 = data_C1_full.drop(['src_port','size_x', 'size_y'], axis=1)
data_C1 = data_C1.groupby(data_C1.columns.tolist(), as_index=False).size()
data_C2 = data_C2_full.drop(['src_port','size_x', 'size_y'], axis=1)
data_C2 = data_C2.groupby(data_C2.columns.tolist(), as_index=False).size()
data_C1.to_csv('iot_C1.csv')
data_C2.to_csv('iot_C2.csv')
\end{lstlisting}

\section{Good GenAI evaluation, fast LLM search, and real randomness}\label{radixdw8}

In this section, I cover several topics in detail. First, I introduce one of the best 
\textcolor{index}{random number generators}\index{random numbers (PRNG}\index{PRNG} (PRNG) with infinite period. Then,
 I show how to evaluate the synthesized numbers using the best metrics. Finally, I illustrate how it applies to
 other contexts, such as \textcolor{index}{large language models}\index{LLM}\index{large language models (LLM)} (LLM). In particular,
 the system is based on words with letters from an arbitrary alphabet, and it can be adapted to any prespecified 
 multivariate distribution, not just uniform: the joint ECDF  (\textcolor{index}{empirical distribution}\index{empirical distribution}\index{empirical distribution!multivariate}) attached to a training set in \textcolor{index}{GenAI}\index{generative AI (GenAI)}\index{GenAI} systems, for instance.
At each step, the focus is both on quality and speed, 
revisiting old methods or inventing new ones, to get solutions performing significantly better and requiring much less computing time. The three components of this system are:
\vspace{1ex}

\noindent{\bf New powerful random number system} 

\noindent In its simplest form, the random numbers are defined as the binary digits $d_n = x_n\bmod{2}$, from 
 the sequence $x_{n+1} = 3 \cdot (x_n // 2)$, where the double slash is the \textcolor{index}{integer division} [\href{https://mathworld.wolfram.com/IntegerDivision.html}{Wiki}]. It is an improvement over binary digits of quadratic irrationals used previously
 (see section 4.4 in \cite{vgchaos}) in the sense that $x_n$ grows only by a factor $3/2$ at each iteration, rather than $2$. All
 sequences $(x_n)$ that do not grow indefinitely necessarily result in periodic numbers. This is the case for all PRNGs on the market.
 
In addition, despite having very long periods, these random generators with finite periods exhibit subtle patterns  in rather low dimensions: in short, lack of randomness. They can be quite sensitive to the \textcolor{index}{seed}\index{seed (random number generators)} and~may require many warm-up iterations before reaching higher randomness. See 
\href{https://github.com/tna0y/Python-random-module-cracker}{here}
how you can crack the
\textcolor{index}{Mersenne twister}\index{Mersenne twister} used in the Numpy random function.
 The question is this: how slowly can $x_n$ grow while preserving perfect randomness, fast implementation, and an infinite period? Read on to see how I managed to reduce the aforementioned exponential growth down to linear, while keeping an infinite period. Proving that the period is infinite is still an open question in number theory, and beyond the scope of this project. 
\vspace{1ex}

\noindent{\bf Ultrafast, robust evaluation metrics} 

\noindent The first step is to define what a strongly random sequence is, when it consists of deterministic digits. Details are again in chapter 4 in \cite{vgchaos}. The takeaway: you need a metric that captures just that, when testing your system. This is true for all GenAI systems. Indeed, here I am re-using the full multivariate 
\textcolor{index}{Kolmogorov-Smirnov distance}\index{Kolmogorov-Smirnov distance} (KS) specifically implemented in the context
 of synthetic data generation: see section 6.4.2 in \cite{vgstats} for details. There, I showed how poorly implemented  metrics used by vendors fail to capture subtle departures from the target distribution. 

In this section, I present a very fast implementation of KS. I also include a few other tests. Very large test batteries exist,
 for instance \textcolor{index}{Diehard}\index{Diehard tests (randomness)} [\href{https://en.wikipedia.org/wiki/Diehard_tests}{Wiki}].  
However, most rely on old statistical practice, offering a large number of disparate, weak tests, rather than
 a centralized approach to dealing with the problem. You can do a lot better with much fewer tests. This is one of the goals of this project, also
 with a focus on hard-to-detect patterns.

Also note that the KS distance relies on the CDF rather than the PDF (probability density function). The latter, used in many tests such as 
\textcolor{index}{Chi-squared}\index{Chi-squared test}, does not  work when you have billions of cross-feature buckets in high dimensions, each with very few observations. 
 As in many GenAI systems, this is what we face.  To give you an idea, think about counting occurrences of billions of
 ``words" such as 
$$321023201031022303412310332310300311023102 
$$
in a sequence of trillions of digits in base 4 (in this case, the alphabet has 4 letters).  Most counts will
 be zero. Likewise, the base (that is, the size of the alphabet) may be a very large integer. The KS distance handles this problem transparently
 by looking at closest strings found in the digit sequences, themselves having only one occurrence most of the time. Also, it easily takes care of conditional probabilities when needed.

My previous KS implementation involved thousands of Pandas SQL queries spanning across many
 features. The new version discussed here is based on the 
\textcolor{index}{radix numeration system}\index{radix numeration system} [\href{https://en.wikipedia.org/wiki/Radix}{Wiki}], turning long strings in big integers (called blocks), allowing
 for fast retrieval with simple \textcolor{index}{binary search}\index{binary search} in a list of big numbers. In this context, a block can have many digits: the $k$-th feature is the $k$-th digit, although blocks may have a varying number of digits. I implicitly rely on the Python
 \textcolor{index}{Bignum library}\index{Bignum library} [\href{https://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic}{Wiki}] to deal with the computations. Finally, the binary search is further improved and called 
\textcolor{index}{weighted binary search}\index{binary search!weighted}, accelerating the computations by a factor 3 or 4 in the examples tested.
 So far, I did not compare with other methods such as \textcolor{index}{vector search}\index{vector search} 
 based on KNN and ANN (approximate nearest neighbors). But these methods are very relevant in this context.
\vspace{1ex}

\noindent {\bf Connection to LLM} 

\noindent The above paragraphs establish the connection to large language models. 
 The problem is strikingly similar to DNA sequence synthetization discussed in section~\ref{dnalove}, where the alphabet has 
 four letters (A, C, G, T) and the words consist of DNA subsequences. The main difference is that DNA sequences are far from random. Yet,
 the methodology presented here can easily be adapted to arbitrary target distributions. In particular to empirical distributions 
like those associated to DNA sequencing, or keyword distributions in ordinary text.

Then, as illustrated in the DNA sequencing problem, predictive analytics for GenAI may rely on conditional probabilities such as $P(B_1 | B_2)$, where $B_1, B_2$ are consecutive blocks. 
Transitioning from KS and the multivariate CDF to conditional probabilities is straightforward with the formula   
$P(B_1 | B_2) = P(B_1, B2)/P(B_2)$.



\subsection{Project description}

We are dealing with sequences of positive integers defined by a recursion $x_{n+1} = f(x_n) \bmod{\tau_n}$, where $x_0$ is the initial condition, referred to as the main seed. The choice of $f$ leads to $x_n$ randomly going up and down as $n$ increases. On average, the trend is up: eventually,  $x_n$ gets bigger and bigger on average, albeit rather slowly to allow for fast computations. The {\bf digits} -- what the generated random numbers are -- are simply defined as $d_n = x_n \bmod{b}$, where $b$ is called the {\bf base}. 

The big contrast with classic random generators is that $\tau_n$ depends on $n$ rather than being fixed. Here $\tau_n$ strictly increases at each iteration, allowing $x_n$ to grow on average. It leads to more random digits, and an infinite period. In the current implementation, $\tau_{n+1} = s + \tau_n$ where $s$ is a moderately large integer. Thus, we have two additional seeds: the step $s$, and $\tau_0$. The function $f$
 depends on two integer parameters $p, q$. More specifically,
\begin{equation}
x_{n+1} = p \cdot (x_n // q) \bmod{\tau_n},\, \text{ with } p > q > 1, \, x_0, \tau_0\geq 0, \text{ and } \tau_{n+1} = s + \tau_n.\label{sd20h}
\end{equation}
Again, the double slash stands for the integer division: $x_n // q = \lfloor x_n / q\rfloor$ where the brackets denote the floor function, 
or \texttt{int} in Python. I use the integer division in Python as it works when $x_n$ becomes extremely large, unlike the standard division combined with \texttt{int}.

I implemented three tests of randomness. Before going into the details, let me introduce the concept of {\bf block}. A block consists of a fixed number of digits, used for testing purposes. They are encoded as big integers. For now, let's pretend that a block $B$ has $m$ digits, each with a variable base:
 the $k$-th digit is in base $b_k$, thus with $0\leq d_k < b_k$. Then $B$ can be uniquely encoded with the one-to-one mapping
\begin{equation}
B = d_1 + d_2 \times b_1 + d_3 \times b_1b_2 + d_4 \times b_1b_2b_3 + \cdots + d_m \times b_1b_2\cdots b_{m-1}\label{rradix}
\end{equation}
In this case, $B$ is represented by an integer strictly smaller than $b_1b_2\cdots b_m$. In the context of real data, each digit represents a feature; the $k$-th feature can take on $b_k$ different values after approximation or truncation, and a block corresponds to a row in a tabular data set. Of course, the $b_k$' can be quite large and different, depending on the feature, especially for numerical features. At the other extreme, for a 2-category feature, 
 $b_k = 2$. The numeration system based on~(\ref{rradix}) is called the 
\textcolor{index}{radix system}\index{radix numeration system}. It is a generalization of the numeration system in fixed base $b$. When a new observation is generated, it is first encoded as a block, then the closest match to any row in the training set can be found with a binary search on all long integers $B$ between $0$ and $(b_1\cdots b_m) -1$, each one representing a potential row in the training set. Note that in general, the existing $B$'s cover a small subset of all potential values: we are dealing with sparsity. 
We will use the same search algorithm here, with a fixed base $b$. It makes sense to call it \textcolor{index}{radix search}.

Now I can provide a quick overview of the three tests of randomness. The most involved is the block test: it is based on the 
multivariate KS distance, which in turn relies on radix search. The tests are:
\vspace{1ex}
\begin{itemize}
\item \textcolor{index}{Run test}\index{run test}.  The max run test is implemented in section~\ref{sqrt2run} for binary digits of quadratic irrationals. The solution presented here
 is more comprehensive, looking at runs or arbitrary length for all possible digits. These run lengths have a 
\textcolor{index}{geometric distribution} 
 if digits show up randomly. When the base $b$ is very large, there are too many values to fit in a table, so I also compute the following statistics:
\begin{equation}  
R(L) = \mathlarger{\sum}_{d=0}^{b-1}  \Bigg(\frac{\rho(L, d) - \rho_0(L, d)}{\sigma(L,d)}\Bigg)^2, \quad L = 1,2,\dots\label{pores}
\end{equation}
where $\rho_0(L, d)$ is the number of runs of length $L$ found in the sequence for digit $d$, and $\rho(L, d)$ is the expected count if the sequence is random; then, it does not depend on $d$. 
 Finally, $\sigma^2(L, d)$ is the theoretical variance, not depending on $d$, to normalize $R(L)$ so that it has a 
\textcolor{index}{Chi-squared distribution}\index{Chi-squared distribution} with $b$ degrees of freedom. In short, an unexpectedly large $R(L)$ indicates lack of randomness. 
\vspace{1ex}

\item \textcolor{index}{Spectral test}\index{spectral test}. 
This test looks at autocorrelations of lag 1, 2, and so on in the digit sequence, to check out whether their behavior is
compatible or not with randomness.  Because the sequences can be extremely long, the computations are done on the flight, 
updated one new digit at a time using a buffer, without having to store the whole sequence anywhere.
\vspace{1ex}

\item \textcolor{index}{Block test}\index{block test}. Create a sample of blocks $B$ of size $m$, for instance equally spaced, and independently of the blocks found in the digit sequence. Here $m$ is the number of digits per block, with each digit in base~$b$. Hence, each block should occur in the infinite digit sequence with
 frequency $b^{-m}$, assuming randomness. 
Compute the CDF value $F(B)$ attached to the underlying theoretical (uniform) distribution, for each of these blocks.  
Then compute the ECDF or empirical CDF value $F_0(B)$ based on block occurrences: mostly nearest neighbors to $B$, found in the digit sequence. 
Finally, $\text{KS} = \sup |F(B) - F_0(B)|$, and the supremum is over all blocks $B$ in your sample. 
To magnify any subtle departure from randomness, plot the following: $\delta(B) = F(B) - F_0(B)$ on the Y-axis, and $B$ (in its integer representation) on the X-axis, for all $B$ in your sample.  
\end{itemize}
\vspace{1ex}

\noindent There is a considerable amount of new material to master, if you were to implement the whole system on your own. The goal is not to create your own code from scratch, but rather to understand and use mine. You are welcome to improve it and create Python objects for the various components. 
One of the easy tasks is to use the code to compare different random generators and assess the impact of parameters. Another goal is to generalize the code to other contexts such as synthesizing DNA sequences, where the target distribution is not uniform, but comes as an ECDF (empirical CDF) computed on the training set. This also involves conditional probabilities: predicting the next block given the previous ones, when blocks are auto-correlated.

\noindent The project consists of the following steps:\vspace{1ex}

\begin{itemize}
\item[] {\bf Step 1}:  Read and understand the code in section~\ref{codesda}. Identify the different components: the three tests, the base, the digits, the generation of the digits, the blocks and block encoding, the seeds and parameters, and the binary search to locate nearby blocks in the sequence, given a specific block. Run the code (it is on GitHub), see and analyze the results. Understand the computations for the autocorrelation  coefficients, 
 as it is done on the fly, in a non-traditional way using a buffer. 
\vspace{1ex}

\item[] {\bf Step 2}: What are the values of $\rho(L, d)$ and $\sigma(L,d)$ in Formula~\ref{pores}? How would you optimize a binary search so that it requires fewer steps? To help answer this question, look at the variable \texttt{trials}, which counts the combined number of steps used in all the binary searches. Finally, save the generated digits in a file if the sequence is not too long.
\vspace{1ex}

\item[] {\bf Step 3}: To improve block tests, work with blocks of various sizes. Also, compute the autocorrelations in the block sequence, in the same way it is done in the digit sequence. Given a small-size block $B$ consisting of two sub-blocks $B_1, B_2$, estimate
 $P(B_2 | B_1)$ on the data, and confirm the independence between $B_1$ and $B_2$, that is, $P(B_2 | B_1) = P(B_2)$. 
\vspace{1ex}

\item {\bf Step 4}: Try different seeds $x_0, \tau_0$ and $s$ to check out how the random generators is sensitive to the seeds. Does it change the quality
 of the randomness? In particular, try to find the lowest possible seed values that still lead to good random numbers. Then, try different combinations of $p,q$ and $b$. Some work, some don't. Can you identify necessary conditions on $p, q, b$ to guarantee good random numbers?
Finally, can you find a good combination that makes the sequence $(x_n)$ grow as slowly as possible, while still generating great random numbers. Obviously, you need $p > q$, for instance $p = q+1$, and $s$ as small as possible. A large base $b$ also helps to extract as much as possible
 from the sequence $(x_n)$, yet some combinations $(p, q, b)$ don't work, and usually $b>q$ causes problems, making $b=q$ ideal.
\end{itemize}

 
 \subsection{Solution}

Since much of the solution is my code in section~\ref{codesda}, I cover only specific items in this section, with a focus on explaining and interpreting the output tables and visualizations. The main parameters are initialized in section [3] in the code, see lines \textcolor{gray}{278} -- \textcolor{gray}{288}. In particular, the current values of $x_n, \tau_n$ are stored in \texttt{x} and \texttt{tau} respectively, while $s$ in Formula~(\ref{sd20h}) is
 denoted as \texttt{step} in the code. 

Figure~\ref{fig:scprngf} shows the function $\delta(B) = F(B) - F_0(B)$, that is, the difference between a perfectly uniform CDF and 
the approximate ECDF computed on the  generated digits, using 500 equally-spaced test blocks, each with 6 digits. The number of blocks (500 here) is specified by \texttt{n\_nodes} in the code. The higher the number, the better the approximation. The total number of digits if $\num{640000}$.
I tried four sets of parameters labeled HM$_1$ to HM$_4$.  The parameter sets are specific combinations of $p, q, b$.
 See lines \textcolor{gray}{294}, \textcolor{gray}{316}, \textcolor{gray}{337} and \textcolor{gray}{358} for the respective values in the code.
I also added Numpy.random for comparison purposes. Note that  HM stands for ``Home-Made", by contrast to Numpy. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.79\textwidth]{prng.png}   
\caption{Four versions of my generator (HMx) compared to numpy.random}
\label{fig:scprngf}
\end{figure}
%imgpy9979_2and3.PNG screen2e.png  
%-------------------------

Clearly, HM$_2$ and HM$_3$ generate non-random numbers. As for HM$_1$, HM$_4$ and Numpy, they pass this test. It does not mean that they generate random digits. More tests are needed for verification, in particular with larger block sizes 
 (the parameter \texttt{block\_size}) and more nodes (the parameter
 \texttt{n\_nodes}).  

Table~\ref{gghuh42s} focuses on HM$_1$ only. It shows the number of occurrences for runs of length $L$, for each of the 4 digits ($b=4$) and various values of $L$.  The sequence has $\num{640000}$ digits. The 4 rightmost columns are summary statistics: \texttt{Exp} and \texttt{Avg} are 
respectively the expected and observed counts, while \texttt{Norm} indicates whether or not all the counts, for a specific $L$,
 are compatible with perfect randomness. If the digits are truly random, then \texttt{Norm} approximately follows a standard normal distribution. In particular,  $\texttt{Norm} = (R(L) -b) / (2b)$ where $R(L)$ is defined by~(\ref{pores}).
Clearly, HM$_1$ passes this test.


\begin{table}[H]
\small
\setlength\extrarowheight{-1pt}
\[
\begin{array}{r|rrrr|rrrr}
\hline
L&      d=0&      d=1&      d=2&      d=3&      \text{Exp}&      \text{Avg}&     \text{Chi2}&     \text{Norm}\\
\hline
\hline
1&     8968&     8932&     9074&     8999&     9000&     8993&     1.44&  -0.3202\\
2&     2296&     2227&     2294&     2149&     2250&     2242&     6.81&   0.3511\\
3&      548&      532&      620&      587&      563&      572&     9.05&   0.6315\\
4&      146&      132&      143&      131&      141&      138&     1.44&  -0.3204\\
5&       36&       39&       38&       36&       35&       37&     0.69&  -0.4136\\
6&        3&       11&        8&       10&        9&        8&     4.61&   0.0759\\
7&        2&        3&        3&        2&        2&        3&     0.62&  -0.4223\\
8&        0&        2&        0&        0&        1&        1&     3.83&  -0.0211\\
\hline
\end{array}
\]
\caption{\label{gghuh42s}Runs of length $L$ per digit for HM$_1$, with summary stats}
\end{table}
%\renewcommand{\arraystretch}{1.0} %%%

Table~\ref{gggtr4} shows  summary statistics for the 4 home-made generators (HM) and Numpy. One of them (HM$_3$) has $b=256$. 
The normal approximation to \texttt{Norm} is especially good when $b > 10$, and bad when the counts are small, that is, when $L$ is large.
All the values not compatible with the randomness assumption are highlighted in red. Thus, HM$_2$ and HM$_3$ do not produce random
 digits, confirming the findings from Figure~\ref{fig:scprngf}. Here, AC stands for autocorrelations within digit sequences 


\begin{table}[H]
\small
\setlength\extrarowheight{-1pt}
\[
\begin{array}{lr|rrrrr}
\hline
\text{Metric}	&	\text{Level}	&	\text{HM}_1	&	\text{HM}_2	&	\text{HM}_3	&	\text{HM}_4	&	\text{Numpy}	\\
\hline
\hline
\text{Norm}	&	L = 1	&	-0.3202	&	\textcolor{red}{1530.7}	&	\textcolor{red}{136.38}	&	0.0885	&	-0.0471	\\
	&	L = 2	&	0.3511	&	\textcolor{red}{249.3}	&	\textcolor{red}{95.80}	&	-0.1944	&	0.3141	\\
	&	L = 3	&	0.6315	&	\textcolor{red}{80.1}	&	\textcolor{red}{41.37}	&	0.5242	&	0.8902	\\
\hline
\text{AC}	&	\text{Lag } 1	&	0.0010	&	-0.0046	&	\textcolor{red}{-0.0731}	&	-0.0002	&	-0.0007	\\
	&	\text{Lag } 2	&	-0.0011	&	0.0022	&	0.0048	&	0.0022	&	-0.0014	\\
\hline
\text{KS}	&		&	0.00676	&	\textcolor{red}{0.06136}	&	\textcolor{red}{0.01932}	&	0.00929	&	0.00859	\\
\hline
\end{array}
\]
\caption{\label{gggtr4}High-level comparison of HM and Numpy generators, with red flags}
\end{table}
%\renewcommand{\arraystretch}{1.0} %%%

Besides listing the parameters used in the simulation, Table~\ref{gds32} features two interesting metrics: last $x_n$, and \texttt{Trials}. The former indicates how fast the
 sequence $x_n$ grows. Numpy is not based on growing sequences, resulting in digits that keep repeating themselves past the finite period. And for random 
binary digits 
 based on quadratic irrationals, the last $x_n$ would be of the order $2^N \approx 10^{\num{193000}}$, where $N = \num{640000}$ is the number of digits in the sequence.
 By contrast, for the HM generators explored here, it is around $10^{10}$.


\begin{table}[H]
\small
\setlength\extrarowheight{-1pt}
\[
\begin{array}{l|rrrrr}
\hline
 \text{Feature} & 	\text{HM}_1	&	\text{HM}_2	&	\text{HM}_3	&	\text{HM}_4	&	\text{Numpy}\\
\hline
\hline
\text{Last } x_n	&	10^{10}	&	10^9	&	10^{10}	&	10^9	&	\text{n/a}	\\
\text{Trials}	&	178	&	2197	&	3497	&	2293	&	150	\\
\hline
\text{Base}	&	4	&	4	&	8	&	256	&	4	\\
\text{Block size}	&	6	&	6	&	6	&	6	&	6	\\
\hline
p	&	7	&	6	&	7	&	401	&	\text{n/a}	\\
q	&	4	&	4	&	4	&	256	&	\text{n/a}	\\
\hline
\end{array}
\]
\caption{\label{gds32}Top parameters and special metrics}
\end{table}
%\renewcommand{\arraystretch}{1.0} %%%

Finally, \texttt{Trials} represents the total number of steps needed in the binary search, combined over the 
500 test blocks (500 = \texttt{n\_nodes}). For each test block, the goal is to find the closest neighbor block in the  digit sequence
 ($\num{640000}$ digits). The \texttt{Trials} value depends on the base, the block size, the number of digits in the sequence, and the number of test blocks.
A very small value means that most of the test blocks are already present in the digit sequence: for these test blocks,  the binary search is not needed. 
In the table, a value of 2197 means that on average, each test block requires $2197 / 500 = 4.39$ trials before finding the closest neighbor block.
 The lower \texttt{Trials}, the faster the computations. Because I use an optimized binary search, it is already 3--4 times faster than the standard method.
The generation of random digits is also just as fast as Numpy.
 
Regarding the project steps, I now provide answers to selected questions. Let $N$ be the number of digits~in base $b$ in the sequence, and $\pi$ be the probability for a run to be of length $L > 0$, for any digit $d<b$. We  are dealing with  a \textcolor{index}{binomial distribution}\index{binomial distribution} of parameter $(N, \pi)$. Thus, 
$$
\rho(L,d) = N \pi, \quad \sigma^2(L, d) = N\pi(1-\pi), \quad \text{ with } \pi = \Big(\frac{b-1}{b}\Big)^2 \cdot \Big(\frac{1}{b}\Big)^L,
$$
Note that  are $\rho(L,d)$ and $\sigma^2(L, d)$ are respectively the expectation 
 and variance of the binomial distribution.  This answers the first question in \textcolor{index}{Step 2}. For the second question about the binary search,
 see lines \textcolor{gray}{184} -- \textcolor{gray}{197} in the code. If you set \texttt{A=1} and \texttt{B=1} respectively
 in lines  \textcolor{gray}{187} and \textcolor{gray}{188}, it becomes a standard binary search, with 
\textcolor{index}{computational complexity}\index{computational complexity} $O(\log_2 n)$ in all cases. With my choice of \texttt{A} and \texttt{B}, 
 it is similar to \textcolor{index}{interpolation search}\index{binary search!interpolation search} [\href{https://en.wikipedia.org/wiki/Interpolation_search}{Wiki}], which is $O(\log_2 \log_2 n)$ for the average case when the underlying distribution is uniform. See also the recent article 
on \textcolor{index}{interpolated binary search}\index{binary search!interpolated binary search} \cite{charleroi}.



\subsection{Python implementation}\label{codesda}

The code is also on GitHub, \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/pnrg_tests.py}{here}.
\vspace{1ex}
\begin{lstlisting}[numbers=left]
import numpy as np
from collections import OrderedDict

#--- [1] Functions to generate random digits

def get_next_digit(x, p, q, tau, step, base, option = "Home-Made"):

    if option == "Numpy":
        digit = np.random.randint(0, base)
    elif option == "Home-Made":
        x =  ((p * x) // q) % tau  # integer division for big int
        tau += step
        digit = x % base
    return(digit, x, tau)


def update_runs(digit, old_digit, run, max_run, hash_runs):

    if digit == old_digit:
        run += 1
    else:
        if (old_digit, run) in hash_runs:
            hash_runs[(old_digit, run)] += 1
        else:
            hash_runs[(old_digit, run)] = 1
        if run > max_run:
            max_run = run
        run = 1
    return(run, max_run, hash_runs)


def update_blocks(digit, m, block, base, block_size, hash_blocks):

    if m < block_size:
        block = base * block + digit
        m += 1
    else:
        if block in hash_blocks:
            hash_blocks[block] += 1
        else:
            hash_blocks[block] = 1
        block = 0
        m = 0
    return(m, block, hash_blocks)


def update_cp(digit, k, buffer, max_lag, N, cp_data):

    # processing k-th digit starting at k=2
    # buffer stores the last max_lag digits
    # cp stands for the cross-product part (in autocorrelation)

    mu = cp_data[0]
    cnt = cp_data[1]
    cp_vals = cp_data[2]
    cp_cnt = cp_data[3]

    buffer[(k-2) % max_lag] = digit
    mu += digit
    cnt += 1

    for lag in range(max_lag):
        if k-2 >= lag: 
            cp_vals[lag] += digit * buffer[(k-2-lag) % max_lag]
            cp_cnt[lag] += 1

    cp_data = (mu, cnt, cp_vals, cp_cnt)
    return(cp_data, buffer)


def generate_digits(N, x0, p, q, tau, step, base, block_size, max_lag, option = "Home-Made"):

    # Main function. Also produces output to test randomnes:
    #     - hash_runs and max_run: input for the run_test function
    #     - hash_blocks: input for the block_test function
    #     - cp_data input for the correl_test function

    # for run and block test
    hash_runs = {}
    hash_blocks = {}
    block = 0 
    m = 0
    run = 0
    max_run = 0
    digit = -1    
 
    # for correl_test
    mu = 0 
    cnt = 0 
    buffer = np.zeros(max_lag) 
    cp_vals = np.zeros(max_lag) # cross-products for autocorrel 
    cp_cnt = np.zeros(max_lag) 
    cp_data = (mu, cnt, cp_vals, cp_cnt)

    x = x0

    for k in range(2, N): 

        old_digit = digit
        (digit, x, tau) = get_next_digit(x, p, q, tau, step, base, option) 
        (run, max_run, hash_runs) = update_runs(digit, old_digit, run, max_run, hash_runs)
        (m, block, hash_blocks) = update_blocks(digit, m, block, base, block_size, hash_blocks)
        (cp_data, buffer) = update_cp(digit, k, buffer, max_lag, N, cp_data)

    print("----------------")        
    print("PRNG = ", option) 
    print("block_size (digits per block), digit base: %d, %d", block_size, base)
    if option == "Home-Made":
        print("p, q: %d, %d" %(p, q))
        print(len(str(x)), "decimal digits in last x")
    return(hash_runs, hash_blocks, max_run, cp_data)


#--- [2] Functions to perform tests of randomness

def run_test(base, max_run, hash_runs):

    # For each run, chi2 has approx. chi2 distrib. with base degrees of freedom
    # This is true assuming the digits are random

    print()
    print("Digit  ", end = " ")
    if base <= 8:
        for digit in range(base):
            print("%8d" %(digit), end =" ")
    print("     Exp", end = " ")
    print("     Avg", end = " ") # count average over all digits 
    print("    Chi2", end = " ") # degrees of freedom = base
    print("    norm", end = " ") 
    print("\n")

    for run in range(1, max_run+1):

        print("Run %3d" % (run), end = " ")
        prob = ((base-1)/base)**2  * (1/base)**run 
        exp  = N*prob 
        var  = N*prob*(1-prob)
        avg  = 0
        chi2 = 0  

        for digit in range(0, base):
            key = (digit, run)
            count = 0
            if key in hash_runs:
                count = hash_runs[key]
                avg += count
                chi2 += (count - exp)**2 / var 
            if base <= 8: 
                print("%8d" %(count), end =" ")

        avg /= base
        norm = (chi2 - base) / (2*base)
        print("%8d" %(int(0.5 + exp)), end =" ")
        print("%8d" %(int(0.5 + avg)), end =" ")
        print("%8.2f" %chi2, end =" ")
        print("%8.4f" %norm, end =" ")
        print()

    return()


def get_closest_blocks(block, list, trials):

    # Used in block_test
    # Return (left_block, right_block) with left_block <= block <= right_block, 
    #   - If block is in list, left_block = block = right_block
    #   - Otherwise, left_block = list(left), right_block = list(right); 
    #                they are the closest neighbors to block, found in list
    #   - left_block, right_block are found in list with weighted binary search
    #   - list must be ordered
    # trials: to compare spped of binary search with weighted binary search

    found = False
    left = 0
    right = len(list) - 1
    delta = 1
    old_delta = 0 
    
    if block in list:
        left_block = block
        right_block = block

    else:
        while delta != old_delta:
            trials += 1
            old_delta = delta   
            A = max(list[right] - block, 0)    # in standard binary search: A = 1
            B = max(block - list[left], 0)     # in standard binary search: B = 1
            middle = (A*left + B*right) // (A + B) 
            if list[middle] > block:
                right = middle
            elif list[middle] < block: 
                left = middle
            delta = right - left

        left_block = list[middle]
        right_block = list[min(middle+1, len(list)-1)]

    return(left_block, right_block, trials)


def true_cdf(block, max_block): 
    # Used in block_test
    # blocks uniformly distributed on {0, 1, ..., max_block}
    return((block + 1)/(max_block + 1))


def block_test(hash_blocks, n_nodes, base, block_size):

    # Approximated KS (Kolmogorov-Smirnov distance) between true random and PRNG
    # Computed only for blocks with block_size digits (each digit in base system)
    # More nodes means better approximation to KS

    hash_cdf = {}
    hash_blocks = OrderedDict(sorted(hash_blocks.items()))
    n_blocks = sum(hash_blocks.values())
    count = 0
    trials = 0  # total number of iterations in binary search


    for block in hash_blocks:
        hash_cdf[block] = count + hash_blocks[block]/n_blocks
        count = hash_cdf[block]

    cdf_list = list(hash_cdf.keys())
    max_block = base**block_size - 1
    KS = 0
    trials = 0     # total number of iterations in binary search
    arr_cdf = []   # theoretical cdf, values
    arr_ecdf = []  # empirical cdf (PRMG), values
    arr_arg = []   # arguments (block number) associated to cdf or ecdf

    for k in range(0, n_nodes): 

        block = int(0.5 + k * max_block / n_nodes)
        (left_block, right_block, trials) = get_closest_blocks(block, cdf_list, trials)
        cdf_val = true_cdf(block, max_block)
        ecdf_lval = hash_cdf[left_block]       # empirical cdf
        ecdf_rval = hash_cdf[right_block]      # empirical cdf
        ecdf_val = (ecdf_lval + ecdf_rval) / 2 # empirical cdf
        arr_cdf.append(cdf_val)
        arr_ecdf.append(ecdf_val)
        arr_arg.append(block)
        dist = abs(cdf_val - ecdf_val)
        if dist > KS:
            KS = dist

    return(KS, arr_cdf, arr_ecdf, arr_arg, trials) 


def autocorrel_test(cp_data, max_lag, base):

    mu = cp_data[0]
    cnt = cp_data[1]
    cp_vals = cp_data[2]
    cp_cnt = cp_data[3]

    mu /= cnt
    t_mu = (base-1) / 2
    var = cp_vals[0]/cp_cnt[0] - mu*mu
    t_var = (base*base -1) / 12 
    print()
    print("Digit mean: %6.2f (expected: %6.2f)" % (mu, t_mu))
    print("Digit var : %6.2f (expected: %6.2f)" % (var, t_var))
    print()
    print("Digit autocorrelations: ")
    for k in range(max_lag):
        autocorrel = (cp_vals[k]/cp_cnt[k] - mu*mu) / var
        print("Lag %4d: %7.4f" %(k, autocorrel))

    return()


#--- [3] Main section

# I tested (p, q) in {(3, 2), (7, 4), (13, 8), (401, 256)}

N = 64000             # number of digits to generate
p = 7                 # p/q must > 1, preferably >= 1.5 
q = 4                 # I tried q = 2, 4, 8, 16 and so on only 
base = q              # digit base, base <= q (base = 2 and base = q work) 
x0 = 50001            # seed to start the bigint sequence in PRNG
tau  = 41197          # co-seed of home-made PRNG
step = 37643          # co-seed of home-made PRNG
digit = -1            # fictitious digit before creating real ones 
block_size = 6        # digits per block for the block test; must be integer > 0
n_nodes = 500         # number of nodes for the block test
max_lag = 3           # for autocorrel test
seed = 104            # needed only with option = "Numpy"
np.random.seed(seed)  

#- [3.1] Home-made PRNG with parameters that work

p, q, base, block_size = 7, 4, 4, 6

# generate random digits, home-made PRNG
(hash_runs, hash_blocks, max_run, cp_data) = generate_digits(N, x0, p, q, tau, step,  
                                                             base, block_size, max_lag, 
                                                             option="Home-Made")
# run_test
run_test(base,max_run,hash_runs)

# block test
(KS, arr_cdf, arr_ecdf1, arr_arg1, trials) = block_test(hash_blocks, n_nodes, 
                                                        base ,block_size)
# autocorrel_test 
autocorrel_test(cp_data, max_lag, base) 

print()
print("Trials = ", trials)
print("KS = %8.5f\n\n" %(KS))


#- [3.2] Home-made, with parameters that don't work

p, q, base, block_size = 6, 4, 4, 6   

# generate random digits, home-made PRNG
(hash_runs, hash_blocks, max_run, cp_data) = generate_digits(N, x0, p, q, tau, step,  
                                                             base, block_size, max_lag, 
                                                             option="Home-Made")
# run_test
run_test(base,max_run,hash_runs)

# block test
(KS, arr_cdf, arr_ecdf2, arr_arg2, trials) = block_test(hash_blocks, n_nodes, 
                                                        base ,block_size)
# autocorrel_test 
autocorrel_test(cp_data, max_lag, base) 

print()
print("Trials = ", trials)
print("KS = %8.5f\n\n" %(KS))

#- [3.3] Home-made, another example of failure

p, q, base, block_size = 7, 4, 8, 6  

# generate random digits, home-made PRNG
(hash_runs, hash_blocks, max_run, cp_data) = generate_digits(N, x0, p, q, tau, step,  
                                                             base, block_size, max_lag, 
                                                             option="Home-Made")
# run_test
run_test(base,max_run,hash_runs)

# block test
(KS, arr_cdf, arr_ecdf3, arr_arg3, trials) = block_test(hash_blocks, n_nodes, 
                                                        base ,block_size)
# autocorrel_test 
autocorrel_test(cp_data, max_lag, base) 

print()
print("Trials = ", trials)
print("KS = %8.5f\n\n" %(KS))

#- [3.4] Home-made, using a large base

p, q, base, block_size = 401, 256, 256, 6  

# generate random digits, home-made PRNG
(hash_runs, hash_blocks, max_run, cp_data) = generate_digits(N, x0, p, q, tau, step,  
                                                             base, block_size, max_lag, 
                                                             option="Home-Made")
# run_test
run_test(base,max_run,hash_runs)

# block test
(KS, arr_cdf, arr_ecdf4, arr_arg4, trials) = block_test(hash_blocks, n_nodes, 
                                                        base ,block_size)
# autocorrel_test 
autocorrel_test(cp_data, max_lag, base) 

print()
print("Trials = ", trials)
print("KS = %8.5f\n\n" %(KS))


#- [3.5] Numpy PRNG (Mersenne twister)

# here p, q are irrelevant
base, block_size = 4, 6 

# generate random digits, home-made PRNG
(hash_runs, hash_blocks, max_run, cp_data) = generate_digits(N, x0, p, q, tau, step,  
                                                             base, block_size, max_lag, 
                                                             option="Numpy")
# run_test
run_test(base,max_run,hash_runs)

# block test
(KS, arr_cdf, arr_ecdf5, arr_arg5, trials) = block_test(hash_blocks, n_nodes, 
                                                        base ,block_size)
# autocorrel_test 
autocorrel_test(cp_data, max_lag, base) 

print()
print("Trials = ", trials)
print("KS = %8.5f\n\n" %(KS))


#--- [4] Scatterplot cdf (true random) versus ecdf (based on the two PRNGs) 

import matplotlib.pyplot as plt
import matplotlib as mpl

mpl.rcParams['axes.linewidth'] = 0.5
plt.rcParams['xtick.labelsize'] = 7
plt.rcParams['ytick.labelsize'] = 7

arr_cdf = np.array(arr_cdf) 
delta_ecdf1 = (np.array(arr_ecdf1) - arr_cdf) 
delta_ecdf2 = (np.array(arr_ecdf2) - arr_cdf) 
delta_ecdf3 = (np.array(arr_ecdf3) - arr_cdf) 
delta_ecdf4 = (np.array(arr_ecdf4) - arr_cdf) 
delta_ecdf5 = (np.array(arr_ecdf5) - arr_cdf) 

# print()
# print("blocks (arguments) used to compute ecdf1:\n")
# print(arr_arg1)

plt.plot(delta_ecdf1, linewidth = 0.4, color = 'red', alpha = 1)
plt.plot(delta_ecdf2, linewidth = 0.3, color = 'blue', alpha = 0.5)
plt.plot(delta_ecdf3, linewidth = 0.3, color = 'darkorange', alpha = 1)
plt.plot(delta_ecdf4, linewidth = 0.4, color = 'purple', alpha = 1)
plt.plot(delta_ecdf5, linewidth = 0.4, color = 'darkgreen', alpha = 1)
plt.axhline(y = 0.0, linewidth = 0.5, color = 'black', linestyle = 'dashed')
plt.legend(['HM1: good', 'HM2: bad', 'HM3: bad', 'HM4: good', 'Numpy'], 
             loc='lower right', prop={'size': 7}, )
plt.show()
\end{lstlisting}

%-------------------------------

\section{Spectacular use of GenAI evaluation as an adaptive  loss function}

Most AI and GenAI algorithms aim at optimizing a criterion. The goal is to get good predictions (see~project~\ref{predxllm}), relevant 
 and exhaustive answers (LLMs) 
 or realistic synthetizations. The quality of the output is assessed using some 
\textcolor{index}{evaluation metric}\index{evaluation (GenAI models)}. However, the algorithm tries to minimize a
 \textcolor{index}{loss function}\index{loss function}. Both are very distinct: one cannot expect to get the best output by optimizing a criterion other than the model evaluation metric. 

So, why are we dealing with two different criteria: loss and evaluation? The reason is simple: good global evaluation metrics are
 unable to handle atomic updates extremely fast, billions of times, for instance each time a neuron is activated in a 
\textcolor{index}{deep neural network}\index{deep neural network (DNN)}. Instead, loss functions are very good at that, and serve as a proxy to the evaluation metric. But what if we could find a great
 evaluation metric that can be updated at the speed of light, at each tiny change? The goal of this project is to answer this question, with
 a case study in the context of tabular \textcolor{index}{synthetic data}\index{synthetic data} generation.

This problem is typically solved with generative adversarial networks\index{GAN (generative adversarial network)} (\textcolor{index}{GANs}),
 see project~\ref{porgan2}. The best evaluation metric, to assess the faithfulness of the generated data,
 is arguably the 
\textcolor{index}{multivariate Kolmogorov-Smirnov distance}\index{Kolmogorov-Smirnov distance!multivariate} (KS). 
It compares two multivariate empirical distributions\index{empirical distribution!multivariate} 
(\textcolor{index}{ECDF}\index{ECDF (empirical distribution)}), corresponding to  the training and synthetic datasets. But GAN does not optimize KS. Instead it tries to minimize a loss function such as 
\textcolor{index}{Wasserstein}\index{Wasserstein GAN (WGAN)}, via 
\textcolor{index}{stochastic gradient descent}\index{gradient descent!stochastic}. 

It would have been fantastic to replace the loss function by KS, but I could not find any way to make it computationally feasible: any tiny
 change to the data -- which happens billions of times with GAN or my other algorithms -- requires to recompute the full KS, a time-consuming task. 
But I found an even more ground-breaking solution. It involves comparing the densities 
(\textcolor{index}{EPDF}\index{EPDF (empirical probability density function)}) rather than 
the distributions (\textcolor{index}{ECDF}). Easier said than done, as it brings new challenges with continuous features; the solution is a lot easier 
 with categorical features.

The seminal idea consists of using more and more granular approximations to the EPDF as the iterative algorithm progresses, eventually converging to the exact yet
 singular EPDF.  The resulting evaluation metric is a variant of the 
\textcolor{index}{Hellinger distance}\index{Hellinger distance}, rather than KS. Thus, it amounts to using an 
\textcolor{index}{adaptive loss function}\index{loss function!adaptive loss}~\cite{adaloss} that converges to the Hellinger distance. 
Then, I tested the method using a special framework where the global optimum is known in advance, hoping to retrieve it. The result is spectacular: 
\vspace{1ex}
\begin{itemize}
\item Brute-force combinatorial approach
 requires $10^{869}$ steps (at most) to find the global optimum. 
\item I found it in less than 2 millions operations. 
\item A typical GAN is nowhere close to the
 optimum after 4 millions weight updates, and it requires more computing time than my method. 
\item The classic \textcolor{index}{Hungarian algorithm}\index{Hungarian algorithm} solves this assignment problem [\href{https://en.wikipedia.org/wiki/Assignment_problem}{Wiki}] in 64 millions operations at most. 
\end{itemize}
\vspace{1ex}

\noindent The global optimum is unique up to a permutation of the generated observations. I found it with no GAN, no neural network. Instead, with a
 probabilistic algorithm. These algorithms tend to progress very fast initially, but quite slowly after a while. One would think that
 if using neural networks, replacing the fixed Wasserstein loss function with adaptive Hellinger approximations (that is, model evaluations that get more accurate over time), 
 you would need much less than 2 millions steps. However, it remains to be seen if a gradient descent -- the core of any neural network -- is suitable in this case. 

My
 solution does not use gradient descent: instead, at each step, a random move is made (thus, not based on a continuous path governed by derivatives of a function), but 
 nevertheless always in the right direction: the move if accepted only if further minimizing the loss.  In addition, there are four more important components to my method:
\vspace{1ex}
\begin{itemize}
\item The algorithm starts with an initial  synthetic dataset where all marginal distributions match those in the training set. So, the univariate Hellinger distances
 are perfect to begin with. The goal is then to optimize the multivariate Hellinger distance, which takes into account the full dependency structure
 in the feature space. 
\item The algorithm is a version of the hierarchical Bayesian \textcolor{index}{NoGAN}\index{NoGAN} described in chapter 7 in~\cite{vgmloptim}. This is
 a resampling method that preservers the marginal distributions throughout all iterations, while attempting to approach the joint distribution in the training set, using a continuous
 loss function as in neural networks. However, here the loss function is replaced by the evaluation metric (Hellinger). 
\item The granularity of the EDPF lattice support increases over time, resulting in an exponential explosion in the number of bins, especially in high dimensions. 
However the sparsity increases in the same proportion: the vast majority of bins are empty and not even stored or visited. The number of active bins (stored in hash tables) is not larger
 than the number of observations, at any given time. 
\item Pre-computation of square roots, stored in tables.  Each of the 2 million atomic updates requires the computation of 16 square roots. The tables in question
significantly contribute to speeding up the algorithm. 
\end{itemize}
\vspace{1ex}

\noindent Most tabular data synthesizers have challenges to generate observations with the prescribed distribution. By replacing the loss function with the
 evaluation metric, I face the opposite problem: generating observations too similar to those in the training set. So instead of trying to achieve a low value
 for the Hellinger distance as in GAN, I must do the opposite:  preventing the Hellinger distance from being too low. This is
 accomplished by setting up constraints. I call my approach \textcolor{index}{constrained synthetization}\index{synthetic data!constrained}. 
See Figure~\ref{fig:xc1oh6} where the Hellinger distance H between the synthetic data and training set (real data) is kept above 0.60 at all times.

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{nogan3b.png}   
\caption{Synth. \hspace{-0.5ex}(H $>$ 0.6, red) vs real (blue) with \href{https://github.com/VincentGranville/Main/blob/main/NoGAN_Hellinger.py}{\texttt{NoGAN\_Hellinger.py}}}
\label{fig:xc1oh6}
\end{figure}
%imgpy9979_2and3.PNG screen2e.png  
%-------------------------

\subsection{Project and solution}

The name for the new algorithm is  \textcolor{index}{constrained NoGAN}\index{NoGAN!constrained}. It blends the best of 
\textcolor{index}{probabilistic NoGan}\index{NoGAN!probabilistic} (based on deep \textcolor{index}{resampling}\index{resampling}, see 
chapter 7 in~\cite{vgmloptim} and Python code \texttt{DeepResampling\_circle.py}
\href{https://github.com/VincentGranville/Main/blob/main/DeepResampling_circle.py}{here}) with the
 efficient multivariate \textcolor{index}{bin structure}\index{binning} found in 
\textcolor{index}{standard NoGAN}\index{NoGAN} 
(see project~\ref{genaiyert} and corresponding code \texttt{NoGan.py} \href{https://github.com/VincentGranville/Main/blob/main/NoGAN.py}{here}). 
The bin structure is used to compute the Hellinger distance.
For the simplest version of constrained NoGAN, see \texttt{NoGAN\_Hellinger.py}, \href{https://github.com/VincentGranville/Main/blob/main/NoGAN_Hellinger.py}{here}. 
This script is great to synthetize data featuring hidden structures (in this case, concentric circles), to check whether or not pattern detection algorithms 
can detect them. The higher the Hellinger distance threshold used in the synthetization, the less visible the circles, making
 detection harder.

In the remaining of this project, 
I focus on \texttt{NoGAN\_hellinger2.py}, with a slightly different architecture leading to the spectacular
 results discussed earlier. The code is in section~\ref{ferment} and also on GitHub,
 \href{https://github.com/VincentGranville/Main/blob/main/NoGAN_Hellinger2.py}{here}. The resampling part works as follows: 
\vspace{1ex}

\begin{itemize}
\item Randomly select two observations $x = (x_1, x_2, x_3)$ and $y = (y_1, y_2, y_3)$ from the synthetic dataset under construction. 
\item Propose the update $x' = (x_1, y_2, x_3)$ and $y' = (y_1, x_2, y_3)$ and compute the change in Hellinger distance, would the proposed update be accepted.
Here the Hellinger distance is measured on the first two features both in the training and synthetic data, while ignoring the third feature.
\item Accept the change if it decreases the Hellinger distance, and repeat these steps millions of times, until the Hellinger distance barely decreases anymore.
\end{itemize}
\vspace{1ex}

\noindent The above describes the first pass: that is, updating the second feature conditionally to the first one. In a second pass, you update the third feature conditionally to the first two ones. This time the Hellinger distance is computed on all 3 features. You have more passes if you have more than 3 features. In practice, each feature is actually a block of features. For simplicity, in this project I stop after the first pass.  

You need to start with an initial synthetic dataset where each feature taken separately has about the same empirical distribution as its sister in the training set. 
This is accomplished with lines \textcolor{gray}{93--97} in the code, using the option \texttt{mode=\textquotesingle Quantiles\textquotesingle}. No matter the number of iterations, the final synthetic data set will be close but different to the training set: it is constrained on the initial synthetization. However, if you use the option
 \texttt{mode=\textquotesingle Shuffle\textquotesingle}, the program uses a scrambled version of the training set for the initial synthetization: each feature column is shuffled separately. 
In this case, the only synthetization minimizing the Hellinger distance (up to a permutation of the rows) is the training set itself! You would think that it will
 take billions of years of computing time to reach it {\em exactly} given the probabilistic nature of the algorithm, albeit getting a very good approximation very quickly.  However, thanks to the small size of the training set (400 observations), it takes just 2 millions atomic updates (also called swaps) to reach the unique, known global optimum. This is faster than any other algorithm
 including neural networks. 

\begin{table}[H]
\small
%\setlength\extrarowheight{-1pt}
\[
\begin{array}{lcccccc}
\hline
\text{Mode}	&	\text{Dim}	& \text{Iterations}	& \text{KS($S,T$)}	&	\text{KS($S_0, T$)}	&  \text{KS($S_0, S$)} & \text{Swaps}	\\
\hline
\text{Shuffle} &  2  & \num{2000000} & 0.0000 & 0.0525 & 0.0500 & \num{12429}\\
\text{Quantiles} &  2  & \num{2000000} & 0.0350 & 0.0725 & 0.0500 & \num{17764}\\
\text{Shuffle} &  3  & \num{2000000} & 0.0200 & 0.2275 & 0.2150 & \num{12177}\\
\text{Quantiles} &  3  & \num{2000000} & 0.0475 & 0.2075 & 0.2025 & \num{12801}\\
\hline
\end{array}
\]
\caption{\label{gh5gf}KS stats: $S =$  synth., $S_0 =$ initial synth., and $T =$ training set}
\end{table}



Table~\ref{gh5gf} shows the quality of the synthetization $S$, depending on the dimension (the number of features), the number of iterations,
 and whether the initial synthetization $S_0$ is independent from the training set $T$ (mode set to `Quantiles') or a scrambled version of the training set 
(mode set to `Shuffle'). 
The parameter \texttt{dim} is specified in line \textcolor{gray}{18} in the code. The values for \texttt{granularity} and \texttt{reset\_granularity}
 depend on \texttt{dim} and are set in the code respectively in line \textcolor{gray}{151}  and \textcolor{gray}{149}. 
The multivariate \textcolor{index}{Kolmogorov-Smirnov distance}\index{Kolmogorov-Smirnov distance} (KS) takes values
 between 0 and 1, with zero for perfect match. It is computed in lines \textcolor{gray}{283--305} in the code, using the 
\textcolor{index}{GenAI-evaluation}\index{Python library!GenAI-evaluation} Python library. The quality of the synthetization is assessed
by comparing the base value KS($S_0,T$) with KS($S, T$). The lower the latter, the better the result.

The training set used in the illustrations so far is the circle dataset available \href{https://github.com/VincentGranville/Main/blob/main/circle8d.csv}{here}. It has 9 features: the last one is categorical (binary) and the other ones
 are concentrated in a tiny portion of the feature space. For instance, in the first two dimensions, the points are concentrated on two concentric circles. 
Combined with the fact that it contains only 400 observations, it is a challenging set for synthetization purposes. All the cross-correlations are either 0, 1 or -1. 
It tricks many synthesizers based on simple loss functions to generate points in a full 2-dimensional square rather than on the one-dimensional circles.   
I compared the solutions offered by several vendors in an notorious article, available \href{https://mltblog.com/3qN7TPw}{here}.  

% beware KS approx. not exact
% put mixture.csv , mixture.py on GitHub
% granul depend on dim
% create new datasets/ explore existing ones
% why is loss value meaningless?
% can get stuck in local minima
% explain why using hellinger2 rather than hellinger or nogan

\noindent The project consists of the following steps:

\vspace{1ex}

\begin{itemize}
\item[] {\bf Step 1: Hyperparameters and notations}.  There are four top parameters: \texttt{granularity}, \texttt{shift}, \texttt{reset\_granularity},
 and \texttt{reset\_shift}, denoted respectively as 
 $\gamma, \tau, \Delta_\gamma$ and $\Delta_\tau$. The grid determining the granularity of the multivariate bins is initialized as follows:
 each feature is binned into $\gamma$ \textcolor{index}{quantile}\index{quantile} intervals of equal length. Every $\Delta_\gamma$ iterations, $\gamma$ is increased by one.  Then, in the main loop starting at line \textcolor{gray}{157}, the Hellinger distance (its square) between the synthetic data under construction and the training set, is discretized using the grid in question. Finally, the vector 
 $\tau$ determines the location of the grid, that is, how much it is shifted from the origin, for each feature separately. It is 
 initially set to zero (the origin)~and then changed to a random location every $\Delta_\tau$ iterations. 

Depending on the number \texttt{dim} of features (the dimension), if you increase or decrease $\gamma, \Delta_\gamma$ or $\Delta_\tau$, what is the expected impact on the final results? Can you retrieve the values used
to produce Table~\ref{gh5gf}, by running the code? Also, does it make sense to plot the values of the Hellinger loss function over time? Why not? Would this plot still convey some 
useful information?

\vspace{1ex}
\item[] {\bf Step 2: Higher dimensions, bigger data}.  Create a training set consisting of 20 features and $\num{10000}$ observations, for instance a 
\textcolor{index}{Gaussian mixture}\index{Gaussian mixture model}. Feel free to use \textcolor{index}{OpenAI}\index{OpenAI} to generate the code (I 
did). Or use my code \texttt{mixture.py} or its output data \texttt{mixture.csv}, available on GitHub
 respectively \href{https://github.com/VincentGranville/Main/blob/main/mixture.py}{here}
and \href{https://github.com/VincentGranville/Main/blob/main/mixture.csv}{here}. When running the synthesizer 
\texttt{NoGAN\_Hellinger2.py} (code in section~\ref{ferment}) on 
 the mixture dataset,  what challenges are you facing when the dimension is larger than 10, and how to address them?  

Note that when I asked OpenAI to produce \texttt{mixture.py}, the   
\textcolor{index}{covariance matrices}\index{covariance matrix} in the mixture are not symmetric and 
\textcolor{index}{positive semidefinite}\index{positive semidefinite (matrix)}\index{matrix!positive semidefinite}. I had to use a second prompt to fix it. 

\vspace{1ex}
\item[] {\bf Step 3: Fine-tuning and evaluation}.  It is easy to assess the impact of the hyperparameters, on the generated (synthetic) data. Design a scheme to efficiently test different combinations of values for $\gamma, \tau, \Delta_\gamma$ and $\Delta_\tau$.  Then, find combinations that do well, based on the number of features (the dimension). 
What about the number of observations to synthesize? For example, when synthesizing a large number, it may be better to split the output 
data into multiple batches generated separately. Assess the impact of the batch size on the results, especially the speed of convergence. How do you evaluate the results?
\vspace{1ex}
\item[] {\bf Step 4: Comparing NoGAN methods}.  That is, 
\href{https://github.com/VincentGranville/Main/blob/main/NoGAN_Hellinger2.py}{\texttt{NoGAN\_hellinger2.py}} (the code in section~\ref{ferment}) with
\href{https://github.com/VincentGranville/Main/blob/main/NoGAN_Hellinger.py}{\texttt{NoGAN\_hellinger.py}}  
(also using Hellinger as the loss function), 
\href{https://github.com/VincentGranville/Main/blob/main/DeepResampling_circle.py}{\texttt{DeepResampling\_circle.py}} 
based on a traditional loss function (discussed in chapter 7 in~\cite{vgmloptim}), 
and the original 
\href{https://github.com/VincentGranville/Main/blob/main/NoGAN.py}{\texttt{NoGAN.py}} 
described in section~\ref{genaiyert}. The latter has no loss function. Finally, also compare with 
GAN-based 
\href{https://github.com/VincentGranville/Main/blob/main/GAN\_circle8d.py}{\texttt{GAN\_circle8d.py}}
 described in section~\ref{porgan2}. 
Focus on quality, speed, and overfitting in particular.   
\end{itemize}
\vspace{1ex}

\noindent Now my answers. Regarding \textcolor{red}{Step 1}, I used $\gamma=20, \Delta_\gamma=\num{10000}$ in two dimensions,
 and then $\gamma=2, \Delta_\gamma=\num{50000}$ in three dimensions, together with \texttt{mode=\textquotesingle Shuffle\textquotesingle} in line \textcolor{gray}{89} 
to produce the results in Table~\ref{gh5gf}. I did not use the shift parameters $\tau,\Delta_\tau$ on the circle dataset.
Starting with a high value for $\gamma$ speeds up convergence in low dimensions (2 or 3) but can get you stuck in higher dimensions.
Use a lower value of $\gamma_\tau$ if you get stuck. Use $\Delta_\tau \leq 0.5 \times \gamma_\tau$ to visit
 the feature space more thoroughly: it can speed up convergence and get you unstuck in higher dimensions. A new $\tau$ is similar to trying another
 location or starting point in \textcolor{index}{gradient descent}\index{gradient descent} to avoid local minima. In higher dimensions,
 use $\gamma=2, \Delta_\gamma > \num{100000}$ and $\Delta_\tau<\num{50000}$. 
In general, the more swaps, the better. Since the loss function is different each time the value of $\gamma$ or $\tau$ changes, 
its graph is meaningless. Still, it can help you identify when the moves (called {\em swaps}) are becoming rare or absent, to fix it: 
see Figure~\ref{fig:xyerwsalk}, where the slope dramatically changes each time $\gamma$ is modified, that is, each time the loss stops decreasing fast enough.  

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{mixture2.png}   
\caption{First 2 dimensions of mixture data (blue) with synthetization in orange}
\label{fig:xcv2q6}
\end{figure}
%imgpy9979_2and3.PNG screen2e.png  
%-------------------------

As for \textcolor{red}{Step 2}, in dimensions above 10, it is customary to work with groups of features, one group at a time. Do the first 10, then 
the next 10 conditionally on the first 10, then the next 10 conditionally on first 20, and so on. The computing time increases linearly with
 the dimension, keeping the same number of iterations, say 2 million, regardless of dimension. This is similar to 
neural networks, where computing time also increases linearly with the dimension, not exponentially. See details in chapter 7 in~\cite{vgmloptim}. 
Finally, I used \texttt{mode=\textquotesingle Quantiles\textquotesingle} on the mixture dataset that I created, available \href{https://github.com/VincentGranville/Main/blob/main/mixture.csv}{here}, working with the first 8 features only. In Figure~\ref{fig:xcv2q6}, the training set has $\num{10000}$ observations. I generated 1000 synthetic ones, thus the reason why orange dots are much rarer than blue ones.

\begin{table}[H]
\small
%\setlength\extrarowheight{-1pt}
\[
\begin{array}{cccccc}
\hline
\gamma & \Delta_\gamma & \Delta_\tau &\text{KS($S,T$)}	&	\text{KS($S_0, T$)} & \text{Swaps}	\\
\hline
 2 & \num{200000} & \infty &  0.0537 & 0.1949 & 8758 \\
 4 & \infty & \num{50000} &  0.0425 & 0.1352 & \num{32786} \\
 2 & \num{200000} & \num{50000}  & 0.0536   & 0.1352 & \num{23127} \\

\hline
\end{array}
\]
\caption{\label{w09igh5gf}Hyperparam impact on KS, mixture dataset}
\end{table}



Now \textcolor{red}{Step 3}.  Why evaluate the results when the loss function is actually the evaluation metric? Yet,
it still makes sense to have another evaluation with a different metric, if anything to confirm that all computations are done correctly. 
In the code, I use the KS distance for final evaluation, see lines \textcolor{gray}{283--305}. 
Table~\ref{w09igh5gf} is based on first 8 columns of the mixture dataset,
 using \texttt{mode=\textquotesingle Quantiles\textquotesingle}. Here $S, S_0,T$ stand respectively for the final synthetization, initial synthetization, and the
 training set.  
The table features some of the best hyperparameter combinations that I tried.
For a systematic approach to find these parameters, you can use \textcolor{index}{smart grid search}\index{grid search!smart grid search}, 
described  in~\cite{vgsmart}. As in table~\ref{gh5gf}, I used 2 million iterations.

%xxxyyy S T T_0 in table xxx quantile mode, dim 8 / smart grid search

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.66\textwidth]{loss.png}   
\caption{Adaptive loss function: steps showing when $\gamma$ changed}
\label{fig:xyerwsalk}
\end{figure}
%imgpy9979_2and3.PNG screen2e.png  
%-------------------------

Regarding \textcolor{red}{Step 4}, \texttt{NoGAN.py} is the best, followed by \texttt{NoGAN\_Hellinger.py} (a probabilistic version of the former). 
This is because they generate observations directly into
 the bin structure attached to the training set. The drawback is potential overfitting, which can be mitigated with 
\textcolor{index}{constrained synthetization}\index{synthetic data!constrained} as discussed earlier. Then \texttt{NoGAN\_Hellinger2.py} usually beats \texttt{DeepResampling\_circle.py}
 because the former uses the evaluation metric as the loss function, while the latter uses a proxy. Finally,
 \texttt{GAN\_circle8d.py} is the last in my ranking: hard to train and fine-tune, 
very sensitive to the seed and to the dataset (volatile results), non-replicable, and non-explainable AI. 

Perhaps one of the greatest benefits of  \texttt{NoGAN\_Hellinger2.py} is its ability to test very fast new features that could be added to
 deep neural networks, such as using the evaluation metric as the loss function. Also, by choosing
\texttt{mode=\textquotesingle Shuffle\textquotesingle}, the ability to check whether the system can retrieve the known global optimum, and if not, how close it gets.
Finally, an adaptive loss function may prevent you  from getting stuck in a local optimum, by changing $\gamma$ or $\Delta_\tau$
when the loss stops decreasing: this is visible in Figure~\ref{fig:xyerwsalk}. 

% swaps with little loss or early vs late loss
%IMG for Q2, table for Q3
%Q2 mode='quantiles'


% more dimension, bigger data [mixtures]
%video //
%alternate datasets on github to answer Q on big data
% vi
% include table with KS1, KS2, swaps in hyperparamer tuning Q + smart grid search
% explain the init synth data with ref to line codes
% geneval KS part of the exercise

%xxx
%project: play with hyperpam in dim 3 or 4 including shift; do dim 9 [last feature is categorical] with bayesian/conditional
%circle data
%genai library

\subsection{Python code}\label{ferment}
 

The code below (\texttt{NoGAN\_Hellinger2.py}) is also on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/NoGAN_Hellinger2.py}{here}.
\vspace{1ex}

\begin{lstlisting}[numbers=left]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib import pyplot


#--- [1] Read data and select dim 

# you can read data from URL below
# https://raw.githubusercontent.com/VincentGranville/Main/main/circle8d.csv
data = pd.read_csv('circle8d.csv') 
features = list(data.columns.values)
X = data.to_numpy()
features = np.array(features)

# use the first dim columns only
dim = 2
X = X[:, 0:dim] 
features = features[0:dim]
nobs_real, dim = X.shape


#--- [2] Functions to build bin structure

def create_quantile_table(x, Hyperparam, shift):

    arr_q = [] 
    for d in range(dim): 
        n = Hyperparam[d]
        arr_qd = np.zeros(n+1)
        for k in range(n):
            q = shift[d] + k/n
            arr_qd[k] = np.quantile(x[:,d], q % 1)
        arr_qd[n] = max(x[:,d])
        arr_qd.sort()
        arr_q.append(arr_qd)
    return(arr_q)


def find_quantile_index(x, arr_quantiles):
    k = 0
    while k < len(arr_quantiles) and x > arr_quantiles[k]: 
        k += 1
    return(max(0, k-1)) 


def create_bin_structure(x, arr_q):

    hash_bins = {}
    hash_bins_median = {}
    hash_index = []

    for n in range(x.shape[0]):

        key = ()
        for d in range(dim):
            kd = find_quantile_index(x[n,d], arr_q[d])
            key = (*key, kd)        
        hash_index.append(key)

        if key in hash_bins: 
            hash_bins[key] += 1
            points = hash_bins_median[key]
            points.append(x[n,:])
            hash_bins_median[key] = points 
        else:
            hash_bins[key] = 1
            hash_bins_median[key] = [x[n,:]]  

    for key in hash_bins: 
        points = hash_bins_median[key]
        # beware: even number of points -> median is not one of the points  
        median = np.median(points, axis = 0) 
        hash_bins_median[key] = median

    return(hash_bins, hash_index, hash_bins_median)


#--- [3] Generate initial nobs_synth obs

# if nobs_synth > 1000, split in smaller batches, do one batch at a time
nobs_synth = nobs_real 
seed = 155
np.random.seed(seed)

# get initial synth. data with same marginal distributions as real data 

mode = 'Shuffle'   # options: 'Shuffle', 'Quantiles'
synth_X = np.empty(shape=(nobs_synth,dim))

if mode == 'Quantiles':
    for k in range(nobs_synth):
        pc = np.random.uniform(0, 1.00000001, dim)
        for d in range(dim):
            synth_X[k, d] = np.quantile(X[:,d], pc[d], axis=0)

elif mode == 'Shuffle':
    nobs_synth == nobs_real  # both must be equal
    synth_X = np.copy(X)
    for d in range(dim):
        col = synth_X[:, d]
        np.random.shuffle(col)
        synth_X[:, d] = col

synth_X_init = np.copy(synth_X)


#--- [4] Main part: create synth obs to minimize Hellinger loss function

def in_bin(x, key, arr_q):
    # test if vector x is in bin attached to key
    status = True
    for d in range(dim):
        arr_qd = arr_q[d]
        kd = key[d]
        if x[d] < arr_qd[kd] or x[d] >= arr_qd[kd+1]:
            status = False  # x is not in the bin
    return(status) 

def array_to_tuple(arr):
    list = ()
    for k in range(len(arr)):
        list = (*list, arr[k])
    return(list)

Hellinger = 40.0   # arbitrary value
swaps = 0  
history_log_H = []
history_log_swaps = []
flist = []  # list of image filenames for the video
frame = 0   # frame number, for video

# to accelerate computations (pre-computed sqrt)
sqrt_real = np.sqrt(nobs_real)
sqrt_synth = np.sqrt(nobs_synth)
n_sqrt = max(nobs_real, nobs_synth) 
arr_sqrt = np.sqrt(np.arange(n_sqrt))

# visualization: graphic parameters
mpl.rcParams['lines.linewidth'] = 0.3
mpl.rcParams['axes.linewidth'] = 0.5
plt.rcParams['xtick.labelsize'] = 7
plt.rcParams['ytick.labelsize'] = 7

video_mode = False

# Hyperparameters
reset_granularity = 10000  # set to 50000 if dim = 3, set to 10000 if dim = 2    
reset_shift = 999999999999 
granularity = 20   #  set to 2 if dim > 2, set to 20 if dim = 2 
Hyperparam = np.full(dim, granularity) 
shift = np.zeros(dim)
n_iter = 2000000  


for iter in range(n_iter):

    if iter % reset_granularity == 0 or iter % reset_shift == 0 or iter == 0:   

        # Get more granular Hellinger approximation

        if iter % reset_granularity == 0: 
            Hyperparam = 1 + Hyperparam    
        if iter % reset_shift == 0: 
            shift = np.random.uniform(0, 1, dim) 
        arr_q = create_quantile_table(X, Hyperparam, shift) 
        ( hash_bins_real, 
          hash_index_real, 
          hash_bins_median_real 
        ) = create_bin_structure(X, arr_q)
        ( hash_bins_synth, 
          hash_index_synth,
          hash_bins_median_synth,  # unused
        ) = create_bin_structure(synth_X, arr_q)

    k = np.random.randint(0, nobs_synth) 
    key_k = hash_index_synth[k]
    scount1 = hash_bins_synth[key_k]
    if key_k in hash_bins_real:
        rcount1 = hash_bins_real[key_k]
    else:
        rcount1 = 0

    l = np.random.randint(0, nobs_synth)  
    key_l = hash_index_synth[l]
    scount2 = hash_bins_synth[key_l]
    if key_l in hash_bins_real:
        rcount2 = hash_bins_real[key_l]
    else:
        rcount2 = 0

    d = np.random.randint(1,dim)  # column 0 can stay fixed

    new_key_k = np.copy(key_k)
    new_key_l = np.copy(key_l)
    new_key_k[d] = key_l[d]
    new_key_l[d] = key_k[d]
    new_key_k = array_to_tuple(new_key_k)
    new_key_l = array_to_tuple(new_key_l)

    if new_key_k in hash_bins_synth:
        scount3 = hash_bins_synth[new_key_k]
    else:
        scount3 = 0
    if new_key_k in hash_bins_real:
        rcount3 = hash_bins_real[new_key_k]
    else:
        rcount3 = 0

    if new_key_l in hash_bins_synth:
        scount4 = hash_bins_synth[new_key_l]
    else:
        scount4 = 0
    if new_key_l in hash_bins_real:
        rcount4 = hash_bins_real[new_key_l]
    else:
        rcount4 = 0

    A = arr_sqrt[scount1]  /sqrt_synth - arr_sqrt[rcount1]/sqrt_real
    B = arr_sqrt[scount1-1]/sqrt_synth - arr_sqrt[rcount1]/sqrt_real
    C = arr_sqrt[scount2]  /sqrt_synth - arr_sqrt[rcount2]/sqrt_real
    D = arr_sqrt[scount2-1]/sqrt_synth - arr_sqrt[rcount2]/sqrt_real
    E = arr_sqrt[scount3]  /sqrt_synth - arr_sqrt[rcount3]/sqrt_real
    F = arr_sqrt[scount3+1]/sqrt_synth - arr_sqrt[rcount3]/sqrt_real
    G = arr_sqrt[scount4]  /sqrt_synth - arr_sqrt[rcount4]/sqrt_real
    H = arr_sqrt[scount4+1]/sqrt_synth - arr_sqrt[rcount4]/sqrt_real
    delta_H = - A*A + B*B - C*C + D*D - E*E + F*F - G*G + H*H

    if delta_H < -0.00001:

        Hellinger += delta_H
        swaps += 1

        # update hash_index_synth and hash_bins_synth

        hash_index_synth[k] = new_key_k
        if new_key_k in hash_bins_synth:
            hash_bins_synth[new_key_k] +=1
        else:
            hash_bins_synth[new_key_k] = 1
        if hash_bins_synth[key_k] == 1:
            del hash_bins_synth[key_k]
        else: 
            hash_bins_synth[key_k] -= 1
   
        hash_index_synth[l] = new_key_l
        if new_key_l in hash_bins_synth:
            hash_bins_synth[new_key_l] += 1
        else:
            hash_bins_synth[new_key_l] =1
        if key_l in hash_bins_synth:
            hash_bins_synth[key_l] -= 1
        else:
            hash_bins_synth[key_l] = 1

        # update synthetic data

        aux = synth_X[k, d]
        synth_X[k, d] = synth_X[l, d]
        synth_X[l, d] = aux

        if video_mode and swaps % 25 == 0:

            # save image for future inclusion in video
            fname='nogan3_frame'+str(frame)+'.png'
            flist.append(fname)
            plt.scatter(synth_X[:,0], synth_X[:,1], s = 1.0) 
            plt.savefig(fname, dpi = 200)
            plt.close() 
            frame += 1

    if iter % 1000 == 0:

        print("Iter: %7d | Loss: %9.6f | Swaps: %5d" 
              %(iter, Hellinger, swaps)) 
        history_log_H.append(Hellinger)
        history_log_swaps.append(swaps)


#--- [5] Evaluation with KS distance

import genai_evaluation as ge

n_nodes = 1000

df_init = pd.DataFrame(synth_X_init, columns = features)
df_synth = pd.DataFrame(synth_X, columns = features)
df_train = pd.DataFrame(X, columns = features) 

query_lst, ecdf_train, ecdf_init = ge.multivariate_ecdf(df_train, 
                df_init, n_nodes, verbose = True) 
ks_base = ge.ks_statistic(ecdf_train, ecdf_init)

query_lst, ecdf_train, ecdf_synth = ge.multivariate_ecdf(df_train, 
                df_synth, n_nodes, verbose = True) 
ks = ge.ks_statistic(ecdf_train, ecdf_synth)

query_lst, ecdf_init, ecdf_synth = ge.multivariate_ecdf(df_init, 
                df_synth, n_nodes, verbose = True) 
ks_diff = ge.ks_statistic(ecdf_init, ecdf_synth)

print("Test ECDF Kolmogorof-Smirnov dist. (synth. vs train.): %6.4f" %(ks))
print("Base ECDF Kolmogorof-Smirnov dist. (init.  vs train.): %6.4f" %(ks_base))
print("Diff ECDF Kolmogorof-Smirnov dist. (init.  vs synth.): %6.4f" %(ks_diff))


#--- [6] Plot some results and create video

mpl.rc('hatch', color='k', linewidth=0.3)
plt.scatter(X[:,0],X[:,1],marker='o',c='deepskyblue',alpha=0.1,s=10) 
plt.scatter(synth_X[:,0],synth_X[:,1],marker='o',c='coral',alpha=0.4,s=10, edgecolors='black',lw=0.2)
plt.grid(linewidth = 0.4, alpha = 1)
plt.show()

x_axis = range(len(history_log_H))
plt.plot(x_axis, history_log_H)
plt.show()
plt.plot(x_axis, history_log_swaps)
plt.show()

if video_mode:
    import moviepy.video.io.ImageSequenceClip
    clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(flist, fps=6)
    clip.write_videofile('nogan3.mp4')
\end{lstlisting}





\chapter{Time Series and Spatial Processes}\label{tschj}

The first project covers non-periodic times series -- more specifically time series with multiple periods -- including modeling, simulation and 
 goodness of fit via the autocorrelation structure. The case study is about ocean tides and distances between planets to detect alignments. 
I then move to random walks and Brownian motions, including integrated Brownian motions, the Hurst exponent to measure smoothness, 
 and ad-hoc smoothing techniques. The last project involves 2D interpolation compared to kriging, applied to the Chicago temperature dataset. 

%xxx
%collision graphs , NN graphs
%integrated brownian motions
%xxxx data normalization
%temperature interpolation + nice chart + kriging
%xxx integrated brownian 2 ways / hurst exponent / 2D brownian/ integral equation
%  smoothing my own method [product of 2 primes]


\section{Time series interpolation: ocean tides}\label{prgt43zas}

The purpose is to reconstruct  daily measurements in a non-periodic time series with monthly observations, and measure the quality of the
results. 
In this project, the daily values can be computed exactly. Let's pretend that we don't know them when we generate them.  In the end we will use the hidden exact values to see how good we were at reconstructing (interpolating) them.  

The problem is similar to the illustration
 in Figure~\ref{fig:tides}: you are dealing with a time series where the observations are the orange dots, in this case equally spaced by 80-min increments. You don't know what the smooth curves look like; you can't tell from the data. Here, I produced 16 modified copies of the observed data (the 80-min measurements). I used \textcolor{index}{interpolation}\index{interpolation}\index{time series!interpolation} -- one of several techniques to synthetize data -- to produce them. Combined together, these 16 sets provide 5-min granularity resulting in the blue curve in Figure~\ref{fig:tides}. In fact here, the 5-min data was actually available and represented by the red curve. I pretended it did no exist, but in the end I used it to assess the quality of the results.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{tides3.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Tides at Dublin (5-min data), with 80 mins between interpolating nodes}
\label{fig:tides}
\end{figure}

The project in this section deals with similar time series. You need to get monthly distances between the planets and the Sun, to see how frequently Earth, Venus (or Jupiter) are aligned on the same side of the Sun. For instance, in case of almost perfect alignment, the apparent locations of Jupiter and Mars are identical to the naked eye in the night sky. Is there a chance you might see that event in your lifetime? You'll get an answer to this curious question, but most importantly, the goal is to get you familiar with one aspect of data reconstruction, sometimes
 called \textcolor{index}{disaggregation}\index{time series!disaggregation}. Rather than 80-min observations, we will use monthly or quarterly observations. And we will reconstruct the more granular data via interpolation. Then, we assessing the quality of the interpolated data, and how more general modeling techniques could be used instead.

%\subsection{The problem}

We first create a dataset with daily measurements of the distance between Earth and Venus, and interpolate the distances to test how little data is needed for good enough performance: 
Can you reconstruct daily data from monthly observations of the distances between planets? What about quarterly or yearly observations? Then, the purpose is to assess how a specific class of models is good at interpolating not only this type of data, but at the same time other types of datasets like the ocean tides in Figure~\ref{fig:tides} or the Riemann zeta function in
 Figure~\ref{fig:zeta}.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{zeta.png} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Interpolating the real part of $\zeta(\frac{1}{2}+it)$ based on orange points}
\label{fig:zeta}
\end{figure}

The planetary fact sheet published by the NASA contains all the information needed to get started. 
It is available \href{https://nssdc.gsfc.nasa.gov/planetary/factsheet/}{here}. I picked up Venus and Earth because they are among the planets with the lowest eccentricities in the solar system. For simplicity, assume that the two orbits are circular. Also assume that at a time denoted as $t=0$, the Sun, Venus and Earth were aligned and on the same side (with Venus between Earth and the Sun). 

Note that all the major planets revolve around the sun in the same direction. 
Let $\theta_V, \theta_E, R_V, R_E$ be respectively the orbital periods of Venus and Earth, and the  distances from Sun for Venus and Earth.  From the NASA table, these quantities are respectively 224.7 days, 365.2 days, $108.2\times 10^6$ km, and 
$149.6  \times 10^6$ km. Let $d_V(t)$ be the distance at time $t$, between Earth and Venus. You first need to convert the orbital periods into angular velocities 
 $\omega_V = 2\pi/\theta_V$ and $\omega_E = 2\pi/\theta_E$ per day.  Then elementary trigonometry leads to the formula
\begin{equation}
d_V^2(t) = R_E^2\Bigg[1 + \Big(\frac{R_V}{R_E}\Big)^2 -2\frac{R_V}{R_E} \cos\Big((\omega_V-\omega_E)t\Big) \Bigg]. \label{resw}
\end{equation}
The distance is thus periodic, and minimum and equal to $R_E - R_V$ when  
$(\omega_V-\omega_E)t$ is a multiple of $2\pi$. This happens roughly every 584 days. 


\subsection{Project description} 

Before starting the project, read section 2.1 about ocean tides in my article 
``New Interpolation Methods  for Synthetization and Prediction" available \href{https://mltblog.com/3Xhm9v9}{here}, and
 the Python program \texttt{interpol\_fourier.py}. The password to open the PDF document is \texttt{MLT12289058}. You can use a different interpolation technique, 
but one of the goals here is to learn how to use code written by a third party, and modify it for your own needs if necessary.  

\noindent The project consists of the following steps: \vspace{1ex}
\begin{itemize}
\item[] {\bf Step 1}:  Use formula~(\ref{resw}) to generate daily values of $d_V(t)$, for 10 consecutive years, starting at $t=0$. \vspace{1ex}
\item[] {\bf Step 2}:  Use the Python code \texttt{interpol\_fourier.py} on my GitHub repository \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/interpol_fourier.py}{here}. Interpolate daily data using one out of every 32 observations: using a power of 2 such as $32=2^5$ will let the code do everything nicely including producing the chart with the red dots, in the same way that I use one observation out of 16 for the ocean tides (80 minutes = $16 \times 5$ minutes). Conclude whether or not using one measurement every 32 days is good enough to reconstruct the daily observations. See how many nodes (the variable \texttt{n} in the code) you need to get a decent interpolation. \vspace{1ex} 
\item[] {\bf Step 3}:  Add planet Mars. The three planets (Venus, Earth, Mars) are aligned with the sun and on the same side when both $(\omega_V-\omega_E)t$ and $(\omega_M-\omega_E)t$ are almost exact multiples of $2\pi$, that is, when both the distance $d_M(t)$ between Earth and Mars, and $d_V(t)$ between Earth and Venus, are minimum. In short, it happens when 
$g(t) = d_V(t) + d_M(t)$ is minimum.  Assume it happened at $t=0$. Plot the function $g(t)$, for a period of time long enough to see a global minimum (thus, corresponding to an alignment). Here $\omega_M$ is the orbital velocity of Mars, and its orbit is approximated by a circle. \vspace{1ex}
\item[] {\bf Step 4}: Repeat steps 1 and 2 but this time for $g(t)$. Unlike $d_V(t)$, the function $g(t)$ is not periodic. Alternatively, use Jupiter instead of Venus, as this leads to alignments visible to the naked eye in the night sky: the apparent locations of the two planets coincide. \vspace{1ex}

\item[] {\bf Step 5}: A possible general model for this type of time series is
\begin{equation}
f(t) = \sum_{k=1}^m A_k \sin(\omega_kt + \varphi_k) + \sum_{k=1}^m A'_k \cos(\omega'_kt + \varphi'_k) \label{tyre}
\end{equation}
where the $A_k, A'_k, \omega_k,\omega'_k,\varphi_k,\varphi'_k$ are the parameters, representing amplitudes, frequencies and phases. Show that this parameter configuration is redundant:  you can simplify while keeping the full modeling capability, by setting
$\varphi_k = \varphi_k'=0$ and re-parameterize. Hint: use the angle sum formula (Google it).\vspace{1ex}
\item[] {\bf Step 6}: Try $10^6$ parameter configurations of the simplified model based on formula~(\ref{tyre}) 
with $m=2$ and $\varphi_k=\varphi'_k=0$, to
 simulate time series via \textcolor{index}{Monte-Carlo simulations}\index{Monte-Carlo simulations}. For each simulated time series, measure how close it is to the ocean tide data (obtained by setting \texttt{mode=\textquotesingle Data\textquotesingle} in the Python code), the functions $g(t)$ or $d_V(t)$ in this exercise, or the Riemann zeta function pictured in Figure~\ref{fig:zeta} (obtained by setting
\texttt{mode=\textquotesingle Math.Zeta\textquotesingle} in the Python code). Use a basic proximity metric of your choice to asses the quality of the fit, and use it 
on the transformed time series obtained after normalization (to get zero mean and unit variance). A possible comparison metric is
a combination of  lag-1, lag-2 and lag-3 \textcolor{index}{autocorrelations}\index{autocorrelation} applied to the 32-day data (planets) or 16-min data (ocean tides), comparing simulated (synthetic) versus observed data. Also, autocorrelations don't require normalizing the data as they are already scale- and location-invariant.\vspace{1ex}
\item[] {\bf Step 7}: Because of the \textcolor{index}{curse of dimensionality}\index{curse of dimensionality} [\href{https://en.wikipedia.org/wiki/Curse_of_dimensionality}{Wiki}], Monte-Carlo is a very poor technique here as we are dealing with $8$ parameters. On the other hand, you can get very good approximations with just 4 parameters, with a lower risk of overfitting. Read
section 1.3.3 in my book ``Synthetic Data and Generative AI"~\cite{vgelsevier} about a better inference procedure, applied to ocean tides. 
Also read chapter 15 on synthetic universes featuring non-standard gravitation laws to generate different types of  synthetic time series. Finally, read chapter 6 on shape generation and comparison: it features a different type of metric to measure the distance between two objects, in this case the time series (their shape: real versus synthetic version). 
\end{itemize}

\subsection{Note on time series comparison}

 Regarding the oscillations of the $g(t)$ function in Steps 3 and 4, a 10-year time period is not enough -- by a long shot -- to find when the next minimum (alignment) will occur. Looking at a 10-year time period is misleading.

One way to check if two time series (after \textcolor{index}{normalization}\index{normalization}) are similar enough is typically done by comparing their estimated parameters (the $\omega_k$, $A_k$ and so on) to see how ``close" they are. This is also how you would measure the similarity between a synthetic time series (or any kind of data for that matter), and the observed data that it is supposed to mimic. You don't compare the shapes: they might be quite different yet represent the same mechanical effect at play. To the contrary, two extracts from different time series may look very similar visually, but may come from very different models: it just happens by coincidence that they look quite similar on some short intervals. 

In some sense, what you want to measure is the {\em stochastic} proximity. Another example is comparing the numbers $\pi$ and $3/7$. If you search long enough, you will find two sequences of \num{5000} digits that are identical in both numbers. Yet these two numbers are very different in nature. Now if you compare $5/11$ and $3/7$, you will never find such identical sequences, not even 2-digit long, and you may conclude that the two numbers are very different, while in fact they are of the same kind. Same if you compare $\pi$ with a rational number very close to~$\pi$.

There is caveat with comparing datasets based on their estimated parameters: if the parameter set is redundant as in Step 5, you can have two perfectly identical datasets that have very different parameters attached to them. This is known as \textcolor{index}{model identifiability}\index{identifiability (statistics)} in statistics. 

Finally, I encourage you to upgrade my Python code \texttt{interpol\_fourier.py} to include a sub-function that computes $g(t)$ defined in Step 3. And of course, you can test this program on your own data, not just the ocean tides or planet-related data. You should try \texttt{\textquotesingle mode=Math.Zeta\textquotesingle} and see if you find anything special about the time series generated. There is something very special about it! Don't forget to install the MPmath Python library to make it work.

%--

\subsection{Solution}

I split the solution into two parts. First the computation of daily distances $d_V(t), d_M(t), g(t)$ in million of km, and how they can be recovered via interpolation. Then, the simulations discussed in steps 5--7. 
 
%\subsubsection{Synthetization: steps 1--4}

Let's start with Steps 1--4. 
The distances for $d_V(t), d_M(t), g(t)$ are respectively in blue, orange and green. They are represented by the vertical axis. The time is represented
 by the horizontal axis. In Figure~\ref{fig:venus}, the time unit is a day (daily observations). In Figure~\ref{fig:venus2}, we are dealing with yearly observations
 instead. The blue curve shows a minimum about every 584 days, confirming that Venus is closest to Earth every 584 days. As for $g(t)$, there is no periodic minimum. Yet after 2400 days, you get another strong minimum, then minima get higher and higher and you have to wait over 400 years to reach a new low 
as low as the first one at $t=0$, when perefct allignment occured by construction. 

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{venusmars1} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{$d_V(t), d_M(t), g(t)$  in $10^6$ km, first $10\times 365$ days after alignment at $t=0$}
\label{fig:venus}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{venusmars2} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{$g(t)$ over $1000$ years, yearly  measurements this time}
\label{fig:venus2}
\end{figure}

Clearly, monthly data is well suited for interpolation, to recover daily data. But yearly data is not granular enough, and you can not expect to use (say) 50-year observations to recover or synthetize yearly observations. 
%\subsubsection{Python code to compute distances}
See below my code to compute the distances. It also produces an output file \texttt{gt\_distances.txt} of daily observations for $g(t)$, 
 used as input file for the interpolation program.\vspace{1ex}

\begin{lstlisting}
import numpy as np
import matplotlib.pyplot as plt

R_V = 108.2 # million km
R_E = 149.6 # million km
R_M = 228.0 # million km
ratio_V = R_V / R_E
ratio_M = R_M / R_E

pi2 = 2*np.pi
t_unit = 1 # (in number of days)

# time unit = 32 days, to interpolate daily values (1 obs ever 32 day)
omega_V = t_unit * pi2 / 224.7 # angular velocity per 32 days
omega_E = t_unit * pi2 / 365.2 # angular velocity per 32 days
omega_M = t_unit * pi2 / 687.0 # angular velocity per 32 days

time = []
d_V = []
d_M = []
d_sum = []
t_incr = 1   # (in number of days)
T = 365 * 10 # time period (in number of days)
OUT = open("gt_distances.txt","w")
for t in np.arange(0, T, t_incr):
    time.append(t)
    dist_V = R_E * np.sqrt(1 + ratio_V**2 - 2*ratio_V * np.cos((omega_V - omega_E)*t))
    dist_M = R_E * np.sqrt(1 + ratio_M**2 - 2*ratio_M * np.cos((omega_M - omega_E)*t))
    d_V.append(dist_V) 
    d_M.append(dist_M)
    d_sum.append(dist_V + dist_M) # near absolute minimum every ~ 400 years
    OUT.write(str(dist_V + dist_M)+"\n")
OUT.close()
plt.plot(time,d_V)
plt.plot(time,d_M)
plt.plot(time,d_sum,c='green')
plt.show()
\end{lstlisting}

\subsubsection{Interpolation} 


I use  \texttt{interpol\_fourier.py} to interpolate the distances for $g(t)$, using \texttt{mode=\textquotesingle Data\textquotesingle} and \texttt{n=8} (the number of interpolation nodes) as for
 the ocean tide dataset. Here the imput file was \texttt{gt\_distances.txt} created by the Python code in the previous section. 


\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{venus3} %0.77
%%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Daily interpolated values for $g(t)$, based on exact 32-day data in orange}
\label{fig:venus3}
\end{figure}

Reducing the number
 of interpolation \textcolor{index}{nodes}\index{node (interpolation)} from $n=8$ to $n=4$ starts showing a small error visible to the naked eye. With $n=8$, you can't see the error
 as illustrated in Figure~\ref{fig:venus3}. I have no doubt that using one out every 64 days to reconstruct daily data (instead of one every 32) would still do a good job. In the process I created 32 synthetic copies of the orange data to fill the gaps: not identical copies but instead different copies with the right distribution compatible
 with the orange data. Also keep in mind that $g(t)$ does not have any period, so any shifted version of it will be different. This is
 in contrast with the function $d_V(t)$. To run \texttt{interpol\_fourier.py}, you need to change the input filename to \texttt{gt\_distances.txt}, and set \texttt{t\_unit=32}. The resulting plot in Figure~\ref{fig:venus} is truncated on the time (horizontal) axis, compared to Figure~\ref{fig:venus3}. Also the time unit on the horizontal axis is 32 days instead of one day as in Figure~\ref{fig:venus}.


\subsubsection{Simulations and comparison with real data}

The answer to Step 5 is as follows. We have
$$
A\cos(\omega t+\varphi) + A'\sin(wt+\varphi') = \alpha \sin\omega t + \beta\sin \omega't + \alpha'\cos\omega t + \beta'\cos\omega' t,
$$
with $\alpha = A\cos\varphi, \beta = -A'\sin\varphi', \alpha' = A\sin\varphi, \beta' = A'\cos\varphi'$. Thus the phase parameters 
$\varphi,\varphi'$ are not necessary. However, removing them requires increasing $m$ in Formula~(\ref{tyre}). Now, ignoring them, let's do the simulations with $m=2$. The Python code below deals with simulating $g(t)$ for the planet dataset. My conclusions follow after the code.
\vspace{1ex}

%\subsubsection{Python code for simulations and model evaluation}

\begin{lstlisting}
import numpy as np
import statsmodels.api as sm
import matplotlib as mpl
from matplotlib import pyplot as plt

# use statsmodel to compute autocorrel lag 1, 2, ..., nlags

#--- read data 

IN = open("gt_distances.txt","r") 
# IN = open("tides_Dublin.txt","r") 
table = IN.readlines()
IN.close()

exact = []
t = 0
for string in table: 
    string = string.replace('\n', '')
    fields = string.split('\t')
    value = float(fields[0]) 
    # value = np.cos(0.15*t) + np.sin(0.73*t) 
    if t % 32 == 0:  # 16 for ocean tides or Riemann zeta, 32 for planets (also try 64)
        exact.append(value)  
    t = t + 1   
nobs = len(exact)
time = np.arange(nobs)
exact = np.array(exact) # convert to numpy array

nlags = 8
acf_exact = sm.tsa.acf(exact, nlags=nlags)  


#--- generate random params, one set per simulation

np.random.seed(104)
Nsimul = 10000

lim = 20
A1 = np.random.uniform(-lim, lim, Nsimul)
A2 = np.random.uniform(-lim, lim, Nsimul)
B1 = np.random.uniform(-lim, lim, Nsimul)
B2 = np.random.uniform(-lim, lim, Nsimul)
norm = np.sqrt(A1*A1 + A2*A2 + B1*B1 + B2*B2)
A1 = A1 / norm
A2 = A2 / norm
B1 = B1 / norm
B2 = B2 / norm
w1 = np.random.uniform(0, lim, Nsimul)
w2 = np.random.uniform(0, lim, Nsimul)
v1 = np.random.uniform(0, lim, Nsimul)
v2 = np.random.uniform(0, lim, Nsimul)

#--- generate Nsimul time series each with nobs according to model
#    measure fit between each realization and the real data
#    identify synthetized series with best fit

best_fit = 9999999.9
best_series_idx = 0
for i in range(Nsimul):
    if i % 5000 == 0:
        print("generating time series #",i)
    asimul = A1[i] * np.cos(w1[i]*time) + A2[i] * np.cos(w2[i]*time) + B1[i] * np.sin(v1[i]*time) + B2[i] * np.sin(v2[i]*time)
    acf = sm.tsa.acf(asimul, nlags=nlags) 
    delta = acf - acf_exact
    metric1 = 0.5 * np.mean(np.abs(delta)) 
    corrm = np.corrcoef(exact,asimul)
    metric2 = 1 - abs(corrm[0,1])
    fit = metric1 # options:  metric1 or metric2 
    if fit < best_fit:
        best_fit = fit
        best_series_idx = i
        best_series = asimul
        acf_best = acf

print("best fit with series #",best_series_idx)
print("best fit:",best_fit)
print()

print("autocorrel lags, observed:")
print(acf_exact)
print()

print("autocorrel lags, best fit:")
print(acf_best)
print()

#--- plotting best fit

mu_exact = np.mean(exact)
stdev_exact = np.std(exact)
mu_best_series = np.mean(best_series)
stdev_best_series = np.std(best_series)
best_series = mu_exact + stdev_exact * (best_series - mu_best_series)/stdev_best_series  # un-normalize

print("min: exact vs best series: %8.3f %8.3f" % (np.min(exact),np.min(best_series)))
print("max: exact vs best series: %8.3f %8.3f" % (np.max(exact),np.max(best_series)))
corrm = np.corrcoef(exact,best_series)
print("|correlation| between both %8.5f" % (abs(corrm[0,1]))) 

plt.scatter(exact, best_series, c='orange') 
plt.show()
\end{lstlisting}\vspace{1ex}

%\subsubsection{Conclusions}

Monte-Carlo simulations are not a good solution with more than 3 or 4 parameters. Using 4 parameters sometimes lead to better results than the full model with 8 parameters in this case. Likewise using fewer simulations -- say $10^4$ instead of $10^6$ -- can lead to better results, especially in a case like this with multiple local minima. Also, using lag-1, lag-2, and lag-3 autororrelations is not enough to measure the ``distance" (called \texttt{fit} in the Python code), between the real and simulated data, to identify among $10^4$
simulated time series which one is the best representative of the type of data we are dealing with. Below is some other highlights and recommendations:\vspace{1ex}
\begin{itemize}
\item All the discussion applies to {\em stationary} time series. It assumes that any \textcolor{index}{non-stationary}\index{stationarity}\index{time series!stationarity} components (such as trends) have been removed, to apply the methods discussed here. The datasets in this project meet this requirement.
\item To measure the quality of fit, it is tempting to use the correlation between simulated and real data. However this approach favors simulated data that is a replicate of the original data. To the contrary, comparing the two autocorrelation structures favors simulated data of the same type as the real data, but not identical. It leads to a richer class of synthetic time series, putting emphasis on structural and stochastic 
 similarity, rather than being ``the same". It also minimizes \textcolor{index}{overfitting}\index{overfitting}. 
\item Try different seeds for the random generator, and see how the solution changes based on the seed. Also, rather than using the sum of absolute value of differences between various \textcolor{index}{autocorrelation lags}\index{time series!autocorrelation function}, try the max, median, or assign a different weight to each lag (such as decaying weights). Or use transformed auto-correlations using a logarithm transform.
\item A classic metric to assess the quality of synthetic data is the \textcolor{index}{Hellinger distance}\index{Hellinger distance}, popular because it yields a value between 0 and 1. It measures the proximity between the two marginal distributions -- here, that of the simulated and real time series. It is not useful for time series though, because you can have the same marginals and very different auto-correlation structures. Note that the metric I use also yields values between 0 and 1, with zero being best, and 1 being worst.
\item The simulation was able to generate values outside the range of observed (real) values. Many synthetic data algorithms fail at that, because they use percentile-based methods (for instance, copulas) for data generation or to measure the quality (Hellinger is in that category). Empirical percentile distributions used for that purpose, including the Python version in the \textcolor{index}{Statsmodels}\index{Statsmodels (Python library)}\index{Python library!Statsmodels} library, have this limitation. 
\end{itemize}

\section{Temperature data: geospatial smoothness and interpolation}\label{georeshy}

The purpose here is twofold. First, using the \textcolor{index}{pykrige}\index{Python library!Pykrige (kriging)} Python library 
 to performs ordinary \textcolor{index}{kriging}, to estimate temperatures outside the sensor locations
 for the Chicago temperature dataset \texttt{sensors.csv} available on GitHub, 
 \href{https://raw.githubusercontent.com/VincentGranville/Statistical-Optimization/main/sensors.csv}{here}. The details are in my book~\cite{vgelsevier}, in chapter 9. In addition, we will use
 the \textcolor{index}{osmnx}\index{Python library!Osmnx (Open Street Map)} library (Open Street Map) to superimpose a map of the Chicago area on the final output, as seen
 in Figure~\ref{fig:gretelbwacxah}.

\begin{figure}[H]
\centering
\includegraphics[width=0.87\textwidth]{ipol1.png}   
\caption{Interpolation of the entire grid; dots are training set locations}
\label{fig:gretelbwacxzas}
\end{figure}

Then, we will use a generated dataset, with temperatures replaced by an arbitrary math function, in this case a mixture of bivariate Gaussian densities, or 
\textcolor{index}{Gaussian mixture model}\index{Gaussian mixture model}. This allows us to simulate hundreds or thousands of values at arbitrary locations, by contrast with the temperature dataset based on 31 observations only. The math function in question is pictured in Figure~\ref{fig:gretelbwacxzad}.
In this case, rather than kriging, we explore an 
\textcolor{index}{exact bivariate interpolation}\index{interpolation} method, also featured in chapter 9 in my book. The goal is to compare with kriging, using \textcolor{index}{cross-validation}\index{cross-validation}, as shown
 in Figure~\ref{fig:gretelbwacxzaf}. The solution offered here is a lot faster than the code in my book, thanks to replacing loops with vector and matrix operations. Eventually, we interpolate the entire grid:
 see Figure~\ref{fig:gretelbwacxzas}. Picking up random locations on the grid, together with the corresponding interpolated values, produces 
\textcolor{index}{synthetic geospatial data}\index{synthetic data!geospatial}\index{geospatial data}.
 It can be used to generate artificial elevation maps, with potential applications in video games.



Last but not least, we define the concept of \textcolor{index}{spatial smoothness}\index{smoothness} for geospatial data, using functions of second-order discrete derivatives. While it is easy to play with parameters of any given algorithm to produce various degrees of smoothness, it is a lot harder
 to define smoothness in absolute terms, allowing you to compare results produced by two different
 algorithms. Another important point is overfitting. Despite using exact interpolation for hundreds of locations -- akin to using a regression model with hundreds of regression coefficients -- we are able to avoid overfitting. 

\begin{figure}[H]
\centering
\includegraphics[width=0.87\textwidth]{ipol3.png}   
\caption{Math function used to sample training set locations and values}
\label{fig:gretelbwacxzad}
\end{figure}

 \subsection{Project description}

The Chicago temperature dataset \texttt{sensors.csv} is available 
  \href{https://raw.githubusercontent.com/VincentGranville/Statistical-Optimization/main/sensors.csv}{here}. It contains latitudes, longitudes and temperatures measured at 31 locations, at one point in time. We will use it to illustrate kriging. The algorithm for exact geospatial interpolation is in chapter 9 
 in my book~\cite{vgelsevier}. The original study of the temperature data set is in the same chapter.
 A Jupyter notebook, published by the University of Illinois,  can be found \href{https://cybergisxhub.cigi.illinois.edu/wp-content/uploads/2019/12/Spatial_interpolation.html}{here}. A shorter version of the interpolation code, applied to temperatures in India, can be found on GitHub, \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/interpol_temperatures_india.py}{here}.
\vspace{1ex}

\noindent The project consists of the following steps:\vspace{1ex}

\begin{itemize}
\item[] {\bf Step 1}:  Using the Pykrige library, write Python code to perform ordinary kriging on the temperature data set and interpolate values in the entire area. You may look at the Jypiter notebook aforementioned, to eventually produce a picture similar to Figure~\ref{fig:gretelbwacxah}. Try different types of kriging and parameters. Does it always lead to smooth interpolation, similar to time series smoothing? What happens when extrapolating, that is, sampling far outside the training set? \\
\item[] {\bf Step 2}: Let us consider the following mathematical function,  a mixture of $m$ bivariate Gaussian densities defined on $D = [0, 2] \times [0, 1]$ with weights $w_k$, centers
 $(c_{xk}, c_{yk})$, variances $\sigma_{xk},\sigma_{yk}$, and correlations $\rho_k$:
\begin{equation}
f(x, y) \propto \mathlarger{\mathlarger{\sum_{k=1}^m}} \frac{w_k}{\sigma_{xk} \sigma_{yk}\sqrt{1-\rho_k^2}}
\exp\Bigg[-\Bigg\{\frac{(x-c_{xk})^2}{\sigma_{xk}^2}
-  \frac{2\rho_k(x-c_{xk})(y-c_{yk})}{\sigma_{xk}\sigma_{yk}}
+ \frac{(y-c_{yk})^2}{\sigma_{yk}^2}
\Bigg\}\Bigg]. \label{pov4dh}
\end{equation}
Here the symbol $\propto$ means ``proportional to"; the proportionality constant does not matter.

Choose specific values for the parameters, and sample two sets of locations on $D$: the training set to define the interpolation formula based on the values of the above function at these locations, and the validation set to check how good the interpolated values are, outside the training set.\vspace{1ex}

\item[] {\bf Step 3}: Use the interpolation formula implemented in section 9.3.2 in my book~\cite{vgelsevier} and also available \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/interpol.py}{here} on GitHub, but this time on the training and validation sets obtained in step 2, rather than on the Chicago temperature dataset. Also interpolate the entire grid, using \num{10000} locations evenly spread
 on $D$. Plot the results using scatterplots and contour maps. Compute the interpolation error on the validation set. Show how the error is sensitive to the choice of sampled locations and parameters. Also show that the contour maps for interpolated values are considerably less smooth than for the underlying math function, due to using exact interpolation. Would this be also true if using kriging instead? What are your conclusions? \vspace{1ex}

\item[] {\bf Step 4}:  The original code in my book runs very slowly. Detect the bottlenecks. How can you improve the speed by several orders of magnitude? Hint: replace some of the loops by array operations, in Numpy.\vspace{1ex}

\item[] {\bf Step 5}:  The goal is to define the concept of smoothness, to compare interpolations and contour maps associated to different algorithms, for instance kriging versus my exact interpolation technique. Unlike 1D time series, there is no perfect answer in 2D: many definitions are possible, depending on the type of data. For instance, it could be based on the amount of chaos or entropy. Here we will use  a generalization of the 
1D metric
$$S(f, D) =  \int_D |f''(w)|^2 dw .$$

Explain why definitions based on first-order derivatives are not good. Search the Internet for a potential solution. I did not find any, but you can check out the answer to the question I posted on Mathoverflow, 
 \href{https://mathoverflow.net/questions/450606/what-are-the-best-definitions-for-smoothness-of-a-2d-curve-real-valued-function}{here}. You may create a definition based on transformed
gradients and \textcolor{index}{Hessians}\index{Hessian}, such as a matrix norm of the Hessian. These objects are respectively the first and second derivatives (a vector for the gradient, a matrix for the Hessian) 
attached to multivariate functions. Compute the smoothness on different interpolated grids, to see if 
 your definition matches intuition. You will need a discrete version of the gradient or Hessian, as we are dealing with data (they don't have derivatives!) rather than mathematical functions. Numpy has functions  such as \texttt{gradient} that do the computations in one line of code, when the input is a grid.
\vspace{1ex}

\item[] {\bf Step 6}: Optional. How would you handle correlated bivariate values, such as temperature and pressure measured simultaneously at various locations? How about spatio-temporal data: temperatures evolving over time across multiple locations? Finally, turn Figure~\ref{fig:gretelbwacxzas} into a video, by continuously modifying 
the parameters in the function defined by~(\ref{pov4dh}), over time, with each video frame corresponding to updated parameters.  This is how \textcolor{index}{agent-based modeling}\index{agent-based modeling} works.
\end{itemize}




\begin{figure}%[H]
\centering
\includegraphics[width=0.87\textwidth]{ipol2.png}   
\caption{Interpolation: dots for training locations, + for validation points}
\label{fig:gretelbwacxzaf}
\end{figure}

\begin{figure}%[H]
\centering
\includegraphics[width=0.87\textwidth]{ipol4.png}   
\caption{Interpolation of entire grid: second-order gradient representing smoothness}
\label{fig:gretelbwacxag}
\end{figure}

\begin{figure}%[H]
\centering
\includegraphics[width=0.83\textwidth]{spatial-vg.png}   
\caption{Kriging, temperature dataset; dots correspond to actual measurements}
\label{fig:gretelbwacxah}
\end{figure}



\subsection{Solution}

The code to solve \textcolor{red}{step 1} is in section~\ref{gtifasw}. In addition, I used the Osmnx library to superimpose the Chicago
 street map on the temperature 2D grid. For \textcolor{red}{step 2}, see the \texttt{gm} function in the code featured
 in section~\ref{uycdk77kuw}. The function \texttt{interpolate} in the same program, is the implementation
 of the interpolation formula discussed in \textcolor{red}{step 3}. More about \textcolor{red}{step 3} and 
\textcolor{red}{step 4} can be found in the same section. As for \textcolor{red}{step 5}, I implemented a discrete version of the following formula, to define and compute the smoothness $S$ on $[0, 2]\times [0, 1]$:
$$
S = \int_{0}^1\int_{0}^2 |\nabla (\lvert\nabla z(x,y)\rvert)| dx dy
$$
where $z$ is the value at location $(x, y)$ on the grid, $\nabla$ is the gradient operator, and $|\cdot|$ is the Eulidean norm. The discrete gradient on the grid is computed in one line of code,
 with the \texttt{gradient} function available in Numpy.

Regarding the difference between kriging and my interpolation method (last
 questions in \textcolor{red}{step 3}), kriging tends to produce much smoother results:
 it is good for measurements such as temperatures, with a smooth gradient. Chaotic processes, for instance the reconstruction of an elevation map or structures similar to \textcolor{index}{Voronoi diagrams}\index{Voronoi diagram} [\href{https://en.wikipedia.org/wiki/Voronoi_diagram}{Wiki}] are much better rendered with my method, especially when few data points are available. It preserves local variations much better than kriging. Finally, despite offering exact interpolation, my method avoids overfitting, unlike
 polynomial regression. This is because I use some normalization in the interpolation formula.
 In short, kriging is a smoothing technique, while my method is best used for data
 reconstruction or synthetization.

There are many other potential topics to address. I listed below a few suggestions for the reader interested in further exploring
this project. \vspace{1ex}

\begin{itemize}
\item Play with the parameters $\alpha, \beta, \kappa, \delta$ to increase or decrease smoothness in the exact interpolation method, and see the impact on the error rate (measured on the validation set).
\item Add noise to the observed values in the training set, and assess sensitivity of interpolated values to various levels of noise.
\item Play with the parameters associated to the \texttt{gm} function, to produce many different sets of observed values. Compute the error (\texttt{error} in the code in section~\ref{uycdk77kuw}) and relative error, for each parameter set. What is the range 
 of the relative error, depending on the size of the training set?
\item Investigate other metrics to measure smoothness (\textcolor{red}{step 5} in the project),  for instance 2D generalizations~of 
 \textcolor{index}{Hurst exponent}\index{Hurst exponent} [\href{https://en.wikipedia.org/wiki/Hurst_exponent}{Wiki}] used for time series. 
\item When is it useful to first transform the data, interpolate the transformed data, then apply the inverse transform? For instance, this is done for the Chicago temperature dataset: see chapter 9 in my book~\cite{vgelsevier}. 
\item How far outside the training set locations can you reasonably interpolate without losing too much accuracy? In this case, it is called extrapolation. Check the accuracy of interpolated values for locations in the validation set that are far away from any training set point.
\item Compute confidence intervals for the interpolated values (validation set). In order to do so, generate~1000 training sets, each with the same number of points, but different locations. Or use the same training set each time, with a \textcolor{index}{resampling}\index{resampling} technique 
 such as \textcolor{index}{bootstrapping}\index{bootstrapping} [\href{https://en.wikipedia.org/wiki/Bootstrapping_(statistics)}{Wiki}].
\end{itemize}\vspace{1ex}

\noindent Regarding \textcolor{red}{step 6}, see how to create a data video in section~\ref{porousbutane} in this textbook.  An 
 example relevant to this project -- an animated elevation map using 
\textcolor{index}{agent-based modeling}\index{agent-based modeling} -- can be found in chapter 14 in my book~\cite{vgelsevier}.


%xxx compare linear regression, my fourier regression, gmm forecasting, kriging, my 2D interpolation on simulated gmm + noise
%xxxx compare smoothness metrics with those for time series


%############### title: Generative AI: Synthetization of Geospatial Data

%# add project: HR GPT crawl GitHub profiles


% link to video... terrain ... step 6

\subsubsection{Kriging}\label{gtifasw}

The Chicago temperature dataset is discussed in chapter 9 in my book~\cite{vgelsevier}.
My code is inspired from a Jupyter notebook posted
 \href{https://cybergisxhub.cigi.illinois.edu/notebook/spatial-interpolation/}{here} by a team working on this project at the University of Chicago.
My Python implementation \texttt{kriging\_temperatures\_chicago.py} listed below is also on GitHub,
 \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/kriging_temperatures_chicago.py}{here}.\vspace{1ex}

\begin{lstlisting}
# compare this with my spatial interpolation: 
# https://github.com/VincentGranville/Statistical-Optimization/blob/main/interpol.py
# This kriging oversmooth the temperatures, compared to my method

import numpy as np
import matplotlib as mpl
from matplotlib import pyplot as plt
from matplotlib import colors  
from matplotlib import cm # color maps
import osmnx as ox
import pandas as pd
import glob
from pykrige.ok import OrdinaryKriging
from pykrige.kriging_tools import write_asc_grid
import pykrige.kriging_tools as kt
from matplotlib.colors import LinearSegmentedColormap

city = ox.geocode_to_gdf('Chicago, IL')
city.to_file("il-chicago.shp")

data = pd.read_csv(
     'sensors.csv',
     delim_whitespace=False, header=None,
     names=["Lat", "Lon", "Z"])

lons=np.array(data['Lon']) 
lats=np.array(data['Lat']) 
zdata=np.array(data['Z'])

import geopandas as gpd
Chicago_Boundary_Shapefile = 'il-chicago.shp'
boundary = gpd.read_file(Chicago_Boundary_Shapefile)

# get the boundary of Chicago 
xmin, ymin, xmax, ymax = boundary.total_bounds

xmin = xmin-0.06
xmax = xmax+0.05
ymin = ymin-0.01
ymax = ymax+0.01
grid_lon = np.linspace(xmin, xmax, 100)
grid_lat = np.linspace(ymin, ymax, 100)

#------
# ordinary kriging

OK = OrdinaryKriging(lons, lats, zdata, variogram_model='gaussian', verbose=True, enable_plotting=False,nlags=20)
z1, ss1 = OK.execute('grid', grid_lon, grid_lat)
print (z1)

#-------
# plots

xintrp, yintrp = np.meshgrid(grid_lon, grid_lat) 
plt.rcParams['axes.linewidth'] = 0.3
fig, ax = plt.subplots(figsize=(8,6))

contour = plt.contourf(xintrp, yintrp, z1,len(z1),cmap=plt.cm.jet,alpha = 0.8)
cbar = plt.colorbar(contour)
cbar.ax.tick_params(width=0.1) 
cbar.ax.tick_params(length=2)
cbar.ax.tick_params(labelsize=7) 

boundary.plot(ax=ax, color='white', alpha = 0.2, linewidth=0.5, edgecolor='black', zorder = 5)
npts = len(lons)

plt.scatter(lons, lats,marker='o',c='b',s=8)  
plt.xticks(fontsize = 7) 
plt.yticks(fontsize = 7)
plt.show()
\end{lstlisting}

\subsubsection{Exact interpolation and smoothness evaluation}\label{uycdk77kuw}

The instruction \texttt{(za,npt)=interpolate(xa,ya,npdata,0.5*delta)} produces interpolated values \texttt{za} for the the validation set locations \texttt{(xa,ya)}.
The parameter \texttt{delta} controls the maximum
 distance allowed between a location, and a training set point used to interpolate the value at the location in question.
 The array \texttt{npt} stores the number of training set points used to compute each interpolated value. 
Also, the instruction  \texttt{z,npt=interpolate(xg,yg,npdata,2.2*delta)}
 interpolates the  entire grid. 

The number of loops has been reduced to make the code run faster. In particular, the arguments
 \texttt{xa,ya} can be arrays; accordingly, the output \texttt{z} of the \texttt{interpolate} function can be an array or even a grid of
 interpolated values. Likewise, \texttt{npt} can be an array or grid, matching the shape of \texttt{z}. Finally, 
 \texttt{error} measures the mean absolute error between exact and interpolated values on the validation set,
  while \texttt{average\_smoothness} measures the smoothness of the grid, original or interpolated.
The Python code, \texttt{interpol\_smooth.py}, is also on GitHub, 
 \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/interpol_smooth.py}{here}.\vspace{1ex}

\begin{lstlisting}[numbers=left]
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import matplotlib as mpl
from matplotlib import pyplot as plt
from matplotlib import colors  
from matplotlib import cm # color maps

n = 60         # number of observations in training set
ngroups = 3     # number of clusters in Gaussian mixture
seed = 13       # to initiate random number generator

#--- create locations for training set

width = 2
height = 1
np.random.seed(seed)
x = np.random.uniform(0, width, n)
y = np.random.uniform(0, height, n)


#--- create values z attached to these locations

weights = np.random.uniform(0.5, 0.9, ngroups)
sum = np.sum(weights)
weights = weights/sum

cx = np.random.uniform(0, width, ngroups)
cy = np.random.uniform(0, height, ngroups)
sx = np.random.uniform(0.3, 0.4, ngroups)
sy = np.random.uniform(0.3, 0.4, ngroups)
rho = np.random.uniform(-0.3, 0.5, ngroups)


def f(x, y, cx, cy, sx, sy, rho):

    # bivariate bell curve

    tx = ( (x - cx) / sx)**2
    ty = ( (y - cy) / sy)**2
    txy = rho * (x - cx) * (y - cy) / (sx * sy)
    z = np.exp(-(tx - 2*txy + ty) / (2*(1 - rho**2)) )
    z = z / (sx * sy * np.sqrt(1 - rho**2))
    return(z)

def gm(x, y, weights, cx, cy, sx, sy, rho):

    # mixture of gaussians

    n = len(x)
    ngroups = len(cx)
    z = np.zeros(n)   
    for k in range(ngroups):
        z += weights[k] * f(x, y, cx[k], cy[k], sx[k], sy[k], rho[k])
    return(z)

z = gm(x, y, weights, cx, cy, sx, sy, rho)
npdata = np.column_stack((x, y, z))

print(npdata)

#--- model parameters

alpha = 1.0       # small alpha increases smoothing
beta  = 2.0       # small beta increases smoothing
kappa = 2.0       # high kappa makes method close to kriging 
eps   = 1.0e-8    # make it work if sample locations same as observed ones
delta = eps + 1.2 * max(width, height)   # don't use faraway points for interpolation

#--- interpolation for validation set: create locations

n_valid = 200   # number of locations to be interpolated, in validation set
xa = np.random.uniform(0, width, n_valid)
ya = np.random.uniform(0, height, n_valid)

#--- interpolation for validation set 

def w(x, y, x_k, y_k, alpha, beta):
    # distance function
    z = (abs(x - x_k)**beta + abs(y - y_k)**beta)**alpha
    return(z)


def interpolate(x, y, npdata, delta):

    # compute interpolated z at location (x, y) based on npdata (observations)
    # also returns npt, the number of data points used for each interpolated value
    # data points (x_k, y_k) with w[(x,y), (x_k,y_k)] >= delta are ignored
    # note: (x, y) can be a location or an array of locations

    if np.isscalar(x):  # transform scalar to 1-cell array
        x = [x]
        y = [y]
    sum  = np.zeros(len(x))
    sum_coeff = np.zeros(len(x)) 
    npt = np.zeros(len(x)) 
    
    for k in range(n):
        x_k = npdata[k, 0]
        y_k = npdata[k, 1]
        z_k = npdata[k, 2]
        coeff = 1
        for i in range(n):
            x_i = npdata[i, 0]
            y_i = npdata[i, 1]
            if i != k:
                numerator = w(x, y, x_i, y_i, alpha, beta)
                denominator = w(x_k, y_k, x_i, y_i, alpha, beta) 
                coeff *= numerator / (eps + denominator) 
        dist = w(x, y, x_k, y_k, alpha, beta)   
        coeff = (eps + dist)**(-kappa) * coeff / (1 + coeff) 
        coeff[dist > delta] = 0.0  
        sum_coeff += coeff
        npt[dist < delta] += 1   
        sum += z_k * coeff  
 
    z = sum / sum_coeff 
    return(z, npt)

(za, npt) = interpolate(xa, ya, npdata, 0.5*delta)

#--- create 2D grid with x_steps times y_steps locations, and interpolate entire grid 

x_steps = 160  
y_steps = 80 
xb = np.linspace(min(npdata[:,0])-0.50, max(npdata[:,0])+0.50, x_steps)
yb = np.linspace(min(npdata[:,1])-0.50, max(npdata[:,1])+0.50, y_steps)
xc, yc = np.meshgrid(xb, yb) 

zgrid = np.empty(shape=(x_steps,y_steps))   # for interpolated values at grid locations
xg = []
yg = []
gmap = {}
idx = 0
for h in range(len(xb)):
    for k in range(len(yb)):
        xg.append(xb[h])
        yg.append(yb[k])
        gmap[h, k] = idx
        idx += 1 
z, npt = interpolate(xg, yg, npdata, 2.2*delta)

zgrid_true = np.empty(shape=(x_steps,y_steps))
xg = np.array(xg)
yg = np.array(yg)
z_true = gm(xg, yg, weights, cx, cy, sx, sy, rho) #  exact values on the grid

for h in range(len(xb)):
    for k in range(len(yb)):
        idx = gmap[h, k]
        zgrid[h, k] = z[idx]  
        zgrid_true[h, k] = z_true[idx]
zgridt = zgrid.transpose() 
zgridt_true = zgrid_true.transpose() 

#--- visualizations

nlevels = 20  # number of levels on contour plots

def set_plt_params():
    # initialize visualizations
    fig = plt.figure(figsize =(4, 3), dpi=200) 
    ax = fig.gca()
    plt.setp(ax.spines.values(), linewidth=0.1)
    ax.xaxis.set_tick_params(width=0.1)
    ax.yaxis.set_tick_params(width=0.1)
    ax.xaxis.set_tick_params(length=2)
    ax.yaxis.set_tick_params(length=2)
    ax.tick_params(axis='x', labelsize=4)
    ax.tick_params(axis='y', labelsize=4)
    plt.rc('xtick', labelsize=4) 
    plt.rc('ytick', labelsize=4) 
    plt.rcParams['axes.linewidth'] = 0.1
    return(fig,ax)

# contour plot, interpolated values, full grid

(fig1, ax1) = set_plt_params() 
cs1 = plt.contourf(xc, yc, zgridt, cmap='coolwarm',levels=nlevels,linewidths=0.1)  
sc1 = plt.scatter(npdata[:,0], npdata[:,1], c=npdata[:,2], s=8, cmap=cm.coolwarm,
      edgecolors='black',linewidth=0.3,alpha=0.8)
cbar1 = plt.colorbar(sc1)
cbar1.ax.tick_params(width=0.1) 
cbar1.ax.tick_params(length=2) 
plt.xlim(0, width)
plt.ylim(0, height)
plt.show()
           
# scatter plot: validation set (+) and training data (o)         

(fig2, ax2) = set_plt_params()
my_cmap = mpl.colormaps['coolwarm']  # old version: cm.get_cmap('coolwarm')
my_norm = colors.Normalize()
ec_colors = my_cmap(my_norm(npdata[:,2]))
sc2a = plt.scatter(npdata[:,0], npdata[:,1], c='white', s=5, cmap=my_cmap, 
    edgecolors=ec_colors,linewidth=0.4)
sc2b = plt.scatter(xa, ya, c=za, cmap=my_cmap, marker='+',s=5,linewidth=0.4)
plt.show()
plt.close()

#--- measuring quality of the fit on validation set
#    zd is true value, za is interpolated value

zd = gm(xa, ya, weights, cx, cy, sx, sy, rho) 
error = np.average(abs(zd - za)) 
print("\nMean absolute error on validation set: %6.2f" %(error))
print("Mean value on validation set: %6.2f" %(np.average(zd)))

#--- plot of original function (true values)

(fig3, ax3) = set_plt_params() 
cs3 = plt.contourf(xc, yc, zgridt_true, cmap='coolwarm',levels=nlevels,linewidths=0.1)
cbar1 = plt.colorbar(cs3)
cbar1.ax.tick_params(width=0.1) 
cbar1.ax.tick_params(length=2) 
plt.xlim(0, width)
plt.ylim(0, height)
plt.show()

#--- compute smoothness of interpolated grid via double gradient 
#    1/x_steps and 1/y_steps are x, y increments between 2 adjacent grid locations

h2 = x_steps**2 
k2 = y_steps**2
dx, dy = np.gradient(zgrid)    # zgrid_true for original function
zgrid_norm1 = np.sqrt(h2*dx*dx + k2*dy*dy)
dx, dy = np.gradient(zgrid_norm1)    
zgrid_norm2 = np.sqrt(h2*dx*dx + k2*dy*dy)  
zgridt_norm2 = zgrid_norm2.transpose()
average_smoothness = np.average(zgrid_norm2) 
print("Average smoothness of interpolated grid: %6.3f" %(average_smoothness)) 

(fig4, ax4) = set_plt_params() 
cs4 = plt.contourf(xc, yc, zgridt_norm2, cmap=my_cmap,levels=nlevels,linewidths=0.1)
plt.xlim(0, width)
plt.ylim(0, height)
plt.show()
\end{lstlisting}



%---------------------------------------------
\chapter{Scientific Computing}

Many projects throughout this book feature scientific programming in Python. This section offers a selection that best illustrates what
 scientific computing is about. In many cases, special libraries are needed, or you have to process numbers with billions of digits.  
Applications range from heavy simulations to cybersecurity. In several instances, very efficient algorithms are required. 

\section{The music of the Riemann Hypothesis: sound generation}\label{music911}

This first project is an introduction to sound generation in Python with wave files, as well as the 
\textcolor{index}{MPmath}\index{MPmath (Python library)}\index{Python library!MPmath} Python library to work with special functions including in the complex plane.  This library is particularly useful to physicists. No special knowledge of complex functions is required: we work with the real and imaginary part separately, as illustrated in Figure~\ref{fig:gretelbwacxzzz}: in this example,
 the frequency attached to a musical note is the real part of the complex 
\textcolor{index}{Dirichlet eta function}\index{Dirichlet eta function} $\eta$ [\href{https://en.wikipedia.org/wiki/Dirichlet_eta_function}{Wiki}] (re-scaled to be positive), the duration of a note is the imaginary part of  $\eta$ after re-scaling, and the volume -- also called amplitude -- is the 
 the modulus [\href{https://en.wikipedia.org/wiki/Complex_modulus}{Wiki}].

\noindent The project consists of the following steps: \vspace{1ex} 

\begin{itemize}
\item[] {\bf Step 1}:  Read my article ``The Sound that Data Makes", available \href{https://mltechniques.com/2022/08/29/the-sound-that-data-makes/}{here}. It contains Python code to turn random data into music. Also download the technical paper
 ``Math-free, Parameter-free Gradient Descent in Python" available \href{https://mltechniques.com/2023/01/28/math-free-parameter-free-gradient-descent-in-python/}{from here}. It contains Python code to compute the Dirichlet eta function (among others) using the MPmath 
 library. \vspace{1ex}
\item[] {\bf Step 2}:  Using the material gathered in step 1, generate a sound file (wav extension) for the Dirichlet eta function
 $\eta(\sigma + it)$, with $\sigma=\frac{1}{2}$ and $t$ between $\num{400000}$ and $\num{400020}$. Create 300 musical notes,
 one for each value of $t$ equally spaced in the interval in question. Each note has 3 components: 
the frequency, duration, and volume, corresponding respectively to the real part, imaginary part, and
 modulus of  $\eta(\sigma + it)$ after proper re-scaling. \vspace{1ex}
\item[] {\bf Step 3}:  Same as step 2, but this time with a different function of your choice. Or better, with actual data rather than sampled
 values from mathematical functions. For instance, try with the ocean tide data, or the planet inter-distance data investigated in
 project~\ref{prgt43zas}.\vspace{1ex}
\item[] {\bf Step 4}: Optional. Create a sound track for the video featured in Figure~\ref{fig:fdxslbwavc}. You can
 watch the video on YouTube, \href{https://www.youtube.com/watch?v=XI5MhyNc7us}{here}.  
The code to produce this video (with detailed explanations) is in section 4.3.2 in my book~\cite{vgelsevier}, 
and also on GitHub, \href{https://github.com/VincentGranville/Visualizations/blob/main/Source-Code/image2R.py}{here}.
To blend the sound track and the video together, see solution on Stack Exchange, 
\href{https://stackoverflow.com/questions/28219049/combining-an-audio-file-with-video-file-in-python}{here}. An alternative
 is to convert the wav file to mp4 formart (see how to do it \href{https://stackoverflow.com/questions/48728145/video-editing-with-python-adding-a-background-music-to-a-video-with-sound}{here}) then use the \textcolor{index}{Moviepy}\index{Moviepy (Python library)}\index{Python library!Moviepy} library to combine them both.
\end{itemize}
\vspace{1ex} 

\noindent Data visualizations offer colors and shapes, allowing you to summarize multiple dimensions in one picture. Data animations (videos) go one step further, adding a time dimension. See examples on 
\href{https://www.youtube.com/c/VincentGranvilleVideos}{my YouTube channel}, and in my book~\cite{vgelsevier}. Then, sound adds multiple dimensions: amplitude, volume and frequency over time. Producing pleasant sound, with each musical note representing a multivariate data point, is equivalent to data binning or bukectization.
Stereo and the use of multiple musical instruments (synthesized) add more dimensions. Once you have a large database of data music, you can use it for generative AI: sound generation to mimic existing datasets. Of course, musical AI art is another application, all the way to creating synthetic movies.

Figure~\ref{fig:gretelbwacxzzz} shows the frequency, duration and volume attached to each of the 300 musical notes in the wav file, prior to re-scaling. The volume is maximum each time the Riemann zeta function hits a zero on the critical line. This is one of the connections to the Riemann Hypothesis. Note that in the solution, I use the Dirichlet eta function $\eta(\sigma + it)$ with $\sigma=\frac{1}{2}$,
 corresponding to the critical line [\href{https://mathworld.wolfram.com/CriticalLine.html}{Wiki}]. According to the Riemann Hypothesis, 
 this is the only positive value of $\sigma$ where all the zeros of the \textcolor{index}{Riemann zeta function}\index{Riemann zeta function} 
 $\zeta(\sigma+it)$ occur. 
 Indeed, there are infinitely many $t$ for which $\zeta(\frac{1}{2}+it)=0$. You can see the first 100 billion of them, 
\href{https://www.lmfdb.org/zeros/zeta/}{here}. 
The Dirichlet eta function has the same zeros. This is the connection to the Riemann Hypothesis.  The notation $\sigma+it$ represents the complex argument of the functions involved, with $\sigma$ the real part and $t$ the imaginary part. More on this topic in
 chapter 17 in my book~\cite{vgelsevier}. 




%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{soundrh.png}   
\caption{300 musical notes, showing volume, duration and frequency}
\label{fig:gretelbwacxzzz}
\end{figure}


\subsection{Solution}

The code in this section provides the answer to \textcolor{red}{step 2}. The variables
 \texttt{z.real} and \texttt{z.imag} correspond respectively to the real and imaginary part of $z$.
The volume in the output wav file (the music) is maximum each time the Riemann zeta or  Dirichlet eta function hits a zero on the critical line. 
The Python code is also
 on my GitHub repository 
\href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/soundRH.py}{here}.

Figure~\ref{fig:fdxslbwavc} shows the final frame of the video discussed in \textcolor{red}{step 4}. It features the convergence path of the Dirichlet 
eta function in the complex plane, for a specific value of the complex argument $\sigma+it$, when adding more and more terms in  the standard sine and cosine series to approximate the function. Here $t$ is very large, and $\sigma$ is in the critical band $\frac{1}{2}\leq \sigma < 1$,
 where the most interesting action takes place.

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{rhp1.png}   
\caption{Last frame from the video featuring the convergence of the Dirichlet eta function}
\label{fig:fdxslbwavc}
\end{figure}


\begin{lstlisting}
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from scipy.io import wavfile
import mpmath

#-- Create the list of musical notes

scale=[] 
for k in range(35, 65): 
    note=440*2**((k-49)/12)
    if k%12 != 0 and k%12 != 2 and k%12 != 5 and k%12 != 7 and k%12 != 10:
        scale.append(note) # add musical note (skip half tones)
n_notes = len(scale) # number of musical notes


#-- Generate the data

n = 300
sigma = 0.5
min_t = 400000
max_t = 400020

def create_data(f, nobs, min_t, max_t, sigma):
    z_real = []
    z_imag = []
    z_modulus = []
    incr_t = (max_t - min_t) / nobs
    for t in np.arange(min_t, max_t, incr_t):   
        if f == 'Zeta':
            z = mpmath.zeta(complex(sigma, t))
        elif f == 'Eta':
            z = mpmath.altzeta(complex(sigma, t))
        z_real.append(float(z.real))
        z_imag.append(float(z.imag))
        modulus = np.sqrt(z.real*z.real + z.imag*z.imag)
        z_modulus.append(float(modulus))
    return(z_real, z_imag, z_modulus)

(z_real, z_imag, z_modulus) = create_data('Eta', n, min_t, max_t, sigma) 

size = len(z_real) # should be identical to nobs
x = np.arange(size)

# frequency of each note     
y = z_real    
min = np.min(y)
max = np.max(y)
yf = 0.999*n_notes*(y-min)/(max-min) 
 
# duration of each note
z = z_imag  
min = np.min(z)
max = np.max(z)
zf = 0.1 + 0.4*(z-min)/(max-min) 

# volume of each note
v = z_modulus 
min = np.min(v)
max = np.max(v)
vf = 500 + 2000*(1 - (v-min)/(max-min)) 

#-- plot data

mpl.rcParams['axes.linewidth'] = 0.3
fig, ax = plt.subplots()
ax.tick_params(axis='x', labelsize=7)
ax.tick_params(axis='y', labelsize=7) 
plt.rcParams['axes.linewidth'] = 0.1
plt.plot(x, y, color='red', linewidth = 0.3)
plt.plot(x, z, color='blue', linewidth = 0.3)
plt.plot(x, v, color='green', linewidth = 0.3)
plt.legend(['frequency','duration','volume'], fontsize="7", 
    loc ="upper center", ncol=3)
plt.show()

#-- Turn the data into music

def get_sine_wave(frequency, duration, sample_rate=44100, amplitude=4096):
    t = np.linspace(0, duration, int(sample_rate*duration))
    wave = amplitude*np.sin(2*np.pi*frequency*t)
    return wave

wave=[]
for t in x: # loop over dataset observations, create one note per observation
    note = int(yf[t])
    duration = zf[t]
    frequency = scale[note]    
    volume = vf[t]  ## 2048
    new_wave = get_sine_wave(frequency, duration = zf[t], amplitude = vf[t])
    wave = np.concatenate((wave,new_wave))
wavfile.write('sound.wav', rate=44100, data=wave.astype(np.int16))
\end{lstlisting}

\section{Cross-correlations in binary digits of irrational numbers}

This short off-the-beaten-path project constitutes an excellent preparation for job interviews. In a nutshell, the task consists of implementing
 the \textcolor{index}{grade-school multiplication algorithm}\index{multiplication algorithm} [\href{https://en.wikipedia.org/wiki/Multiplication_algorithm}{Wiki}] for numbers with millions of digits in base 2, in this case random digits.
The practical goal is to assess when and how sequences of bits or binary digits are correlated or not. It is an important issue 
in the quadratic irrational \textcolor{index}{random number generator}\index{pseudo-random number generator} (PRNG) discussed 
 in chapter 11 in my book~\cite{vgelsevier}. 

Indeed, this PRNG relies on blending billions of digits from millions of irrational numbers, using a very fast algorithm. Three of these numbers could be $\sqrt{2341961}, \sqrt{1825361}$ and $\sqrt{2092487}$. It turns out that
 the binary digits of $\sqrt{2341961}$ and $\sqrt{1825361}$ are correlated, when computing the empirical correlation on a growing
 sequence of digits. But those of $\sqrt{2341961}$ and $\sqrt{2092487}$ are not, 
 at the limit when the number of digits becomes infinite. In the former case, the correlation
 is about $1.98\times 10^{-5}$. The exact value is $1/50429$. While very close to zero,
 it is not zero, and this is a critical cybersecurity issue when designing military-grade PRNGs. Irrational numbers must be selected 
 in such a way that all cross-correlations are zero.

The theoretical explanation is simple, though hard to prove. If $X$ is an irrational number, and $p,q$ are two positive co-prime odd integers, then the correlation between the binary digits of $pX$ and $qX$, is $1/(pq)$. Again, this is the limit value when the number $n$ of digits
 in each sequence tends to infinity. In my example, 
$2341961 = 239^2 \times 41, 1825361 = 211^2 \times 41,
2092487 = 211^2\times 47$, and $50429 = 239\times 211$.

\subsection{Project and solution}
 

Write a program that computes the binary digits of $pX$ using grade-school multiplication. Here $p$ is a positive integer, and $X$ is a random number in $[0, 1]$. Use this program to compute the correlation between the sequences of binary digits of $pX$ and $qX$, where $p,q$ are positive integers, and $X$ a number in $[0, 1]$ with random binary digits. Focus on the digits after the decimal point (ignore the other ones).

For the solution, see my Python code below. It is also on GitHub, \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/lottery_correl.py}{here}. It creates the digits of $X$, then those of $pX$ and $qX$, starting backward with the last digits. Finally it computes the correlation in question, assuming the digits of $X$ are random. It works if $X$ has a finite number of digits, denoted as \texttt{kmax} in the code. By increasing \texttt{kmax}, you can approximate any $X$ with infinitely many digits, arbitrarily closely.
\vspace{1ex} \\ 

\begin{lstlisting}
# Compute binary digits of X, p*X, q*X backwards (assuming X is random)
# Only digits after the decimal point (on the right) are computed
# Compute correlations between digits of p*X and q*X
# Include carry-over when performing grade school multiplication

import numpy as np

# main parameters
seed = 105
np.random.seed(seed)
kmax = 1000000
p  = 5
q  = 3

# local variables
X, pX, qX = 0, 0, 0
d1, d2, e1, e2 = 0, 0, 0, 0
prod, count = 0, 0 

# loop over digits in reverse order
for k in range(kmax): 

    b = np.random.randint(0, 2)  # digit of X
    X = b + X/2  

    c1 = p*b
    old_d1 = d1
    old_e1 = e1 
    d1 = (c1 + old_e1//2) %2  # digit of pX
    e1 = (old_e1//2) + c1 - d1
    pX = d1 + pX/2

    c2 = q*b
    old_d2 = d2
    old_e2 = e2 
    d2 = (c2 + old_e2//2) %2  #digit of qX
    e2 = (old_e2//2) + c2 - d2
    qX = d2 + qX/2

    prod  += d1*d2
    count += 1 
    correl = 4*prod/count - 1

    if k% 10000 == 0:  
        print("k = %7d, correl = %7.4f" % (k, correl))  

print("\np = %3d, q = %3d" %(p, q))
print("X = %12.9f, pX  = %12.9f, qX  = %12.9f" % (X, pX, qX))
print("X = %12.9f, p*X = %12.9f, q*X = %12.9f" % (X, p*X, q*X))    
print("Correl = %7.4f, 1/(p*q) = %7.4f" % (correl, 1/(p*q))) 
\end{lstlisting}


\section{Longest runs of zeros in binary digits of $\sqrt{2}$}\label{sqrt2run}

Studying the longest head runs in coin tossing has a very long history, starting in gaming and probability theory. Today, it 
 has applications in cryptography and insurance~\cite{g1594d3wes}. For random sequences
 or \textcolor{index}{Bernoulli trials}\index{Bernoulli trials}, the associated statistical properties and distributions
 have been studied in details~\cite{k1594djj}, even when the proportions of zero and one are different. Yet, I could barely find any discussion on deterministic sequences, such as the digits
 or irrational numbers~\cite{boundrun7}. The case study investigated here fills this gap, focusing on one of the deepest and most challenging problems in number theory:  almost all the questions about the distribution of these digits, even the most basic ones such as the proportions of zero and one, are still unsolved conjectures to this day. 

In this context, a \textcolor{index}{run}\index{run (statistical theory)} is a sequence of successive, identical digits. In random sequences of bits,
 runs have a specific probability distribution. In particular, the maximum length of a run in a random sequence of $n$ binary digits has expectation $\log_2 n$. For details and additional properties, see~\cite{runs45}.
This fact can be used to test if a sequence violates the laws of randomness. 
Pseudo-random number generators (\textcolor{index}{PRNG}\index{pseudo-random number generator}\index{PRNG}) that do not pass 
 this test are not secure.

 The focus here is on sequences of binary digits
 of quadratic irrational numbers of the form $x_0 = \sqrt{p/q}$, where $p,q$ are positive coprime integers.  The goal is to show,
 using empirical evidence, that indeed such sequences pass the test. More precisely, the project consists of computing runs of zeros 
 in billions of successive binary digits of $x_0$ for specific $p$ and $q$. Let $L_n$ be the length of such a run starting at position $n$ in the digit
 expansion of $x_0$. Do we have 
 $L_n / \log_2 n \leq \lambda$ where $\lambda$ is a positive constant? Based on the computations, is it reasonable to conjecture that $\lambda = 1$ if
 $n$ is large enough? Very little is known about these digits. However, before discussing the project in more details, I share a much weaker yet spectacular result, easy to prove.
By contrast to the digits investigated here, there is an abundance of far more accurate theoretical results, proved long ago, for random bit sequences. See for instance~\cite{aslr4e, rizlr4e, aslimoi}.

\subsection{Surprising result about the longest runs}\label{nntserde}

For numbers such as $\sqrt{p/q}$ where $p,q$ are coprime positive integers and $p/q$ is not the square of a rational number, a run of zeros starting at position $n$ can not be longer than $n + C$ where $C$ is a constant depending on $p$ and $q$. Due to symmetry, the same is true for runs of ones. 

\begin{proof} 
\, \\
% \vspace{1ex}\\
A run of length $m$ starting at position $n$, consisting of zeros, means that the digit $d_{n}$ at position $n$ is one, followed by $m$ digits all zero, and then by 
 at least one non-zero digit. For this to happen, we must have
$$
\frac{1}{2} < 2^n x_0 - \Big\lfloor 2^n x_0 \Big\rfloor = \frac{1}{2} +\alpha,
$$ 
where
$$ x_0 = \sqrt{\frac{p}{q}} \quad \text{ and } \quad \frac{1}{2^{m+2}}\leq \alpha < \sum_{k=0}^\infty \frac{1}{2^{m+2+k}} = \frac{1}{2^{m+1}}.
$$
Here the brackets represent the floor function. Let $K_n = 1 + 2 \cdot \lfloor 2^n x_0 \rfloor $. Then, 
$K_n < 2^{n+1} x_0 = K_n +2\alpha$. After squaring both sides, we finally obtain
$$
qK_n^2 < 4^{n+1}\cdot p = q (K_n + 2\alpha)^2.
$$
Now, let $\delta_n = 4^{n+1}p - qK_n^2$. Note that $\delta_n$ is a strictly positive integer, smallest when $p,q$ are coprime. After some simple rearrangements, we
 obtain
\begin{align}
2\alpha & = \frac{1}{\sqrt{q}}\bigg[2^{n+1}\sqrt{p} -\sqrt{4^{n+1}p - \delta_n}\bigg] \nonumber \\
  & = \frac{1}{\sqrt{q}}
\Bigg[\frac{4^{n+1}p - (4^{n+1}p-\delta_n)}{2^{n+1}\sqrt{p} + \sqrt{4^{n+1}p -\delta_n}}\Bigg] \nonumber \\
  & = \frac{1}{\sqrt{q}} \cdot \frac{\delta_n}{2^{n+1}\sqrt{p} + \sqrt{4^{n+1}p - \delta_n}} \nonumber \\
  & \sim \frac{1}{\sqrt{q}}\cdot \frac{\delta_n}{2\cdot 2^{n+1}\sqrt{p}} \text{ as } n\rightarrow\infty. \nonumber
\end{align}
In the last line, I used the fact that $\delta_n$ is at most of the order $2^n$, thus negligible compared to $4^{n+1}p$. 

\noindent Since a run of length $m$ means $2^{-(m+2)} \leq \alpha < 2^{-(m+1)}$, combining with the above (excellent) 
 asymptotic result, we have, for large $n$:
$$
\frac{1}{2^{m+2}}\leq \frac{\delta_n}{4\cdot 2^{n+1}\sqrt{pq}} < \frac{1}{2^{m+1}}.
$$
Taking the logarithm in base 2, we obtain
$$
m+ 1 \leq - \log_2 \delta_n +\frac{1}{2}\log_2(pq) + n+3 < m+2,
$$
and thus, assuming $L_n$ denotes the length of the run at position $n$ and $n$ is large enough:
\begin{equation}
L_n =\Big\lfloor -\log_2\delta_n + \frac{1}{2}\log_2(pq) + n+ 2 \Big\rfloor \leq n + 2 +\frac{1}{2}\log_2(pq). \label{eqstr4nzs}
\end{equation}
This concludes the proof. It provides an upper bound for the maximum possible run length at position $n$: in short, $L_n \leq n +C$, where $C$ 
 is an explicit constant depending on $p$ and $q$. \qed
\end{proof}

Conversely, a number whose binary digits do not satisfy (\ref{eqstr4nzs}) can not be of the prescribed form.
 Note that if $p=1$ and $q$ is large, there will be a long run of zeros at the very beginning, and thus $C$ will be larger than usual. 
I implemented the equality in Formula~(\ref{eqstr4nzs}) in my Python code in section~\ref{xzz4kj}. It yielded  the exact run length 
 in all instances, for all $n$ where a new run starts. In the code, I use the fact that $\delta_n$ can be written as $u-qv^2$,
 where $u = 4^{n+1}p$ and $v$ is a positive odd integer, the largest one that keeps $\delta_n = u-qv^2$ positive. It leads
 to an efficient implementation where $L_n$ is computed iteratively as $n$ increases, rather than from scratch for each new $n$.



\subsection{Project and solution}\label{xzz4kj}

You need to be able to correctly compute a large number of binary digits of numbers such as $\sqrt{2}$. In short, you must work with
 exact arithmetic (or infinite precision). This is one of the big takeaways of this project. As previously stated, the goal is to assess whether
 the maximum run length in binary digits of $x_0=\sqrt{p/q}$, grows at the same speed as you would expect if the digits were random. We focus on runs of zeros only. A positive answer would be one more indication (when combined with many other tests) that these digits indeed behave like random bits, 
 and can be used to generate random sequences. Fast, secure random number generators based on quadratic irrationals are described in details in~\cite{vgchaos}.
%\pagebreak 

\begin{table}[H]
\small
\setlength\extrarowheight{-2pt}
\[
\begin{array}{rrrc}
\hline
 n & L_n & \log_2 n & L_n / \log_2 n \\
\hline
1	&	1	&	0.0000	&		\\
8	&	5	&	3.0000	&		\\
453	&	8	&	8.8234	&	0.9067	\\
1302	&	9	&	10.3465	&	0.8699	\\
\num{5334}	&	10	&	12.3810	&	0.8077	\\
\num{8881}	&	12	&	13.1165	&	0.9149	\\
\num{24001}	&	18	&	14.5508	&	1.2370	\\
\num{574130}	&	19	&	19.1310	&	0.9932	\\
\num{3333659}	&	20	&	21.6687	&	0.9230	\\
\num{4079881}	&	22	&	21.9601	&	1.0018	\\
\num{8356568}	&	23	&	22.9945	&	1.0002	\\
\num{76570752}	&	25	&	26.1903	&	0.9546	\\
\num{202460869}	&	26	&	27.5931	&	0.9423	\\
\num{457034355}	&	28	&	28.7677	&	0.9733	\\
\hline
\end{array}
\]
\caption{\label{tafghurds} Record runs of zeros in binary digits of $\sqrt{2}/2$}
\end{table}
%\renewcommand{\arraystretch}{1.0} %%%

\noindent The project consists of the following steps: \vspace{1ex}

\begin{itemize}
\item[] {\bf Step 1:  Computing binary digits of quadratic irrationals}. Let $x_0 = \sqrt{p/q}$. Here I assume that $p,q$ are positive coprime integers, and $p/q$ is not the square of
 a rational number. There are different ways to compute the binary digits of $x_0$, see~\cite{vgchaos}. I suggest to use the 
 \textcolor{index}{Gmpy2}\index{Python library!Gmpy2} Python library: it is written in C, thus very fast, and it offers a lot of functions to
 work with arbitrary large numbers. In particular, \texttt{gmpy2.isqrt(z).base(b)} returns the 
\textcolor{index}{integer square root}\index{integer square root} [\href{https://en.wikipedia.org/wiki/Integer_square_root}{Wiki}] of the integer \texttt{z}, in base \texttt{b}. 
Thus, to get the first $N$ binary digits of $x_0$,
 use $z = \lfloor 2^{2N} \cdot p/q \rfloor$ and $b=2$. The digits are stored as a string. \vspace{1ex}
\item[] {\bf Step 2: Computing run lengths}. Let $L_n$ be the length of the run of zeros starting at position $n$ in the binary expansion 
 of $x_0$. A more precise definition is offered in section~\ref{nntserde}. If there is no such run starting at $n$, set $L_n=0$.
Once the digits have been computed, it is very easy to obtain the run lengths: see Table~\ref{tafghur}, corresponding to $p=1, q=2$.
However, I suggest using Formula~(\ref{eqstr4nzs}) to compute $L_n$. Not only it shows that the formula is correct, but it also offers an
 alternative method not based on the binary digits, thus also confirming that the binary digits are correctly computed. The end result should
 be an extension of Table~\ref{tafghur}, ignoring the columns labeled as $s_n$ for now. Try with the first $N=10^6$ digits,
with $p=1$ and $q=2$.  

\vspace{1ex}
\item[]{\bf Step 3: Maximum run lengths}. Use a fast technique to compute the successive 
\textcolor{index}{records}\index{records (statistical distribution)} for $L_n$, for the first $N=10^9$ binary digits
 of $\sqrt{2}/2$. Is it reasonable to conjecture that asymptotically, $L_n/\log_2 n$ is smaller than or  equal to $1$? In other words, could
  we have $\lim\sup L_n/\log_2 n = 1$? See [\href{https://en.wikipedia.org/wiki/Limit_inferior_and_limit_superior}{Wiki}] for the definition 
of \textcolor{index}{lim sup}\index{lim sup}. Then instead of using the binary digits of some number, do the same test for random bits. Is the  behavior similar? Do we get the same upper bound for the record run lengths? The records for $L_n$, and when their occur (location $n$) in the
 binary digit expansion, are displayed in Table~\ref{tafghurds}. You should get the same values.
\end{itemize}
\vspace{1ex}

\noindent A fast solution to compute both the digits and the run lengths is to use the Gmpy2 library, as illustrated in the first code snippet below
 (also available on GitHub, \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/max_runs_fast.py}{here}).
The output is Table~\ref{tafghurds}. The conclusion is that indeed, $\lambda_n = L_n / \log_2 n$ seems to be bounded by $1$, at 
least on average as $n$ increases. It does not mean that $\lim \sup \lambda_n = 1$. 
For instance, if $S_n$ is the number of zeros in the first $n$ digits, assuming the digits are random, then the ratio 
 $\rho_n = |S_n - n/2|/\sqrt{n}$ seems bounded. However $\lim \sup \rho_n = \infty$. To get a strictly positive, finite constant
 for $\lim \sup$, you need to further divide $\rho_n$ by $\sqrt{\log \log n}$. This is a consequence
 of \textcolor{index}{the law of the iterated logarithm}\index{law of the iterated logarithm} [\href{https://en.wikipedia.org/wiki/Law_of_the_iterated_logarithm}{Wiki}]. More details are available in my book on dynamical systems~\cite{vgchaos}.  
For $\lambda_n$ applied to random binary digits, see \href{https://math.stackexchange.com/questions/1409372/what-is-the-expected-length-of-the-largest-run-of-heads-if-we-make-1-000-flips}{here}.
This completes \textcolor{red}{Step 1} and \textcolor{red}{Step 3}.
\vspace{1ex}


\begin{lstlisting}
import gmpy2

p = 1 
q = 2 
N = 1000000000 # precision, in number of binary digits 

# compute and store in bsqrt (a string) the N first binary digits of sqrt(p/q)
base = 2
bsqrt = gmpy2.isqrt( (2**(2*N) * p) // q ).digits(base) 

last_digit = -1
L = 0
max_run = 0

for n in range(0, N):
    d = int(bsqrt[n])  # binary digit
    if d == 0:
        L += 1
    if d == 1 and last_digit == 0:
            run = L 
            if run > max_run:
                max_run = run
                print(n-L, run, max_run)
            L = 0
    last_digit = d
\end{lstlisting}

\noindent At the bottom of this section, I share my Python code for \textcolor{red}{Step 2}, with the implementation of formula~(\ref{eqstr4nzs}) to compute $L_n$. The results are in Table~\ref{tafghur}, and compatible with those obtained in \textcolor{red}{Step 1} and displayed in Table~\ref{tafghurds}. The method based on formula~(\ref{eqstr4nzs}) is a lot slower. So why try it, you may ask? It is slower because Gmpy2 
 is implemented more efficiently, and closer to machine arithmetic. And the goal is different: formula~(\ref{eqstr4nzs})  allows you to 
  double check the earlier computations, using a method that does not require producing the binary digits to determine $L_n$. 






%%------------

%\renewcommand{\arraystretch}{0.99999} %%%
\begin{table}[H]
\small
\setlength\extrarowheight{-2pt}
\[
\begin{array}{rccc|rccc|rccc|rccc|rccc}
\hline
n	&  d_n & L_n & s_n & n	&  d_n & L_n & s_n & n	&  d_n & L_n & s_n & n	&  d_n & L_n & s_n & n	&  d_n & L_n & s_n\\ 
\hline
1	&	1	&	1	&	1	&	21	&	0	&		&	0	&	41	&	1	&		&	1	&	61	&	0	&		&	1	&	81	&	1	&	3	&	1	\\
2	&	0	&		&	0	&	22	&	0	&		&	1	&	42	&	1	&	1	&	1	&	62	&	1	&	3	&	2	&	82	&	0	&		&	0	\\
3	&	1	&		&	2	&	23	&	1	&		&	2	&	43	&	0	&		&	0	&	63	&	0	&		&	0	&	83	&	0	&		&	1	\\
4	&	1	&	1	&	1	&	24	&	1	&	2	&	1	&	44	&	1	&		&	2	&	64	&	0	&		&	1	&	84	&	0	&		&	1	\\
5	&	0	&		&	0	&	25	&	0	&		&	0	&	45	&	1	&		&	1	&	65	&	0	&		&	1	&	85	&	1	&	2	&	2	\\
6	&	1	&	1	&	2	&	26	&	0	&		&	1	&	46	&	1	&		&	1	&	66	&	1	&	1	&	2	&	86	&	0	&		&	0	\\
7	&	0	&		&	0	&	27	&	1	&		&	2	&	47	&	1	&	2	&	1	&	67	&	0	&		&	0	&	87	&	0	&		&	1	\\
8	&	1	&	5	&	2	&	28	&	1	&	2	&	1	&	48	&	0	&		&	0	&	68	&	1	&		&	2	&	88	&	1	&		&	2	\\
9	&	0	&		&	0	&	29	&	0	&		&	0	&	49	&	0	&		&	1	&	69	&	1	&	2	&	1	&	89	&	1	&	1	&	1	\\
10	&	0	&		&	1	&	30	&	0	&		&	1	&	50	&	1	&		&	2	&	70	&	0	&		&	0	&	90	&	0	&		&	0	\\
11	&	0	&		&	1	&	31	&	1	&		&	2	&	51	&	1	&	2	&	1	&	71	&	0	&		&	1	&	91	&	1	&		&	2	\\
12	&	0	&		&	1	&	32	&	1	&		&	1	&	52	&	0	&		&	0	&	72	&	1	&	1	&	2	&	92	&	1	&	2	&	1	\\
13	&	0	&		&	1	&	33	&	1	&		&	1	&	53	&	0	&		&	1	&	73	&	0	&		&	0	&	93	&	0	&		&	0	\\
14	&	1	&	2	&	2	&	34	&	1	&		&	1	&	54	&	1	&	2	&	2	&	74	&	1	&		&	2	&	94	&	0	&		&	1	\\
15	&	0	&		&	0	&	35	&	1	&		&	1	&	55	&	0	&		&	0	&	75	&	1	&		&	1	&	95	&	1	&		&	2	\\
16	&	0	&		&	1	&	36	&	1	&		&	1	&	56	&	0	&		&	1	&	76	&	1	&		&	1	&	96	&	1	&	1	&	1	\\
17	&	1	&		&	2	&	37	&	1	&	2	&	1	&	57	&	1	&	4	&	2	&	77	&	1	&		&	1	&	97	&	0	&		&	0	\\
18	&	1	&		&	1	&	38	&	0	&		&	0	&	58	&	0	&		&	0	&	78	&	1	&	1	&	1	&	98	&	1	&		&	2	\\
19	&	1	&		&	1	&	39	&	0	&		&	1	&	59	&	0	&		&	1	&	79	&	0	&		&	0	&	99	&	1	&		&	1	\\
20	&	1	&	2	&	1	&	40	&	1	&		&	2	&	60	&	0	&		&	1	&	80	&	1	&		&	2	&	100	&	1	&	1	&	1	\\
\hline
\end{array}
\]
\caption{\label{tafghur} Binary digit $d_n$, run length $L_n$ for zeros, and steps $s_n$ at position $n$, for $\sqrt{2}/2$}
\end{table}
%\renewcommand{\arraystretch}{1.0} %%%




\noindent Most
 importantly, the slow method is valuable because it is the first step to make progress towards a better (smaller) upper bound than
 that featured in section~\ref{nntserde}. To get a much stronger bound for the run lengths $L_n$,
 one has to investigate $\delta_n$, denoted as \texttt{delta} in the code below. The code is also
on GitHub, \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/max_runs_slow.py}{here}.
Note that the variable \texttt{steps} can only take on three values: 0, 1, and 2. It is represented as $s_n$ in Table~\ref{tafghur}. Improving
 the asymptotic upper bound $L_n/n \leq 1$ in~(\ref{eqstr4nzs}) as $n\rightarrow\infty$, is incredibly hard. I spent a considerable amount of time to non avail, even though anyone who spends a small amount of time on this problem will be convinced that  asymptotically, $L_n/\log_2 n \leq 1$
 as $n\rightarrow\infty$, a much stronger result.
 Proving the stronger bound, even though verified in Table~\ref{tafghurds} for $n$ up to $10^9$,  is beyond the capabilities of the mathematical tools currently available. It may as well not be true or undecidable, nobody knows.
 \vspace{1ex}


%-----
%Add theorem to Chaos book, in appendix


\begin{lstlisting}
import math
import gmpy2

# requirement: 0 < p < q
p = 1 
q = 2 
x0 = math.sqrt(p/q)
N = 1000 # precision, in number of binary digits for x0 

# compute and store in bsqrt (a string) the N first binary digits of x0 = sqrt(p/q)
base = 2
bsqrt = gmpy2.isqrt( (2**(2*N) * p) // q ).digits(base) 

for n in range(1, N):

    if n == 1:
        u = p * 4**n  
        v = int(x0 * 4**n) 
        if v % 2 == 0:
            v = v - 1
    else: 
        u = 4*u
        v = 2*v + 1
    steps = 0
    while q*v*v < u:
        v = v + 2
        steps += 1   # steps is always 0, 1, or 2
    v = v - 2
    delta = u - q*v*v 
    d = bsqrt[n-1]    # binary digit of x0 = sqrt(p/q), in position n 
    
    ## delta2 = delta >> (n - 1)
    ## res = 5/2 + n - math.log(delta,2) - math.log(n, 2)

    run = int(n + 1 + math.log(p*q, 2)/2 - math.log(delta, 2) ) 
    if d == "0" or run == 0:
        run = "" 

    print("%6d %1s %2s %1d" % (n, d, str(run), steps))
\end{lstlisting}


\section{Quantum derivatives, GenAI, and the Riemann Hypothesis}

If you are wondering how close we are to proving the 
\textcolor{index}{Generalized Riemann Hypothesis}\index{Riemann Hypothesis} (GRH), you should read on. The purpose of this project is to uncover 
intriguing patterns
 in prime numbers, and gain new insights on the GRH. I stripped off all the unnecessary math, focusing on 
 the depth and implications of the material discussed here.  You will also learn to use the remarkable 
\textcolor{index}{MPmath}\index{Python library!MPmath} library for scientific computing. This is a cool project for people who love math, 
 with the opportunity to work on state-of-the-art research even if you don't have a PhD in number theory. 

Many of my discoveries
 were made possible thanks to pattern detection algorithms (in short, AI) before leading to actual proofs, or disproofs. This 
 data-driven, bottom-up approach is known as
 \textcolor{index}{experimental math}\index{experimental math}. It contrasts with the top-down, classic theoretical academic framework.
 The potential of AI and its computing power should not be underestimated to make progress on the most difficult mathematical problems. It offers a big competitive advantage over professional mathematicians focusing on theory exclusively. 

My approach is unusual as it is based on the \textcolor{index}{Euler product}\index{Euler product}. The benefit is that you immediately know when the target function, 
say the \textcolor{index}{Riemann zeta function}\index{Riemann zeta function} $\zeta(s)$, has a root or not, wherever the product converges. 
 Also, these products represent \textcolor{index}{analytic function}\index{analytic functions} [\href{https://en.wikipedia.org/wiki/Analytic_function}{Wiki}]  
 wherever they converge. 

I use the standard notation in the complex plane:
 $s = \sigma + it$, where $\sigma, t$ are respectively the real and imaginary parts. I focus on the real part only (thus $t=0$) because of the 
 following result: if for some $s=\sigma_0$, the product converges, then it converges for all $s = \sigma + it$ with $\sigma > \sigma_0$.
 Now let's define the Euler product. The finite version with $n$ factors is a function of $s$, namely
$$f(s, n) = \prod_{p\in P_n} \Bigg(1 - \frac{\chi(p)}{p^s}\Bigg)^{-1} = \prod_{k=1}^n \Bigg(1 - \frac{\chi(p_k)}{p_k^s}\Bigg)^{-1}. $$
Here $P_n = \{2, 3, 5, 7, 11,\dots\}$ is the set of the first $n$ prime numbers, and $p_k$ denotes the $k$-th prime with $p_1 = 2$.
The function $\chi(p)$ can take on three vales only: $0, -1, +1$. This is not the most generic form, but the one 
 that I will be working with in this section. More general versions are investigated in chapter 17, in~\cite{vgelsevier}.
Of course, we are interested in the case $n\rightarrow\infty$, where convergence becomes the critical issue. Three particular cases are:
\vspace{1ex}

\begin{itemize} 
\item Rienmann zeta, denoted as $\zeta(s, n)$ or $\zeta(s)$ when $n=\infty$. In this case $\chi(p)= 1$ for all primes $p$. The resulting
 product converges only if $\sigma > 1$. Again, $\sigma$ is the real part of $s$.
\item \textcolor{index}{Dirichlet $L$-function}\index{Dirichlet $L$-function}  $L_4(s,n)$ 
 [\href{https://en.wikipedia.org/wiki/Dirichlet_L-function}{Wiki}] with 
\textcolor{index}{Dirichlet modular character}\index{Dirichlet character} $\chi=\chi_4$ [\href{https://en.wikipedia.org/wiki/Dirichlet_character}{Wiki}]. Denoted as $L_4(s)$ when $n=\infty$. Here $\chi_4(2) = 0$, $\chi_4(p)= 1$ if $p-1$ is a multiple of $4$, and $\chi_4(p)=-1$ otherwise.
The product is absolutely convergent if $\sigma>1$, but convergence status is unknown if $\frac{1}{2} < \sigma \leq 1$.
\item Unnamed function $Q_2(s, n)$, denoted as $Q_2(s)$ when $n=\infty$.
 Here $\chi(2)=0$. Otherwise, $\chi(p_k) = 1 $ if $k$ is even, and $\chi(p_k)=-1$ if $k$ is odd. 
Again, $p_k$ is the $k$-th prime with $p_1=2$. The product is absolutely convergent if $\sigma>1$,
 and \textcolor{index}{conditionally convergent}\index{conditional convergence} [\href{https://en.wikipedia.org/wiki/Conditional_convergence}{Wiki}] if $\frac{1}{2} < \sigma \leq 1$.
\end{itemize} \vspace{1ex}
All these products can be expanded into \textcolor{index}{Dirichlet series}\index{Dirichlet series} [\href{https://en.wikipedia.org/wiki/Dirichlet_series}{Wiki}], and the corresponding $\chi$ 
 expanded into
 \textcolor{index}{multiplicative functions} [\href{https://en.wikipedia.org/wiki/Completely_multiplicative_function}{Wiki}] over all positive integers.
 Also, by construction, Euler products have no zero in their conditional and absolute convergence domains.
Most mathematicians believe that the Euler product for $L_4(s)$ conditionally converges when $\frac{1}{2}<\sigma \leq 1$.
 Proving it would be a massive accomplishment. This would be make $L_4$ the first example of a function satisfying all the requirements of the Generalized Riemann Hypothesis.
The Unnamed function $Q_2$ actually achieves this goal, with the exception that its associated $\chi$ is not periodic. Thus,
 $Q_2$ lacks some of the requirements.  The Dirichlet series associated to $Q_2$ (the product expansion as a series) is known to convergence and
 thus equal to the product if $\sigma > \frac{1}{2}$.  

The rest of the discussion is about building the framework to help solve this centuries-old problem. 
It can probably be generalized to $L$-functions other than $L_4$, with one notable exception: the Riemann function itself, which was the
 one that jump-started all this vast and beautiful mathematical theory.

\subsection{Cornerstone result to bypass the roadblocks} \label{raduse}%prime number distribution}

The goal here is to prove that the Euler product $L_4(s,n)$ converges to some constant $c(s)$ as $n\rightarrow\infty$, for some $s = \sigma_0 + it$, with $t=0$ and some $\sigma_0 < 1$. In turns, it implies that it converges at $s=\sigma +it$, for all $t$ and for all $\sigma > \sigma_0$.
 It also implies that $c(s) = L_4(s)$, the true value obtained by \textcolor{index}{analytic continuation}\index{analytic continuation} [\href{https://en.wikipedia.org/wiki/Analytic_continuation}{Wiki}]. Finally, 
 it implies that $L_4(s)$ has no zero if $\sigma > \sigma_0$. This would provide a partial solution to
 the Generalized Riemann Hypothesis, for $L_4$ rather than the Riemann zeta function $\zeta(s)$, and not with
 $\sigma_0 = \frac{1}{2}$ (the conjectured lower bound), but a least for some $\sigma_0<1$. This is enough to make countless mathematical 
theorems true, rather than ``true conditionally on the fact that the Riemann Hypothesis is true". It also leads to much more precise 
 results regarding the distribution of primes numbers: results that to this day, are only conjectures. The implications are numerous, 
 well beyond number theory.

The chain of implications that I just mentioned, follows mainly from expanding the Euler product into a Dirichlet-$L$ series. In this case, the expansion is as follows, with $s=\sigma+it$ as usual:
\begin{equation}
\prod_{k=1}^\infty \Bigg(1 - \frac{\chi_4(p_k)}{p_k^s}\Bigg)^{-1} =\sum_{k=0}^\infty \frac{(-1)^{k+1}}{(2k+1)^s}. \label{epdll4}
\end{equation}
The series obviously converges when $\sigma>0$. The product converges for sure when $\sigma > 1$. It is believed that it converges as well 
 when $\sigma > \frac{1}{2}$. The goal here is to establish that it converges when $\sigma > \sigma_0$, for some $\frac{1}{2} < \sigma_0 < 1$. When both converge, they converge to the same value, namely $L_4(s)$ as the series is the analytic continuation of the product, for all $\sigma>0$.
 And of course, the product can not be zero when it converges. Thus $L_4(s) \neq 0$ if $\sigma > \sigma_0$.

The big question is how to find a suitable $\sigma_0$, and show that it must be strictly smaller than $1$. I now focus on 
 this point, leading to some unknown $\sigma_0$, very likely in the range $0.85 <\sigma_0 < 0.95$, for a number of reasons.
 The first step is to approximate the Euler product $L_4(s,n)$ with spectacular accuracy around $\sigma=0.90$,  using statistical techniques and a simple formula. This approximation amounts to denoising the irregularities caused by the prime number distribution,
 including \textcolor{index}{Chebyshev's bias}\index{Chebyshev's bias} [\href{https://en.wikipedia.org/wiki/Chebyshev\%27s_bias}{Wiki}]. 
After this step, the remaining is standard real analysis, trying to establish a new generic asymptotic result for a specific class 
 of functions, and assuring that it encompasses our framework. The new theorem~\ref{5rhgf} in question, albeit independent from
 number theory, has yet to be precisely stated, let alone proved.  The current version is as follows:\vspace{1ex}

\begin{theorem}\label{5rhgf}
Let $A_n = \{a_1, \dots, a_n\}$ and $B_n = \{b_1,\dots, b_n\}$ be two finite sequences of real numbers, with $a_n\rightarrow 0$ as $n\rightarrow\infty$. Also assume that
 $b_{n+1}-b_n \rightarrow 0$. Now, define $\rho_n$ as the ratio of the standard deviations, respectively computed on $A_n$  (numerator) and $B_n$ (denominator). If $\rho_n$ converges to a non-zero value as $n\rightarrow\infty$, then 
$b_n$ also converges.
\end{theorem}\vspace{1ex}

\noindent The issue to finalize the theorem is to make sure that it is applicable in our context, and add any additional requirements needed (if any). Is it enough to require $\inf \rho_n > 0$
 and $\sup \rho_n < \infty$, rather than the convergence of $\rho_n$ to non-zero? A stronger version, assuming  $\sqrt{n}\cdot a_n$  
 is bounded and $\lim\inf \rho_n = \rho > 0$, leads to
\begin{equation}
\rho b_n - a_n \sim c + \frac{\alpha}{\sqrt{n}} +\frac{\beta}{\sqrt{n\log n}} +\cdots   \label{yuietgv}
\end{equation}
 where $c, \alpha, \beta$ are constants. As a result,  $b_n \rightarrow c/\rho$. For the term $\beta/\sqrt{n\log n}$ to be valid, additional conditions
  on the asymptotic behavior of $a_n$ and $b_n$ may be required. Note that $a_n$ and $\alpha/\sqrt{n}$ have the same order of magnitude. As we shall see, $a_n$ captures most of the chaotic part of $L_4(s,n)$, while the term 
$\beta/\sqrt{n\log n}$ significant improves the approximation. 

The following fact is at the very core of the GRH proof that I have in mind. Let us assume that $b_n$ depends 
{\em continuously} on some parameter $\sigma$.
 If $\rho_n\rightarrow 0$ when $\sigma = \sigma_1$, and $\rho_n\rightarrow \infty$ when $\sigma = \sigma_2$,
 then there most be some $\sigma_0$ 
 with $\sigma_1 \leq \sigma_0\leq \sigma_2$ such that $\rho_n$ converges to non-zero, or at least $\lim\sup \rho_n<\infty$ and $\lim\inf \rho_n>0$ when $\sigma=\sigma_0$. 
 This in turn allows us to use the proposed theoretical framework (results such as theorem~\ref{5rhgf}) to prove the
 convergence of $L_4(s, n)$ at $\sigma = \sigma_0$. The challenge in our case is to 
 show that there is such a $\sigma_0$,  satisfying $\sigma_0< 1$.  However, the difficulty is not caused by crossing the line $\sigma=1$, and thus unrelated to the prime number distribution. Indeed, most of the interesting action -- including crossing our red line -- takes place around 
$\sigma=0.90$. Thus the problem now appears to be generic,  rather than specific to GRH.

Now I establish the connection to the convergence of the Euler product $L_4(s, n)$. First, I introduce two new functions: 
\begin{equation}
\delta_n(s) = L_4(s,n)-L_4(s), \quad \Lambda_n = \frac{1}{\varphi(n)} \sum_{k=1}^n \chi_4(p_k),\label{8c3jfcd}
\end{equation}
 with  $\varphi(n) = n$ for $ n = 2, 3, 4$ and so on. An important requirement is that $\Lambda(n) \rightarrow 0$.  I also tested $\varphi(n) = n\log n$. Then, in formula~(\ref{yuietgv}), I use the following:
\begin{equation}
a_n = \Lambda_n, \quad b_n = \delta_n(s).\label{oi7xc}
\end{equation}
Here, $L_4(s)$ is obtained via analytic continuation, not as the limit of the Euler product $L_4(s, n)$. The reason is because
 we don't know if the Euler product converges if $\sigma <1$, although all evidence suggests that this is the case. 
 Convergence of $\delta_n(s)$ translates to $c=0$ in formula~(\ref{yuietgv}). Finally, in the figures, the X-axis represents $n$.

\subsection{Quantum derivative of functions nowhere differentiable}

Discrete functions such as $L_4(s,n)$, when $s$ is fixed and $n$ is the variable, can be scaled to represent a continuous function. 
 The same principle is used to transform a \textcolor{index}{random walk}\index{random walk} into a 
 \textcolor{index}{Brownian motion}\index{Brownian motion}, as discussed in section 1.3 in my book 
 on chaos and dynamical systems~\cite{vgchaos}, and pictured in Figure~\ref{fig:lollog1xx}. This is true whether the function is deterministic or random. 
In this context, $n$ represents the time.

In general, the resulting function is nowhere differentiable. You can use the integrated function rather than the original one to study the properties,
 as integration turns chaos into a smooth curve. But what if we could use the derivative instead? The derivative is even more chaotic than the
 original function, indeed it does not even exist! Yet there is a way to define the derivative and make it particularly useful to discover
 new insights about the function of interest. This new object called \textcolor{index}{quantum derivative}\index{quantum derivative} is not a function, but rather a set of points with a specific shape, boundary, and configuration. 
 In some cases, it may consist of multiple curves, or a dense set with homogeneous or non-homogeneous point density.  Two chaotic functions that look identical to
 the naked eye may have different quantum derivatives. 



The goal here is not to formally define the concept of quantum derivative, but to show its potential. For instance, in section 
 3.3.2.1 of the same book~\cite{vgchaos}, I compute the moments of a cumulative distribution function (CDF) that is nowhere differentiable. For that purpose, I use
 the density function (PDF), which of course is nowhere defined. Yet, I get the correct values. While transparent to the reader, I implicitly 
 integrated weighted 
 quantum derivatives of the CDF. In short, the quantum derivative of a discrete function $f(n)$ is based on $f(n) - f(n-1)$. 
 If the time-continuous (scaled) version of $f$ is continuous, then the quantum derivative corresponds to the standard derivative. Otherwise, it takes 
 on multiple values, called \textcolor{index}{quantum states}\index{quantum state} [\href{https://en.wikipedia.org/wiki/Quantum_state}{Wiki}] in quantum physics. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.84\textwidth]{quantum.png}  
\caption{Two shifted legs of $\delta_n$ (left), and their quantum derivatives (right) [$\sigma=0.90$]}
\label{fig:lollod34sd}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

Now in our context, in Figure~\ref{fig:lollod34sd}, I show two legs of $\delta_n(s)$: one where $\chi_4(p_n) =+1$, and
 the other one where $\chi_4(p_n) =-1$. Both give rise to time-continuous functions that are nowhere differentiable,
 like the Brownian motions in Figure~\ref{fig:lollog1xx}. Unlike Brownian motions, the variance tends to zero over time. 
The two functions are almost indistinguishable to the naked eye, so I separated them on the left plot in Figure~\ref{fig:lollod34sd}. The corresponding quantum derivatives consist of a set of curves (right plot, same figure). They contain a lot of useful information about $L_4(s)$. In particular:
\vspace{1ex}

\begin{itemize}
\item The left plot in Figure~\ref{fig:lollod34sd} shows an asymmetrical distribution of the quantum derivatives around the X-axis. 
 This is caused by the \textcolor{index}{Chebyshev bias}\index{Chebyshev's bias}, 
also called \textcolor{index}{prime race}\index{prime race}: among the first $n$ primes numbers, the difference between the  proportion of primes $p_k$ with $\chi_4(p_k)=+1$, and those with $\chi_4(p_k)=-1$, is of the order $1/\sqrt{n}$, in favor of the latter. 
See~\cite{primerace, primerace3, primerace4}. This is known as \textcolor{index}{Littlewood's oscillation theorem}\index{Littlewood's oscillation theorem}~\cite{primerace2}. 
\item The various branches in the quantum derivative (same plot) correspond to runs of different lengths in the sequence $\{\chi_4(p_n)\}$: shown as positive or negative depending on the sign of $\chi_4(p_n)$. Each branch has it own point density, asymptotically equal to $2^{-\lambda}$ (a geometric distribution) for the
 branch featuring runs of length $\lambda$, for $\lambda=1,2$ and so on. A similar number theory problem with the distribution of run lengths 
 is discussed in section~\ref{sqrt2run}, for the binary digits of $\sqrt{2}$.
\end{itemize}
\vspace{1ex}

\subsection{Project and solution}\label{tuasbu5}
The project consists of checking many of the statements made in section~\ref{raduse}, via computations. Proving the empirical results is beyond the scope of this work. 
 The computations cover not only the case $L_4(s,n)$, but also other similar functions, including synthetic ones
 and \textcolor{index}{Rademacher random multiplicative functions}
\index{Rademacher function (random)}
\index{multiplicative function (random)}~\cite{harper2020bb, harper2020, RH1002}. It also includes an empirical verification of theorem~\ref{5rhgf}, and assessing whether its converse might be true. Finally, you will try different functions $\varphi$ in formula~(\ref{8c3jfcd}) to check the
 impact on approximation~(\ref{yuietgv}).  In the process, you will get
 familiar with a few Python libraries: \vspace{1ex}
\begin{itemize}
\item \textcolor{index}{MPmath}\index{Python library!MPmath} to compute $L_4(s)$ for complex arguments when $\sigma<1$. 
\item \textcolor{index}{Primepy}\index{Python library!PrimePy} to obtain a large list of prime numbers.
\item \textcolor{index}{Scipy}\index{Python library!Scipy} for curve fitting, when verifying the approximation~(\ref{yuietgv}).
\end{itemize}
\vspace{1ex}
The curve fitting step is considerably more elaborate than the standard implementation in typical machine learning projects. 
First, it consists of finding the best $c, \alpha, \beta$ in~(\ref{yuietgv}) for a fixed $n$, and identifying the best model: in this case,
 the choice of $\sqrt{n}$ and $\sqrt{n\log n}$ for the curve fitting function (best fit). Then, using different $n$, assess whether or not
 $c, \alpha, \beta, \rho$ depend on $n$ or not, and whether $c=0$ (this would suggest that the Euler product converges).

Finally, you will run the same curve fitting model for random functions, replacing the sequence $\{\chi_4(p_n)\}$ by 
 random sequences of independent $+1$ and $-1$ evenly distributed, to mimic the behavior of $L_4(s, n)$ and $\Lambda_n$. One would expect a better fit when the functions are perfectly random, yet the opposite is true. A possible explanation is the fact that the 
\textcolor{index}{Chebyshev bias}\index{Chebyshev's bias} in $L_4(n, s)$
 is very well taken care of by the choice of $\Lambda_n$, while for random functions, there is no such bias, and thus no correction.

%---


%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{linear.png}  
\caption{Integrated Brownian (top left), Brownian (top right) and quantum derivatives (bottom)}
\label{fig:lollog1xx}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------




%xxx images 1) from book 2) differentiated L_4 and 3) original L4 // ref to Python code







\noindent The project consists of the following steps: \vspace{1ex}

\begin{itemize}
\item[] {\bf Step 1:  MPmath library, complex and prime numbers}. Compute $L_4(s)$ using the MPmath library. See my code in 
section~\ref{l4code}. The code is designed to handle complex numbers, even though in the end I only use the real part. Learn how to manipulate 
 complex numbers by playing with the code. Also, see how to create a large list of prime numbers: look at how I use the PrimePy library. 
Finally, look at the Python \texttt{curve\_fitting} and \texttt{r2\_score} functions, to understand how it works, as you will have to use them. 
 The former is from the Scipy library, and the latter (to measure \textcolor{index}{goodness-of-fit}\index{goodness-of-fit}) is from the Sklearn library.
\vspace{1ex}
\item[] {\bf Step 2:  Curve fitting, part A}. Implement $\Lambda_n$ and the Euler Product $L_4(s, n)$ with $n$ (the number of factors) up to
 $10^5$.  My code runs in a few seconds for $n\leq 10^5$. Beyond
  $n=10^7$, a distributed architecture may help. In the code, you specify $m$ rather than $n$, and $p_n \approx m/\log m$ is the largest prime smaller than $m$. For $s = \sigma + it$, choose $t=0$, thus avoiding complex numbers, and various values of $\sigma$ ranging from $0.55$ to $1.75$. The main step is the \textcolor{index}{curve fitting}\index{curve fitting} procedure, similar to a linear regression but for non linear functions. 
The goal is to approximate $\delta_n(s)$, focusing for now on $\sigma = 0.90$. \vspace{1ex} 

The plots of $\delta_n(s)$ and $\Lambda_n$ at the top in Figure~\ref{fig:lollog1xx}, with $n = \num{80000}$, look very similar. 
It seems like there must be some $c_n(s)$ and a strictly positive 
$\rho_n(s)$  such that
 $\rho_n(s) \delta_k(s) - \Lambda_k\approx c_n(s)$ for $k=1,2,\dots, n$. Indeed, the \textcolor{index}{scaling factor}\index{scaling factor}  
$$
\rho_n(s) = \frac{\text{Stdev}[\Lambda_1,\dots,\Lambda_n]}{\text{Stdev}[\delta_1(s),\dots,\delta_n(s)]},
$$
 together with $c_n(s)=0$, 
work remarkably well. 
The next step is to refine the linear approximation based on $\rho=\rho_n(s)$,  
 using~(\ref{yuietgv}) combined with~(\ref{oi7xc}). This is where the curve fitting takes place; the parameters to estimate are $c, \alpha$ and $\beta$, with $c$ close to zero. 
You can check the spectacular result of the fit, here with $\sigma=0.90$ and $n\approx 1.25\times 10^6$, on the bottom left plot in Figure~\ref{fig:lollog1xx}. Dropping the $\sqrt{n\log n}$ term in~(\ref{yuietgv}) results in a noticeable drop in performance (test it).
For $\rho_n(s)$, also try different options.
\vspace{1ex}
\item[] {\bf Step 3:  Curve fitting, part B}. Perform the same curve fitting as in Step 2, but this time for different values of $n$. Keep
 $s = \sigma +it$ with $t=0$ and $\sigma = 0.90$. The results should be identical to those in Table~\ref{tafghuh42s}, where $\gamma_n=\sqrt{n}\cdot\Lambda_n$. The coefficients $c,\alpha,\beta$ and $R^2$ (the \textcolor{index}{R-squared}\index{R-squared} or quality of the fit)
 depend on $n$ and $s$, explaining the notation in the table. Does $\rho_n(s)$ tend to a constant depending only on $s$, as $n\rightarrow\infty$? Or does it stay bounded? What about the other coefficients?
\vspace{1ex}

Now do the same with $\sigma= 0.70$ and $\sigma=1.10$, again with various values of $n$. Based on your computations, do you think that $\rho_n(s)$ decreases to zero, stays flat, or increases to infinity, depending on whether $s=0.70$, $s=0.90$ or $s=1.10$? If true, what are the potential implications?

\vspace{1ex}
\item[] {\bf Step 4: Comparison with synthetic functions}.  First, try $\varphi(n) = n\log n$ rather $\varphi(n) = n$, 
 in~(\ref{8c3jfcd}). Show that the resulting curve fitting is not as good. Then, replace $\chi_4(p_k)$, both
 in $L_4(s,n)$ and $\Lambda_n$, by independent  
\textcolor{index}{Rademacher distributions}\index{Rademacher distribution} 
 [\href{https://en.wikipedia.org/wiki/Rademacher_distribution}{Wiki}], taking the values $+1$ and $-1$ with the same probability $\frac{1}{2}$.
Show that again, the curve fitting is not as good, especially if $n\leq 10^5$. 
Then, you may even replace $p_k$ (the $k$-th prime) by $k\log k$.
The goal of  these substitutions is to compare the results
 when $\chi_4$ is replaced by \textcolor{index}{synthetic functions}\index{synthetic function} that mimic the behavior of the  Dirichlet character modulo 4. Also, you want to assess how much leeway you have in the choice of these functions, for the conclusions to stay valid.
\vspace{1ex}

The use of synthetic functions is part of a general approach known as \textcolor{index}{generative AI}\index{generative AI (GenAI)}. If all
 the results remain valid for such synthetic functions, then the theory developed so far is not dependent on special properties of prime numbers: we isolated
 that problem, opening the path to an easier proof that the Euler product $L_4(s, n)$ converges to $L_4(s)$ at some location 
 $s = \sigma_0+ it$ with $\sigma_0 < 1$ inside the critical strip. 

\vspace{1ex}
\item[] {\bf Step 5: Application outside number theory}. Using various  pairs of sequences $\{a_n\}$, $\{b_n\}$, empirically verify when the statistical theorem~\ref{5rhgf} might be correct, and when it might not. 

\vspace{1ex}
\end{itemize}


\noindent The Python code in section~\ref{l4code} allows you to perform all the tasks except \textcolor{red}{Step 5}.
In particular, for \textcolor{red}{Step 4}, set \texttt{mode=\textquotesingle rn\textquotesingle} in the code. As for the curve fitting plot -- the bottom left plot in
 Figure~\ref{fig:lollog1xx} -- I multiplied both the target function $\delta_n(s)$ and the fitted curve by $\sqrt{n}$, here with $n = 1.25 \times 10^6$.
 Both tend to zero, but after multiplication by $\sqrt{n}$, they may or may not tend to a constant strictly above zero. Either way,
 it seems to indicate that the Euler product converges when $\sigma = 0.90$. What's more, the convergence looks strong, non-chaotic,
 and the second-order term involving $\sqrt{n\log n}$ in the approximation error, seems to be correct.  





%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{L4fitting.png}  
\caption{Top: $\delta_n$ (left), $\Lambda_n$ (right); bottom: fitting $\delta_n$ (left), integrated $\delta_n$ (right) [$\sigma=0.90$]}
\label{fig:lollog1xx}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

Regarding \textcolor{red}{Step 3}, Table~\ref{tafghuh42s} is the answer when $\sigma = 0.90$. It seems to indicate that $\rho_n(s)$
 convergences (or is at least bounded and strictly above zero) when $\sigma = 0.90$ (remember that $s=\sigma +it$, with $t=0$). 
With $\sigma=0.70$, it seems that $\rho_n(s)$ decreases probably to zero, while with $\sigma = 1.10$, $\rho_n(s)$ is increasing without upper bound.
 The highest stability is around $\sigma = 0.90$. There, theorem~\ref{5rhgf} may apply, which would prove the convergence of the Euler product strictly
 inside to critical strip. As stated earlier, this would be a huge milestone if it can be proved, partially solving GRH not for $\zeta(s)$, but for the second most famous function of this nature, namely $L_4(s)$. By partial solution, I mean proving it for (say) $\sigma_0 =0.90 <1$, but  not
 yet for $\sigma_0 = \frac{1}{2}$.

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.78\textwidth]{l4rn.png}  
\caption{Same as Figure~\ref{fig:lollog1xx}, replacing $\delta_n(s),\Lambda_n$ by synthetic functions}
\label{fig:lollo562}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

 

\begin{table}[H]
\small
\setlength\extrarowheight{-1pt}
\[
\begin{array}{ccccccc}
\hline
 n & \gamma_n & c_n(s) & \alpha_n(s) & \beta_n(s) & \rho_n(s) & R^2 \\
\hline
\num{127040}	&	-0.27	&	0.00	&	0.41	&	0.94	&	5.122	& 	0.940	\\
\num{254101}	&	-0.40	&	0.00	&	0.39	&	0.98	&	5.082	& 	0.976	\\
\num{381161}	&	-0.35	&	0.00	&	0.37	&	1.03	&	5.151	& 	0.986	\\
\num{508222}	&	-0.44	&	0.00	&	0.36	&	1.05	&	5.149	& 	0.989	\\
\num{635283}	&	-0.32	&	0.00	&	0.36	&	1.04	&	5.085	& 	0.989	\\
\num{762343}	&	-0.22	&	0.00	&	0.36	&	1.03	&	5.047	& 	0.989	\\
\num{889404}	&	-0.10	&	0.00	&	0.35	&	1.06	&	5.123	& 	0.990	\\
\num{1016464}	&	-0.44	&	0.00	&	0.35	&	1.07	&	5.139	& 	0.990	\\
\num{1143525}	&	-0.30	&	0.00	&	0.34	&	1.08	&	5.121	& 	0.991	\\
\num{1270586}	&	-0.24	&	0.00	&	0.34	&	1.08	&	5.111	& 	0.991	\\
\hline
\end{array}
\]
\caption{\label{tafghuh42s}One curve fitting per row, for $\delta_n(s)$ with $\sigma=0.90$}
\end{table}
%\renewcommand{\arraystretch}{1.0} %%%

Unexpectedly, Figure~\ref{fig:lollo562} shows that the fit is not as good when using a random sequence of $+1$ and $-1$, evenly distributed, to
 replace and mimic $\chi_4$. The even distribution is required by the \textcolor{index}{Dirichlet theorem}\index{Dirichlet's theorem}, a
 generalization of the prime number theorem to 
  arithmetic progressions [\href{https://en.wikipedia.org/wiki/Dirichlet\%27s_theorem_on_arithmetic_progressions}{Wiki}]. 

Finally, see the short code below as the answer to \textcolor{red}{Step 5}. 
The code is also on GitHub, 
 \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/Dirichlet_L4_product_synthetic.py}{here}.
The parameters $p,q$ play a role similar to $\sigma$, 
 and \texttt{r} represents $\rho_n$ in theorem~\ref{5rhgf}. The coefficient $\rho_n$ may decrease to zero, increase to infinity, or converge
 depending on $p$ and $q$. Nevertheless, in most cases when $p, q$ are not too small, $b_n$ converges. Applied to $L_4(s,n)$, it means that convergence 
 may occur at $s$ even if $\rho_n(s)$ does not converge. 
 

The existence of some $\sigma_1$ for which $\rho_n(s)$ decreases to zero, 
  and some $\sigma_2$ for which $\rho_n(s)$ increases to infinity, implies that there must be 
 a $\sigma_0$ in the interval $[\sigma_1, \sigma_2]$,  for which $\rho_n(s)$ converges or is bounded. This in turn implies that the 
 Euler product $L_4(s, n)$ converges at $s=\sigma+it$ if $\sigma > \sigma_0$. The difficult step is to show that 
 the largest $\sigma_1$
  resulting in $\rho_n(s)$ decreasing to zero, is $<1$. Then, $\sigma_0<1$, concluding the proof.
\vspace{1ex}

\begin{lstlisting}
import numpy as np

N = 10000000
p = 1.00 
q = 0.90  
stdev = 0.50
seed = 564
np.random.seed(seed)
start = 20

u = 0
v = 0
a = np.zeros(N)
b = np.zeros(N)

for n in range(2, N):

    u += -0.5 + np.random.randint(0, 2) 
    v += np.random.normal(0, stdev)/n**q  
    a[n] = u / n**p 
    b[n] = v

    if n % 50000 == 0:
        sa = np.std(a[start:n])
        sb = np.std(b[start:n])
        r = sa / sb
        c = r * b[n] - a[n]
        print("n = %7d r =%8.5f an =%8.5f bn =%8.5f c =%8.5f sa =%8.5f sb=%8.5f" 
                  %(n, r, a[n], b[n], c, sa, sb)) 
    
\end{lstlisting}
\vspace{1ex}
%xxxyyy
%MLT article on percentile extrapolation


%when product oscillate use cesaro product [geometric mean]
%can not go to infinity ; by using 1/L4, can not go to 0 by applying result to 1/L4


%xxx
%add agglomerative process to stats book - collision graph
%gmpy2 long runs in quadratic irrationals: runs6b.py and maxrun.py

%link to scientific computing, collision graphs
%nice graph in LaTeX, nice one with star collisions
%agglomerative processes

\noindent {\bf Important note}. When dealing with the Euler product $L_4(s, n)$, the ratio $\rho_n(s)$ is rather stable (bounded strictly above zero, chaos-free, barely depending on $n$) and may even converge when $\sigma=0.90$ and $t=0$. Again, $s=\sigma+it$. Indeed, both the numerator and denominator appear well-behaved and seemingly chaos-free. Both of them tend to zero  
 as $n$ increases, at the same speed as $1/\sqrt{n}$. The chaos is in $L_4(s,n)$ and $\Lambda_n$. This fact can be leveraged to make 
 progress towards proving the convergence of $L_4(s,n)$ at $\sigma=0.90$. If not at $\sigma =0.90$, there has to be at least one value $\sigma_0 <1$ (close to $0.90$) for which everything I just wrote, apply.


\subsection{Python code}\label{l4code}

The implementation of the quantum derivatives is in section [4] in the following code. The coefficient
 $\rho_n(s)$ is denoted as \texttt{r}, while the parameter $\gamma$ in table~\ref{tafghuh42s} is denoted as \texttt{mu}.
In addition to addressing \textcolor{red}{Step 1} to \textcolor{red}{Step 4}, the computation of the Dirichlet-$L$ series  
 and the $Q_2$ function are in section [2.1]. For \textcolor{red}{Step 5}, see the Python code at the end of section~\ref{tuasbu5}.
Finally, to use synthetic functions rather than $\chi_4$, set \texttt{mode=\textquotesingle rn\textquotesingle}.
The code is also on GitHub, 
\href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/Dirichlet_L4_product_fitting.py}{here}.
\vspace{1ex}

\begin{lstlisting}[numbers=left]
# DirichletL4_EulerProduct.py
# On WolframAlpha: DirichletL[4,2,s], s = sigma + it
#     returns Dirichlet L-function with character modulo k and index j.
#
# References:
#     https://www.maths.nottingham.ac.uk/plp/pmzcw/download/fnt_chap4.pdf
#     https://mpmath.org/doc/current/functions/zeta.html
#     f(s) = dirichlet(s, [0, 1, 0, -1]) in MPmath

import matplotlib.pyplot as plt
import matplotlib as mpl
import mpmath
import numpy as np
from primePy import primes
from scipy.optimize import curve_fit
from sklearn.metrics import r2_score
import warnings
warnings.filterwarnings("ignore")

#--- [1] create tables of prime numbers

m = 1000000  # primes up to m included in Euler product
aprimes  = []  

for k in range(m):
    if k % 100000 == 0:
        print("Creating prime table up to p <=", k)
    if primes.check(k) and k > 2:
        aprimes.append(k)


#--- [2] Euler product

#--- [2.1] Main function

def L4_Euler_prod(mode = 'L4', sigma = 1.00, t = 0.00):

    L4 = mpmath.dirichlet(complex(sigma,t), [0, 1, 0, -1]) 
    print("\nMPmath lib.: L4(%8.5f + %8.5f i) = %8.5f + %8.5f i" 
          % (sigma, t,L4.real,L4.imag))

    prod = 1.0
    sum_chi4 = 0 
    sum_delta = 0
    run_chi4 = 0
    old_chi4 = 0
    DLseries = 0
    flag = 1

    aprod = [] 
    adelta = []
    asum_delta = []
    achi4 = []
    arun_chi4 = []
    asum_chi4 = []

    x1 = []
    x2 = []
    error1 = []
    error2 = []
    seed = 116  # try 103, 105, 116 & start = 2000 (for mode = 'rn')
    np.random.seed(seed)
    eps = 0.000000001

    for k in range(len(aprimes)): 
        
        if mode == 'L4':
            condition  = (aprimes[k] % 4 == 1)  
        elif mode == 'Q2':
            condition  = (k % 2 == 0)           
        elif mode == 'rn':
            condition = (np.random.uniform(0,1) < 0.5)
        
        if condition:   
            chi4 = 1
        else:
            chi4 = -1

        sum_chi4 += chi4 
        achi4.append(chi4)
        omega = 1.00 # try 1.00, sigma or 1.10
        # if omega > 1, asum_chi4[n] --> 0 as n --> infty
        # asum_chi4.append(sum_chi4/aprimes[k]**omega)
        asum_chi4.append(sum_chi4/(k+1)**omega) 
        # asum_chi4.append(sum_chi4/(k+1)*(np.log(k+2))) 

        if chi4 == old_chi4:
            run_chi4 += chi4
        else:
            run_chi4 = chi4
        old_chi4 = chi4
        arun_chi4.append(run_chi4) 
 
        factor = 1 - chi4 * mpmath.power(aprimes[k], -complex(sigma,t))
        prod *= factor
        aprod.append(1/prod)    

        term = mpmath.power(2*k+1, -complex(sigma,t))
        DLseries += flag*term
        flag = -flag

    limit = -eps + 1/prod   # full finite product (approx. of the limit)
    if mode == 'L4':
        limit = L4   # use exact value instead (infinite product if it converges)
  
    for k in range(len(aprimes)): 

        delta = (aprod[k] - limit).real  # use real part
        adelta.append(delta)
        sum_delta += delta
        asum_delta.append(sum_delta)
        chi4 = achi4[k]
       
        if chi4 == 1: 
            x1.append(k)
            error1.append(delta)
        elif chi4== -1: 
            x2.append(k)
            error2.append(delta)

    print("Dirichlet L: DL(%8.5f + %8.5f i) = %8.5f + %8.5f i" 
        % (sigma, t, DLseries.real, DLseries.imag))
    print("Euler Prod.: %s(%8.5f + %8.5f i) = %8.5f + %8.5f i\n" 
        % (mode, sigma, t, limit.real, limit.imag))

    adelta = np.array(adelta)
    aprod = np.array(aprod)
    asum_chi4 = np.array(asum_chi4)  
    asum_delta = np.array(asum_delta) 
    error1 = np.array(error1)
    error2 = np.array(error2)

    return(limit.real, x1, x2, error1, error2, aprod, adelta, asum_delta, 
             arun_chi4, asum_chi4)

#--- [2.2] Main part
    
mode = 'L4'  # options: 'L4', 'Q2', 'rn' (random chi4)
(prod, x1, x2, error1, error2, aprod, adelta, asum_delta, arun_chi4, 
     asum_chi4) = L4_Euler_prod(mode, sigma = 0.90, t = 0.00)


#--- [3] Plots (delta is Euler product, minus its limit)

mpl.rcParams['axes.linewidth'] = 0.3
plt.rcParams['xtick.labelsize'] = 7
plt.rcParams['ytick.labelsize'] = 7

#- [3.1] Plot delta and cumulated chi4

x = np.arange(0, len(aprod), 1)

# offset < len(aprimes), used to enhance visualizations 
offset = int(0.02 * len(aprimes))  

# y1 = aprod / prod
# plt.plot(x[offset:], y1[offset:], linewidth = 0.1)
# plt.show()

y2 = adelta 
plt.subplot(2,2,1)
plt.plot(x[offset:], y2[offset:], marker=',', markersize=0.1, 
   linestyle='None', c='red')

y3 = asum_chi4 
plt.subplot(2,2,2)
plt.plot(x[offset:], y3[offset:], marker=',', markersize=0.1, 
   linestyle='None', c='red')

#- [3.2] Denoising L4, curve fitting

def objective(x, a, b, c):

    # try c = 0 (actual limit)
    value = c + a/np.sqrt(x) +  b/np.sqrt(x*np.log(x)) 
    return value

def model_fit(x, y2, y3, start, offset, n_max):
   
    for k in range(n_max):

        n = int(len(y2) * (k + 1) / n_max) - start  
        stdn_y2 = np.std(y2[start:n])
        stdn_y3 = np.std(y3[start:n])
        r = stdn_y3 / stdn_y2
        
        # note: y3 / r ~ mu / sqrt(x)  [chaotic part]
        mu = y3[n] * np.sqrt(n)  # tend to a constant ?
        y4 = y2 * r - y3
        y4_fit = []
        err = -1

        if min(y4[start:]) > 0: 
            popt, pcov = curve_fit(objective, x[start:n], y4[start:n], 
                         p0=[1, 1, 0], maxfev=5000)
            [a, b, c] = popt
            y4_fit = objective(x, a, b, c)
            err = r2_score(y4[offset:], y4_fit[offset:])
            print("n = %7d  mu =%6.2f  c =%6.2f  a =%5.2f  b =%5.2f  r =%6.3f  err =%6.3f"
                        %(n, mu, c, a, b, r, err))

    return(y4, y4_fit, err, n)

n_max = 10   # testing n_max values of n, equally spaced
start = 20   # use Euler products with at least 'start' factors 
if mode == 'rn':
    start = 1000
if start > 0.5 * offset:
    print("Warning: 'start' reduced to 0.5 * offset")
    start = int(0.5 * offset)
(y4, y4_fit, err, n) = model_fit(x, y2, y3, start, offset, n_max)
ns = np.sqrt(n)

if err != -1:
    plt.subplot(2,2,3)
    plt.plot(x[offset:], ns*y4[offset:], marker=',', markersize=0.1, 
       linestyle='None', c='orange')
    plt.plot(x[offset:], ns*y4_fit[offset:], linewidth = 0.2, c='black')
else:
    print("Can't fit: some y4 <= 0 (try different seed or increase 'start')")

#--- [3.3] Plot integral of delta

y5 = asum_delta
plt.subplot(2,2,4)
plt.plot(x[offset:], y5[offset:], linewidth = 0.4, c='red')
plt.show()


#--- [4] Quantum derivative

#- [4.1] Function to differentiated: delta, here broken down into 2 legs

plt.subplot(1,2,1)
shift = 0.001
plt.plot(x1[offset:], error1[offset:], marker=',', markersize=0.1, 
   linestyle='None', alpha = 1.0, c='red')
plt.plot(x2[offset:], shift + error2[offset:], marker=',', markersize=0.1, 
   linestyle='None', alpha = 0.2, c='orange')

#- [4.2] Quantum derivative

def d_error(arr_error):

    diff_error = []  # discrete derivative of the error
    positives = 0
    negatives = 0
    for k in range(len(arr_error)-1):
        diff_error.append(arr_error[k+1] - arr_error[k])
        if arr_error[k+1] - arr_error[k] > 0: 
            positives +=1
        else:
            negatives += 1
    return(diff_error, positives, negatives)

(diff_error1, positives1, negatives1) = d_error(error1)
(diff_error2, positives2, negatives2) = d_error(error2)
ymin = 0.5 * float(min(min(diff_error1[offset:]), min(diff_error1[offset:])))
ymax = 0.5 * float(max(max(diff_error1[offset:]), max(diff_error2[offset:])))

plt.subplot(1,2,2)
plt.ylim(ymin, ymax)
plt.plot(x1[offset:len(x1)-1], diff_error1[offset:len(x1)-1], marker=',', markersize=0.1, 
         linestyle='None', alpha=0.8, c = 'red')
plt.plot(x2[offset:len(x2)-1], diff_error2[offset:len(x2)-1], marker=',', markersize=0.1, 
         linestyle='None', alpha=0.8, c = 'orange')
plt.show()

print("\nError 1: positives1: %8d  negatives1: %8d" % (positives1, negatives1)) 
print("Error 2: positives2: %8d  negatives2: %8d" % (positives2, negatives2)) 


\end{lstlisting}


%------------------------------------------
\chapter{Generative AI}

The projects in this chapter are related to various aspects of generative AI, such as data synthetization (tabular data), agent-based modeling and model-based generative AI relying on stochastic systems. Emphasis is on assessing the quality of the generated data and reducing the time required to train the underlying algorithms such as GAN (generative adversarial network). For time series generation and geospatial applications, see chapter~\ref{tschj}. 
For LLMs (large language models), see chapter~\ref{nlpllm}.  Computer vision, graph and even sound generation, are included in this chapter.
   

%\section{Synthetic data}

The goal of data synthetization is to produce artificial data that mimics the patterns and features present in existing, real data. Many generation methods and evaluation techniques are available, depending on purposes, the type of data, and the application field. Everyone is familiar with synthetic images in the context of computer vision, or synthetic text in applications such as GPT. Sound, graphs, shapes, mathematical functions, artwork, videos, time series, spatial phenomena — you name it — can be synthesized. In this article, I focus on tabular data, with applications in fintech, the insurance industry, supply chain, and health care, to name a few.

The word “synthetization” has its origins in drug synthesis, or possibly music. Interestingly, the creation of new molecules also benefits from data synthetization, by producing virtual compounds, whose properties (if they could be produced in the real world) are known in advance to some degree. It also involves tabular data generation, where the features replicated are various measurements related to the molecules in question. Historically, data synthetization was invented to address the issue of missing data, that is, as a data imputation technique. It did not work as expected as missing data is usually very different from observed values. But the technique has evolved to cover many applications.

You can synthesize data using interpolation, agent-based modeling, adding correlated zero-mean noise to the real data, using copulas or generative adversarial networks (GANs). All these techniques are discussed in details in my book on Generative AI~\cite{vgelsevier}. For time series synthetization via interpolation, see project~\ref{prgt43zas}.

%http://mirrors.ibiblio.org/CTAN/macros/latex/contrib/exercise/exercise.pdf
%\begin{Exercise}\hspace{-1.5ex}{\bf: Evaluate synthetizations with holdout method}  \vspace{1ex}
%\end{Exercise}

\section{Holdout method to evaluate synthetizations}\label{holfghre3}

The goal is to compare synthetic data vendors, in the context of tabular or transactional data generation. Using real datasets, run synthetizations through various vendors and evaluate the quality of the results: how well the real data is reproduced. In order words, the goal is to 
 assess the \textcolor{index}{faithfulness}\index{faithfulness (synthetic data)} of the synthetizations, using a sound methodology.

The \textcolor{index}{holdout method}\index{holdout method} consists of using 50\% of the real data to train the synthesizer, and using the remaing 50\% to assess the quality of the 
 synthesized data. It is
 similar to cross-validation, but applied in the context of synthetic data, where a response or dependent variable may or may not be present. 
There are many metrics to compare two datasets, in this case real versus synthetic data. Here, I focus on the following metrics:\vspace{1ex}

\begin{itemize}
\item The \textcolor{index}{correlation distance matrix}\index{correlation distance} $\Delta$. This symmetric $m\times m$ matrix is defined as $\Delta_{ij} = | R_{ij} - S_{ij}|$, where $R,S$ are the correlation matrices respectively measured on the real and synthetic data. Here $m$ is the number of features, including the response or outcome if there is one. The indices $i,j$ represent two features. In particular $\Delta_{\text{avg}}$ and $\Delta_{\text{max}}$ are the average and maximum value of $\Delta$. These values are always between 0 (best match) and 1 (worst match). 
\item The \textcolor{index}{Kolmogorov-Smirnov distance}\index{Kolmogorov-Smirnov distance} vector $K$. This vector has $m$ components, one for each feature. Each component represents the 
  normalized distance between two empirical distributions, for the corresponding feature: one attached to the synthetic data, and the other one
 to the real data. In particular $K_{\text{avg}}$ and $K_{\text{max}}$ are the average and maximum value of $K$. These values are always between 0 (best match) and 1 (worst match). 
 \item Additional metrics capturing non-linear inter-dependencies among features, and how well these non-linear patterns are reproduced in the synthetic data. I use scatterplots such as those in Figures~\ref{fig:gretelvc} and~\ref{fig:gretelbwa} to show the match (or lack of) between real and synthetic data. These metrics are important, as correlations alone focus on linear dependencies only, and Komogorov-Smirnov are one-dimensional summaries and do not take into account the feature dependencies. 
\end{itemize}\vspace{1ex}
For details and to get started, read my article ``Generative AI: Synthetic Data Vendor Comparison and Benchmarking Best Practices"~\cite{vgvendors},
 available \href{https://mltechniques.com/2023/06/16/generative-ai-synthetic-data-vendor-comparison-and-benchmarking-best-practices/}{here}.
 Download the technical document, and look at the Jupyter notebook referenced in the article in question.  

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{plots_insurance.png}   
\caption{Insurance data scatterplot, age (X-axis) versus charges (Y-axis)}
\label{fig:gretelvc}
\end{figure}
%imgpy9979_2and3.PNG screen2e.png  
%-------------------------



My article features 3 datasets and several vendors, as well as a case study with the holdout method: the insurance dataset synthesized with
 YData.ai. We will start with the insurance dataset. As for the vendors, at the time of writing, Gretel and Mostly.ai offer web APIs. Synthesizing data is as easy as uploading a csv file (the real data), run it through the API, and download the result. Alternatively you can use the YData platform (Fabric). You can run the synthetization with 3 lines of Python code provided in their documentation.  You can also use the 
 open source SDV library (synthetic data vault) or my own home-made synthesizers described in chapter 10 in my book on generative AI~\cite{vgelsevier}.

\subsection{Project description}

\noindent The project consists of the following steps: \vspace{1ex}
\begin{itemize}
\item[] {\bf Step 1}:  Use the insurance dataset \texttt{incurance.csv} available \href{https://github.com/VincentGranville/Main}{here}. Produce an histogram for each of the non-categorical features: age, charges and bmi (body mass index). The dataset is described in chapter 10 in my book on generative AI~\cite{vgelsevier}. Does it require data cleaning or preprocessing such as transformations to normalize the data?
\vspace{1ex}
\item[] {\bf Step 2}:  Use the Python code \texttt{insurance\_compare.py} on my GitHub repository 
\href{https://github.com/VincentGranville/Main/blob/main/insurance_compare.py}{here}, to perform evaluations and vendor comparisons.
The $\Delta$ and $K$ metrics are implemented in this script, as well as histograms and scatterplots.
\vspace{1ex}
\item[] {\bf Step 3}: Produce synthetic data using one of the vendors.
  Update the \texttt{insurance\_compare.csv} file found in the same GitHub folder, once downloaded in your local environment,  by adding the generated data. 
Proceed as follows: use 50\% of the insurance data set (real data) to produce the synthetization, and use the other 50\% (the validation set) to compare with the synthesized data. Produce a file similar to 
\texttt{insurance\_compare\_holdout.csv} (in the same GitHub folder) but for a vendor other than YData.ai, as I already did the analysis for this one.
\item[] {\bf Step 4}: The first 50\% of the real data is called the training set. Compare the training set with the validation set, and the synthesized data with the validation set. It is assumed that these three datasets have the same number of observations. Do you observe a loss of quality in the syntheric data, when using the holdout method just described, compared to using the full real data (without validation set)? \vspace{1ex}
\item[] {\bf Step 5}: Run two synthetizations from the same vendor: in other words, produce two synthetic datasets based on the same real data. 
 For each synthetization, use the holdout method to evaluate the quality. The goal is to evaluate not only the difference between a real dataset and its synthetization, but also between two synthetizations of the same dataset.  Are differences between two synthetizations of a same dataset larger or smaller than between a real dataset and its synthetization?
\end{itemize}\vspace{1ex}

\noindent The holdout method is used to verify that vendors are not using artifacts to boost performance. If this was the case,
 it would result in overfitting with very good performance measured against the training data, but poor performance when measured against the  validation set. Indeed, the actual performance should always be assessed by comparison with the validation set, not with the data used to train the synthesizer.


\subsection{Solution}

The files \texttt{insurance\_compare\_holdout.csv} (input data)
 and \texttt{insurance\_compare\_holdout.py} illustrate the holdout method on YData.ai. More details are
 available in one of my articles posted \href{https://github.com/VincentGranville/Main/blob/main/vendors.pdf}{here}: see section 3.2. 
  in the same GitHub repository. The Jupyter notebook  \texttt{sd\_vendors.ipynb} available 
 \href{https://github.com/VincentGranville/Notebooks/blob/main/sd_vendors.ipynb}{here} illustrates how to  compute the evaluation
 metrics $\Delta, K$ and 
 produce the various plots such as Figure~\ref{fig:gretelbwa}  (scatterplots for the circle data) and 
Figure~\ref{fig:gretelgf} (histogram for the insurance dataset).  For convenience, I also included the Python code in this section. \vspace{1ex}



\begin{lstlisting}
import pandas as pd
import numpy as np
import scipy
from scipy.stats import ks_2samp
from statsmodels.distributions.empirical_distribution import ECDF

dataset = 'insurance_compare.csv'
url = "https://raw.githubusercontent.com/VincentGranville/Main/main/" + dataset 
df = pd.read_csv(url)
# df = pd.read_csv(dataset)
if dataset == 'insurance_compare.csv':
    df = df.drop('region', axis=1)
    df = df.dropna(axis='columns')
print(df.head())

data_real = df.loc[df['Data'] == 'Real']
data_real = data_real.drop('Data', axis=1)
data_real = data_real.to_numpy()
print(data_real)

r_corr = np.corrcoef(data_real.T) # need to transpose the data to make sense
print(r_corr)

ltests = df.Data.unique().tolist()
popped_item = ltests.pop(0)   # remove real data from the tests
print(ltests)

#--- main loop

for test in ltests:

    data_test = df.loc[df['Data'] == test]
    data_test = data_test.drop('Data', axis=1)
    data_test = data_test.to_numpy()
    t_corr = np.corrcoef(data_test.T) 
    delta = np.abs(t_corr - r_corr)
    dim = delta.shape[0]   # number of features
  
    ks = np.zeros(dim)
    out_of_range = 0
    for idx in range(dim):
        dr = data_real[:,idx]
        dt = data_test[:,idx]
        stats = ks_2samp(dr, dt)
        ks[idx] = stats.statistic
        if np.min(dt) < np.min(dr) or np.max(dt) > np.max(dr):
            out_of_range = 1
    str = "%20s %14s %8.6f %8.6f %8.6f %8.6f %1d" % (dataset, test, np.mean(delta), 
              np.max(delta), np.mean(ks), np.max(ks), out_of_range)
    print(str)

#--- visualizing results

def vg_scatter(df, test, counter):

    # customized plots, insurance data 
    # one of 6 plots, subplot position based on counter

    data_plot = df.loc[df['Data'] == test]
    x = data_plot[['age']].to_numpy()
    y = data_plot[['charges']].to_numpy()
    plt.subplot(2, 3, counter)
    plt.scatter(x, y, s = 0.1, c ="blue")
    plt.xlabel(test, fontsize = 7)
    plt.xticks([])
    plt.yticks([])
    plt.ylim(0,70000)
    plt.xlim(18,64)
    return()


def vg_histo(df, test, counter):

    # customized plots, insurance data 
    # one of 6 plots, subplot position based on counter

    data_plot = df.loc[df['Data'] == test]
    y = data_plot[['charges']].to_numpy()
    plt.subplot(2, 3, counter)
    binBoundaries = np.linspace(0, 70000, 30)
    plt.hist(y, bins=binBoundaries, color='white', align='mid',edgecolor='red',
              linewidth = 0.3) 
    plt.xlabel(test, fontsize = 7)
    plt.xticks([])
    plt.yticks([])
    plt.xlim(0,70000)
    plt.ylim(0, 250)
    return()

import matplotlib.pyplot as plt
import matplotlib as mpl
mpl.rcParams['axes.linewidth'] = 0.3

vg_scatter(df, 'Real', 1)
vg_scatter(df, 'YData1', 2)
vg_scatter(df, 'Gretel', 3)
vg_scatter(df, 'Mostly.ai', 4)
vg_scatter(df, 'Synthesize.io', 5)
vg_scatter(df, 'SDV', 6)
plt.show()

vg_histo(df, 'Real', 1)
vg_histo(df, 'YData1', 2)
vg_histo(df, 'Gretel', 3)
vg_histo(df, 'Mostly.ai', 4)
vg_histo(df, 'Synthesize.io', 5)
vg_histo(df, 'SDV', 6)
plt.show()
\end{lstlisting}\vspace{1ex}

\noindent Table~\ref{ycpuj3eure} compares real with synthetic data using holdout. Each dataset (a row in the table) is compared with the validation set. Thus the row ``Validation" is filled with zeros (the best possible fit) as you compare the validation set with itself.


%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{plots_circle.png}   
\caption{Circle data scatterplot, first two coordinates}
\label{fig:gretelbwa}
\end{figure}
%imgpy9979_2and3.PNG screen2e.png  
%-------------------------



%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{plots_histo.png}   
\caption{Insurance data, charges distribution, real (top left) vs synthetic}
\label{fig:gretelgf}
\end{figure}
%imgpy9979_2and3.PNG screen2e.png  
%-------------------------

Besides the quality metrics investigated in this project, there are other ways to compare vendors. For instance, how long it takes to train a synthesizer, particularly GAN (generative adversarial networks) which is a combination of deep neural networks. Methods to improve the speed are discussed in project~\ref{porgan2}. The ease of use is also an important factor. For instance, is a Pyhon SDK available? Can you run it on the
 vendor's platform without interfering with your own environment, and does it require a lot of parameter fine-tuning?

Other factors to consider is replicability and being able to sample outside the observation range. At the time of this writing, 
none of the vendors offer these features. However, my home-made GAN does, see chapter 10 in my book on generative AI~\cite{vgelsevier}.
 

\begin{table}[H]
%\[\
\begin{center}
\scalebox{0.9}{
\begin{tabular}{llcccc}
\hline
  Data & Type & $\Delta_{\text{avg}}$ & $\Delta_{\text{max}}$ & $K_\text{avg}$ & $K_\text{max}$ \\
\hline
Training&Real&0.0314&0.0687&0.0361&0.0732 \\
Validation&Real&0.0000&0.0000&0.0000&0.0000\\
YData1&Synthetic& 0.0394&0.1081&0.0433&0.0792\\
YData2&Synthetic&0.0241&0.0822&0.0386&0.0807\\
\hline		
\end{tabular}
}
%\]
\caption{\label{ycpuj3eure} Insurance data, distances to validation set}
\end{center}
\end{table}



%---
\section{Enhanced synthetizations with GANs and copulas}\label{porgan2}


The goal is to synthesize a real dataset, understand the steps involved (data transformation if necessary, synthesizing, assessing the quality of the results) and then use tools that automate this process. Along the way, we investigate various enhancements to the base technique, and the value that they bring, especially in terms of speeding up the training of the deep neural networks involved, reducing the amount of data needed, replicability, sampling outside the observation range and so on. Two types of synthesizers are considered: GAN and copula-based, in the context of tabular data generation.

% transform: decorrelate; standardize
% augmented data
%xxxxx nlp -- scores for GPT // taxonomy
%agglomerative processes / agent base modeling / random graphs / PB processes / zeta-geometric distrib.
%smart search grid, data thinning, feature clustering
%clustering to assess synth. quality
%gan enhancement
%create clusters with pb processes
%  add index and orange KWs
% zeta-geom distrib.
% include my python code for gan, zeta-geom, smart grid search, link to jupyter

\subsection{Project description}\label{ot6zej7}

The material about GAN (\textcolor{index}{generative adversarial network}\index{generative adversarial network}\index{GAN (generative adversarial network)}) and \textcolor{index}{copula}\index{copula} is discussed in detail in chapter 10 in my book on generative AI~\cite{vgelsevier}.  This project also features techniques described in separate papers.
 It covers feature clustering to reduce the dimensionality of the problem, smart grid search for hyperparameter tuning, stopping rules for GAN to speed up training, parametric copulas and alternative to GMM (Gaussian mixture models), customizing the loss function, dealing with categorical features, as well as fast data distillation to reduce the size of the training set. Two important issues are replicability and the ability to generate synthetic data outside the observation range. 

This project is rather large, and can easily be broken down into sub-projects. Participants enrolled in the Generative AI certification program may choose to work on two of the steps below. I use the insurance and diabetes datasets in this project. They are available on GitHub, 
 \href{https://github.com/VincentGranville/Main/blob/main/}{here}. Look for \texttt{diabetes\_clean.csv} and \texttt{insurance.csv}.



\noindent The project consists of the following steps: \vspace{1ex}
\begin{itemize}
\item[] {\bf Step 1}:  {\bf Data transformation}. Use a synthetization tool from one of the vendors listed in project~\ref{holfghre3}, 
 or my GAN synthesizer \texttt{GAN\_diabetes.py} available
 \href{https://github.com/VincentGranville/Main/blob/main/GAN_diabetes.py}{here} and explained in my book~\cite{vgelsevier}. Apply the technique to the diabetes dataset. 
 My GAN is not doing great on the diabetes dataset. Decorrelate the data using PCA
 (\textcolor{index}{principal component analysis}\index{principal component analysis}\index{PCA (principal component analysis)}), then synthesize the transformed data, then apply the inverse PCA transform to the synthetic data, to reproduce the correct correlation structure. Measure the  improvement 
 over the base synthetization,  using the correlation distance metric $\Delta_{\text{avg}}$ described in section~\ref{holfghre3}.
See Stack Exchange question on this topic, \href{https://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-com}{here}. Section 7.1 in my book~\cite{vgelsevier} offers a different perspective on the subject, using square roots of covariance matrices, without advanced matrix algebra. \vspace{1ex}
\item[] {\bf Step 2}:   {\bf Data augmentation}. Synthesize the \texttt{diabetes\_clean.csv} dataset as in step 1. One of the
 fields named ``Outcome" is the binary response: the patient either has cancer, or not. The other features are predictors (called independent variables by statisticians) such as glucose level, body mass index, or age. The goal is to predict the risk of cancer based on the values attached to the predictors. Train a predictive classifier such as \textcolor{index}{logistic regression}\index{logistic regression} to predict cancer outcome, on the real data. Then, train the same classifier on \textcolor{index}{augmented data}\index{augmented data}, consisting of 50\% of the real data, the other 50\% being synthetic data. Assess correct classification rate on the unused portion of the training set, called 
\textcolor{index}{validation set}\index{validation set}. Finally, compare the results if doing the same test but without adding synthetic data to the training set.
\vspace{1ex}

\item[] {\bf Step 3}: {\bf Data grouping}. Use the Gaussian copula method described in chapter 10 in my book~\cite{vgelsevier} and in my Jupyter notebook
 \href{https://github.com/VincentGranville/Notebooks/blob/main/copula_insurance_nogroup.ipynb}{here}, to synthesize the insurance dataset.
  Ignore the categorical features ``gender", ``smoking status" and ``region".  Instead, for each multivariate bucket, use a separate copula. 
Then, assess the improvement over using the same copula across all buckets. An example of bucket, also called 
 bin or \textcolor{index}{flag vector}, is \texttt{[gender=male,smoking=yes,region=Northeast]}. The 
 \textcolor{index}{bucketization}\index{bucketization} process is performed manually here. But it could be automated using \textcolor{index}{XGboost} or similar 
\textcolor{index}{ensemble methods}\index{ensemble method} based on many decision trees, such as my Hidden Decision Trees technique
 described in chapter 2 in my book~\cite{vgelsevier}. Buckets that are too small (less than 30 observations) can be aggregated into larger buckets or treated separately.
\vspace{1ex}

\item[] {\bf Step 4}: {\bf Data distillation}. Can randomly removing 50\% of the observations in the training set (real data) speed up the training process by a factor two? Implement this strategy, and evaluate the results, both in term of algorithmic speed and the quality of the generated data. It is possible to be highly selective in the choice of observations to be deleted, for instance to boost the quality of the synthesized data at each deletion. However, this method, known as \textcolor{index}{data distillation}\index{data distillation}~\cite{ieeedistil}, can be time consuming and erase all gains in the training process. See also step 7 for stopping the training process when the loss function has stabilized to a low, rather than using a fixed number of epochs. \vspace{1ex}

\item[] {\bf Step 5}: {\bf Feature clustering}. This method is an alternative to principal component analysis. It does not 
 transform the features into meaningless, artificial variables. Thus, 
 it belongs to a set of techniques known as \textcolor{index}{explainable AI}\index{explainable AI}. It consists of putting all the highly correlated features into a number of clusters, and creating individual clusters for features barely correlated to any other ones. How woulsd you use
 this technique on the diabetes data set? While clustering the observations is based on the distance (Euclidean, cosine and so on) between any two observations, clustering features is based on the absolute value of the correlation between any two features.
 Features barely correlated to any other one can be synthesized separately rather than jointly, thus saving time in the training process, via \textcolor{index}{parallel computing}\index{parallel computing}. \vspace{1ex}

\item[] {\bf Step 6}: {\bf Parameter fine-tuning}. Implement the smart grid search algorithm described in~\cite{vgsmart}, to fine-tune the \textcolor{index}{learning rate}\index{learning rate} 
 hyperparameter in \texttt{GAN\_diabetes.py}. In the end, GANs are \textcolor{index}{gradient descent}\index{gradient descent} algorithms to minimize a \textcolor{index}{loss function}\index{loss function}, and the learning rate applies to the gradient method being used (in my GAN and many other implementations,
 ADAM is the preferred gradient method). \vspace{1ex}

\item[] {\bf Step 7}: {\bf Loss function and stopping rule}. In some gradient descent implementations such as GANs or linear regression, the loss function is the mean squared  or mean absolute error. GANs actually consist of two different neural networks with opposite goals: the generator to synthesize data, and the discriminator to check how close real and synthetic data are to each other. Thus two loss functions are needed, and can be blended into a single one: for details, see
 \href{https://neptune.ai/blog/gan-loss-functions}{here}. It is possible to use a customized loss function for each model (generator and discriminator) when calling the \texttt{mode.compile} \textcolor{index}{Keras}\index{Keras (Python library)}\index{Python library!Keras} function: see example in \texttt{GAN\_diabetes.py}. 

Another, possibly easier strategy, consists of computing the distance between the real and synthesized data, (say) every 50 
\textcolor{index}{epochs}\index{epoch (neural networks)}, and stop
 when it is low enough. This is possible in my GAN by setting \texttt{mode=\textquotesingle Enhanced\textquotesingle }. For instance, I use the distance (or loss function) 
$L = \Delta_{\text{avg}}$ 
defined in section~\ref{holfghre3}, and implemented as the \texttt{GAN\_distance} function in the Python code. Modify the code to use 
 $L = \Delta_{\text{avg}}+K_{\text{avg}}$ instead. The first is the average \textcolor{index}{correlation distance}\index{correlation distance}, and the second is the average \textcolor{index}{Kolmogorov-Smirnov distance}\index{Kolmogorov-Smirnov distance}. Computation examples can be found in \texttt{insurance\_compare.py}, 
in the same GitHub folder \href{https://github.com/VincentGranville/Main}{here},
 and in the \texttt{sd\_vendors.ipynb} notebook, \href{https://github.com/VincentGranville/Notebooks/blob/main/sd_vendors.ipynb}{here}. 
\vspace{1ex}

\item[] {\bf Step 8}: {\bf Synthesizing outliers}. Most of the vendors do not produce synthetizations outside the observation range: for instance, if ``age" ranges from 18 to 64 in your real dataset, it will be the same in the synthesized version.  
 My copula method based on empirical quantiles has the same problem. In some contexts, it is important to sample outside the observation range,
 for instance when testing new algorithms. How would you do to achieve this goal?  \vspace{1ex}

\item[] {\bf Step 9}: {\bf Sensitivity analysis}. Add noise to the real data. Evaluate how sensitive your synthetization is to changes in the real data, depending on the amount of noise. Fine-tune parameters in the synthesizer to see how to reduce sensitivity. Identify synthesizers most robust against noise.
\end{itemize}\vspace{1ex}
 
\noindent Another interesting question is whether your synthesizer leads to replicable results. Those based on neural networks generally don't, and this includes all the solutions offered by the vendors tested. My home-made GAN does: two runs with the same \texttt{seed} parameter lead to the same synthetizations. This fact can be leveraged to get better synthetizations, by trying different \textcolor{index}{seeds}\index{seed (random number generators)}. Related to this, a potential project consists of testing the variations between two different runs from the same synthesizer on the same data. Are these differences bigger than the average discrepancy between a synthetization and the real data? See Table~\ref{ycpuj3eure} for an answer, based on testing the YData.ai platform. The answer is usually negative.

On a different topic, Figure~\ref{fig:gretcocol} illustrates the evolution of the loss function discussed in step 7, over $\num{10000}$ epochs.
 Note that there are two loss functions: one for the generator (in orange), and one for the discriminator model (in blue). The two plots correspond to using two different seeds: 103 (top) and 102 (bottom). Clearly, seed 103 is the winner, as it leads to a lower value of the loss function over time, and thus to a stronger local optimum. It is also obvious that we could have stopped training the GAN after about $6000$ epochs.


%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.67\textwidth]{GAN_history_nice.png}   
\caption{Loss function history, two versions of GAN}
\label{fig:gretcocol}
\end{figure}
%imgpy9979_2and3.PNG screen2e.png  

Finally, one way to accelerate GAN training is to use a fast version of the gradient descent algorithm, such as 
\textcolor{index}{lightGBM}\index{lightGBM} [\href{https://en.wikipedia.org/wiki/LightGBM}{Wiki}]. This is implemented 
in \textcolor{index}{TabGAN}\index{TabGAN (Python library)}\index{Python library!TabGAN} [\href{https://github.com/Diyago/GAN-for-tabular-data}{Wiki}],
as well as in the 
 light version of \textcolor{index}{SDV}\index{SDV (Python library)}\index{Python library!SDV} (the synthetic data vault library). It may result in a noticeable drop in quality.

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{pdf3.png}   
\caption{Modeling the number of children, insurance data (green is best fit)}
\label{fig:gretfg1009}
\end{figure}

\subsection{Solution}\label{pureas}

For convenience, this section contains the Python code for the Gaussian copula method based on grouping (\textcolor{red}{step 3}), my home-made GAN method
 (most useful to work on \textcolor{red}{step 7}), and the smart grid search related to \textcolor{red}{step 6} and \textcolor{index}{step 8}. The choice of the copula -- Gaussian or not -- has nothing to do with the observed distributions in the real data: most features are not Gaussian in my examples, some are multimodal and not symmetric. 
However, non-Gaussian copulas are sometimes preferred when dealing with very thick tails: the reader is invited to check out articles 
on this topic~\cite{copul22eg}. 
Gumbel, Vine, Frank and other copulas are available in Python libraries such as \textcolor{index}{SDV} or \textcolor{index}{Copula}\index{Python library!Copula}.
 See code \href{https://github.com/VincentGranville/Main/blob/main/copula.py}{here}.  Likewise, the choice of a Gaussian distribution
 for latent variables in GAN is unimportant, though uniform distributions might be more GAN-friendly.



Copulas methods based on \textcolor{index}{empirical quantiles}\index{empirical quantile} do not allow you to generate data outside the observation range. This includes my own version.
To fix this issue, replace the empirical quantiles by those of a parametric distribution that fits the real data well. The parameters are estimated 
 on the real data. I explain how this works in my article about smart grid search~\cite{vgsmart}. See Figure~\ref{fig:gretfg1009} with 
 the ``number of children" -- one of the features in the insurance dataset -- modeled using a two-parameter zeta-geometric distribution.
 Univariate and multivariate \textcolor{index}{Gaussian mixture models}\index{Gaussian mixture model}\index{GMM (Gaussian mixture model)}  (GMM) are popular in this context when dealing with continuous variables. Parameters are then estimated
 via the \textcolor{index}{EM algorithm}\index{EM algorithm}, and the resulting synthetizations are not limited to the observation range, thus answering the question in \textcolor{red}{step 8}. Hierarchical Bayesian models are a generalization of GMM.
 Adding noise is the easiest way to sample outside the observation range. It is discussed in chapter 7 in my book~\cite{vgelsevier}.

The answer to \textcolor{red}{step 5} can be found in my article on feature clustering~\cite{vgfclust}. I did not include the code here, but you can find two 
 different implementations in section~10.4.6 in my book~\cite{vgelsevier}, and 
\href{https://github.com/VincentGranville/Main/blob/main/featureClusteringScipy.py}{here}. The first version uses the 
\textcolor{index}{Scipy}\index{Scipy (Python library)}\index{Python library!Scipy} library for 
\textcolor{index}{hierarchical clustering}\index{hierarchical clustering}; the second one is based on detecting \textcolor{index}{connected components}\index{connected components}, a graph theory algorithm. Figure~\ref{fig:gretde3bn} shows the 9 features of the diabetes dataset on the X-axis; the Y-axis represents the distance between two feature clusters, measured as $1-|\rho|$ where $\rho$ is the correlation between features from two different clusters. Clearly, Family history and blood pressure (features 6 and 2 respectively) are the least correlated to other features, and can be treated separately.

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{hcluster.png}   
\caption{Feature clustering on the diabetes dataset}
\label{fig:gretde3bn}
\end{figure}

Table~\ref{tabebbphuyfr} lists the features (label with description) for the diabetes dataset. Figure~\ref{fig:pihg} shows the correlation matrix.
Regarding \textcolor{red}{step 4}, see my article on stochastic thinning~\cite{vgthin}. 

Finally, 
regarding \textcolor{red}{step 2}, see my code \texttt{GAN\_diabetes.py} on GitHub, 
 \href{https://github.com/VincentGranville/Main/blob/main/GAN_diabetes.py}{here}, also dealing with the same dataset. The feature ``Outcome"  (cancer status) is the response, and the goal is to predict the risk of cancer given the other features. I use the 
\textcolor{index}{random forest classifier}\index{random forest classifier} to perform supervised classification into the two groups (cancer versus no cancer), by calling the \texttt{RandomForestClassifier} \textcolor{index}{ensemble method}\index{ensemble method} from the \textcolor{index}{Sklearn}\index{Sklearn (Python library)}\index{Python library!Sklearn} 
Python library. I first do it on the real data before training the GAN model, and then again at the end, but this time on the synthetic data for comparison purposes. To complete this step, blend real with synthetic data, then run the random forest classifier on this 
\textcolor{index}{augmented data}\index{augmented data}, then evaluate the results on a \textcolor{index}{validation set}\index{validation set}. This latter set with known outcome, part of the real data and also called holdout, is not used to train the random forest classifier, but to evaluate the results. You can generate it with the function  \texttt{train\_test\_split}
 available in Sklearn, as illustrated in the Python code. 











\subsection{Python code}

In this section, you will find the Python code for the Gaussian copula method with a separate copula for each group, the GAN method, and 
 the smart grid search algorithm with an application to optimize parameters. The application in question consists of estimating the parameters in a 
 a 2-parameter distribution to fit the ``number of children" feature in the insurance dataset, in the context of parametric copulas. The code and datasets are also available on GitHub: see each subsection for the link to the GitHub locations. For examples using the 
\textcolor{index}{SDV}\index{SDV (Python library)}\index{Python library!SDV} open source library, including how to
 handle \textcolor{index}{metadata}\index{metadata}, 
 see my code snippet \texttt{SDV\_example} on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/SDV_example.py}{here}.

\renewcommand{\arraystretch}{1.2} %%%
\renewcommand{\arraystretch}{1.2} %%%
\begin{table}[H]
%\small
\[
\begin{tabular}{cll}
\hline
 Code &  Feature name & Description  \\
\hline
\hline
 0 & Pregnancies & Number of pregnancies \\
 1 & Glucose & Plasma glucose concentration\\
 2 & BloodPressure &  Diastolic blood pressure in mm/Hg \\
 3 & SkinThickness &  Triceps skinfold thickness in mm\\
4 &  Insulin & Insulin in U/mL\\
5 &  BMI & Body mass index \\
6 &  DiabetesPedigreeFunction & Risk based on family history\\
7 & Age & Age of patient\\
8 & Outcome & Patient had diabetes  or not \\
\hline
\end{tabular}
\]
\caption{\label{tabebbphuyfr} Feature mapping table for the diabetes dataset}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{fc4453.png}  
\caption{Correlation matrix for the diabetes dataset}
\label{fig:pihg}
\end{figure}

\subsubsection{Gaussian copula by group}

This program generates synthetic observations for the insurance dataset, using a different copula for each group.  
Groups are defined in step 3 in project~\ref{porgan2}. A detailed description of the code is in my Jupyter notebook, \href{https://github.com/VincentGranville/Notebooks/blob/main/copula_insurance_byGroup.ipynb}{here}, as well as in chapter 10 in my book~\cite{vgelsevier}. \vspace{1ex}

\begin{lstlisting}[numbers=left]
import pandas as pd
from scipy.stats import norm
import numpy as np

# source: https://www.kaggle.com/datasets/teertha/ushealthinsurancedataset
# Fields: age, sex, bmi, children, smoker, region, charges
    
url="https://raw.githubusercontent.com/VincentGranville/Main/main/insurance.csv"
# make sure fields don't contain commas
data = pd.read_csv(url)
print(data.head(10))

groupCount = {}
groupReal = {}
for k in range(0, len(data)):  
    obs = data.iloc[k]   # get observation number k
    group = obs[1] +"\t"+obs[4]+"\t"+obs[5]
    if group in groupCount:
        cnt = groupCount[group]
        groupReal[(group,cnt)]=(obs[0],obs[2],obs[3],obs[6]) 
        groupCount[group] += 1    
    else:
        groupReal[(group,0)]=(obs[0],obs[2],obs[3],obs[6]) 
        groupCount[group] = 1

for group in groupCount:
    print(group, groupCount[group])

print(groupReal[("female\tyes\tsouthwest",0)])
print(groupReal[("female\tyes\tsouthwest",1)])
print(groupReal[("female\tyes\tsouthwest",2)])
print(groupReal[("female\tyes\tsouthwest",3)])
print(groupReal[("female\tyes\tsouthwest",20)])

def create_table(group, groupCount, groupReal):

    # extract data corresponding to specific group, from big table groupReal

    nobs = groupCount[group]
    age = []
    bmi = []
    children = []
    charges = []
    for cnt in range(nobs):
        features = groupReal[(group,cnt)]
        age.append(float(features[0]))       # uniform outside very young or very old
        bmi.append(float(features[1]))       # Gaussian distribution?
        children.append(float(features[2]))  # geometric distribution?
        charges.append(float(features[3]))   # bimodal, not gaussian 
        real = np.stack((age, bmi, children, charges), axis = 0)
    return(real)

def gaussian_to_synth(real, gfg, group, nobs_synth, groupSynth):

    # turn multivariate gaussian gfg into synth. data, update groupSynth 
    # this is done for a specific group, creating nobs_synth obs.

    age = real[0,:]
    bmi = real[1,:]
    children = real[2,:]
    charges = real[3,:]

    g_age = gfg[:,0]
    g_bmi = gfg[:,1]
    g_children = gfg[:,2]
    g_charges = gfg[:,3]

    for k in range(nobs_synth):   

        u_age = norm.cdf(g_age[k])                     # u stands for uniform[0, 1]
        u_bmi = norm.cdf(g_bmi[k])
        u_children = norm.cdf(g_children[k])
        u_charges = norm.cdf(g_charges[k])

        s_age = np.quantile(age, u_age)                # synthesized age 
        s_bmi = np.quantile(bmi, u_bmi)                # synthesized bmi
        s_children = np.quantile(children, u_children) # synthesized children
        s_charges = np.quantile(charges, u_charges)    # synthesized charges

        # add k-th synth. obs. for group in question, to groupSynth
        groupSynth[(group, k)] = [s_age, s_bmi, s_children, s_charges] 

    return()

seed = 453
np.random.seed(seed)
groupSynth = {}

for group in groupCount:

    real = create_table(group, groupCount, groupReal) 
    n_var = real.shape[0] 
    zero = np.zeros(n_var) 
    corr = np.corrcoef(real)       # correlation matrix for Gaussian copula for this group
    nobs_synth = groupCount[group] # number of synthetic obs to create for this group
    gfg = np.random.multivariate_normal(zero, corr, nobs_synth) 
    gaussian_to_synth(real, gfg, group, nobs_synth, groupSynth)


for group, k in groupSynth:

    obs = groupSynth[(group,k)] # this is k-th synth. obs. for group in question

    # print synth. data for sample group: age, bmi, children, charges
    if group == "female\tyes\tsouthwest":
        print("%6.2f %7.2f %6.2f %10.2f" % (obs[0], obs[1], obs[2], obs[3]))


\end{lstlisting}

\subsubsection{Generative adversarial networks}

This Python program below shows the different steps in the implementation of a GAN synthesizer using TensorFlow and Keras, including
 the architecture and training the two neural networks involved: the generator and discriminator models. The code
 is also on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/GAN_diabetes.py}{here}. Explanations are in chapter 10 in my 
book~\cite{vgelsevier}, and in the corresponding Jupyter notebook, \href{https://github.com/VincentGranville/Notebooks/blob/main/GAN_diabetes.ipynb}{here}. I applied the technique to the diabetes dataset, avaliable as \texttt{diabetes.csv}, 
\href{https://github.com/VincentGranville/Main/blob/main/diabetes.csv}{here}. 

The code does a lot more than training a standard GAN to produce synthetizations. It includes data cleaning, classifying patients as cancer or not  using random forests as a predictive model, evaluating the quality of the synthetizations, and stopping before reaching $\num{10000}$
epochs  when \texttt{mode} is set to \texttt{\textquotesingle Enhanced\textquotesingle }. As usual, it produces some plots, in this case a time series of historical values for the loss functions, computed at each \textcolor{index}{epoch}\index{epoch (neural networks)}. \vspace{1ex}

\begin{lstlisting}[numbers=left]
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import random as python_random
from tensorflow import random
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam    # type of gradient descent optimizer
from numpy.random import randn
from matplotlib import pyplot
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics

data = pd.read_csv('diabetes.csv')
# rows with missing data must be treated separately: I remove them here
data.drop(data.index[(data["Insulin"] == 0)], axis=0, inplace=True) 
data.drop(data.index[(data["Glucose"] == 0)], axis=0, inplace=True) 
data.drop(data.index[(data["BMI"] == 0)], axis=0, inplace=True) 
# no further data transformation used beyond this point
data.to_csv('diabetes_clean.csv')

print (data.shape)
print (data.tail())
print (data.columns)


seed = 103     # to make results replicable
np.random.seed(seed)     # for numpy
random.set_seed(seed)    # for tensorflow/keras
python_random.seed(seed) # for python

adam = Adam(learning_rate=0.001) # also try 0.01
latent_dim = 10
n_inputs   = 9   # number of features
n_outputs  = 9   # number of features


#--- STEP 1: Base Accuracy for Real Dataset

features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']
label = ['Outcome']  # OutCome column is the label (binary 0/1) 
X = data[features]
y = data[label] 

# Real data split into train/test dataset for classification with random forest

X_true_train, X_true_test, y_true_train, y_true_test = train_test_split(X, y, test_size=0.30, random_state=42)
clf_true = RandomForestClassifier(n_estimators=100)
clf_true.fit(X_true_train,y_true_train)
y_true_pred=clf_true.predict(X_true_test)
print("Base Accuracy: %5.3f" % (metrics.accuracy_score(y_true_test, y_true_pred)))
print("Base classification report:\n",metrics.classification_report(y_true_test, y_true_pred))


#--- STEP 2: Generate Synthetic Data

def generate_latent_points(latent_dim, n_samples):
    x_input = randn(latent_dim * n_samples) 
    x_input = x_input.reshape(n_samples, latent_dim)
    return x_input

def generate_fake_samples(generator, latent_dim, n_samples):
    x_input = generate_latent_points(latent_dim, n_samples) # random N(0,1) data
    X = generator.predict(x_input,verbose=0) 
    y = np.zeros((n_samples, 1))  # class label = 0 for fake data
    return X, y

def generate_real_samples(n):
    X = data.sample(n)   # sample from real data
    y = np.ones((n, 1))  # class label = 1 for real data
    return X, y

def define_generator(latent_dim, n_outputs): 
    model = Sequential()
    model.add(Dense(15, activation='relu',  kernel_initializer='he_uniform', input_dim=latent_dim))
    model.add(Dense(30, activation='relu'))
    model.add(Dense(n_outputs, activation='linear'))
    model.compile(loss='mean_absolute_error', optimizer=adam, metrics=['mean_absolute_error']) # 
    return model

def define_discriminator(n_inputs):
    model = Sequential()
    model.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))
    model.add(Dense(50, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy']) 
    return model

def define_gan(generator, discriminator):
    discriminator.trainable = False # weights must be set to not trainable
    model = Sequential()
    model.add(generator) 
    model.add(discriminator) 
    model.compile(loss='binary_crossentropy', optimizer=adam)  
    return model

def gan_distance(data, model, latent_dim, nobs_synth): 

    # generate nobs_synth synthetic rows as X, and return it as data_fake
    # also return correlation distance between data_fake and real data

    latent_points = generate_latent_points(latent_dim, nobs_synth)  
    X = model.predict(latent_points, verbose=0)  
    data_fake = pd.DataFrame(data=X,  columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'])
 
    # convert Outcome field to binary 0/1
    outcome_mean = data_fake.Outcome.mean()
    data_fake['Outcome'] = data_fake['Outcome'] > outcome_mean
    data_fake["Outcome"] = data_fake["Outcome"].astype(int)

    # compute correlation distance
    R_data      = np.corrcoef(data.T) # T for transpose
    R_data_fake = np.corrcoef(data_fake.T)
    g_dist = np.average(abs(R_data-R_data_fake))
    return(g_dist, data_fake) 

def train(g_model, d_model, gan_model, latent_dim, mode, n_epochs=10000, n_batch=128, n_eval=50):   
    
    # determine half the size of one batch, for updating the  discriminator
    half_batch = int(n_batch / 2)
    d_history = [] 
    g_history = [] 
    g_dist_history = []
    if mode == 'Enhanced':
        g_dist_min = 999999999.0  

    for epoch in range(0,n_epochs+1): 
                 
        # update discriminator
        x_real, y_real = generate_real_samples(half_batch)  # sample from real data
        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)
        d_loss_real, d_real_acc = d_model.train_on_batch(x_real, y_real) 
        d_loss_fake, d_fake_acc = d_model.train_on_batch(x_fake, y_fake)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # update generator via the discriminator error
        x_gan = generate_latent_points(latent_dim, n_batch)  # random input for generator
        y_gan = np.ones((n_batch, 1))                        # label = 1 for fake samples
        g_loss_fake = gan_model.train_on_batch(x_gan, y_gan) 
        d_history.append(d_loss)
        g_history.append(g_loss_fake)

        if mode == 'Enhanced': 
            (g_dist, data_fake) = gan_distance(data, g_model, latent_dim, nobs_synth=400)
            if g_dist < g_dist_min and epoch > int(0.75*n_epochs): 
               g_dist_min = g_dist
               best_data_fake = data_fake
               best_epoch = epoch
        else: 
            g_dist = -1.0
        g_dist_history.append(g_dist)
                
        if epoch % n_eval == 0: # evaluate the model every n_eval epochs
            print('>%d, d1=%.3f, d2=%.3f d=%.3f g=%.3f g_dist=%.3f' % (epoch, d_loss_real, d_loss_fake, d_loss,  g_loss_fake, g_dist))       
            plt.subplot(1, 1, 1)
            plt.plot(d_history, label='d')
            plt.plot(g_history, label='gen')
            # plt.show() # un-comment to see the plots
            plt.close()
       
    OUT=open("history.txt","w")
    for k in range(len(d_history)):
        OUT.write("%6.4f\t%6.4f\t%6.4f\n" %(d_history[k],g_history[k],g_dist_history[k]))
    OUT.close()
    
    if mode == 'Standard':
        # best synth data is assumed to be the one produced at last epoch
        best_epoch = epoch
        (g_dist_min, best_data_fake) = gan_distance(data, g_model, latent_dim, nobs_synth=400)
       
    return(g_model, best_data_fake, g_dist_min, best_epoch) 

#--- main part for building & training model

discriminator = define_discriminator(n_inputs)
discriminator.summary()
generator = define_generator(latent_dim, n_outputs)
generator.summary()
gan_model = define_gan(generator, discriminator)

mode = 'Enhanced'  # options: 'Standard' or 'Enhanced'
model, data_fake, g_dist, best_epoch = train(generator, discriminator, gan_model, latent_dim, mode)
data_fake.to_csv('diabetes_synthetic.csv') 
    

#--- STEP 3: Classify synthetic data based on Outcome field

features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']
label = ['Outcome']
X_fake_created = data_fake[features]
y_fake_created = data_fake[label]
X_fake_train, X_fake_test, y_fake_train, y_fake_test = train_test_split(X_fake_created, y_fake_created, test_size=0.30, random_state=42)
clf_fake = RandomForestClassifier(n_estimators=100)
clf_fake.fit(X_fake_train,y_fake_train)
y_fake_pred=clf_fake.predict(X_fake_test)
print("Accuracy of fake data model: %5.3f" % (metrics.accuracy_score(y_fake_test, y_fake_pred)))
print("Classification report of fake data model:\n",metrics.classification_report(y_fake_test, y_fake_pred))


#--- STEP 4: Evaluate the Quality of Generated Fake Data With g_dist and Table_evaluator

from table_evaluator import load_data, TableEvaluator

table_evaluator = TableEvaluator(data, data_fake)
table_evaluator.evaluate(target_col='Outcome')
# table_evaluator.visual_evaluation() 

print("Avg correlation distance: %5.3f" % (g_dist))
print("Based on epoch number: %5d" % (best_epoch))
\end{lstlisting}

\subsubsection{Smart grid search}\label{smartgs}

The Python code below is also on GitHub, \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/ZetaGeom.py}{here},
 with a Jupyter notebook version available \href{https://github.com/VincentGranville/Notebooks/blob/main/copula_insurance_nogroup.ipynb}{here} (see section 10 in the notebook in question). It illustrates the smart grid search algorithm to optimize the parameters associated to the
 distribution fit to ``number of children" in the insurance dataset, as illustrated in Figure~\ref{fig:gretfg1009}. The context is parametric copulas,
 where empirical quantiles are replaced by those of a known parametric distribution, with parameters estimated on the real data. It is an alternative to gradient descent. The loss function to minimize is pictured in Figure~\ref{fig:gretddfaq}. The minimum is in the middle of the narrow, elongated basin. Narrow valleys represent a challenge to most optimization techniques. Here, the X-axis represents the value of one of the two parameters, the Y-axis is for the other parameter, and the contour levels represent the value of the loss function. Best fit to real data is attained when loss is minimum.

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{Zipfgeom.png}   
\caption{Loss function for the 2D smart grid search, with minimum in the basin}
\label{fig:gretddfaq}
\end{figure} 





\begin{lstlisting}[numbers=left]
import numpy as np

#--- compute mean and stdev of ZetaGeom[p, a]

def ZetaGeom(p, a):
    C = 0
    for k in range(200):
        C += p**k/(k+1)**a
    mu = 0
    m2 = 0
    for k in range(200):
        mu += k*p**k/(k+1)**a
        m2 += k*k*p**k/(k+1)**a
    mu /= C
    m2 /= C
    var = m2 - mu*mu
    stdev = var**(1/2)
    return(mu, stdev)

#--- optimized grid search to find optimal p and a

def grid_search(grid_range):
    p_min = grid_range[0][0]
    p_max = grid_range[0][1]
    a_min = grid_range[1][0]
    a_max = grid_range[1][1]
    p_step = (p_max - p_min)/10
    a_step = (a_max - a_min)/10
    min_delta = 999999999.9
    for p in np.arange(p_min, p_max, p_step):
        for a in np.arange(a_min, a_max, a_step):
            (mu, std) = ZetaGeom(p, a)
            delta = np.sqrt((mu - target_mu)**2 + (std - target_std)**2)
            if delta < min_delta:
                p_best = p
                a_best = a
                mu_best = mu
                std_best = std
                min_delta = delta
    return(p_best, a_best, mu_best, std_best, min_delta)

#--- estimating p and a based on observed mean and standard deviation

target_mu    = 1.095  # mean
target_std   = 1.205  # standard deviation

p = 0.5
a = 0.0
step_p = 0.4
step_a = 3.0

for level in range(3):
    step_p /= 2
    step_a /= 2
    p_min = max(0, p - step_p)
    p_max = p + step_p
    a_min = a - step_a
    a_max = a + step_a
    grid_range = [(p_min, p_max),(a_min, a_max)]
    (p, a, mu, std, min_delta) = grid_search(grid_range)
    print("delta: %6.4f mu: %6.4f std: %6.4f p: %6.4f a: %6.4f" 
         % (min_delta, mu, std, p, a))

# now (p_fit, a_fit) is such that (mean, std) = (target_mu, target_std)
p_fit = p  
a_fit = a

# now we found the correct p, a to fit to target_mu, target stdev

#--- sampling from ZetaGeom[p, a]

def CDF(p, a):
    C = 0
    for k in range(100):
        C += p**k/(k+1)**a
    arr_CDF = []
    CDF = 0
    for k in range(100):
        CDF += (p**k/(k+1)**a)/C
        arr_CDF.append(CDF)
    return(arr_CDF)

def sample_from_CDF(p, a):
    u = np.random.uniform(0,1)
    k = 0
    arr_CDF = CDF(p, a)
    while u > arr_CDF[k]:
        k = k+1
    return(k)

#--- sample using estimated p, a to match target mean and stdev

nobs = 50000  # number of deviates to produce
seed = 500
np.random.seed(seed)
sample1 = np.empty(nobs)
for n in range(nobs):
    k = sample_from_CDF(p_fit, a_fit)
    sample1[n] = k

mean = np.mean(sample1)
std  = np.std(sample1)
maxx = max(sample1)
print("\nSample stats: mean: %5.3f std: %5.3f max: %5.3f" 
   % (mean, std, maxx))

#--- optional: plotting approximation error for p, a 

from mpl_toolkits import mplot3d
import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib import cm # color maps

xa = np.arange(0.0, 0.6, 0.005)
ya = np.arange(-3.0, 0.0, 0.025)
xa, ya = np.meshgrid(xa, ya)
za = np.empty(shape=(len(xa),len(ya)))

kk = 0
for p in np.arange(0.0, 0.6, 0.005):
    hh = 0
    for a in np.arange(-3.0, 0.0, 0.025):
        (mu, std) = ZetaGeom(p, a)
        delta = np.sqrt((mu - target_mu)**2 + (std - target_std)**2)
        za[hh, kk] = delta
        hh += 1
    kk += 1

mpl.rcParams['axes.linewidth'] = 0.5
fig = plt.figure() 
axes = plt.axes()
axes.tick_params(axis='both', which='major', labelsize=8)
axes.tick_params(axis='both', which='minor', labelsize=8)
CS = axes.contour(xa, ya, za, levels=150, cmap=cm.coolwarm, linewidths=0.35)
cbar = fig.colorbar(CS, ax = axes, shrink = 0.8, aspect = 5)
cbar.ax.tick_params(labelsize=8)
plt.show()

#--- compare with zeta with same mean mu = 1.095

p = 1.0
a = 2.33  # a < 3 thus var is infinite
sample2 = np.empty(nobs)
for n in range(nobs):
    k = sample_from_CDF(p, a)
    sample2[n] = k

mean = np.mean(sample2)
std  = np.std(sample2)
maxx = max(sample2)
print("Sample stats Zeta: mean: %5.3f std: %5.3f max: %5.3f" 
   % (mean, std, maxx))

#--- compare with geom with same mean mu = 1.095

p = target_mu/(1 + target_mu)
a = 0.0
sample3 = np.empty(nobs)
for n in range(nobs):
    k = sample_from_CDF(p, a)
    sample3[n] = k

mean = np.mean(sample3)
std  = np.std(sample3)
maxx = max(sample3)
print("Sample stats Geom: mean: %5.3f std: %5.3f max: %5.3f" 
   % (mean, std, maxx))

#--- plot probability density functions

axes.tick_params(axis='both', which='major', labelsize=4)
axes.tick_params(axis='both', which='minor', labelsize=4)
mpl.rc('xtick', labelsize=8) 
mpl.rc('ytick', labelsize=8) 
plt.xlim(-0.5,9.5)
plt.ylim(0,0.8)

cdf1 = CDF(p_fit, a_fit) 
cdf2 = CDF(1.0, 2.33) 
cdf3 = CDF(target_mu/(1+target_mu), 0.0) 

for k in range(10):
    if k == 0:
        pdf1 = cdf1[0]
        pdf2 = cdf2[0]
        pdf3 = cdf3[0]
    else:
        pdf1 = cdf1[k] - cdf1[k-1]
        pdf2 = cdf2[k] - cdf2[k-1]
        pdf3 = cdf3[k] - cdf3[k-1]
    plt.xticks(np.linspace(0,9,num=10))
    plt.plot([k+0.2,k+0.2],[0,pdf1],linewidth=5, c='tab:green', label='Zeta-geom')
    plt.plot([k-0.2,k-0.2],[0,pdf2],linewidth=5, c='tab:orange',label='Zeta')
    plt.plot([k,k],[0,pdf3],linewidth=5, c='tab:gray',label='Geom')

plt.legend(['Zeta-geom','Zeta','Geom'],fontsize = 7)
plt.show()
\end{lstlisting}

\section{Difference between synthetization and simulation}


I focus here on some particular aspects of data synthetizations, that differentiate them from other techniques.
Simulations do not simulate joint distributions.
Sure, if all your features behave like a mixture of multivariate normal distributions, you can use GMMs (Gaussian mixture models) for synthetization. This is akin to 
\textcolor{index}{Monte-Carlo simulation}\index{Monte-Carlo simulations}. The parameters of the mixture -- number of clusters, covariance matrix attached to each Gaussian distribution (one per cluster), and the mixture proportions -- can be estimated using the EM algorithm. It is subject to \textcolor{index}{model identifiability}\index{identifiability (statistics)} issues, but it will work.

If the interdependence structure among the features is essentially linear, in other words well captured by the correlation matrix, you can decorrelate the features using a linear transform such as PCA to remove cross-correlations, then sample each feature separately using standard simulation techniques, and finally apply the inverse transform to add the correlations back. This is similar to what the copula method accomplishes. Each decorrelated feature can be modeled using a parametric \textcolor{index}{metalog distribution}\index{metalog distribution} to fit with various shapes, akin to Monte-Carlo simulations.

\subsection{Frequently asked questions}

The following questions are frequently asked by participants working on the data synthetizations projects. I added the most popular ones in this section, with my answer.

\subsubsection{Dealing with a mix of categorical, ordinal, and continuous features}

This is when synthetization becomes most useful. The copula method can handle it easily. For categorical variables, you can create buckets also called flag vectors. For instance, 
\texttt{[smoker=yes,region=South,gender=F]} is a bucket.  Frequency counts are computed for each bucket in the real dataset. Generate the estimated frequencies for each bucket when synthetizing data. You may aggregate all small buckets into one catch-all bucket. This method, similar to decision trees and \textcolor{index}{XGboost}\index{XGboost}, is a good alternative to turning your categorical features into a large number of binary, numerical 
\textcolor{index}{dummy variables}\index{dummy variables}.

To deal with non-linear interdependencies among your features, GAN synthetizations  are usually superior to copula-based methods. Of course, the two methods can be blended: remove the cross-correlations first, then synthetize decorrelated features using GAN, then add back the cross-correlations, with the same linear transform and inverse transform as discussed earlier.
One issue is how to measure the correlation between categorical features, or between categorical and numerical features. Metrics such as 
\textcolor{index}{Cramér's V}\index{Cramér's V} accomplish this, returning a value between 0 and 1, instead of between -1 and 1 for standard correlations.

\subsubsection{Do Gaussian copulas work on non-Gaussian observations?}

Copulas and GANs aimed at replicating the whole joint distribution, not each component separately, but also the correlations (for copulas) and non-linear feature interactions (GANs). It works with discrete and multimodal distributions combined together, regardless of the underlying distribution (copulas are based on empirical quantiles, although parametric versions are available).
Whether you use a Gaussian or Frank or Vine copula does not really matter, except when dealing with distributions with very long tails. Same with GAN: you use Gaussian distributions for latent variables regardless of the actual distributions in the real data.

\subsubsection{My simulations do as well as synthetizations, how so?}

You need to compare the feature dependencies structures as well, not just feature-to-feature (1D) comparisons. Perfect replication of univariate distributions is easy, but replication of cross-interdepencies is the challenging part. See how I measure the quality of the results using the 
$\Delta_{\text{avg}}$ in section~\ref{holfghre3} .

Basically, I compute the correlation matrix $M_1$ on real data, then $M_2$ on synthetic data, then $\Delta = M_1 – M_2$ except that I take the absolute value of the difference for each entry in $M_1 – M_2$. This 
\textcolor{index}{correlation distance matrix}\index{correlation distance matrix} is denoted as $\Delta$. Then $\Delta_{\text{avg}}$ is the value averaged over all elements or $\Delta$. 
So if there are m features, the matrix $\Delta$ is $m \times m$, symmetric, the main diagonal is zero, and each element has a value between 0 (great fit) and 1 (bad fit). Note that this method focuses on bivariate comparisons only, and linear interactions only. There are methods to compare more complex multidimensional interactions. 
More on this in my book on synthetic data and generative AI~\cite{vgelsevier}. See also~\cite{ieeeaccess22}.

%---

\subsubsection{Sensitivity to changes in the real data}

To avoid over-fitting, you assess the quality of the resulting synthetization using the \textcolor{index}{holdout method}\index{holdout method}. Say 50\% of your real data is used to train GAN or set up the copula, and the remaining 50\% (called validation set) used to compare with synthetic data. This cross-validation technique makes your comparison more meaningful. Sensitivity to the original distribution should not be too large unless you introduce a lot of noise to assess sensitivity to the point that the difference between $D_1$ and $D_2$ is larger than between $S_1$ and $D_1$ or $S_2$ and $D_2$. Here $D_1, D_2$ are the real data and real data after adding noise, while $S_1, S_2$ are the corresponding synthetizations.

My own GAN is quite sensitive (compared to vendors) but there are ways to reduce this problem by choosing the loss function and other techniques. My copula is more robust than the open source SDV library, as SDV is a combo GAN/Copula and Lite SDV (the version tested) uses a fast but poor gradient descent algorithm for GAN. Some parameter fine-tuning might be needed to reduce sensitivity. On the circle data,  my GAN does better than the copula, with some vendors (YData.ai in particular) doing even much better, and some vendors including SDV Lite doing much worse. 
\textcolor{index}{Wasserstein GANs}\index{Wasserstein GAN (WGAN)}\index{GAN (generative adversarial network)!Wasserstein (WGAN)} [\href{https://en.wikipedia.org/wiki/Wasserstein_GAN}{Wiki}] is an alternative~\cite{ieeewgan}, also designed to avoid 
\textcolor{index}{mode collapse}\index{mode collapse}. This happens when the underlying \textcolor{index}{gradient descent}\index{gradient descent} method gets stuck in some local optimum. 
See example of implementation, \href{https://machinelearningmastery.com/how-to-code-a-wasserstein-generative-adversarial-network-wgan-from-scratch/}{here}.

\subsection{Project: synthetizations with categorical features}

The goal is to compare two synthetizations based on the insurance dataset: one obtained without ignoring the dependencies between the categorical and quantitative features, and the other one treating the 
 categorical and quantitative features independently. Here GroupID represents a \textcolor{index}{bucket}\index{bucketization}, as illustrated in
 Table~\ref{ycpufd1re}. Since ``gender" and ``smoking status" have 2 potential values, while ``region" has 4, the total number
 of buckets is at most $2\times 2\times 4 = 16$. Thus GroupID ranges from 0 to 15.

\pagebreak %---
\noindent The project consists of the following steps: \vspace{1ex}
\begin{itemize}
\item[] {\bf Step 1}: Generate 1300 observations, synthesizing the categorical features ``gender", ``smoking status", and ``region", but ignoring the quantitative features ``charges", ``bmi", ``number of children", and ``age". Analyze the computational complexity of your method. Can you improve it?  \vspace{1ex}
\item[] {\bf Step 2}: For each observation, in addition to the categorical features synthesized in step 1, generate the quantitative features. Use a separate copula
 for each GroupID in table~\ref{ycpufd1re}. \vspace{1ex}
\item[] {\bf Step 3}: Same as step 2, but this time, for the quantitative features use the same global copula for all the observations regardless of GroupID.\vspace{1ex}
\item[] {\bf Step 4}: Compute the average charge per GroupID, both for the synthetizations obtained in steps 2 and 3, and for the real data. 
Conclude that step 2 yields superior results compared to step 3,  because step 3 ignores the dependencies between the categorical
 and quantitative features. 
\end{itemize}

\subsection{Solution}

\noindent The Python code in this section answers \textcolor{red}{step 1}.  The computational complexity of my solution is equal to the square of the number of buckets  (here $16^2$) multiplied by the number of observations to generate. Note that the counts in the synthetization follow a 
 \textcolor{index}{multinomial distribution}\index{multinomial distribution} with 16 parameters: the frequencies attached to each GroupID. Thus the 1300 observations could be generated faster, without a loop, using the multinomial generator available in the Numpy library.

In this project, each of the 16 potential buckets have at least 20 observations. However, when granular categorical features such as zip code are present, some buckets may have too few or no observations, even if the dataset is very large. You can bundle these small buckets with the closest ones, or use county rather than zip code. Or treat these small buckets separately, as explained in my hidden decision trees algorithm: see chapter 2 in my book~\cite{vgelsevier}.
 
  





\begin{table}[H]
%\[\
\begin{center}
\scalebox{0.9}{
\begin{tabular}{llccrr}
\hline
  GroupID & Gender & Smoker & Region & real & synth. \\
\hline
0&female&yes&southwest&21&29\\
1&male&no&southeast&134&141\\
2&male&no&northwest&132&152\\
3&female&no&southeast&139&129\\
4&female&no&northwest&135&137\\
5&male&no&northeast&125&126\\
6&female&yes&southeast&36&34\\
7&male&no&southwest&126&106\\
8&male&yes&southeast&55&47\\
9&female&no&northeast&132&130\\
10&male&yes&southwest&37&29\\
11&female&no&southwest&141&132\\
12&female&yes&northeast&29&25\\
13&male&yes&northeast&38&44\\
14&male&yes&northwest&29&28\\
15&female&yes&northwest&29&33\\
\hline		
\end{tabular}
}
%\]
\caption{\label{ycpufd1re} Group counts, real versus synthesized}
\end{center}
\end{table}


To answer \textcolor{red}{step 2}, see the data grouping step in Project~\ref{ot6zej7}. It is based on the same dataset. The results from step 1 tells you how many observations to synthetisize for each GroupID, that is, for each copula. The improvement obtained by using a separate copula for each GroupID, as opposed to a same global copula, 
 is discussed in section 4 (assessing quality of synthetized data) \href{https://github.com/VincentGranville/Notebooks/blob/main/copula_insurance_byGroup.ipynb}{in this notebook}. This provides the answer to \textcolor{red}{step 4}. 
The Python code below, solving \textcolor{red}{step 1}, is also on GitHub,
\href{https://github.com/VincentGranville/Main/blob/main/synthesize_categories.py}{here}.

Categorical features in GAN can be handled with \textcolor{index}{softmax}\index{softmax function} output
 [\href{https://en.wikipedia.org/wiki/Softmax_function}{Wiki}]. See the section ``Wasserstein GAN on categorical data" in \href{https://medium.com/jungle-book/towards-data-set-augmentation-with-gans-9dd64e9628e6}{this paper}, and ``Generating Multi-Categorical Samples with Generative Adversarial Networks"~\cite{gancat18}, with the accompanying code \href{https://github.com/rcamino/multi-categorical-gans}{here}. Finally, to evaluate the quality of synthetic data, you should not focus on raw features only, but compare ratios. For instance, in the diabetes dataset, compare the cancer rate per age group, between real and synthesized data. In this case, cancer and age are the raw features: the former being binary (yes/no, for each patient), and the latter being ordinal.    
\vspace{1ex}

\begin{lstlisting}
import pandas as pd
import numpy as np

url="https://raw.githubusercontent.com/VincentGranville/Main/main/insurance.csv"
# make sure fields don't contain commas
data = pd.read_csv(url)
print(data.head(10))

groupID = {}
groupLabel = {}
groupCount = {}
ID = 0

Nobs = len(data)
for k in range(0, Nobs):  
    obs = data.iloc[k]   # get observation number k
    group = obs[1] +"\t"+obs[4]+"\t"+obs[5]
    if group in groupID: 
        groupCount[group] += 1
    else:
        groupCount[group] = 1
        groupID[group] = ID 
        groupLabel[ID] = group          
        ID += 1
Ngroups = len(groupID)

Nobs_synth = Nobs
seed = 453
np.random.seed(seed)

GroupCountSynth = {}
Synth_group = {}
for k in range(Nobs_synth):
    u = np.random.uniform(0.0, 1.0)
    p = 0
    ID = -1
    while p < u:
        ID = ID + 1
        group = groupLabel[ID]
        p += groupCount[group]/Nobs
    group = groupLabel[ID]
    if group in GroupCountSynth:
        GroupCountSynth[group] += 1 
    else:
        GroupCountSynth[group] = 0
    Synth_group[k] = group  # GroupID assigned to synthetic observation k

for group  in groupCount:
    print(group, groupCount[group], GroupCountSynth[group])

\end{lstlisting}

\section{Music, synthetic graphs, LLM, and agent-based models}

Besides the projects described in this chapter, there are other projects throughout this book, related to
 generative AI, yet not involving generative adversarial networks. In particular, sections~\ref{prgt43zas} and~\ref{georeshy} deal respectively with time series and geospatial synthetizations, using interpolation techniques. Then, 
section~\ref{genaiyert} features a very fast synthetizer for tabular data, consistently leading to better synthetizations than those produced by GANs.
For DNA sequence synthetization based on LLM techniques, see section~\ref{dnalove}. To synthesize music,
 see section~\ref{music911}. Finally, synthetic graphs and agent-based models are covered in section~\ref{misc911}.











%---------------------------------
\chapter{Data Visualizations and Animations}

This chapter is organized differently. Rather than working on specific project steps, the goal is to learn how to understand, reproduce 
 and fine-tune advanced visualizations. In the process, you will learn various visual techniques and how to optimize their parameters. Most importantly, you want to apply what you learned to your own datasets.  To illustrate the visualizations,  I use various case studies. 
Typically, each section has the following components:\vspace{1ex}

\begin{itemize}
\item An AI algorithm performing some task, for instance curve fitting, using some input data, and producing some output. You don't need
 to understand how the AI algorithm works. You only need to think about potential insightful visualizations, to tell what the algorithm 
 accomplishes. In short, thinking in terms of images rather than words.
\vspace{1ex}
\item The visualization part, typically at the end of the Python code, once the output is created. In some cases, the visualization is a data animation (video). In this case, I provide a link to the actual video on YouTube. But I also include a number of frames in this textbook, to give you an idea. For instance, see Figure~\ref{ringersap}
\end{itemize}
\vspace{1ex}

\noindent The topics are selected based both on the usefulness of the associated AI algorithm, and the value of the 
accompanying visualizations. Everything is in Python. I do not cover dashboards and techniques based on BI tools. 
 The focus is on high quality, insightful visual output, as well as on covering fundamental computer vision concepts:
 DPI, frames per second, RGB channels, color opacity, optimum palettes, graph visualization, bitmaps and grids, contour
 levels, orthogonal trajectories and so on. As a starter, I encourage you to read section~\ref{vesurine}, featuring a 3D visualization 
 with contour levels and rotating shapes seen from various angles over time.

\section{Synthesizing data outside the observation range}

Given a continuous feature such as annual charges per policy holder for an insurance company, how do you synthesize data mimicking the
 distribution observed in the training set? In this case, one observation corresponds to one policy holder. Using the 
 \textcolor{index}{empirical cumulative distribution}\index{empirical distribution}
 (ECDF), the solution is straightforward.  In fact, many \textcolor{index}{synthetic data}\index{synthetic data} generators rely on
  the \textcolor{index}{quantile function}\index{quantile!quantile function} -- the inverse of the ECDF -- available in various Python libraries. 

But there is a big caveat. All implementations allow you to sample between the minimum and maximum observed in the training set.
 If you synthesize a large number of observations, this becomes a serious issue. For instance, if your training data
 has 1000 observations, you want the 0.9995 quantile to be above the maximum, and the 0.0005 quantile to be below the minimum. But by how much? 
 In short, you want to write you own quantile function: one that generates 
 \textcolor{index}{extrapolated quantiles}\index{quantile!extrapolated quantile}. I discuss how to do it 
 \href{https://mltblog.com/3QUj6qP}{here}. This is the core AI algorithm used in this section, with Python code
 in section~\ref{refresa}. 


\subsection{Animated histograms for extrapolated quantiles}\label{quantcare}

The algorithm generating  the extrapolated quantiles has one important parameter: $v$. When $v=0$, the output is
 identical to traditional quantiles. As $v$ increases, the quantiles get more and more extrapolated, with a smoother distribution
 and wider range. Figure~\ref{ringersap} illustrates this transition, with $v=0$ for the top left histogram, and maximum for the bottom right.
 The goal is to produce a video showing the exptrapolated quantiles, with 500 frames. Each frame corresponds to a specific value of $v$, with $v$ (starting at zero) increasing over time. You can watch the video \href{https://mltechniques.com/2023/11/26/easy-trick-to-debias-genai-models-quantile-convolution/}{here}. Note that unlike the selected 64 frames in Figure~\ref{ringersap}, the video has ticks and labels on both axes (vertical, horizontal), as well as a legend showing the value of $v$ at any time, thus, updated in real time when you watch it.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{eqthumb2.png}   
\caption{From raw data histogram (top left) to extrapolated quantiles (bottom right)}
\label{ringersap}
\end{figure}

The code in section~\ref{refresa} produces both the video and Figure~\ref{ringersap}. Now, let's discuss the graphical elements. 
 For easy presentation, I broke them down into the following categories:
\vspace{1ex}
\begin{itemize}
\item {\bf Video library and parameters}. I used the \textcolor{index}{MoviePy}\index{Python library!Moviepy} library, in particular
 \texttt{ImageSequenceClip} that reads PNG images and combine them into a video. You might want to check options  
 such as conversion to GIF, compression mode or resolution (dots per inch or DPI). Here I use \texttt{fps=10}, that is, 10 frames per second.
\vspace{1ex}
\item {\bf Matplotlib parameters}. The options and plot types (histograms, scatterplots, contour maps, 3D, grids, and so on) are too numerous to list. The goal in this chapter 
 is to cover the most important ones. Here, note the use of \texttt{plt.savefig} to save the video frames produced 
 by \texttt{plt.hist}, as PNG images. General Matplotlib options are specified using 
\texttt{mpl.rcParams} and \texttt{plt.rcParams}.
\vspace{1ex}
\item {\bf Adaptive legend}.  In \texttt{plt.hist}, the option \texttt{label='v=\%6.4f'\%v}, combined in the following line~of code with the
instruction \texttt{plt.legend(loc='upper right',prop=\{'size':6\})},  allows you to display  the legend in the upper right corner in each frame,
 with a different value of $v$ depending on the frame. The formatting (float with 4 digits) is specified by  \texttt{\textquotesingle v=\%6.4f\textquotesingle} while
 the actual argument $v$ ia passed via \texttt{\%v}. The argument \texttt{'size':6} specifies the size of the font.
\vspace{1ex}
\item {\bf Image transforms}. Images must have even dimensions (length and height, in pixels), and they must all have the same size,
 to produce a viewable video. See how I do it my \texttt{save\_image} function in the code. In
 other cases, I had to use \textcolor{index}{antialiasing}\index{antialiasing} techniques to eliminate 
\textcolor{index}{pixelation} effects. See how I did it, \href{https://github.com/VincentGranville/Visualizations/blob/main/Source-Code/image2R.py}{here}.
\vspace{1ex}
\item {\bf GIF format}. You can use the MoviePy library to produce animated GIFs instead of videos. But I~was not satisfied with the results. 
 Instead I used free online tools such as Ezgif to convert MP4 videos (produced by MoviePy) to GIF. 
\end{itemize}
\vspace{1ex}


\subsection{Python code: video, thumbnails}\label{refresa}

The code listed here produces both the video and Figure~\ref{ringersap}. 
The code is also on GitHub,  \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/equantile_thumbnails.py}{here}.
\vspace{1ex}

\begin{lstlisting}[numbers=left]
# equantileThumbnails.py: extrapolated quantiles, with thumbnails production

import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import pandas as pd
from PIL import Image
import moviepy.video.io.ImageSequenceClip

seed = 76
np.random.seed(seed)

def get_test_data(n=100):
    data = []
    for k in range(n):
        u = np.random.uniform(0, 1)
        if u < 0.2:
            x = np.random.normal(-1, 1)
        elif u < 0.7:
            x = np.random.normal(0, 2)
        else: 
            x = np.random.normal(5.5, 0.8)
        data.append(x)
    data = np.array(data)
    return(data)

def get_real_data():
    url = "https://raw.githubusercontent.com/VincentGranville/Main/main/insurance.csv"
    data = pd.read_csv(url)
    # features = ['age', 'sex', 'bmi', 'children', 'smoker', 'region', 'charges'] 
    data = data['bmi']    # choose 'bmi' or 'charges'
    data = np.array(data)
    return(data)

#--

def truncated_norm(mu, sigma, minz, maxz):
    z = np.random.normal(mu, sigma)
    if minz < maxz:
        while z < minz or z > maxz:
            z = np.random.normal(mu, sigma)
    return(z)

#- sample from mixture

def mixture_deviate(N, data, f, sigma, minz, maxz, verbose=False):
    sample = []
    point_idx = np.random.randint(0, len(data), N) 
    mu = data[point_idx]
    for k in range(N):
        z = truncated_norm(mu[k], sigma, minz, maxz)
        sample.append(z)
        if verbose and k%10 == 0:
            print("sampling %6d / %6d" %(k, N))
    sample = np.array(sample)
    sample = np.sort(sample)
    return(sample)

#--- Main part

data = get_test_data(100)
# data = get_real_data()
N = 1000000
truncate = False

# minz > maxz is the same as (minz = -infinity, maxz = +infinity)
if truncate == True:
    minz = 0.50 * np.min(data)  # use 0.95 for 'charges', 0.50 for 'bmi'
    maxz = 1.50 * np.max(data)  # use 1.50 for 'charges', 1.50 for 'bmi'
else:
    minz = 1.00
    maxz = 0.00

#--- Making video

mpl.rcParams['axes.linewidth'] = 0.9
plt.rcParams['xtick.labelsize'] = 7
plt.rcParams['ytick.labelsize'] = 7
bins=np.linspace(-7.0, 10.0, num=100)

pbins = 1000
step = N / pbins      # N must be a multiple of pbins
my_dpi = 300          # dots per each for images and videos 
width  = 2400         # image width
height = 1800         # image height
flist = []            # list image filenames for video
nframes = 100 
velocity = 1.75

def save_image(fname,frame):
    global fixedSize
    plt.savefig(fname, bbox_inches='tight')    
    # make sure each image has same size and size is multiple of 2
    # required to produce a viewable video   
    im = Image.open(fname)
    if frame == 0:  
        # fixedSize determined once for all in the first frame
        width, height = im.size
        width=2*int(width/2)
        height=2*int(height/2)
        fixedSize=(width,height)
    im = im.resize(fixedSize) 
    im.save(fname,"PNG")
    return()

for frame in range(nframes):

    print("Processing frame", frame)
    v = 0.4*(frame/nframes)**velocity  ### np.log(1 + frame)/100
    sigma = v * np.std(data) 
    sample = mixture_deviate(N, data, truncated_norm, sigma, minz, maxz)
    equant = []

    for k in range(pbins):
        p = (k + 0.5) / pbins
        eq_index = int(step * (k + 0.5))
        equant.append(sample[eq_index])

    plt.figure(figsize=(width/my_dpi, height/my_dpi), dpi=my_dpi)
    plt.hist(equant,color='orange',edgecolor='red',bins=bins,linewidth=0.3,label='v=%6.4f' %v)
    # plt.legend(loc='upper right', prop={'size': 6}, ) 
    plt.ylim(0,60)
    plt.xticks([])  # comment out for the video
    plt.yticks([])  # comment out for the video
    fname='equant_frame'+str(frame)+'.png'
    flist.append(fname)
    save_image(fname,frame)
    plt.close()

clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(flist, fps=10)
clip.write_videofile('equant.mp4')


#--- Making picture with thumbmails
#
# note: nframes must be a multiple of n_thumbnails

columns  = 10
rows = 10
n_thumbnails = columns * rows
increment = int(nframes / n_thumbnails) 

import matplotlib.image as mpimg
cnt = 1

for frame in range(0, nframes, increment):

    fname = flist[frame]
    img = mpimg.imread(fname)
    plt.subplot(rows, columns, cnt)
    plt.xticks([])  
    plt.yticks([])
    plt.axis('off')
    plt.subplots_adjust(wspace = 0.05, hspace = 0.05)
    plt.imshow(img)
    cnt += 1

plt.show()
\end{lstlisting}

\section{Curve fitting in bulk}\label{assamero}

I decided to include this material for two reasons. First, the underlying AI algorithm is a particular case of a generic technique that can perform any kind of regression, supervised or not, under one umbrella. In particular, there is no dependent variable in the example featured in this section.
 The technique uses \textcolor{index}{gradient descent}\index{gradient descent} with a \textcolor{index}{Lagrange multiplier}\index{Lagrange multiplier} to satisfy some arbitrary constraints on the regression coefficients. 
 In neural network contexts, the constraints are called \textcolor{index}{regularization}\index{regularization}. 
The generic method, called \textcolor{index}{cloud regression}\index{cloud regression}, 
is described in chapter~1 in my book on synthetic data~\cite{vgsynthetic}. Here I focus on a particular application:
 \textcolor{index}{curve fitting}\index{curve fitting}.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{cf25.png}   
\caption{Twenty five frames from the curve fitting video}
\label{ringersap25r}
\end{figure}

Then, the visualization features a powerful method to summarize a large number of tests, with just one short data animation.
 To illustrate the potential, I tested the curve fitting procedure on 500 training sets, each one generated with
 a specific set of parameters and a different amount of noise. The goal is to check how well the curve fitting technique works depending on the data.
The originality lies in the choice of a continuous path in the parameter space, so that moving from one training set to the other (that is, from one frame
 to the next in the video) is done smoothly, yet covering a large number of possible combinations.
See 25 of the 500 video frames in Figure~\ref{ringersap25r}. Note how the transitions are smooth, yet over time cover various situations:
 a changing rotation angle, training sets (the red dots) of various sizes, ellipse eccentricity that varies over time, and noise ranging from strong to weak. 



\subsection{Regression without dependent variable}

The curve fitting method is an example of regression without dependent variable. You don't predict a response $y$ given a set of features $x$, using
 a formula such as $y = f(x, \theta)$ where $\theta$ is the model parameter (a vector). Instead, you fit $f(y, x, \theta)=0$ under some
 constraint such as $\theta\cdot\theta^T = 1$.


 If you look at Figure~\ref{ringersap25r}, the curve $f$  is a 
\textcolor{red}{multivalued function}\index{multivalued function} [\href{https://en.wikipedia.org/wiki/Multivalued_function}{Wiki}]. 
 Thus, the method is more general~than regression. At the limit, when the eccentricity (one of the parameters) is infinite, the ellipse becomes a straight line. Assuming $\theta\cdot\theta^T=1$, the problem is equivalent to a \textcolor{index}{Deming regression}\index{Deming regression} 
 [\href{https://en.wikipedia.org/wiki/Deming_regression}{Wiki}]. The regression line~can~be vertical: standard regression fails in that case. Then, depending on $\theta$, the method encompasses polynomial regression. If the features are properly scaled, taking values between $-1$ and $+1$, it is actually a {\em robust} polynomial regression with no risk of overfitting. 
Also you can have a curve consisting of two ellipses rather than one. Then, it becomes a clustering problem! 
Finally, see in Figure~\ref{ringersap25r} how it works with partial ellipse arcs. 

Now I focus on the visualizations. What I wrote in section~\ref{quantcare} regarding video generation,  also applies here.
In the code in section~\ref{pcrf}, you should focus on the following:
\vspace{1ex}
\begin{itemize}
\item Line \textcolor{gray}{293} produces the animated GIF, while \textcolor{gray}{295} -- \textcolor{gray}{310} generates Figure~\ref{ringersap25r}. 
The video frames
 are created in lines \textcolor{gray}{251} -- \textcolor{gray}{286}, with the video produced in lines \textcolor{gray}{289} -- \textcolor{gray}{290}. Watch the video on YouTube, 
\href{https://www.youtube.com/watch?v=QSAT546tITM}{here}. 
\item Parameters controlling the video (DPI, number of frames) are set in 
 lines \textcolor{gray}{240} and \textcolor{gray}{243}. Frames per second (\texttt{fps}) is set in 
 line \textcolor{gray}{289}, and the name of the PNG images in line \textcolor{gray}{256}. I invite you to fine-tune them.
\item Parameters controlling the curve fitting algorithm are set in lines \textcolor{gray}{259} -- \textcolor{gray}{271}.
Some parameters depend on a real number \texttt{p} between 0 and 1 (see line \textcolor{gray}{266}), representing the time in the video from beginning to end. 
Again, you can play with these parameters. 
\end{itemize}




% \begin{lstlisting}[numbers=left,firstnumber=4,stepnumber=5,escapeinside={\%}{\%}]


\subsection{Python code}\label{pcrf}

This version is different from the original listed \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/fittingEllipse.py}{here}. In the original version, you can also generate confidence intervals for the fitted ellipse, that is, for the estimated parameters.
This feature is not included in the code below. However, this new version produces better visualizations.  You can find it on GitHub, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/fittingEllipse_v2.py}{here}.
\vspace{1ex}

\begin{lstlisting}[numbers=left]
# Fitting ellipse via least squares

import numpy as np
import matplotlib.pyplot as plt
import moviepy.video.io.ImageSequenceClip  # to produce mp4 video
from PIL import Image  # for some basic image processing

def fit_ellipse(x, y):

    # Fit the coefficients a,b,c,d,e,f, representing an ellipse described by
    # the formula F(x,y) = ax^2 + bxy + cy^2 + dx + ey + f = 0 to the provided
    # arrays of data points x=[x1, x2, ..., xn] and y=[y1, y2, ..., yn].

    # Based on the algorithm of Halir and Flusser, "Numerically stable direct
    # least squares fitting of ellipses'.

    D1 = np.vstack([x**2, x*y, y**2]).T
    D2 = np.vstack([x, y, np.ones(len(x))]).T
    S1 = D1.T @ D1
    S2 = D1.T @ D2
    S3 = D2.T @ D2
    T = -np.linalg.inv(S3) @ S2.T
    M = S1 + S2 @ T
    C = np.array(((0, 0, 2), (0, -1, 0), (2, 0, 0)), dtype=float)
    M = np.linalg.inv(C) @ M
    eigval, eigvec = np.linalg.eig(M)
    con = 4 * eigvec[0]* eigvec[2] - eigvec[1]**2
    ak = eigvec[:, np.nonzero(con > 0)[0]]
    return np.concatenate((ak, T @ ak)).ravel()

def cart_to_pol(coeffs):

    # Convert the cartesian conic coefficients, (a, b, c, d, e, f), to the
    # ellipse parameters, where F(x, y) = ax^2 + bxy + cy^2 + dx + ey + f = 0.
    # The returned parameters are x0, y0, ap, bp, e, phi, where (x0, y0) is the
    # ellipse centre; (ap, bp) are the semi-major and semi-minor axes,
    # respectively; e is the eccentricity; and phi is the rotation of the semi-
    # major axis from the x-axis.

    # We use the formulas from https://mathworld.wolfram.com/Ellipse.html
    # which assumes a cartesian form ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0.
    # Therefore, rename and scale b, d and f appropriately.
    a = coeffs[0]
    b = coeffs[1] / 2
    c = coeffs[2]
    d = coeffs[3] / 2
    f = coeffs[4] / 2
    g = coeffs[5]

    den = b**2 - a*c
    if den > 0:
        raise ValueError('coeffs do not represent an ellipse: b^2 - 4ac must'
                         ' be negative!')

    # The location of the ellipse centre.
    x0, y0 = (c*d - b*f) / den, (a*f - b*d) / den

    num = 2 * (a*f**2 + c*d**2 + g*b**2 - 2*b*d*f - a*c*g)
    fac = np.sqrt((a - c)**2 + 4*b**2)
    # The semi-major and semi-minor axis lengths (these are not sorted).
    ap = np.sqrt(num / den / (fac - a - c))
    bp = np.sqrt(num / den / (-fac - a - c))

    # Sort the semi-major and semi-minor axis lengths but keep track of
    # the original relative magnitudes of width and height.
    width_gt_height = True
    if ap < bp:
        width_gt_height = False
        ap, bp = bp, ap

    # The eccentricity.
    r = (bp/ap)**2
    if r > 1:
        r = 1/r
    e = np.sqrt(1 - r)

    # The angle of anticlockwise rotation of the major-axis from x-axis.
    if b == 0:
        phi = 0 if a < c else np.pi/2
    else:
        phi = np.arctan((2.*b) / (a - c)) / 2
        if a > c:
            phi += np.pi/2
    if not width_gt_height:
        # Ensure that phi is the angle to rotate to the semi-major axis.
        phi += np.pi/2
    phi = phi % np.pi

    return x0, y0, ap, bp, phi

def sample_from_ellipse_even(x0, y0, ap, bp, phi, tmin, tmax, npts):

    npoints = 1000
    delta_theta=2.0*np.pi/npoints
    theta=[0.0]
    delta_s=[0.0]
    integ_delta_s=[0.0]
    integ_delta_s_val=0.0
    for iTheta in range(1,npoints+1):
        delta_s_val=np.sqrt(ap**2*np.sin(iTheta*delta_theta)**2+ \
                            bp**2*np.cos(iTheta*delta_theta)**2)
        theta.append(iTheta*delta_theta)
        delta_s.append(delta_s_val)
        integ_delta_s_val = integ_delta_s_val+delta_s_val*delta_theta
        integ_delta_s.append(integ_delta_s_val)
    integ_delta_s_norm = []
    for iEntry in integ_delta_s:
        integ_delta_s_norm.append(iEntry/integ_delta_s[-1]*2.0*np.pi)    
    
    x=[]
    y=[] 
    for k in range(npts):
        t = tmin + (tmax-tmin)*k/npts
        for lookup_index in range(len(integ_delta_s_norm)):
            lower=integ_delta_s_norm[lookup_index]
            upper=integ_delta_s_norm[lookup_index+1]
            if (t >= lower) and  (t < upper):
                t2 = theta[lookup_index]
                break    
        x.append(x0 + ap*np.cos(t2)*np.cos(phi) - bp*np.sin(t2)*np.sin(phi))
        y.append(y0 + ap*np.cos(t2)*np.sin(phi) + bp*np.sin(t2)*np.cos(phi))

    return x, y

def sample_from_ellipse(x0, y0, ap, bp, phi, tmin, tmax, npts): 

    x = np.empty(npts)
    y = np.empty(npts)
    x_unsorted = np.empty(npts)
    y_unsorted = np.empty(npts)
    angle = np.empty(npts)

    global urs, vrs

    if frame == 0:
        cov=[[ap,0],[0,bp]]
        urs, vrs = np.random.multivariate_normal([0, 0], cov, size = npts_max).T

    # sample from multivariate normal, then rescale 
    count = 0
    index = 0
    while count < npts:
        u = urs[index]
        v = vrs[index]
        index += 1
        d=np.sqrt(u*u/(ap*ap) + v*v/(bp*bp))
        u=u/d
        v=v/d
        t = np.pi + np.arctan2(-ap*v,-bp*u)   
        if t >= tmin and t <= tmax:
            x_unsorted[count] = x0 + np.cos(phi)*u - np.sin(phi)*v
            y_unsorted[count] = y0 + np.sin(phi)*u + np.cos(phi)*v
            angle[count]=t
            count=count+1

    # sort the points x, y for nice rendering with mpl.plot
    hash={}
    hash = dict(enumerate(angle.flatten(), 0)) # convert array angle to dictionary
    idx=0
    for w in sorted(hash, key=hash.get):
        x[idx]=x_unsorted[w]
        y[idx]=y_unsorted[w]
        idx=idx+1

    return x, y

def get_ellipse_pts(params, npts=100, tmin=0, tmax=2*np.pi, sampling='Standard'):

    # Return npts points on the ellipse described by the params = x0, y0, ap,
    # bp, e, phi for values of the parametric variable t between tmin and tmax.

    x0, y0, ap, bp, phi = params
    
    if sampling=='Standard':
        t = np.linspace(tmin, tmax, npts)
        x = x0 + ap * np.cos(t) * np.cos(phi) - bp * np.sin(t) * np.sin(phi)
        y = y0 + ap * np.cos(t) * np.sin(phi) + bp * np.sin(t) * np.cos(phi)
    elif sampling=='Enhanced':
        x, y = sample_from_ellipse(x0, y0, ap, bp, phi, tmin, tmax, npts) 
    elif sampling=='Even':
        x, y = sample_from_ellipse_even(x0, y0, ap, bp, phi, tmin, tmax, npts) 

    return x, y

def vgplot(x, y, color, npts, tmin, tmax):

    plt.plot(x, y, linewidth=0.8, color=color) # plot exact ellipse 
    # fill gap (missing segment in the ellipse plot) if plotting full ellipse
    if tmax-tmin > 2*np.pi - 0.001:
        gap_x=[x[nlocs-1],x[0]]
        gap_y=[y[nlocs-1],y[0]]
        plt.plot(gap_x, gap_y, linewidth=0.8, color=color)
    plt.xticks([])  
    plt.yticks([])  
    plt.xlim(-1 + min(x), 1 + max(x)) 
    plt.ylim(-1 + min(y), 1 + max(y)) 
    return()

def main(npts, noise, seed, tmin, tmax, params, sampling):

    # params = x0, y0, ap, bp, phi (input params for ellipse)
    global ur, vr 

    # Get points x, y on the exact ellipse and plot them
    x, y = get_ellipse_pts(params, npts, tmin, tmax, sampling)

    # perturb x, y on the ellipse with some noise, to produce training set
    if frame == 0: 
      cov = [[1,0],[0,1]]  
      np.random.seed(seed)
      ur, vr = np.random.multivariate_normal([0, 0], cov, size = npts_max).T ### npts).T
    x += noise * ur[0:npts]  
    y += noise * vr[0:npts]  

    # get and print exact and estimated ellipse params
    coeffs = fit_ellipse(x, y) # get quadratic form coeffs
    print('True ellipse    :  x0, y0, ap, bp, phi = %+.5f %+.5f %+.5f %+.5f %+.5f' % params)
    fitted_params = cart_to_pol(coeffs)  # convert quadratic coeffs to params
    print('Estimated values:  x0, y0, ap, bp, phi = %+.5f %+.5f %+.5f %+.5f %+.5f' % fitted_params)
    print()

    # plot training set points in red
    plt.scatter(x, y,s = 3.5,color = 'red') 
 
    # get nlocs points on the fitted ellipse and plot them
    x, y = get_ellipse_pts(fitted_params, nlocs, tmin, tmax, sampling) 
    vgplot(x, y,'blue', nlocs, tmin, tmax)

    # save plots in a picture [filename is image]
    plt.savefig(image, bbox_inches='tight',dpi=dpi)  
    plt.close() # so, each video frame contains one curve only
    return()

#--- Main Part: Initializationa

sampling= 'Enhanced'         # options: 'Enhanced', 'Standard', 'Even' 
npts_max = 50000         # max size of random arrays
nlocs = 2500             # number of points used to represent true ellipse 

dpi =240       # image resolution in dpi (100 for gif / 300 for video)
flist = []     # list of image filenames for the video
gif = []       # used to produce the gif image
nframes = 500  # number of frames in video

# intialize plotting parameters
plt.rcParams['axes.linewidth'] = 0.8
plt.rc('axes',edgecolor='black') # border color
plt.rc('xtick', labelsize=6) # font size, x axis  
plt.rc('ytick', labelsize=6) # font size, y axis

#--- Main part: Main loop

for frame in range(0,nframes): 

    # Global variables: dpi, frame, image
    image='ellipse'+str(frame)+'.png' # filename of image in current frame
    print("Creating image",image) # show progress on the screen

    # params = (x0, y0, ap, bp, phi) : first two coeffs is center of ellipse, last one 
    # is rotation angle, the two in the middle are the semi-major and semi-minor axes.
    #
    # Also: 0 <= tmin < tmax <= 2 pi determine start / end of ellipse arc

    # parameters used for current frame 
    seed = 100            # same seed (random number generator) for all images 
    p = frame/(nframes-1) # assumes nframes > 1
    noise = (1-p)*(1-p)   # amount of noise added to to training set
    npts = int(100*(2-p)) # number of points in training set, < npts_max 
    tmin= (1-p)*np.pi     # training set: ellipse arc starts at tmin >= 0
    tmax= 2*np.pi         # training set: ellipse arc ends at tmax  < 2*Pi 
    params = 4, -3.5, 7, 1+6*(1-p), (p+np.pi/3) # ellipse parameters 

    # call to main function 
    main(npts, noise, seed, tmin, tmax, params, sampling)

    # processing images for video and animated gif production (using pillow library)
    im = Image.open(image)
    if frame==0:  
        width, height = im.size  # determines the size of all future images
        width=2*int(width/2)
        height=2*int(height/2)
        fixedSize=(width,height) # even number of pixels for video production 
    im = im.resize(fixedSize)  # all images must have same size to produce video
    gif.append(im)       # to produce Gif image [uses lots of memory if dpi > 100] 
    im.save(image,"PNG") # save resized image for video production
    flist.append(image)

# output video / fps is number of frames per second
clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(flist, fps=20) 
clip.write_videofile('ellipseFitting.mp4')

# output video as gif file 
gif[0].save('ellipseFitting.gif',save_all=True, append_images=gif[1:],loop=0)  

#--- Making picture with thumbmails
#
# note: nframes must be a multiple of n_thumbnails

columns  = 5
rows = 5
n_thumbnails = columns * rows
increment = int(nframes / n_thumbnails) 

import matplotlib.image as mpimg
cnt = 1

for frame in range(0, nframes, increment):

    fname = flist[frame]
    img = mpimg.imread(fname)
    plt.subplot(rows, columns, cnt)
    plt.xticks([])  
    plt.yticks([])
    plt.axis('off')
    plt.subplots_adjust(wspace = 0.05, hspace = 0.05)
    plt.imshow(img)
    cnt += 1

plt.show()

\end{lstlisting}



\section{Gradient descent: grid, orthogonal trajectories}\label{poures}

Gradient descent is one of the most fundamental algorithms used in AI and machine learning. The goal here is not to understand how it works, but instead,
 to understand the visualizations and graphical elements used in the Python code, to be able to reuse them in other contexts. I already explained how to produce data videos in section~\ref{assamero},
 so I do not discuss  the portion of the code producing the data animations. But it creates spectacular videos showing 2D gradient descent in action, starting with 100 locations
 simultaneously: see video on YouTube, \href{https://www.youtube.com/watch?v=pqQsLpPkvbw}{here}. 
The code is self-contained, producing the graphics (our focus) and also implementing a simple, math-free version of 
\textcolor{index}{gradient descent}\index{gradient descent} (I will mostly ignore that part).   

%----
\begin{figure}[H]
\centering
\includegraphics[width=0.68\textwidth]{RH4_ortho_zeta.png}   
\caption{Yellow dots on a contour line (blue) and orthogonal trajectories (red)}
\label{ringhouls}
\end{figure}
%---- 

The mathematical background is explained in section 1.2 in~\cite{vgmloptim}. Contour lines and orthogonal trajectories are discussed in details in section 1.4.1, 
while I describe video generation in section 1.4.2, all in the same book. You may ignore section 1.4.3. 

Figure~\ref{ringhouls} shows all the important elements: the \textcolor{index}{contour lines}\index{contour line} in blue attached 
to the function of interest (actually, a dataset, not an actual math function), and the 
\textcolor{index}{orthogonal trajectories}\index{orthogonal trajectory}\index{orthogonal trajectory} in red perpendicular to 
the contour lines. Each contour line corresponds to a particular \textcolor{index}{contour level}\index{contour level}, where the function takes the same value. Starting from any location
 -- a yellow dot in this case -- the goal is to follow an orthogonal trajectory, also referred to as \textcolor{index}{gradient path}\index{gradient descent!gradient path},  
until you reach a minimum: one of the two dots colored in cyan. Thus the goal is to find the local minima. 
The background color in Figure~\ref{ringhouls} indicates the value of the function,
 ranging from dark red (high value) to dark blue (low value), computed at a large number of locations on a very granular grid, giving the
 impression to cover all possible locations whithin the window.  Before reading further, try to identify in the code where each of the aforementioned 
elements is implemented. 

\noindent The solution is as follows: 
\vspace{1ex}

\begin{itemize}
\item To map the function to a grid \texttt{[xa,ya]} -- actually, the data points where the function is evaluated -- see lines \textcolor{gray}{31--39} in the code.
 I use the Python function \texttt{meshgrid}. This grid is used as input to the contour plotting functions
 in lines \textcolor{gray}{120} and \textcolor{gray}{145}. 
Note that the \texttt{contour} and \texttt{contourf} functions produce two different visualizations to show the same information.
Look at the arguments, in particular \texttt{cmap} for the color gradient, and \texttt{levels} specifying the number of contour levels.
\item You can also extract data from the contour lines generated by Matplotlib. See how I do it in lines 
\textcolor{gray}{234--241} from the contour object \texttt{CS} created in line \textcolor{gray}{120} or \textcolor{gray}{145}. 
I use it to generate the yellow dots in Figure~\ref{ringhouls}, all located on a same contour line; I plot them 
in line \textcolor{gray}{168} in the code.
\end{itemize}
\vspace{1ex}

In practice, you don't need to store the whole grid in memory, but only a small, local moving window as you follow a descending path (orthogonal trajectory). 
The reason to store the whole grid is due to starting~from 100 locations at the same time. Usually, you only start from one location. In that case,
 you need far less memory, and the algorithm can easily be adapted to higher dimensions, although losing visual capabilities. 
The source code below is on GitHub, \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/gradient.py}{here}. 

%get contour levels data from python

\vspace{1ex}

\begin{lstlisting}[numbers=left]
import matplotlib.pyplot as plt
from matplotlib import cm # color maps
import numpy as np
import mpmath

View = 'Local'       # options: 'Local' or 'Global'
Function = 'Sinh2'   # options: 'Zeta', 'Eta', 'Sinh1', 'Sinh2'
Contour = 'Lines'    # options: 'Lines', 'Surface'
Video = True         # options: True or False
Granularity = 'Low'  # options: 'Low' or 'High'

if View == 'Local':
    min_t = 201.4    # for zeta, choose 201.0 
    max_t = 202.601  # for zeta, choose 203.301 
    min_sigma = 0.26 # for zeta, choose 0.25 
    max_sigma = 0.85 # for zeta, choose 1.60 
    if Granularity == 'High':
        incr_t = 0.0025 # slow, also requires 4x more memory
    elif Granularity == 'Low':
        incr_t = 0.005  # 4x faster than 'High', less accurate 
    incr_sigma = incr_t*(max_sigma - min_sigma)/(max_t - min_t)    
elif View == 'Global':
    min_t = 200
    max_t = 300
    min_sigma = 0.50
    max_sigma = 1.10
    incr_t = 0.05 
    incr_sigma = 0.05
nlevels = 180 # number of contour levels (120 for zeta)

#--- Store values of bivariate function in grid za[( , )]
#     xa[], y[a] are the x and y coordinates

xa = np.arange(min_t, max_t, incr_t)
ya = np.arange(min_sigma, max_sigma, incr_sigma)
xa, ya = np.meshgrid(xa, ya)
k_steps = 1 + int((max_t - min_t)/incr_t)
h_steps = 1 + int((max_sigma - min_sigma)/incr_sigma)
za = np.empty(shape=(len(xa),len(ya)))  # set dimensions for za

k=0
for t in np.arange(min_t, max_t, incr_t):
    print("t=",t) 
    h = 0
    for sigma in np.arange(min_sigma, max_sigma, incr_sigma):    
        if Function == 'Zeta':
            z = mpmath.zeta(complex(sigma, t)) 
        elif Function == 'Eta':
            z = mpmath.altzeta(complex(sigma, t))
        elif Function == 'Sinh2':
            p = np.log(2)
            z = mpmath.cosh(complex(sigma-0.5, t*np.log(t)/4))* \
                   mpmath.cosh(complex(sigma-0.7, p*t)) 
        elif Function == 'Sinh1':
            z = mpmath.cosh(complex(sigma-0.5, t*np.log(t)/4))
        modulus=np.sqrt(z.real*z.real + z.imag*z.imag)
        za[h,k]=modulus
        h = h + 1
    k = k + 1

#--- 3D surface plot 

fig = plt.figure(figsize =(12, 8), dpi=240)
axes = plt.axes(projection ='3d')
axes.set_xlabel('t', fontsize=6)
axes.set_ylabel('sigma',fontsize=6)
axes.set_zlabel('Eta Modulus',fontsize=6)
axes.tick_params(axis='both', which='major', labelsize=6)
axes.tick_params(axis='both', which='minor', labelsize=6)
axes.set_xlim3d(min_t, max_t)
axes.set_ylim3d(min_sigma, max_sigma)
axes.set_zlim3d(0.0, float(np.max(za)))

surf = axes.plot_surface(xa, ya, za, cmap=cm.coolwarm,
                       linewidth=1, antialiased=True)

#--- Create video of 3D surface plot (rotate the plot)

if Video:

    import moviepy.video.io.ImageSequenceClip  # to produce mp4 video
    from PIL import Image  # for some basic image processing

    Nframes = 250 
    flist=[]               # list of image filenames for the video
    w, h, dpi = 4, 3, 300  # width and heigh in inches
    fps=10                 # frames per second

    for frame in range(0,Nframes): 
        image='RH4_'+str(frame)+'.png' # filename of image in current frame
        print("Creating image",image) # show progress on the screen
        angle_vertical = 30 + 30 * np.sin(12*frame/Nframes)
        angle_horizontal = 80+ frame * 360 / Nframes
        axes.view_init(angle_vertical, angle_horizontal)
        plt.savefig(image,bbox_inches='tight')
        im = Image.open(image)
        if frame == 0:  # all images must have the same size
            width, height = im.size
            width=2*int(width/2)
            height=2*int(height/2)
            fixedSize=(width,height)
        im = im.resize(fixedSize) 
        im.save(image,"PNG")
        flist.append(image)

    # output video 
    clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(flist, fps=fps) 
    clip.write_videofile('RH4.mp4')

#--- Create contour map

fig = plt.figure(figsize =(12, 8), dpi=240)
axes = plt.axes()
fig.colorbar(surf, ax = axes, shrink = 0.5, aspect = 5)
# Add horizontal dashed line at sigma = 0.5 (the 'critical line') 
#     https://matplotlib.org/stable/gallery/lines_bars_and_markers/linestyles.html
plt.plot((min_t,max_t),(0.5,0.5),color='black',linestyle=(0,(15,15)),linewidth=0.2)

if View == 'Global':
    CS = axes.contourf(xa, ya, za, levels=nlevels, cmap=cm.coolwarm) 
    plt.savefig('RH4_contours.png',bbox_inches='tight')
    sigma_low  = 0.5 # must be > min_sigma
    sigma_mid  = 0.7
    sigma_high = 1.0   # must be < max_sigma (h_high < h_steps -1)
    sigma_max  = max_sigma
    h_low  = int(0.5+(sigma_low - min_sigma)/incr_sigma)
    h_mid  = int(0.5+(sigma_mid - min_sigma)/incr_sigma)
    h_high = int(0.5+(sigma_high - min_sigma)/incr_sigma)
    h_max  = h_steps-2
    OUT=open("gradient_LH.txt","w")
    header="\tsigma=%f\tsigma=%f\tsigma=%f\tsigma=%f\n" \
            % (sigma_low,sigma_mid,sigma_high,sigma_max)
    OUT.write(header)
    for k in range(k_steps-1):
        t = min_t + incr_t*k
        z_low  = za[h_low, k]
        z_mid  = za[h_mid, k]
        z_high = za[h_high, k]
        z_max  = za[h_max, k]
        line = str(t)+"\t"+str(z_low)+"\t"+str(z_mid)+"\t"+str(z_high)+"\t"+str(z_max)+"\n"
        OUT.write(line)
    OUT.close()
    exit() 
else:
    CS = axes.contour(xa, ya, za, levels=nlevels, cmap=cm.coolwarm, linewidths=0.75) 
    plt.savefig('RH4_contours.png',bbox_inches='tight')

#--- Steepest descent on contour map: sample paths to minimum (a root of |Zeta|)
#    requires: Granulary = 'High', View = 'Local'

def gradient_descent(t, sigma, showStart, showEnd, showPath, mode, n_iter, \
    learn_t, learn_sigma, type):

#   mode = Ascent or Descent, starting point is (t, sigma)
#   type = Gradient (orthogonal trajectory) or Contour
#   learn = learning rate in gradient method; n_iter = number of iterations
#   showStart, showEnd display start/end points if true, on the plot
#   in all cases, the full path is displayed in red on the plot

    x = []
    y = []
    x.append(t)
    y.append(sigma)
    h = int((sigma - min_sigma)/incr_sigma)
    k = int((t - min_t)/incr_t)
    old_z = za[h,k]
    if showStart:
        plt.plot(t, sigma, marker="o", markersize=5, markerfacecolor="yellow",color="black")
    if mode == 'Descent': 
        sign = +1
    elif mode == 'Ascent':
        sign = -1
         
    iter = 0
    while iter < n_iter:
        if type == 'Gradient':
            t = t - learn_t * sign * dx[h,k]/incr_t    
            sigma = sigma - learn_sigma * sign * dy[h,k]/incr_sigma     
        elif type == 'Contour':
            t = t - learn_t * sign * dy[h,k]/incr_sigma  
            sigma = sigma + learn_sigma * sign * dx[h,k]/incr_t 
        if t>min_t and t<max_t and sigma>min_sigma and sigma<max_sigma:   
            x.append(t)
            y.append(sigma)
        old_z = za[h, k]
        h = int(0.5+(sigma - min_sigma)/incr_sigma)
        k = int(0.5+(t - min_t)/incr_t)             
        if h<h_steps-2 and k<k_steps-2 and h>0 and k>0: 
            z = za[h, k]
        else:
            iter = 99999999999
            showEnd = False
        iter = iter + 1

    # smooth the path x, y
    n = len(x)
    if smooth > 0:
        for i in range(1,n-2):
            x[i] = np.mean(x[max(0,i-smooth):min(n-1,i+smooth)])
            y[i] = np.mean(y[max(0,i-smooth):min(n-1,i+smooth)])

    # plot path from intitial point to convergence
    if showPath:
        plt.plot(x,y,color='red',linewidth=0.2)
    if showEnd:   
        # show where the iteration converged
        sigma=int(0.5+100*sigma)/100
        t=int(0.5+100*t)/100
        if t>min_t and t<max_t and sigma>min_sigma and sigma<max_sigma:
            plt.plot(t, sigma, marker="o", markersize=5, markerfacecolor="palegreen",color="black")

    return(x, y)  

#--- Steepest descent: main part 


dy, dx = np.gradient(za)      # matrices with same dim as za 
norm = np.sqrt(dx*dx + dy*dy) # matrix with same dim as za
angle = np.arctan2(dx,dy)     # angle of descent (unused)

smooth = 0                    # integer, to smooth tajectories (0 = no smoothing)
learn_t = incr_t              # learning parameter in gradient method  
learn_sigma = incr_sigma      # learning parameter in gradient method 

showStart = False
showEnd   = False
showPath  = False

# sample points on a contour line
#     need Ascent and Descent for full loop of contour line

level = 40 # level=1 is the one with lowest za[,]. For zeta, set level=36 
           # level must be an integer between 1 and nlevels
lines = []  
x = []
y = []
for line in CS.collections[level].get_paths():
    lines.append(line.vertices)
x = lines[0][:, 0]
y = lines[0][:, 1]
n = len(x)
showStart = True
showEnd   = False
showPath  = True
n_iter = 3000 
step = 5 # for zeta choose step=10

for i in range(0, n, step):  
    t = x[i]
    sigma = y[i]
    # need Ascent and Descent for full path up and down
    gradient_descent(t, sigma, showStart, showEnd, showPath, 'Descent', n_iter, \
       learn_t, learn_sigma, 'Gradient')
    gradient_descent(t, sigma, showStart, showEnd, showPath, 'Ascent', n_iter, \
       learn_t, learn_sigma, 'Gradient')

plt.savefig('RH4_ortho.png',bbox_inches='tight')
\end{lstlisting}


% gradient desct / geospatial interpolation
%xxxyyy
%##contour levels and orthogonal trajectories}

%xxx
%xxxx temperature geo map: under gradient [grid]
%gradient can do it for millions of params
%gradient descent 1 million features, 1000 observ

%xxx LLM add HDT project

% maps: georeshy






\section{More spectacular videos and image generation}\label{misc911}

You can zoom in or click on any picture in Figure~\ref{fig:hvge67f32}. Each one features new graphical elements summarized
 in Table~\ref{souffre}, and not covered in other projects. When you click on a picture, it may lead to a YouTube video,
 the Python source code used to generate it, or the corresponding reference and documentation.  

%terrain: https://www.youtube.com/watch?v=RE3Lz559aM0
%gradient: https://www.youtube.com/watch?v=pqQsLpPkvbw
%3D: https://www.youtube.com/watch?v=Jpjw8wtoZrM
%collision graph: https://www.youtube.com/watch?v=-kMJzCd8v0Q

 



\begin{figure}[H]%
    \centering
    \subfloat[\centering Math]{\href{https://www.youtube.com/watch?v=H77ULp6HVsE}{\fbox{\includegraphics[width=.18\linewidth]{thumb1b.png}}} }%
   \qquad
   \subfloat[\centering Graphs]{\href{https://mltechniques.com/product/intuitive-machine-learning/}{\fbox{\includegraphics[width=.18\linewidth]{cliques.png}}} }%
    \qquad
    \subfloat[\centering Chaos]{\href{https://mltechniques.com/product/ebook-gentle-introduction-to-chaotic-dynamical-systems/}{\fbox{\includegraphics[width=.18\linewidth]{art3.png}}} }%
  \qquad
    \subfloat[\centering Dendrogram]{\href{https://mltechniques.com/2024/05/04/breakthrough-zero-weight-llm-for-accurate-predictions-and-high-performance-clustering/}{\fbox{\includegraphics[width=.18\linewidth]{vdendo.png}}} }%

\hfill

    \subfloat[\centering Clustering]{\href{https://www.youtube.com/watch?v=dNPSEh-X6uw}{\fbox{\includegraphics[width=.18\linewidth]{vpbcl2.png}}} }%
    \qquad
    \subfloat[\centering Geospatial]{\href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/kriging_temperatures_chicago.py}{\fbox{\includegraphics[width=.18\linewidth]{geo.png}}} }%
    \qquad
    \subfloat[\centering 3-Dim]{\href{https://www.youtube.com/watch?v=Jpjw8wtoZrM}{\fbox{\includegraphics[width=.18\linewidth]{v3d.png}}} }%
  \qquad
    \subfloat[\centering Geometry]{\href{https://www.youtube.com/watch?v=wl0vfOdaGD8}{\fbox{\includegraphics[width=.18\linewidth]{vcomet.png}}} }%

\hfill

    \subfloat[\centering Terrain]{\href{https://www.youtube.com/watch?v=RE3Lz559aM0}{\fbox{\includegraphics[width=.18\linewidth]{vterrain.png}}} }%
    \qquad
    \subfloat[\centering Diagrams]{\href{https://www.youtube.com/watch?v=-kMJzCd8v0Q}{\fbox{\includegraphics[width=.18\linewidth]{vgraph.png}}} }%
    \qquad
    \subfloat[\centering Gradient]{\href{https://www.youtube.com/watch?v=pqQsLpPkvbw}{\fbox{\includegraphics[width=.18\linewidth]{vgradient.png}}} }%
 \qquad
    \subfloat[\centering Interpolation]{\href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/interpol2.png}{\fbox{\includegraphics[width=.18\linewidth]{vgeo2.png}}} }%


    \caption{Selection of clickable images and videos produced with Python libraries}%
    \label{fig:hvge67f32}%
\end{figure}

%code not included but referenced
%link to book section and github code
%add dendogram
%ellipses
%interpol / dots

%3D see also section 1

%--------
\begin{table}[H]
%\[\
\begin{center}
\scalebox{0.9}{
\begin{tabular}{p{\dimexpr2.5cm-2\tabcolsep}|p{\dimexpr10.5cm-2\tabcolsep}|c p{\dimexpr1.5cm-2\tabcolsep}}%{|l|l p{10cm}|l|}
\hline
  Name & Description & GitHub  \\
\hline
\hline
Math & Successive approximations of the Dirichlet eta function in the complex plane, with special wavelets to represent pastel colors evolving harmoniously over time.   See chapter 4 in~\cite{vgmlbook2022}. & \href{https://github.com/VincentGranville/Visualizations/blob/main/Source-Code/image2R.py}{Link}\\
\hline
Graphs &  Nearest neighbors graph: built using fast connected components algorithm. See section 1.5.4 in~\cite{vgsimulnew}. & 
 \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN_graph.py}{Link}\\
\hline
Chaos & Basins of attraction, discrete chaotic dynamical systems. See section 2.4 in~\cite{vgchaos}. & \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/basin.py}{Link}\\
\hline
Dendrogram & LLM hierarchical clustering based on keyword relationships. Figure~\ref{fig:z0m4rgv098} offers a larger version. Details in section~\ref{predxllm}. & \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/nlp_scoring.py}{Link}\\
\hline
Clustering & Features fractal clustering of tabular data, performed as a convolution filter using image filtering techniques in computer vision. See section 2.4.2
 and 3.4.3
 in~\cite{vgsimulnew}. Includes discussion on creating optimum palettes (cluster coloring scheme). & \href{https://github.com/VincentGranville/Point-Processes/blob/main/Videos/PB_clustering_video.py}{Link}\\
\hline
Geospatial & Map superimposed on the grid: temperature data, Chicago area. Interpolation via kriging. & \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/kriging_temperatures_chicago.py}{Link}\\
\hline
3-Dim& Video showing 3-D shapes changing over time, with contour levels and 3-D rotations. See section~\ref{porousbutane}.  & \href{https://github.com/VincentGranville/Visualizations/blob/main/Source-Code/contourvideoplotly.py}{Link}\\
\hline
Geometry & Elliptic orbits in progress (video). See chapter 4 in~\cite{vgmlbook2022}. & \href{https://github.com/VincentGranville/Visualizations/blob/main/Source-Code/image2.py}{Link}\\
\hline
Terrain &  Four videos running in parallel. Special terrain palette, GenAI, palette morphing. See chapter 14 in~\cite{vgelsevier}. & \href{https://github.com/VincentGranville/Visualizations/blob/main/Source-Code/terrain.py}{Link}\\
\hline
Diagrams & Collision graph representing stars colliding over time, in a synthetic galaxy. See section 15.3 in~\cite{vgelsevier}. Click on thumbnail to see how the galaxy evolves. & \href{https://github.com/VincentGranville/Visualizations/blob/main/Source-Code/nbody_graph.py}{Link}\\
\hline
Gradient & Gradient descent starting with 100 locations at once. Features grid, contour levels and orthogonal trajectories. See section~\ref{poures}. & \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/gradient.py}{Link}\\
\hline
Interpolation &  Exact bivariate interpolation, Chicago temperature dataset, see section~\ref{georeshy}. Color gradient for the temperatures. Click on thumbnail to see the corresponding scatterplot,
with round dots for training set locations, and + for validation set locations.  & \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/interpol_smooth.py}{Link}\\
\hline		
\end{tabular}
}
%\]
\caption{\label{souffre} Details about the visualizations featured in Figure~\ref{fig:hvge67f32}}
\end{center}
\end{table}

%\subsection{Supervised classification with image bitmaps}
%
%book : pb binomial or intuitive ML

%\subsection{Geospatial data, with maps}

%link to gradient descent

%https://github.com/VincentGranville/Statistical-Optimization/blob/main/kriging_temperatures_chicago.py
%interpol.py
%fcontour python plot
%section 4.3.2 in stats optim book (python code)
%section 4.1.4 in same vooks for explanation
%\cite{vgmloptim}

%\subsection{Agent-based modeling: collision graphs}\label{graph911}

%optimum palette

%\subsection{Terrain generation: image and palette morphing}

%\subsection{Mathematical art}


%xxx
% ref to chapter 1
%stochastic models genAI // grapg ~ agent based [palettes] [RGBA][img resizing][high quality resolution]
%AI art in visualizations: RH sine/cosine colors colors
%geospatial interpol. measure of smoothness in 2D
% gradient descent with no math - contour lines
% curve fitting 250 training sets
%\section{Graph synthetization} // terrain generation

%xxx star collisions graphs [agglomerative processes --> add to stats book] / nn graphs
%ellipse comets
% graphs done in Python

%xxx 4 videos in one plot
%meteorite exercises
%250 training sets
%gradient descent
%star clusters, collision graphs , NN graphs
% making a graph diagram
% since color, optimum palette, agent-based modeling, matplotly param, plotly, RGBA [clustering]

%------------------------
\chapter{NLP and Large Language Models}\label{nlpllm}

If you tried apps such as \textcolor{index}{GPT}\index{GPT} (generative pre-training transformer), you may be surprised by the quality of the sentences, images, sound, videos, or code generated. Yet, in the end, the value is the depth and relevance or the content generated, more than the way it is presented. My interest started when I did a Google search for ``variance of the range for Gaussian distributions". I vaguely remember that it is of the order $1/\sqrt{n}$ where $n$ is the number of observations, but could not find the reference anymore. Indeed I could not find anything at all on this topic. The resources I found on the subject 10 years ago are all but gone, or at least very hard to find. As search evolved over time, it now caters to a much larger but less educated audience. As a result, none of the search results were even remotely relevant to my question. This is true for pretty much any research question that I ask. 

Using OpenAI, I found the answer I was looking for, even with more details than expected, yet with no link to an actual reference, no matter how I change my prompt. OpenAI could not find my answer right away, and I had to rephrase my prompt as 
``what is the asymptotic variance of the range for Gaussian distributions". More general prompts on specific websites, such as ``asymptotic distribution of sample variance" lead to a number of articles which in turn lead to some focusing on Gaussian distributions. Even today, 
 automatically getting a good answer in little time, with a link, is still a challenge.

In this chapter, I focus on building better \textcolor{index}{LLMs}\index{LLM} to address the issues in question. Project~\ref{dnalove} is peculiar in the sense
 that the alphabet consists of 4 symbols. The goal is to synthesize sequences that exhibit the same autocorrelation structure as found in the original data, and then evaluate the results. Then, in projects~\ref{oudopre}, \ref{bernoc}, and~\ref{beurreblanc}, 
I discuss in details a new multi-LLM app based on crawled data and reconstructed taxonomies, with specialized sub-LLMs (one per top category), self-tuning,
 variable-length embeddings with multi-token words, and much smaller tables. The material covers the core components of this new, energy-efficient 
\textcolor{index}{RAG}\index{RAG} architecture,
 including smart crawling. There is no neural network involved. 

\section{Synthesizing DNA sequences with LLM techniques}\label{dnalove}

This project is not focused on genome data alone. The purpose is to design a generic solution that may also work in other contexts, such as
 synthesizing molecules. The problem involves dealing with a large amount of ``text". Indeed, the sequences discussed here consist of 
 letter arrangements, from an alphabet that has 5 symbols:  A, C, G, T and N.  The first four symbols stand for the  types of bases found in a DNA molecule: adenine (A), cytosine (C), guanine (G), and thymine (T). The last one (N) represents missing data. No prior knowledge of
  genome sequencing is required.

The data consists of DNA sub-sequences from a number of individuals, and categorized according to the type of genetic patterns found in each
 sequence. Here I combined the sequences together. The goal is to synthesize realistic DNA sequences, evaluate the quality of the synthetizations, and
 compare the results with random sequences.  The idea is to look at a DNA string $S_1$ consisting of $n_1$ consecutive symbols, to identify
  potential candidates for the next string $S_2$ consisting of $n_2$ symbols. Then, assign a probability to each string $S_2$ conditionally
 on $S_1$, use these transition probabilities to sample $S_2$ given $S_1$, then move to the right by $n_2$ symbols, do it again, and so on. 
 Eventually you build a synthetic sequence of arbitrary length. There is some analogy to \textcolor{index}{Markov chains}\index{Markov chain}. 
 Here, $n_1$ and $n_2$ are fixed, but arbitrary.

\subsection{Project and solution}\label{pscaokesour}

Let's look at 3 different DNA sequences. The first one is from a real human being. The second one is synthetic, replicating some of the patterns found in real data. The third one is purely random, with each letter 
 independently distributed from each other, with the same $25\%$ marginal frequency. Can you tell the differences 
just by looking at the 3 sequences in Table~\ref{tafresa}? If not, see Figure~\ref{rinyltualoer} and accompanying description. 
\vspace{1ex}


\begin{table}
\begin{center}
\begin{tabular}{c}
\begin{lstlisting}[frame=none]
Real DNA

ATGCCCCAACTAAATACTACCGTATGGCCCACCATAATTACCCCCATACTCCTTACACTATTCCTCATCACCCAACTA
AAAATATTAAACACAAACTACCACCTACCTCCCTCACCAAAGCCCATAAAAATAAAAAATTATAACAAACCCTGAGAA
CCAAAATGAACGAAAATCTGTTCGCTTCATTCATTGCCCCCACAATCCTAGNATGAACGAAAATCTGTTCGCTTCATT
CATTGCCCCCACAATCCTAGGCCTACCCGCCGCAGTACTGATCATTCTATTTCCCCCTCTATTGATCCCCACCTCCAA
ATATCTCATCAACAACCGACTAATCACCACCCAACAATGACTAATCAAACTAACCTCAAAACAAATGATAACCATACA

Synthetic DNA

TTGTTTTCTTCACCTAAATGCACAAGAATGGTGGGCCGAGGAGCCATGTCAAGTGGGGATGGGTCTATCGAACCTGAG
GGCCCCCCACTTCAGATGCTTCGTACTGTCTTTGGGACTTCTCACCGTCTCATGGTCTGCCCTGCCCCGCAGTGTGGC
CTGGTATTTTTAACCCTATTATAGAAACAACAATTTATGGGCTCCTTGAAGCTTATACAATACAACAGTAAAGGGCCC
CTCCTCCAGTCAGCCTCTTTCCCTCTTAGGGTAAATGAGGATATCCAAGTGCCCACCTCATCATCAACTCCGCCACCA
GTTTGCAGCCCTTGCAGGAGATTTCTGGTGATGAAAGTTCAGTGGACTTGGGAAAAGCCGTCATGCTGTCTGCCAACC

Random DNA

ATCCTGCTTCATATGTAGGAAGGGTTGTAGGTTCCCGGAGGGCGCATTGCAAAGACCGGCCAGACTACTTATGGCCGC
GTCCTAAGCACCATATGCTAAGCCTGATTAACATCGCGCGGATGTAACTACACGCGCGCTACGTGAATCCTAGGCAGC
CGTCACGATTGACTCCTCATACTCATCGAGGCGCTCGCGTCATAGACCGACCATCGCGTCACCATAATAAGTAGAGTC
TTTACGGTAGGCCTTCAAAATACGGACAAGGCATTTGTATTCTTCATGTCATGTAGCTGAAGAATACCATTAAGTTTA
TAGGCGGGTGTACGACAAGACTGCCAGGTGGCAGTGTCGTCACAAGAGCGCGTAAACTTTTTGCCGGTAATAGACCGT
\end{lstlisting}
\end{tabular}
\caption{\label{tafresa}Genome data, three DNA sub-sequences: real, synthetic, and random}
\end{center}
\end{table}
\vspace{1ex}

\noindent For this project, I start with real sequences to train the DNA synthesizer. I then evaluate the quality, and show how synthetic DNA  is superior to random sequences. Any symbol other than A, C, G, or T must be labeled as N. It represents missing data and I ignore it in the Python code. Figures~\ref{rinyltualoer} and~\ref{fcasswelbwacxd} illustrate the end result: comparing string frequencies
 in real, synthetic and random DNA sequences, for specific ``words" consisting of $n_3$ consecutive symbols. The main diagonal (red line) represents perfect fit  with real DNA. The number of ``words" (dots in the pictures) varies from $\num{4000}$ to $\num{10000}$, and are called
 \texttt{nodes} in the Python code. It shows that these DNA sequences are anything but random, with synthetic fitting real data quite well.

\begin{figure}[H]
\centering
\includegraphics[width=0.78\textwidth]{genome6.png}   
\caption{PDF scatterplots, $n_3=6$: real DNA vs synthetic (blue), and vs random (orange)}
\label{rinyltualoer}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[width=0.78\textwidth]{genome8.png}   
\caption{PDF scatterplots, $n_3=8$: real DNA vs synthetic (blue), and vs random (orange)}
\label{fcasswelbwacxd}
\end{figure}

\noindent The project consists of the following steps: \vspace{1ex}

\begin{itemize}
\item[] {\bf Step 1:  Understanding the data}. Look at the URL in the Python code in section~\ref{pupipr} to access the data. Ignore the
 ``class'' feature as the purpose here is not to classify DNA sequences. A small extract of a real DNA sequence is featured in Table~\ref{tafresa}, at the top. Note that the order of the letters is important.
 \vspace{1ex}
\item[] {\bf Step 2: Compute summary statistics}.  For $n_1$ and $n_2$ fixed, extract all distinct strings $S_1, S_2$ of length respectively $n_1$ and $n_2$,
 and compute their occurrences. Do the same for strings $S_{12}$ of length $n_1+n_2$. The goal is to predict, given a string $S_1$ of length $n_1$, the probability
 to be followed by a specific string $S_2$ of length $n_2$. That is, $P[S_2 = s_2 | S_1 = s_1]$. Here a string is a sub-sequence of letters. Strings are  called words, and letters are also called characters.
 The four letters are `A', `B', `C', `D'. Ignore the letter `N'. Then, do the same when the strings $S_1$ and $S_2$ are separated by a gap of
 $g$ letters, for $g=1,2,3$.
\vspace{1ex} 
\item[] {\bf Step 3: String associations}. Two specific strings $S_1, S_2$ may frequently be found together, or rarely. Characterize this  string association using the \textcolor{index}{pointwise mutual information}\index{pointwise mutual information (PMI)} (PMI) [\href{https://en.wikipedia.org/wiki/Pointwise_mutual_information}{Wiki}]. Rare
 occurrences may indicate a rare genetic condition. Order the $(S_1, S_2)$ found in the data according to the PMI metric.

\vspace{1ex}
\item[] {\bf Step 4: Synthesize a DNA sequence}. Proceed as follows. 
Start with an arbitrary string $S_1$ of length $n_1$. Then add $n_2$ letters at a time, sequentially, to the DNA sequence being generated. In other words, at each step, sample $S_2$ from
 $P(S_2 | S_1)$, where $S_2$ is the new string of length $n_2$ to be added, and $S_1$ is the last string of length $n_1$ built so far.
\vspace{1ex}
\item[] {\bf Step 5: Evaluate the quality}. In this context, the \textcolor{index}{Hellinger distance}\index{Hellinger distance} 
 [\href{https://en.wikipedia.org/wiki/Hellinger_distance}{Wiki}] is simple and convenient, to assess the quality of
 the synthetic DNA sequence, that is, how well it replicates the patterns found in real DNA. The value is between 0 (great fit) and 1 (worst fit).
 Randomly select $n = \num{10000}$ strings $S_3$ of length $n_3$ found in real DNA. These strings are referred to as 
{\em nodes}.  Compute the frequency $P_\text{real}(S_3)$ for each of them. 
 Also compute the frequency $P_\text{synth}(S_3)$ on the synthetic sequence. The Hellinger distance is then
$$
\text{HD} = \sqrt{1 - \sum \sqrt{P_\text{real}(S_3)\cdot P_\text{synth}(S_3)}},
$$
where the sum is over all the selected strings $S_3$. Also compare real DNA with a random sequence,
 using HD. Show that synthetic DNA is a lot better than random sequences, to mimic real DNA. Finally, try different values of $n_1, n_2, n_3$
 and check whether using $n=1000$ nodes provides a good enough approximation to HD (it is much faster than $n=\num{10000}$, especially
 when $n_3$ is large).
\end{itemize}
\vspace{1ex}


\noindent The solution to the first four steps correspond to steps [1--4] in the Python code in section~\ref{pupipr}, while
 \textcolor{red}{Step 5} corresponds to step [6] in the code. To compute summary statistics (\textcolor{red}{Step 2}) when $S_1$ and $S_2$ are separated by a gap of $g$ letters, replace \texttt{string2=obs[pos1:pos2]}
 by \texttt{string2=obs[pos1+g:pos2+g]} in step [2] in the code. The interest in doing this is to assess whether there are
 long-range associations between strings. By default, in the current version of the code, $g=0$. 

Figures~\ref{rinyltualoer} and~\ref{fcasswelbwacxd} show scatterplots with probability vectors 
$[P_\text{real}(S_3),$ $P_\text{synth}(S_3)]$ in blue, 
 for thousands of strings $S_3$ found in the real DNA sequence. 
 For orange dots, the second component $P_\text{synth}(S_3)$ is replaced by $P_\text{rand}(S_3)$, the value computed on a random sequence.
 Clearly, the synthetic DNA is much more realistic than the random DNA, especially when $n_3 = 6$. Note that 
 the probabilities are associated to overlapping events: for instance, the strings `AACT' and `GAAC' are not independent, even in the random sequence. The Hellinger distance used here is not adjusted for this artifact. 

To dive deeper into DNA synthetization, you might want to investigate problems such as the minimal DNA sequence that determines the gender, or some other genetic features.

%xxxx explain the 2 figures ### the probas are overlapping
 

\subsection{Python code}\label{pupipr}

The code is also on GitHub, \href{https://github.com/VincentGranville/Main/blob/main/genome.py}{here}. For explanations,
 see section~\ref{pscaokesour}.
\vspace{1ex}

\begin{lstlisting}[numbers=left]
# genome.py : synthesizing DNA sequences
# data: https://www.kaggle.com/code/tarunsolanki/classifying-dna-sequence-using-ml

import pandas as pd
import numpy as np
import re   # for regular expressions


#--- [1] Read data

url = "https://raw.githubusercontent.com/VincentGranville/Main/main/dna_human.txt"
human = pd.read_table(url)
# human = pd.read_table('dna_human.txt')
print(human.head())


#--- [2] Build hash table architecture
#
# hash1_list[string1] is the list of potential string2 found after string1, separated by ~

nobs = len(human)
print(nobs)
hash12 = {}
hash1_list = {}
hash1 = {}
hash2 = {}
count1 = 0
count2 = 0
count12 = 0
sequence = ''

for k in range(nobs):
   obs = human['sequence'][k]
   sequence += obs
   sequence += 'N'
   type = human['class'][k]
   length = len(obs)
   string1_length = 4
   string2_length = 2
   pos0 = 0
   pos1 = pos0 + string1_length
   pos2 = pos1 + string2_length

   while pos2 < length:

       string1 = obs[pos0:pos1]
       string2 = obs[pos1:pos2]

       if string1 in hash1: 
           if string2 not in hash1_list[string1] and 'N' not in string2:
               hash1_list[string1] = hash1_list[string1] + '~' + string2
           hash1[string1] += 1
           count1 += 1
       elif 'N' not in string1:
           hash1_list[string1] = '~' + string2
           hash1[string1] = 1
       key = (string1, string2)

       if string2 in hash2:
           hash2[string2] += 1
           count2 += 1
       elif 'N' not in string2:
           hash2[string2] = 1

       if key in hash12:
           hash12[key] += 1
           count12 += 1
       elif 'N' not in string1 and 'N' not in string2:
           hash12[key] = 1

       pos0 += 1
       pos1 += 1
       pos2 += 1

   if k % 100 == 0:
       print("Creating hash tables: %6d %6d %4d" %(k, length, type))


#--- [3] Create table of string associations, compute PMI metric 

print()
index = 0
for key in hash12:
    index +=1
    string1 = key[0]
    string2 = key[1]
    n1 = hash1[string1]  # occurrences of string1 
    n2 = hash2[string2]  # occurrences of string2 
    n12 = hash12[key]    # occurrences of (string1, string2) 
    p1 = n1 / count1     # frequency of string1
    p2 = n2 / count2     # frequency of string2
    p12 = n12 / count12  # frequency of (string1, string2)
    pmi = p12 / (p1 * p2)
    if index % 100 == 0:
        print("Computing string frequencies: %5d %4s %2s %4d %8.5f" 
                %(index, string1, string2, n12, pmi))
print()


#--- [4] Synthetization
#
# synthesizing word2, one at a time, sequencially based on previous word1

n_synthetic_string2 = 2000000
seed = 65
np.random.seed(seed)

synthetic_sequence = 'TTGT'    # starting point (must be existing string1)
pos1 = len(synthetic_sequence)
pos0 = pos1 - string1_length 


for k in range(n_synthetic_string2):

    string1 = synthetic_sequence[pos0:pos1]
    string = hash1_list[string1]
    myList = re.split('~', string)

    # get target string2 list in arr_string2, and corresponding probabilities in arr_proba
    arr_string2 = []
    arr_proba   = []
    cnt = 0
    for j in range(len(myList)):
        string2 = myList[j]
        if string2 in hash2:
            key = (string1, string2)
            cnt += hash12[key]
            arr_string2.append(string2)
            arr_proba.append(hash12[key])
    arr_proba = np.array(arr_proba)/cnt

    # build cdf and sample word2 from cdf, based on string1 
    u = np.random.uniform(0, 1) 
    cdf = arr_proba[0]
    j = 0
    while u > cdf:
        j += 1
        cdf += arr_proba[j]
    synthetic_string2 = arr_string2[j]
    synthetic_sequence += synthetic_string2
    if k % 100000 == 0:
        print("Synthesizing %7d/%7d: %4d %8.5f %2s" 
                  % (k, n_synthetic_string2, j, u, synthetic_string2))

    pos0 += string2_length
    pos1 += string2_length

print()
print("Real DNA:\n", sequence[0:1000])
print()
print("Synthetic DNA:\n", synthetic_sequence[0:1000])
print()


#--- [5] Create random sequence for comparison purposes

print("Creating random sequence...")
length = (1 + n_synthetic_string2) * string2_length
random_sequence = ""
map = ['A', 'C', 'T', 'G']

for k in range(length):
    random_sequence += map[np.random.randint(4)]
    if k % 100000 == 0:
        print("Creating random sequence: %7d/%7d" %(k,length))
print()
print("Random DNA:\n", random_sequence[0:1000])
print()


#--- [6] Evaluate quality: real vs synthetic vs random DNA

max_nodes = 10000  # sample strings for frequency comparison
string_length = 6  # length of sample strings (fixed length here)

nodes = 0
hnodes = {}
iter = 0

while nodes < max_nodes and iter < 5*max_nodes:
    index = np.random.randint(0, len(sequence)-string_length)
    string = sequence[index:index+string_length]
    iter += 1
    if string not in hnodes and 'N' not in string:
        hnodes[string] = True
        nodes += 1
        if nodes % 1000 == 0:
            print("Building nodes: %6d/%6d" %(nodes, max_nodes))
print()

def compute_HD(hnodes, sequence, synthetic_sequence):

    pdf1 = []
    pdf2 = []
    cc = 0

    for string in hnodes:
        cnt1 = sequence.count(string) 
        cnt2 = synthetic_sequence.count(string) 
        pdf1.append(float(cnt1))
        pdf2.append(float(cnt2))
        ratio = cnt2 / cnt1
        if cc % 100 == 0:
            print("Evaluation: computing EPDFs: %6d/%6d: %5s %8d %8d %10.7f" 
                   %(cc, nodes, string, cnt1, cnt2, ratio))
        cc += 1

    pdf1 = np.array(pdf1)   # original dna sequence
    pdf2 = np.array(pdf2)   # synthetic dna sequence
    pdf1 /= np.sum(pdf1)
    pdf2 /= np.sum(pdf2)

    HD = np.sum(np.sqrt(pdf1*pdf2))
    HD = np.sqrt(1 - HD)
    return(pdf1, pdf2, HD) 

pdf_dna, pdf_synth,  HD_synth  = compute_HD(hnodes, sequence, synthetic_sequence)
pdf_dna, pdf_random, HD_random = compute_HD(hnodes, sequence, random_sequence)

print()
print("Total nodes: %6d" %(nodes))
print("Hellinger distance [synthetic]: HD = %8.5f" %(HD_synth))
print("Hellinger distance [random]   : HD = %8.5f" %(HD_random))

#--- [7] Visualization (PDF scatterplot)

import matplotlib.pyplot as plt
import matplotlib as mpl

mpl.rcParams['axes.linewidth'] = 0.5
plt.rcParams['xtick.labelsize'] = 7
plt.rcParams['ytick.labelsize'] = 7

plt.scatter(pdf_dna, pdf_synth, s = 0.1, color = 'red', alpha = 0.5)
plt.scatter(pdf_dna, pdf_random, s = 0.1, color = 'blue', alpha = 0.5)
plt.legend(['real vs synthetic', 'real vs random'], loc='upper left', prop={'size': 7}, )
plt.plot([0, np.max(pdf_dna)], [0, np.max(pdf_dna)], c='black', linewidth = 0.3)
plt.show()
\end{lstlisting}

\section{Creating high quality LLM embeddings}\label{sxllm5}

The purpose of this project is twofold. First, learning how to efficient crawl large, well structured websites to extract and reconstruct comprehensive keyword taxonomies, as well as gathering vast amounts of text. Here, the focus is on one particular component of human knowledge: statistics and probability theory. Then use~the crawled data, structure it, and provide high quality answers -- along with the sources -- to specific questions or prompts. At the time of this writing, OpenAI fails at both.

Unlike \textcolor{index}{OpenAI}\index{OpenAI}, the goal is not to produce wordy English prose explaining rudimentary principles at length, blended with more advanced material. Instead, the output may consist of a few bullet points, keywords and links. That is, what is most useful to busy professionals 
who are  experts in their field. The motivation behind this project is to eliminate bottlenecks in standard search technology. In 
 particular, Google search (whether targeted to a specific website or not) as well as search boxes found on Wikipedia, Stack Exchange, ArXiv, Wolfram and other websites, including ``related content" links found on these websites, are of limited value. 

Likewise, \textcolor{index}{GPT}\index{GPT} frequently provides slightly wrong answers to scientific questions, without providing the sources that would allow you to easily fix the errors. This may result in considerable time spent in searching specialized information, using methods that could be automated. The example at the origin of this project was my search to find the asymptotic expectation of the range for Gaussian distributions. Why I was interested in this topic, and the problems that I faced even with GPT, are described \href{https://mltblog.com/3GStuLc}{here}. 
\vspace{1ex}

\begin{figure}[H]
\centering
\includegraphics[width=0.96\textwidth]{LLM.png}   
\caption{Extract from reconstructed taxonomy structure, Wolfram website}
\label{rinyltuag5ttc}
\end{figure}

\subsection{Smart, efficient, and scalable crawling}\label{oudopre}

Here, the purpose is to identify target web sites to crawl, identify the kind of information to gather, and perform the actual 
initial crawling. Enterprise-grade crawling must meet the following requirements:\vspace{1ex}
\begin{itemize}
\item Smart: Extracting all the relevant data and nothing more. Avoid getting stuck in infinite loops. For large repositories, start by crawling the category pages when possible (building a knowledge taxonomy in the process), then crawl the actual content. For the latter, assign a category, parent category, and category depth to each crawled URL. A web page may have multiple categories and parent categories. Choose an appropriate 
 organization for the crawling algorithm: tree, graph, or stack, and heavy use of \textcolor{index}{hash tables}\index{hash table} (key-value pairs).
\item Efficient: You may start with a small crawl, for instance a subcategory, to optimize the parameters.
Do not crawl too fast to avoid being blocked. Add a timeout to each URL request, to avoid getting stuck on occasions. 
Assign a status to each URL: queued (added to crawling list but not crawled yet), parsed (crawled), final (no downstream URLs attached to it), or failed. Failed requests can be re-tried later.  Use a distributed architecture. 
Finally, you want to store the raw crawled content efficiently,  typically in compressed format and organized for easy retrieval. 
\item Scalable: Using a stack architecture leads to simple distributed implementation and avoids recursivity. Saving each web page to a file
right after being crawled, along with other summary statistics, 
 provides the ability to easily resume 
 from where your algorithm stopped in case of crash, as the crawling 
may last for months or years. Then you want to avoid being blocked by the target website or being able to recover if it happens. Using anonymous proxies may help with
 this.  You may skip images and other binary files (video, audio) when crawling, if you don't plan on using them.
\end{itemize}
\vspace{1ex}

\noindent In this project, I crawled the entire Wolfram website, consisting of thousands of categories and about 
$\num{15000}$ web pages, totaling 150 MB in compressed format. Because web pages can be modified by the owner over time, my Python code may need to be adapted. 
However, I stored all the crawled content and full taxonomy, 
\href{https://github.com/VincentGranville/Large-Language-Models/tree/main/crawl_dump_wolfram}{here}. So, you can skip Part 1 (crawling) and move to Part 2 (building the LLM engine)
 if you experience crawling issues.  

\noindent The project consists of the following steps: \vspace{1ex}

\begin{itemize}
\item[] {\bf Step 1:  Introduction}. The goal is to crawl  \href{https://mathworld.wolfram.com/}{mathworld.wolfram.com}, more specifically the 
Probability \& Statistics section. Also identify other relevant websites
 worth crawling. Besides the directory structure and actual content, what other types of pages should you consider, to build your LLM app?
Which Python libraries should you use? How about using anonymous proxies and other strategies to avoid blocking when crawling?
 \vspace{1ex}
\item[] {\bf Step 2: Initial taxonomy}.  Identify three types of pages: non-terminal category pages (pointing to subcategories), 
terminal category pages (pointing to content pages), and actual content pages. Reconstruct the full list of categories, creating a table such as
 the one featured in Figure~\ref{rinyltuag5ttc}. Produce a file with all the content pages found (URLs), with category, parent category, and category depth for each URL (one row per URL).
These URLs will be crawled in the next step. Make sure that accented, non-English and special characters are preserved. 
Also, prove that the crawling algorithm used in the Python code, always ends without missing any URL.
\vspace{1ex}
\item[] {\bf Step 3: Extracting content}.  Crawl all the URLs obtained in the previous step. Use the smart, efficient, scalable methodology.
The output should be text files, each one containing 500 URLs, with one (very long) line per URL, with the following fields:
 URL, category, parent category, category depth, and full content attached to the URL in question. 
Use appropriate separators for these fields. The HTML content, consisting of text, should have all line breaks (the $\text{\texttt{\textbackslash n}}$ character)
 replaced by (say) a space, so that one web page fits in one row. Make sure that accented, non-English and special characters are preserved. 
\end{itemize}
\vspace{1ex}

\noindent 
Let's start with the answer to \textcolor{red}{Step 1}. Content worth crawling includes search result pages linked to pre-specified keywords (for instance, `quantile' on Wolfram search,
 using \href{https://mathworld.wolfram.com/search/?query=quantile}{this URL}), website-targeted Google search  
(for instance, `quantile + Wolfram.com' using \href{https://www.google.com/search?q=quantile+\%2B+Wolfram.com}{this URL}), exact search as opposed to broad
 search,
lists of ``related material" or ``related questions" found on crawled pages,
 indexes or glossaries when available (see example \href{https://mathworld.wolfram.com/letters/}{here}), 
 \textcolor{index}{metada}\index{metadata}  [\href{https://en.wikipedia.org/wiki/Meta_element}{Wiki}], and tags. All these elements help build 
 relationships between keywords or concepts. 

As for websites, consider crawling Wikipedia (with its own taxonomy), Stack Exchange (each website focusing on one topic, for instance mathematics), ArXiv, Google Scholar, and GitHub. Finally, I used the \textcolor{index}{Requests}\index{Python library!Request} Python library for crawling. 
For crawling examples with anonymous proxies, see 
\href{https://github.com/VincentGranville/Large-Language-Models/blob/main/tor_crawling.py}{here} with the 
\textcolor{index}{Torpy}\index{Python library!Torpy} library, and 
\href{https://github.com/VincentGranville/Large-Language-Models/blob/main/brightdata.py}{here} 
with the Bright Data library. The latter is not free.

Regarding \textcolor{red}{Step 2}, sample page types can be found 
\href{https://github.com/VincentGranville/Large-Language-Models/blob/main/sample-crawled-page-type-1.txt}{here} 
(non-terminal category page referred to as Type 1 in the code), 
\href{https://github.com/VincentGranville/Large-Language-Models/blob/main/sample-crawled-page-type-2.txt}{here} (terminal category page referred to as Type 2), and
\href{https://github.com/VincentGranville/Large-Language-Models/blob/main/sample-crawled-page-type-3.txt}{here}
 (actual content page, with metadata at the top). The first two types require ad-hoc parsing to recursively build the categories and to extract the final URLs:  the actual content pages. For output file management, note the \texttt{encoding=\textquotesingle utf-8\textquotesingle} directive to handle non-standard characters, 
and the use of the \texttt{flush} command to avoid buffering.

As for \textcolor{red}{Step 3}, see the Python code below. Crawling the content pages (lines \textcolor{gray}{135} -- \textcolor{gray}{171}) can be done with a stand-alone script, as all the necessary input data is stored in a text file in lines
 \textcolor{gray}{124} -- \textcolor{gray}{132}. The output files are on GitHub, \href{https://github.com/VincentGranville/Large-Language-Models}{here}. This repository contains sample logfiles monitoring crawling  progress, the taxonomy structure featured
 in Figure~\ref{rinyltuag5ttc}, as well as the full text (HTML code) of all the content web pages. Both for the entire Wolfram website, and
 the Statistics \& Probability category.
The source code is also on GitHub, \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/crawl_directory.py}{here}.

As a final note, \textcolor{index}{OpenAI}\index{OpenAI} uses the \textcolor{index}{Common Crawl} repository [\href{https://en.wikipedia.org/wiki/Common_Crawl}{Wiki}]. This resource is claimed to contain ``all the Internet" and it is also available as open source. You might consider it as an alternative to running your own crawling algorithm. Also, while OpenAI did not provide useful links to answer my questions, Edge (Bing) and \textcolor{index}{Bard}\index{Bard (Google AI)} have that capability. Bing blends AI results with standard search result; in the AI-generated section, it provided 
\href{https://math.stackexchange.com/questions/89030/expectation-of-the-maximum-of-gaussian-random-variables}{this useful link} to 
 \href{https://www.bing.com/search?q=what+is+the+asymptotic+expectation+of+the+range+for+gaussian+distributions}{my question}.
 I was less lucky with Bard. However, keep in mind that these apps keep being improved all the time. The tests reported here were performed in January 2024.

\vspace{1ex}




\begin{lstlisting}[numbers=left]
import requests
import time
  
URL_list = []
URL_parent_Category = {}
categoryLevel = {}
history = {}
final_URLs = {}

URL_base1 = "https://mathworld.wolfram.com/topics/"  # for directory pages (root)
URL_base2 = "https://mathworld.wolfram.com/"         # for final pages

seed_URL = "https://mathworld.wolfram.com/topics/ProbabilityandStatistics.html"
seed_category = "Probability and Statistics"  # "Root" if starting at URL_base1
categoryLevel[seed_category] = 1  # set to 0 if starting at URL_base1

#seed_URL = "https://mathworld.wolfram.com/topics/"
#seed_category = "Root"  # "Root" if starting at URL_base1
#categoryLevel[seed_category] = 0  # set to 0 if starting at URL_base1

URL_list.append(seed_URL)   # URL stack
URL_parent_Category[seed_URL] = seed_category 

parsed = 0       # number of URLs already parsed
n_URLs = 1       # total number of URLs in the queue 
max_URLs = 5000  # do not crawl more than max_URLs directory pages 

def validate(string):
    Ignore = ['about/','classroom/','contact/','whatsnew/','letters/']
    validated = True  
    if len(string) > 60 or string in Ignore or string.count('topics') > 0:
        validated = False
    return(validated)

def update_lists(new_URL, new_category, parent_category, file):
    URL_parent_Category[new_URL] = new_category
    # if new_category was encountered before, special processing required 
    #   --> in this case, not a one-to-one mapping (ignore here)
    categoryLevel[new_category] = 1 + categoryLevel[parent_category]
    level = str(categoryLevel[new_category])
    file.write(level+"\t"+new_category+"\t"+parent_category+"\n")
    file.flush()
    return()


#---[1] Creating category structure and list of webpages

# file1 allows you to resume from where it stopped in case of crash

file1 = open("crawl_log.txt","w",encoding="utf-8")
file2 = open("crawl_categories.txt","w",encoding="utf-8")

while parsed < min(max_URLs, n_URLs):  

    URL = URL_list[parsed]    # crawl first non-visited URL on the stack
    parent_category = URL_parent_Category[URL]
    level = categoryLevel[parent_category]
    time.sleep(5.0)  #  slow down crawling to avoid being blocked
    parsed += 1

    if URL in history:

        # do not crawl twice the same URL
        print("Duplicate: %s" %(URL)) 
        file1.write(URL+"\tDuplicate\t"+parent_category+"\t"+str(level)+"\n")

    else:   

        print("Parsing: %5d out of %5d: %s" % (parsed, n_URLs, URL))
        # req = requests.get(server, auth=('user',"pass"))
        resp = requests.get(URL, timeout=5)
        history[URL] = resp.status_code

        if resp.status_code != 200:

            print("Failed: %s" %(URL)) 
            file1.write(URL+"\tError:"+str(resp.status_code)+"\t"+
                                                    parent_category+"\t"+str(level)+"\n")
            file1.flush()

        else: # URL successfully crawled

            file1.write(URL+"\tParsed\t"+parent_category+"\t"+str(level)+"\n")
            page = resp.text
            page = page.replace('\n', ' ')
            page1 = page.split("<a href=\"/topics/")
            page2 = page.split("<a href=\"/")
            n_URLs_old = n_URLs

            # scraping Type-1 page (intermediate directory node) 

            for line in page1:  
                line = line.split("<span>")
                line = line[0]
                if line.count(">") == 1:
                    line = line.split("\">")
                    if len(line) > 1:
                        new_URL = URL_base1 + line[0]
                        new_category = line[1]  
                        URL_list.append(new_URL)
                        update_lists(new_URL, new_category, parent_category, file2)
                        file1.write(new_URL+"\tQueued\t"+new_category+"\t"+str(level+1)+"\n")
                        file1.flush()
                        n_URLs += 1

            # scraping Type-2 page (final directory node)

            if n_URLs == n_URLs_old:

                for line in page2:
                    line = line.split("</a>") 
                    line = line[0].split("\">")
                    if validate(line[0]) and len(line) > 1:
                        new_URL = URL_base2 + line[0]
                        new_category = line[1] 
                        update_lists(new_URL, new_category, parent_category, file2)
                        file1.write(new_URL+"\tEndNode\t"+new_category+"\t"+str(level+1)+"\n")
                        file1.flush()
                        final_URLs[new_URL] = (new_category, parent_category, level+1)

file1.close()
file2.close()

# save list of final URLs to use in step [2]

count = 0
file = open("list_final_URLs.txt","w",encoding="utf-8")
for URL in final_URLs:
    count += 1
    file.write(str(count)+"\t"+URL+"\t"+str(final_URLs[URL])+"\t\n")
file.close()
print()


#---[2] Extracting content from final URLs

# file_log + file_input allows you to resume from where it stopped (in case of crash)

file_input = open("list_final_URLs.txt","r",encoding="utf-8")
file_log = open("crawl_content_log.txt","w",encoding="utf-8")
file_output = open("crawl_final.txt","w",encoding="utf-8")
separator = "\t~"

Lines = file_input.readlines()
file_input.close()
n = len(Lines)      # number of final URLs (final pages)

for line in Lines:

    line = line.split("\t")
    count = int(line[0])
    URL = line[1]
    category = line[2]
    category.replace('\n','')
    print("Page count: %d/%d %s" %(count, n, URL))
    time.sleep(2.5)  #  slow down crawling to avoid being blocked
 
    resp = requests.get(URL, timeout=5)

    if resp.status_code == 200:
        # add page content: one line per page in the output file
        page = resp.text
        page = page.replace('\n', ' ')
        file_output.write(URL+"\t"+category+separator+page+"\t\n")
        file_output.flush()

    file_log.write(str(count)+"\t"+URL+"\t\n")
    file_log.flush()

file_log.close()
file_output.close()
https://drive.google.com/file/d/1H_xhfhzIPnO8oe9xlwCDWWM9OR5m81wd/view
\end{lstlisting}

%\subsection{Creating the weighted keyword association graph}\label{bernoc}

%----------------------
\subsection{User queries, category-specific embeddings and related tables}\label{bernoc}

Extreme LLM (\textcolor{index}{xLLM}\index{xLLM}) is a multi-LLM architecture with a separate, specialized and simple LLM for each top category.  Each LLM has its own set of tables:
 embeddings, subcategories, related contents, stopwords, URLs, stemmed words, and so on. Some tables are identical across all categories, 
 for instance the mapping between accented characters and the ASCII version.   The idea is to use the
 best Internet websites covering the entire human knowledge, reconstruct the structure and taxonomy of each website, blend the taxonomies, and extract
 condensed content. The other
  component consists of processing 
user queries and returning 
 results in real-time, including links. The user selects or suggests top categories, along with his prompt, to get more relevant results, faster. 

So far, I crawled the entire Wolfram website to cover all math and statistics categories. This was accomplished in project~\ref{oudopre}. Here I focus
 on one portion of this content: the top category ``Probability \& Statistics". The crawled data -- the input to the Python code --
 is available as a text file named \texttt{crawl\_final\_stats.txt}, on Google Drive \href{https://drive.google.com/file/d/1H_xhfhzIPnO8oe9xlwCDWWM9OR5m81wd/view}{here},
 consisting of about 600 webpages. 

The starting point is Appendix~\ref{aasdaaqw}, providing a lot more details and featuring 
\texttt{xllm5\_util.py} (see section~\ref{xllm5util}), a utility with several functions used in this project as well as in project~\ref{beurreblanc}. 
This library is also on GitHub, \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm5/xllm5_util.py}{here}. 
The whole xLLM application is broken down into multiple parts, see Table~\ref{tab:over4l}.

Project~\ref{bernoc} (this section) focuses on the third bullet point in Table~\ref{tab:over4l}. While xLLM may look like the most advanced and complicated project in
 this entire book, it is actually relatively simple and light. First, there is no neural network or training involved. Reinforcement learning is accomplished via
 self-tuning, described later, relying on the most popular parameter sets based on usage. Then, the apparent complexity is due to minimal use of existing
 Python libraries. The reason is because I designed much more efficient, faster, smaller yet better solutions: for instance 
 \textcolor{index}{variable-length embeddings}\index{embeddings!variable length}\ (VLE) encoded as hash tables (though graphs would be a great alternative), 
rather than vectors. Also, there are many other summary tables besides embeddings; these satellite tables are derived from the reconstructed taxonomy, and
 deliver the best improvements. By contrast, in traditional LLMs, embeddings are the central table. The way $n$-grams are treated, or the weights attached to tokens, are 
 non-standard and work better, but require customized code rather than available libraries.
\vspace{1ex}



\begin{table}[ht]
    \centering
    \begin{tabular}{|p{0.3\linewidth} | p{0.6\linewidth}|}
     \hline
      Component  & Description \\ 
     \hline
    \hline

      Web crawling &  Project~\ref{oudopre}. It nncludes \texttt{crawl\_directory.py}, also on GitHub, 
 \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/crawl_directory.py}{here}.\\
     \hline

      Utilities & Appendix~\ref{aasdaaqw}. It includes \texttt{xllm5\_util.py}, also on GitHub, \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm5/xllm5_util.py}{here}\\
\hline

Creating \& updating tables (embeddings and so on), processing user queries & Project~\ref{bernoc}. Main program, for developers: 
\texttt{xllm5.py}, also on GitHub, \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm5/xllm5.py}{here}. 
See section~\ref{porctupues} for details about embeddings and cosine distance.\\
\hline

Adding other sources (Wikipedia, books), processing user queries & Project~\ref{beurreblanc}. Based on 
\texttt{xllm5\_short.py} (\href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm5/xllm5.py}{here} on GitHub), a short version of \texttt{xllm5.py} that uses the
 summary tables created in project~\ref{bernoc} (embeddings and so on), but does not create them. \\
     \hline

Fast embedding search & A better alternative to vector search. See projects~\ref{radixdw8} and~\ref{f8v2koyuy}.\\
\hline

Glossary & Appendix~\ref{aasdaaq32} is a glossary focusing on LLM, and worth reading before moving forward. \\
\hline

    \end{tabular}
    \caption{xLLM: projects breakdown}
    \label{tab:over4l}
\end{table}

\subsubsection{Project and solution}

When I crawled \href{https://mathworld.wolfram.com/}{mathworld.wolfram.com}, I found different types of pages: hierarchical category listings including the home page
 (see \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/sample-crawled-page-type-1.txt}{here}) and 
actual content pages (see \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/sample-crawled-page-type-3.txt}{here}). 
I discuss the details in Step 2 in project~\ref{oudopre}. Content pages have important navigation features, such as ``related pages" and ``See also" references.  I isolated them to build special summary tables, besides embeddings. 
Many websites, for instance Wikipedia, have a very similar structure. 

See Figure~\ref{de2pufd1re} for embedding examples. I added words and tokens found in categories and
 other navigation features, to the main summary tables including embeddings. Parsing content pages and extraction of these navigation features
 is performed in the \texttt{split\_page} function, line \textcolor{gray}{76} in the python code in section~\ref{og4u9b}. For partial xLLM sample output -- extract from an answer to a user query -- see Figure~\ref{fig:log10trmt}. Embeddings are not included. 

The goal here is not to create an LLM from scratch, but to understand the different components, adapt it to other sources, augment my current version of xLLM with extra sources, and improve some of the features. Thus, you should start by looking at the code 
\texttt{xllm5.py} in section~\ref{og4u9b} as well as the accompanying library \texttt{xllm5\_util.py} in section~\ref{xllm5util}. 

\noindent The project consists of the following steps:
\vspace{1ex}
\begin{itemize}
\item[] {\bf Step 1:  Summary tables}. Besides \textcolor{index}{embeddings}\index{embeddings}, what are the summary tables and underlying data structure, for each of them? How are they linked to 
 the output returned to a user query? How to adapt the methodology if the input source is Wikipedia rather than Wolfram?
 \vspace{1ex}
\item[] {\bf Step 2: Best practices}.  By looking at my comments in the code or by experimenting yourself, identify bad side effects from
 standard Python libraries such as \textcolor{index}{NLTK}\index{Python library!NLTK}. Examples include
 stopwords, auto-correct, and singularize. The problems arise due to the specialization of my xLLM, but is otherwise minor in generic
 applications targeted to non-experts. Suggest workarounds.
\vspace{1ex}
\item[] {\bf Step 3: Improving xLLM}.  I implemented several enhancements, for instance: ignoring \textcolor{index}{$N$-grams}\index{$n$-gram} (permutations of $N$ tokens) 
[\href{https://en.wikipedia.org/wiki/N-gram}{Wiki}] not found in the crawled data, 
 using normalized \textcolor{index}{PMI}\index{pointwise mutual information (PMI)} in embeddings as the association metric or weight between two 
\textcolor{index}{tokens}\index{token}, working with \textcolor{index}{variable-length embeddings}\index{embeddings!variable length} (VLE) rather than vectors, 
managing accented characters, not combining tokens separated by punctuation signs, and minimizing stemming such as singular form.  
Find other potential enhancements and how to implement them. For instance, making sure that ``analysis of variance" and ``ANOVA" return the same results, or ``San Francisco" is not split into two tokens. Hint: use mappings (synonyms) and double-tokens treated as a single tokens. Treat uppercase and lowercase differently.
\vspace{1ex}
\item[] {\bf Step 4: Taxonomy creation}.  The architecture of xLLM relies heavily on having a great categorization of the crawled content. In the case of Wolfram, 
 the underlying man-made taxonomy (the category hierarchy) can easily be reconstructed. How would you proceed if you do not find any pre-built taxonomy? How to build one from scratch? How to detect subcategories that should not be included? In the case of Wolfram, a sub-category can have multiple parent categories, albeit rarely. How do
 you handle this? 
\vspace{1ex}
\item[] {\bf Step 5: Parameters and evaluation}.  In the code, what are compressed $N$-grams and their purpose? How are user queries matched to embeddings and other back-end tables? Is there any training in the algorithm?
How would you evaluate the results? What are the main parameters? Hint: allow some users to play with the parameters; automatically detect the most popular values across these users; 
 popular values become the default setting. For customized results, allow users to keep playing with their favorite values. The key concept here is \textcolor{index}{self-tuning}\index{self-tuning}.
\vspace{1ex}
\item[] {\bf Step 6: Monetization}.  How would you monetize the app, while keeping it free and open-source?
\end{itemize}
\vspace{1ex}

 

\noindent I now answer my own questions, starting with \textcolor{red}{Step 1}. For summary tables,  see lines \textcolor{gray}{22-38}
in \texttt{xllm5\_short.py} in section~\ref{beurreblanc},
 and the corresponding text files (saved tables)
on GitHub, 
\href{https://github.com/VincentGranville/Large-Language-Models/tree/main/xllm5}{here}.
To check out the size, format, or key-value elements for hash tables, 
click on the corresponding file. For hash tables, the key can be a token as in \texttt{word\_list}, or a word (a combination of up to 4 tokens, separated 
 by the tilde symbol) as in \texttt{dictionary} and \texttt{ngram}. In some cases (\texttt{embeddings}, \texttt{related}) the value is also a hash table, or a list (\texttt{ngram}).  
There are about $\num{3500}$ valid tokens, and $\num{46000}$ valid words in the dictionary table, for the top-category that I am interested in. 

To process a user query, see the code starting at line \textcolor{gray}{371} in section~\ref{og4u9b}. The main function for this task
 is \texttt{process\_query} (line \textcolor{gray}{330}). First, I \textcolor{index}{transform}\index{transformer} the query in the same way the crawled data was originally transformed (auto-correct, removing stopwords, and so on), and then sort it by token: the query ``normal and Poisson distributions" becomes ``distribution normal poisson".
I ignore tokens not in \texttt{dictionary}, and then generate all token combinations (lines \textcolor{gray}{337-346}). In this case:
distribution, normal,  poisson,  distribution normal, distribution poisson, normal poisson, distribution normal poisson. 
I then match these combinations with keys in the compressed \textcolor{index}{$N$-gram}\index{$n$-gram} table. For instance, ``distribution normal" has an entry (key) and the corresponding value 
 is ``normal distribution" (the most common $N$-gram for this word, as found when crawling the data). That is, ``normal distribution" is also a key in the summary tables (at least some of them). In particular, I can retrieve links, embeddings, categories,  and related content associated to ``normal distribution", 
respectively in the tables \texttt{url\_map}, \texttt{hash\_embeddings}, \texttt{hash\_category}, and \texttt{hash\_related}. 

Finally, the Wikipedia website also has a structure quite similar to Wolfram. For the machine learning top category, see the directory page, 
 \href{https://en.wikipedia.org/wiki/Category:Machine_learning}{here}: 
this is the starting point for crawling, visiting content pages, and~to reconstruct the taxonomy. Like Wolfram, content pages sometimes 
have ``See also" navigation links, and related categories are listed at the bottom of the page. 
For sample code crawling Wikipedia, see \href{https://github.com/rajiviyer/llm/blob/main/wikipedia_ml_scraping.ipynb}{here}.

Now moving to \textcolor{red}{Step 2}. Examples of problems are auto-correct libraries erroneously changing ``feller" to ``seller", or 
 ``hypothesis" singularized to ``hypothesi", or ``even" flagged as a \textcolor{index}{stopword}\index{stopword} by some libraries and thus removed, while you need it to distinguish between even and odd
 functions in your specialized LLM. San Francisco should map to one single token, not two. To avoid these problems, do not singularize a word in the user query if the plural (but not the presumed singular) is found in the dictionary table.
Do not auto-correct proper names such as ``Feller". Create a table of words that can not be corrected or singularized, especially high frequency words 
such as ``hypothesis". Find words frequently associated, such as ``San" and ``Francisco", or use available tables.  
And build your own stopword list. Stopwords, do-not-stem, do-not-auto-correct, and do-not-singularize tables should be specific to
 a top category, that is, to a sub LLM in your multi-LLM architecture. 

To answer \textcolor{red}{Step 3}, I recommend creating a synonym dictionary. It would have an entry mapping ``ANOVA" to ``analysis of variance", and
 conversely. You can build such a dictionary by browsing glossaries, or use one from a Python library. Then allowing tokens to
 be double, for instance considering ``Gaussian", ``distribution" and ``Gaussian+distribution" as three tokens. The latter is a double token and represents two terms frequently found together. In this case, ``Gaussian" and ``distribution" are both tokens on their own, and sub-tokens. 
 
Regarding \textcolor{red}{Step 4}, you can build a taxonomy by identifying the top 200 words (up to 4 tokens) in the crawled sources, and manually assign labels to those that best represent a
 top category. At level two, look at top words attached to a specific top category (using embeddings or other tables), to create level-2 sub-categories. Do the same for level-3 and so on. The process can be
 automated to some extent. To give you an idea, the Wolfram taxonomy has about $\num{5000}$ categories. 

Note that the pre-built Wolfram taxonomy is not a tree: some sub-categories have multiple parent categories. Use a graph rather than a tree
 to represent such structures. Finally, some sub-categories such as ``linear algebra", that naturally fit under ``Probability \& Statistics" (the top category in this project), are found under the top category ``Algebra" in the Wolfram taxonomy. However, the ``Algebra" category is filled with irrelevant material (``Lie algebra"), so you want to eliminate all top categories from ``Algebra" that are not relevant to statistics. Then, the Wolfram sub-category ``Bayesian statistics" is rather weak: you need to augment it with other sources, such as Wikipedia. If you want to add ``gradient descent" to ``Probability \& Statistics", you need to look at the top category ``Calculus". 

Part of the answer to \textcolor{red}{Step 5} ($N$-grams and matching user queries to summary tables) is discussed in the second paragraph in my solution to \textcolor{red}{Step 1}. The compressed $N$-gram table is a key-value hash, where the key is an ordered $N$-gram (with tokens in alphabetical order)
and the value is a list of non-ordered versions of the $N$-gram in question,  found in the sources during crawling. Thus these non-ordered versions or natural words 
 are present as keys in the \texttt{dictionary} and satellite summary key-value tables.  To check if a user query matches some words in the summary tables, 
extract ordered $N$-grams from the query, look for them in the compressed $N$-gram table, and from there find the associated non-ordered versions. Finally, these
 non-ordered versions exist as keys in the summary tables.

There is no actual training in the algorithm. The idea is to identify the best parameters based on usage, and set them as default. Important parameters are 
 the minimum counts or thresholds to decide whether or not to include some entries in the output results. For instance: \texttt{ccnt1}, \texttt{ccnt2}, and 
\texttt{maxprint}
 in lines \textcolor{gray}{360-362}. The max number of tokens per word is set to 4, but could be changed. Separators include comma and point, but could be expanded. 
Instead of \texttt{pmi}, you can use \texttt{word2\_weight} in line  \textcolor{gray}{321} in \texttt{xllm5\_util.py} (see code in section~\ref{xllm5util}). Or you can
 change the \texttt{exponent} value in the computation of \texttt{pmi} (same code, line \textcolor{gray}{310}). If you use multiple sources, say Wolfram and Wikipedia, you
 can assign a separate trust score to each one and allow the user to fine-tune it. Finally, you can offer the user to choose between different stopword lists.

Finally, for \textcolor{red}{Step 6} (monetization), there are two options. First, blend organic results with relevant, sponsored links from advertisers.  Then offer a paid version with API access, and larger summary tables (compared to the free version) based on additional input sources.  Or give access to fresh summary tables (regularly updated) and parameter tuning, to
 paid customers only.




\begin{comment}
%compare with gpt for p-value
crawl itl.nist.gov/div898/handbook/index.htm


## crawl my books char char 
## identify \index{xxx} and \section \subsection \subsubsection ; for each index add list of section / chapters / books
##      use it to complement my LLM

### twitter2.xlsx on google drive, and DSC-master-capri5.xlsx
###                 to predict pageviews
##  Wikipedia directory: wikipedia.org/wiki/Category:Machine_learning (+ indexes)
##       poor : https://en.wikipedia.org/wiki/Category:Statistics

    ## Lemmatization and stemming are the techniques of keyword normalization, while Levenshtein and Soundex are techniques of string matching.
    ##             https://en.wikipedia.org/wiki/Edit_distance
    ##             https://huggingface.co/tasks/sentence-similarity
    ##             https://en.wikipedia.org/wiki/Semantic_similarity
    ### in stackexchange, use tags to extend categoriries // use top words

#querry = "empirical distribution" "berry" ## "Entropy" # "Convolution" # "Spectral analysis" # "time series" # "Autocorrelation" ### "Maximum"  # "Outlier" # "Extreme"  #  "Range"  ### "Markov"   ### "Beta"
######### add categories: Applied Mathematics / Calculus and Analysis / Discrete Mathematics
\end{comment}

\subsubsection{Python code}\label{og4u9b}

The program, called \texttt{xllm5.py}, is also on GitHub, \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm5/xllm5.py}{here}. 
The input file \texttt{crawl\_final\_stats.txt}~(Wolfram crawled pages, ``Probability \& Statistics" top category)   is on my Google drive, \href{https://drive.google.com/file/d/1H_xhfhzIPnO8oe9xlwCDWWM9OR5m81wd/view?usp=drive_link}{here}. Then the \texttt{xllm5\_util.py} library is discussed in 
Appendix~\ref{xllm5util}, and also on GitHub, \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm5/xllm5_util.py}{here}. 
\vspace{1ex}

\begin{lstlisting}[numbers=left]
import numpy as np
import xllm5_util as llm5  
from autocorrect import Speller
from pattern.text.en import singularize


#--- [1] some utilities

# words can not be any of these words
stopwords = ( "of", "now", "have", "so", "since", "but", "and", 
              "thus", "therefore", "a", "as", "it", "then", "that",
              "with", "to", "is", "will", "the", "if", "there", "then,",
              "such", "or", "for", "be", "where", "on", "at", "in", "can",
              "we", "on", "this", "let", "an", "are", "has", "how", "do",
              "each", "which", "nor", "any", "all", "al.", "by", "having",
              "therefore", "another", "having", "some", "obtaining",
              "into", "does", "union", "few", "makes", "occurs", "were",
              "here", "these", "after", "defined", "takes", "therefore,",
              "here,", "note", "more", "considered", "giving", "associated", 
              "etc.", "i.e.,", "Similarly,", "its", "from", "much", "was",
              "given", "Now,", "instead", "above,", "rather", "consider",
              "found", "according", "taking", "proved", "now,", "define",
              "showed", "they", "show", "also", "both", "must", "about",
              "letting", "gives", "their", "otherwise", "called", "descibed",
              "related", "content", "eg", "needed", "picks", "yielding",
              "obtained", "exceed", "until", "complicated", "resulting",
              "give", "write", "directly", "good", "simply", "direction",
              "when", "itself", "ie", "al", "usually", "whose", "being",
              "so-called", "while", "made", "allows", "them", "would", "keeping",
              "denote", "implemented", "his", "shows", "chosen", "just",
              "describes", "way", "stated", "follows", "approaches", "known",
              "result", "sometimes", "corresponds", "every", "referred",
              "produced", "than", "may", "not", "exactly", "&nbsp;", "whether",
              "illustration", ",", ".", "...", "states", "says", "known", "exists",
              "expresses", "respect", "commonly", "describe", "determine", "refer",
              "often", "relies", "used", "especially", "interesting", "versus",
              "consists", "arises", "requires", "apply", "assuming", "said",
              "depending", "corresponding", "calculated", "depending", "associated",
              "corresponding", "calculated", "coincidentally", "becoming", "discussion",
              "varies", "compute", "assume", "illustrated", "discusses", "notes",
              "satisfied", "terminology", "scientists", "evaluate", "include", "call",
              "implies", "although", "selected", "however", "between", "explaining",
              "featured", "treat", "occur", "actual", "authors", "slightly",
              "specified"
            )

# map below to deal with some accented / non-standard characters
utf_map = { "&nbsp;"   : " ", 
            "&oacute;" : "o",
            "&eacute;" : "e",
            "&ouml;"   : "o",
            "&ocirc;"  : "o",
            "&#233;"   : "e",
            "&#243;"   : "o",
            "  "       : " ",
            "'s"       : "",   # example: Feller's --> Feller
          }

def get_top_category(page):

    # useful if working with all top categories rather than just one 
    # create one set of tables (dictionary, ngrams...) for each top category
    # here we mostly have only one top category: 'Probability & Statistics'
    # possible cross-links between top categories (this is the case here)

    read = (page.split("<ul class=\"breadcrumb\">"))[1]
    read = (read.split("\">"))[1]
    top_category = (read.split("</a>"))[0]
    return(top_category)


def trim(word):
    return(word.replace(".", "").replace(",",""))


def split_page(row):

    line = row.split("<!-- Begin Content -->")  
    header = (line[0]).split("\t~")
    header = header[0]
    html = (line[1]).split("<!-- End Content -->")
    content = html[0] 
    related = (html[1]).split("<h2>See also</h2>")
    if len(related) > 1:
        related = (related[1]).split("<!-- End See Also -->")
        related = related[0]
    else:
        related = ""
    see = row.split("<p class=\"CrossRefs\">")
    if len(see) > 1:
        see = (see[1]).split("<!-- Begin See Also -->")
        see = see[0]
    else:
        see = ""
    return(header, content, related, see) 


def list_to_text(list):
    text = " " + str(list) + " "
    text = text.replace("'", " ")
    text = text.replace("\"", " ")
    # text = text.replace("-", " ")   
    text = text.replace("(", "( ")
    text = text.replace(")", ". )").replace(" ,",",")
    text = text.replace("  |",",").replace(" |",",")
    text = text.replace(" .", ".")
    text = text.lower()
    return(text)


#--- [2] Read Wolfram crawl and update main tables

file_html = open("crawl_final_stats.txt","r",encoding="utf-8")
Lines = file_html.readlines() 

# unless otherwise specified, a word consists of 1, 2, 3, or 4 tokens

dictionary = {}              # words with counts
word_pairs = {}              # pairs of 1-token words found in same word, with count
url_map = {}                 # URL IDs attached to words in dictionary
arr_url = []                 # maps URL IDs to URLs (one-to-one)
hash_category = {}           # categories attached to a word
hash_related = {}            # related topics attached to a word
hash_see = {}                # topics from "see also" section, attached to a word
word_list = {}               # list of 1-token words associated to a 1-token word 

url_ID = 0  # init for first crawled page

# process content from Wolfram crawling, one page at a time
# for Probability & Statistics category

for row in Lines:

    #-- cleaning; each row is a full web page + extra info

    category = {}

    for key in utf_map:
        row = row.replace(key, utf_map[key])

    (header, content, related, see) = split_page(row)
    url = (header.split("\t"))[0] 
    cat = (header.split("\t"))[1]
    cat = cat.replace(",", " |").replace("(","").replace(")","")
    cat = cat.replace("'","").replace("\"","")
    category[cat] = 1

    # top_category not always "Probability & Statistics"
    top_category = get_top_category(row)

    # processing "related content" on web page
    list = related.split("\">")
    related = ()
    for item in list:
        item = (item.split("<"))[0]
        if item != "" and "mathworld" not in item.lower():
            related = (*related, item)

    # processing "see also" on web page
    if see != "":
        list = see.split("\">")
        see = ()
        for item in list:
            item = (item.split("<"))[0]
            if item != "" and item != " ":
                see = (*see, item)
   
    text_category = list_to_text(category)
    text_related = list_to_text(related)
    text_see = list_to_text(see)
    content += text_category + text_related + text_see

    # skip all chars between 2 quotes (it's just HTML code)
    flag = 0
    cleaned_content = ""
    for char in content:
        if char == "\"":
            flag = 1 - flag
        if flag == 0:
            cleaned_content += char

    cleaned_content = cleaned_content.replace(">", "> ")
    cleaned_content = cleaned_content.replace("<", ". <")
    cleaned_content = cleaned_content.replace("(", "( ")
    cleaned_content = cleaned_content.replace(")", ". )")
    cleaned_content = cleaned_content.lower()
    data = cleaned_content.split(" ")
    stem_table = llm5.stem_data(data, stopwords, dictionary)

    # update tables after each parsed webpage
    # data is an array containing cleaned words found in webpage

    url_ID = llm5.update_core_tables(data, dictionary, url_map, arr_url, hash_category, 
                                     hash_related, hash_see, stem_table, category, url, 
                                     url_ID, stopwords, related, see, word_pairs, word_list)


#--- [3] create embeddings and ngrams tables, once all sources are parsed

pmi_table               = llm5.create_pmi_table(word_pairs, dictionary)    
embeddings              = llm5.create_embeddings(word_list, pmi_table)
ngrams_table            = llm5.build_ngrams(dictionary)
compressed_ngrams_table = llm5.compress_ngrams(dictionary, ngrams_table)


#--- [4] print some stats (utilities)

def hprint(title, hash, maxprint, output_file, format = "%3d %s"):
    print("\n%s\n" %(title))
    output_file.write("\n%s\n\n" %(title))
    hash_sorted = dict(sorted(hash.items(), 
                        key=lambda item: item[1], reverse=True))
    printcount = 0
    for item in hash_sorted:
        value = hash[item]
        if "URL" in title: 
            item = arr_url[int(item)] 
        if item != "" and printcount < maxprint:
            print(format % (value, item))
            output_file.write(str(value) + " " + str(item) + "\n")
            printcount += 1

    return()


def word_summary(word, ccnt1, ccnt2, maxprint, output_file):  

    if word not in dictionary:
        print("No result")
        output_file.write("No result\n")
        cnt = 0
    else:   
        cnt = dictionary[word]

    if cnt > ccnt1:

        dashes = "-" * 60
        print(dashes)
        output_file.write(dashes + "\n")
        print(word, dictionary[word])
        output_file.write(word + " " + str(dictionary[word]) + "\n")

        hprint("ORGANIC URLs", url_map[word], maxprint, output_file)
        hprint("CATEGORIES & LEVELS", hash_category[word], maxprint, output_file) 
        hprint("RELATED", hash_related[word], maxprint, output_file)
        hprint("ALSO SEE", hash_see[word], maxprint, output_file)

        if word in word_list and word in embeddings:

            # print embedding attached to word

            hash = {}
            weight_list = llm5.collapse_list(word_list[word])
            embedding_list = embeddings[word]

            for word2 in embedding_list:
               if word2 != word:
                   pmi = embedding_list[word2]
                   count = weight_list[word2]
                   product = pmi * count
                   hash[word2] = product 
            
            hprint("EMBEDDINGS", hash, maxprint, output_file, "%8.2f %s")

    print()
    return()


#--- [5] main loop 

def save_tables(): 
   
    list = { "dictionary" : dictionary,
             "ngrams_table" : ngrams_table,
             "compressed_ngrams_table" : compressed_ngrams_table,
             "word_list" : word_list,
             "embeddings" : embeddings,
             "url_map" : url_map,
             "hash_category" : hash_category,
             "hash_related" : hash_related,
             "hash_see" : hash_see,
           }

    for table_name in list:
        file = open("xllm5_" + table_name + ".txt", "w")
        table = list[table_name]
        for word in table:
            file.write(word + "\t" + str(table[word]) + "\n")
        file.close()

    file = open("xllm5_arr_url.txt", "w")
    for k in range(0, len(arr_url)):
        file.write(str(k) + "\t" + arr_url[k] + "\n")
    file.close()

    file = open("stopwords.txt","w")
    file.write(str(stopwords))
    file.close()
    file = open("utf_map.txt","w")
    file.write(str(utf_map))
    file.close()
    return()


dump = False  # to save all potential query results (big file)
save = True   # to save all tables

if save:
    save_tables()
 
if dump:

    # hperparameters 
    ccnt1 = 0  # 5
    ccnt2 = 0  # 1
    maxprint = 200  # up to maxprint rows shownn per word/section
    
    dump_file = open("xllm5_dump.txt", "w")
    for word in dictionary:
        word_summary(word, ccnt1, ccnt2, maxprint, dump_file)
    dump_file.close()
     
print("Words (up to 4 tokens):", len(dictionary))
print("1-token words with a list:", len(word_list))
print("1-token words with an embedding:", len(embeddings))


#--- [6] process sample user queries

def process_query(query, ccnt1, ccnt2, maxprint, output_file = ""):
    # query is a sorted word, each token is in dictionary
    # retrieve all sub-ngrams with a dictionary entry, print results for each one

    get_bin = lambda x, n: format(x, 'b').zfill(n)
    n = len(query)

    for k in range(1, 2**n): 

        binary = get_bin(k, n)
        sorted_word = ""
        for k in range(0, len(binary)):
            if binary[k] == '1':
                if sorted_word == "":
                    sorted_word = query[k]
                else:
                    sorted_word += "~" + query[k]

        if sorted_word in compressed_ngrams_table:
            list = compressed_ngrams_table[sorted_word]
            # the word below (up to 4 tokens) is in the dictionary
            word = list[0]
            print("Found:", word)
            output_file.write("Found:" + word + "\n")
            word_summary(word, ccnt1, ccnt2, maxprint, output_file)
            
    return()
   

# hyperparameters 
ccnt1     = 0  
ccnt2     = 0   # 2  
maxprint = 10  # up to maxprint rows shownn per word/section

spell = Speller(lang='en') 


query = " "
print("\n")
output_file = open("xllm5_results.txt", "w")

while query != "":  

    # entries separated by commas are treated independently
    query = input("Enter queries (ex: Gaussian distribution, central moments): ") 
    queries = query.split(',')
    token_list = []
    token_clean_list = []

    for query in queries:

        tokens = query.split(' ')
        for token in tokens:
            # note: spell('feller') = 'seller', should not be autocorrected
            token = token.lower()
            if token not in dictionary:
                token = spell(token) 
            token_list.append(token)
        stemmed = llm5.stem_data(token_list, stopwords, dictionary, mode = 'Internal')   

        for old_token in stemmed:
            token = stemmed[old_token]
            if token in dictionary:
                token_clean_list.append(token)
        token_clean_list.sort()

        if not token_clean_list: 
            if query != "":
                print("No match found")
                output_file.write("No match found\n")
        else:
            print("Found: ", token_clean_list) 
            output_file.write("Found: " + str(token_clean_list) + "\n") 
            process_query(token_clean_list, ccnt1, ccnt2, maxprint, output_file)

output_file.close()

\end{lstlisting}

%-------------------------------------
\subsection{RAG: retrieval augmented generation using book catalogs}\label{beurreblanc}

The purpose is to augment the \textcolor{index}{xLLM}\index{xLLM} system introduced in section~\ref{bernoc}, with external input data consisting of books. 
The focus is still on one particular
 sub-LLM: the ``probability \& statistics" category, initially built on the Wolfram crawl.  The additional input (the books) leads to an alternate taxonomy, 
 complementing sub-categories that don't have much coverage in Wolfram, such as ``Bayesian analysis". 
The alternate taxonomy based on the books may lead to a separate sub-LLM, or used for fusion with the existing one built earlier. Other input sources to augment the back-end tables include the metadata attached to crawled documents, and the front-end user prompts. For more on LLM fusion, see~\cite{serg24}.

In the process,  I also create a synonyms table built on glossaries and other structure found in the books, to further enhance output results to user prompts. Then, I  introduce double-tokens and scores: the latter measures the relevancy of the results returned to the user, for a specific query. 
Finally, I further investigate self-tuning, a better alternative to standard LLM evaluation metrics described in~\cite{24erw2}.

\noindent The project consists of the following steps:
\vspace{1ex}
\begin{itemize}
\item[] {\bf Step 1:  xLLM pluses and minuses}. The benefits of xLLM are listed and contrasted to standard LLMs in appendix~\ref{ruipof}. What are the potential negatives?
 \vspace{1ex}
\item[] {\bf Step 2: Data augmentation}. How to incorporate additional input sources  such as PDFs? In short, explain how to reconstruct the 
underlying \textcolor{index}{taxonomy}\index{taxonomy creation} of a PDF repository, and blend the augmented data with the existing summary tables built in Step 1 in project~\ref{bernoc}. What other sources could be used? The \LaTeX \, sources
 of my books are on GitHub, \href{https://github.com/VincentGranville/Large-Language-Models/tree/main/data_augmentation}{here}. The PDF version of this document can be found \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/Projects4.pdf}{here}. In these documents, what elements would you use to create / update the summary tables such as categories or \textcolor{index}{embeddings}\index{embeddings}? These additional input sources (PDFs and so on) 
 are referred to as \textcolor{index}{augmented data}\index{augmented data}, corresponding to the letter {\bf A} in the \textcolor{index}{RAG}\index{RAG}  architecture
 (retrieval-augmentation-generation).
\vspace{1ex}
\item[] {\bf Step 3: Scoring, evaluation, self-tuning}. The results displayed to the user, broken down per section (URLs, categories, tokens, related items and so on 
 as in Figure~\ref{fig:log10trmt}) are sorted according to relevancy: see the numbers on the left column, attached to each item in Figure~\ref{fig:log10trmt}. Lines \textcolor{gray}{131-147} in the Python
 script \texttt{xllm5\_short.py} at the end of this section, take care of this. 

How to create a global quality score attached to the output displayed to the user, based on the various relevancy scores assigned to each returned item? How do you weight the various sources (original crawl, augmented data such as PDFs)? Finally, if you allow the user to play with the 
\textcolor{index}{hyperparameters}\index{hyperparameter} (see Step 5 
 in project~\ref{bernoc}), you can deliver customized results: two users with the same query get different outputs. Discuss the challenges of creating
 a universal evaluation metric to compare various LLMs such as OpenAI, Mistral, Claude, xLLM and so on. How would you identify the optimum set of parameters and set it as default, based on user preferences? That is, based on the individual favorite hyperparameter sets selected by the users. 
\vspace{1ex}
\item[] {\bf Step 4: Enhancements}.  Some queries such as ``Bayes" or ``Anova" return very little, while ``Bayesian" or ``analysis of variance" return a lot more. How to address this issue? How do you improve or build a good stopwords list for a specific top category? Finally, what are the benefits of double tokens such as ``Gaussian+distribution", in addition to the simple tokens ``Gaussian" and ``distribution"? How to implement double tokens in embeddings and other tables?
\vspace{1ex}
\item[] {\bf Step 5: Combining multiple specialized LLMs}. How do you manage multiple LLMs, one per top category? The solution here is known as 
\textcolor{index}{multi-agent system}\index{multi-agent system}. 
\vspace{1ex}
\end{itemize}

\noindent I now offer my answers, starting with \textcolor{red}{Step 1}. Not working with the right in-house experts (people who know~your repository very well and what users are looking for) may result in poor design. Not enough testing may lead~to many bugs. You must create a list of who can access what to avoid data leaks and security issues.
Other issues: poor documentation; the expert engineer who knows all the details leaves your company.  

Now my answer to \textcolor{red}{Step 2}. To augment the Wolfram
``Probability \& Statistics" top category (the specialized sub-LLM of interest here),  you can parse my statistics books:
 either the PDF documents, or the \LaTeX \, sources that I share on GitHub, \href{https://github.com/VincentGranville/Large-Language-Models/tree/main/data_augmentation}{here}. For PDF documents, you can use Python libraries to retrieve tables, images, and content: 
 see my sample code \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/data_augmentation/pdf.py}{here}, based on the 
\textcolor{index}{PyPDF2} library\index{Python library!PyPDF2}. But I recommend working with the \LaTeX \, sources, for two reasons. First, the process that
 produces the PDFs may result in information loss. Then, it may~be easier to retrieve the hidden structure from the original sources: index keywords, glossary terms, 
table of contents, bibliography items and references, formatting including color (for instance to separate standard text from Python code elements) as well as 
titles, sections and subsections to help build a taxonomy. 

With the newest version of xLLM, it is easier to reconstruct the underlying taxonomy, or build one from scratch. See details in section~\ref{xllm6tr}. You may use the existing Wolfram taxonomy
 to match multi-token words or some \textcolor{index}{$n$-gram} permutation found in the \LaTeX \, sources, with related Wolfram-based \textcolor{index}{x-embeddings} entries in the \texttt{embeddings2} table, and their corresponding category
 in the \texttt{hash\_category} table.  More specifically, for \texttt{word$_1$} found in the source text, look for all
 \texttt{word$_2$} found in \texttt{embedding2[word$_1$]}, and then look for 
\texttt{hash\_category[word$_1$]} and  \texttt{hash\_category[word$_2$]}. Note that x-embeddings (the \texttt{embeddings2} table) are available
 in xLLM6 discussed in section~\ref{xllm6tr}, not in the xLLM5 code in this section. Words with high count and not found in Wolfram-based tables may lead to new categories or sub-categories.  The \texttt{compressed\_ngrams\_table} indexed by sorted $n$-grams, helps you retrieve existing token permutations
 found in Wolfram tables, when the original word is not present in these tables.  

Finally, other input sources for data augmentation include metadata, synonyms dictionaries, Wikipedia (featuring a taxonomy like Wolfram, 
see \href{https://en.wikipedia.org/wiki/Category:Machine_learning}{here}) and the prompts entered by the users.
Some Wikipedia sub-categories are irrelevant and should be skipped, otherwise it will add noise in the back-end tables. Some are missing. 

Regarding \textcolor{red}{Step 3}, different types of users (expert versus layman) may rate the results differently. To build a default set of hyperparameters, look at all
 the values favored by your users. This assumes that users can select hyperparameters to fine-tune results according to their liking. 
Based on these choices, perform \textcolor{index}{smart grid search}\index{grid search!smart grid search} using the technique described in section~\ref{smartgs}.

As for evaluating the results, in the output displayed to the user, each item in each section has a relevancy scored $z$ shown on the left: see example in Figure~\ref{fig:xembeddings}.
The first step is to normalize these scores, for instance with the transform $z\mapsto 1-\exp(-\lambda z)$ where $\lambda>0 $ is an hyperparameter.  The normalized score
 is then between 0 and 1. Compute an aggregate score per section, also taking into account the total number of items returned for a specific section. Then compute a global score for each query, as a weighted average over the sections. Use a different weight for each section. More about
 LLM evaluation can be found in~\cite{24erw2} and~\cite{eval34edr}, published in 2024. See also the GitHub repository on this topic,
 \href{https://github.com/aishwaryanr/awesome-generative-ai-guide/blob/main/free_courses/Applied_LLMs_Mastery_2024/week6_llm_evaluation.md}{here}. 


Now moving to \textcolor{red}{Step 4} and \textcolor{red}{Step 5}. Several major enhancements are discussed in section~\ref{xllm6tr}, 
featuring the differences between xLLM5 (the Python code in this section) and xLLM6. To deal with multiple specialized LLMs -- one per top category -- do the same
 separately  for 
 each of the 15 top categories on Wolfram. Here I focused on just one of them. Allow the user to specify a category along with his query, so that
 retrieving the content from the backend tables (x-embeddings and so on) is focused and fast, boosting relevancy. There will be redundancy among the various LLMs. 
Also, a top layer can be added to jointly manage the separate LLMs. This is accomplished with a 
\textcolor{index}{multi-agent system}\index{multi-agent system} architecture.  

% add related and so on unto data to compute word2_pairs
% use dictionary of recognized kw to eliminate 'procedure endeavors', 'theorem asymptotic' and the like
% yellowpages restaurant user queries as input source

You may use the \texttt{xllm5\_short.py} program below to do various testing and implement the enhancements discussed in this section. 
The code is also on GitHub, \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm5/xllm5_short.py}{here}.  For
the most recent version \texttt{xllm6\_short.py}, see section~\ref{xllm6tr}. 
\vspace{1ex}

% gent as a coach, draft 0.7.4
%Gen AI Felloship: new xLLM project added, more datasets #genaifellowhip
% rheye.py


\begin{comment}
%latex files
%xxx pdf.py xllm5_short.py

% wikipedia category link to rajiv / analogy to wolfram .. put that in next section
% https://github.com/rajiviyer/llm/blob/main/wikipedia_ml_scraping.ipynb
% https://en.wikipedia.org/wiki/Category:Machine_learning [category at bottom of each page / "see also" section

%---

%maybe wolfram uses page metadata for search

%crawl google cache
    % https://webcache.googleusercontent.com/search?q=cache:http://mltechniques.com
% add lines to all lstlisting

%compare with gpt for p-value
crawl itl.nist.gov/div898/handbook/index.htm


## crawl my books char char 
## identify \index{xxx} and \section \subsection \subsubsection ; for each index add list of section / chapters / books
##      use it to complement my LLM

### twitter2.xlsx on google drive, and DSC-master-capri5.xlsx
###                 to predict pageviews
##  Wikipedia directory: wikipedia.org/wiki/Category:Machine_learning (+ indexes)
##       poor : https://en.wikipedia.org/wiki/Category:Statistics

    ## Lemmatization and stemming are the techniques of keyword normalization, while Levenshtein and Soundex are techniques of string matching.
    ##             https://en.wikipedia.org/wiki/Edit_distance
    ##             https://huggingface.co/tasks/sentence-similarity
    ##             https://en.wikipedia.org/wiki/Semantic_similarity
    ### in stackexchange, use tags to extend categoriries // use top words

#querry = "empirical distribution" "berry" ## "Entropy" # "Convolution" # "Spectral analysis" # "time series" # "Autocorrelation" ### "Maximum"  # "Outlier" # "Extreme"  #  "Range"  ### "Markov"   ### "Beta"
######### add categories: Applied Mathematics / Calculus and Analysis / Discrete Mathematics
\end{comment}

\begin{lstlisting}[numbers=left]
# xllm5_short.py : Extreme LLM (light version), vincentg@mltechniques.com

import requests
from autocorrect import Speller
from pattern.text.en import singularize

# Unlike xllm5.py, xllm5_short.py does not process the (huge) crawled data.
# Instead, it uses the much smaller summary tables produced by xllm5.py

#--- [1] get tables if not present already

# First, get xllm5_util.py from GitHub and save it locally as xllm5_util.py
#     note: this python code does that automatically for  you
# Then import everything from that library with 'from xllm5_util import *'
# Now you can call the read_xxx() functions from that library
# In addition, the tables stopwords and utf_map are also loaded
#
# Notes:
#    - On first use, dowload all locally with overwrite = True
#    - On next uses, please use local copies: set overwrite = False 

# Table description: 
#
# unless otherwise specified, a word consists of 1, 2, 3, or 4 tokens
# word_pairs is used in xllm5.py, not in xllm5_short.py
#
# dictionary = {}      words with counts: core (central) table
# word_pairs = {}      pairs of 1-token words found in same word, with count
# url_map = {}         URL IDs attached to words in dictionary
# arr_url = []         maps URL IDs to URLs (one-to-one)
# hash_category = {}   categories attached to a word
# hash_related = {}    related topics attached to a word
# hash_see = {}        topics from "see also" section, attached to word
# word_list = {}       list of 1-token words associated to a 1-token word 
# ngrams_table = {}    ngrams of word found when crawling
# compressed_ngrams_table = {}     only keep ngram with highest count
# utf_map = {}         map accented characters to non-accented version
# stopwords = ()       1-token words not accepted in dictionary

path = "https://raw.githubusercontent.com/VincentGranville/Large-Language-Models/main/llm5/"

overwrite = False

if overwrite:

    response = requests.get(path + "xllm5_util.py")
    python_code = response.text

    local_copy = "xllm5_util"
    file = open(local_copy + ".py", "w")
    file.write(python_code)
    file.close()

    # get local copy of tables

    files = [ 'xllm5_arr_url.txt', 
              'xllm5_compressed_ngrams_table.txt',
              'xllm5_word_list.txt',
              'xllm5_dictionary.txt',
              'xllm5_embeddings.txt',
              'xllm5_hash_related.txt',
              'xllm5_hash_category.txt',
              'xllm5_hash_see.txt',
              'xllm5_url_map.txt',
              'stopwords.txt'
            ]

    for name in files:
        response = requests.get(path + name)
        content = response.text
        file = open(name, "w")
        file.write(content)
        file.close()  

import xllm5_util as llm5

# if path argument absent in read_xxx(), read from GitHub
# otherwise, read from copy found in path

arr_url       = llm5.read_arr_url("xllm5_arr_url.txt",       path="")
dictionary    = llm5.read_dictionary("xllm5_dictionary.txt", path="")
stopwords     = llm5.read_stopwords("stopwords.txt",         path="")

compressed_ngrams_table = llm5.read_table("xllm5_compressed_ngrams_table.txt", 
                                                           type="list", path="")
word_list     = llm5.read_table("xllm5_word_list.txt",     type="list", path="")
embeddings    = llm5.read_table("xllm5_embeddings.txt",    type="hash", path="", 
                                                                 format="float") 
hash_related  = llm5.read_table("xllm5_hash_related.txt",  type="hash", path="")
hash_see      = llm5.read_table("xllm5_hash_see.txt",      type="hash", path="")
hash_category = llm5.read_table("xllm5_hash_category.txt", type="hash", path="")
url_map       = llm5.read_table("xllm5_url_map.txt",       type="hash", path="")


#--- [2] some utilities

def singular(data, mode = 'Internal'):

    stem_table = {}

    for word in data:
        if mode == 'Internal': 
            n = len(word)
            if n > 2 and "~" not in word and \
                  word[0:n-1] in dictionary and word[n-1] == "s":
                stem_table[word] = word[0:n-1]
            else:
                stem_table[word] = word
        else:
            # the instruction below changes 'hypothesis' to 'hypothesi'
            word = singularize(word)

            # the instruction below changes 'hypothesi' back to 'hypothesis'
            # however it changes 'feller' to 'seller' 
            # solution: create 'do not singularize' and 'do not autocorrect' lists
            stem_table[word] = spell(word) 

    return(stem_table)


#--- [3] print some stats (utilities)  

def fformat(value, item, format):
    format = format.split(" ")
    fmt1 = format[0].replace("%","")
    fmt2 = format[1].replace("%","")
    string = '{:{fmt1}} {:{fmt2}}'.format(value,item,fmt1=fmt1,fmt2=fmt2)
    return(string)


def hprint(title, hash, maxprint, output_file, format = "%3d %s"):
    print("\n%s\n" %(title))
    output_file.write("\n%s\n\n" %(title))
    hash_sorted = dict(sorted(hash.items(), 
                        key=lambda item: item[1], reverse=True))
    printcount = 0
    for item in hash_sorted:
        value = hash[item]
        if "URL" in title: 
            item = arr_url[int(item)] 
        if item != "" and printcount < maxprint:
            print(format % (value, item))
            string = fformat(value, item, format)
            output_file.write(string + "\n")
            printcount += 1

    return()


def word_summary(word, ccnt1, ccnt2, maxprint, output_file):  

    if word not in dictionary:
        print("No result")
        output_file.write("No result\n")
        cnt = 0
    else:   
        cnt = dictionary[word]

    if cnt > ccnt1:

        dashes = "-" * 60
        print(dashes)
        output_file.write(dashes + "\n")
        print(word, dictionary[word])
        output_file.write(word + " " + str(dictionary[word]) + "\n")

        hprint("ORGANIC URLs", url_map[word], maxprint, output_file)
        hprint("CATEGORIES & LEVELS", hash_category[word], maxprint, output_file) 
        hprint("RELATED", hash_related[word], maxprint, output_file)
        hprint("ALSO SEE", hash_see[word], maxprint, output_file)

        if word in word_list and word in embeddings:

            # print embedding attached to word

            hash = {}
            weight_list = llm5.collapse_list(word_list[word])
            embedding_list = embeddings[word]

            for word2 in embedding_list:
               if word2 != word:
                   pmi = embedding_list[word2]
                   count = weight_list[word2]
                   product = pmi * count
                   hash[word2] = product 
            
            hprint("EMBEDDINGS", hash, maxprint, output_file, "%8.2f %s")

    print()
    return()


#--- [4] main loop 

dump = False  
 
if dump:

    # hperparameters 
    ccnt1 = 0  # 5
    ccnt2 = 0  # 1
    maxprint = 200  # up to maxprint rows shownn per word/section
    
    dump_file = open("xllm5_dump.txt", "w")
    for word in dictionary:
        word_summary(word, ccnt1, ccnt2, maxprint, dump_file)
    dump_file.close()
     
print("Words (up to 4 tokens):", len(dictionary))
print("1-token words with a list:", len(word_list))
print("1-token words with an embedding:", len(embeddings))


#--- [5] process sample user queries 

def process_query(query, ccnt1, ccnt2, maxprint, output_file = ""):
    # query is a sorted word, each token is in dictionary
    # retrieve all sub-ngrams with a dictionary entry, print results for each one

    get_bin = lambda x, n: format(x, 'b').zfill(n)
    n = len(query)

    for k in range(1, 2**n): 

        binary = get_bin(k, n)
        sorted_word = ""
        for k in range(0, len(binary)):
            if binary[k] == '1':
                if sorted_word == "":
                    sorted_word = query[k]
                else:
                    sorted_word += "~" + query[k]

        if sorted_word in compressed_ngrams_table:
            list = compressed_ngrams_table[sorted_word]
            # the word below (up to 4 tokens) is in the dictionary
            word = list[0]
            print("Found:", word)
            output_file.write("Found:" + word + "\n")
            word_summary(word, ccnt1, ccnt2, maxprint, output_file)

    return()
   

# hyperparameters
ccnt1     = 0  
ccnt2     = 0   # 2  
maxprint = 10  # up to maxprint rows shownn per word/section 

spell = Speller(lang='en')

query = " "
print("\n")
output_file = open("xllm5_results.txt", "w")

while query != "":  

    # entries separated by commas are treated independently
    query = input("Enter queries (ex: Gaussian distribution, central moments): ") 
    queries = query.split(',')
    token_list = []
    token_clean_list = []

    for query in queries:

        tokens = query.split(' ')
        for token in tokens:
            # note: spell('feller') = 'seller', should not be autocorrected
            token = token.lower()
            if token not in dictionary:
                token = spell(token) 
            token_list.append(token)
        stemmed = singular(token_list, mode = 'Internal')   

        for old_token in stemmed:
            token = stemmed[old_token]
            if token in dictionary:
                token_clean_list.append(token)
        token_clean_list.sort()

        if not token_clean_list: 
            if query != "":
                print("No match found")
                output_file.write("No match found\n")
        else:
            print("Found: ", token_clean_list) 
            output_file.write("Found: " + str(token_clean_list) + "\n") 
            process_query(token_clean_list, ccnt1, ccnt2, maxprint, output_file)

output_file.close()

\end{lstlisting}




\chapter{Miscellaneous Projects}

This chapter features projects that span across multiple categories, such as generative AI, large language models and NLP, or machine learning optimization and scientific computing. They allow you to broaden your horizon in multiple directions. You don't need to be an expert in many areas to work on them. 
 Typically,  the first step consists of checking my Python code, create a minimum viable version, then add features, and use the code on your own data. Each
 project is self-contained, with an introduction presenting relevant material. References to literature and other chapters are provided as needed.  

%xxxx yyyy  replace links by ref to other sections

\section{Fast probabilistic nearest neighbor search (pANN)}\label{f8v2koyuy}

ANN -- \textcolor{index}{approximate nearest neighbors}\index{nearest neighbors!approximate (ANN)} --  is at the core of fast 
\textcolor{index}{vector search}\index{vector search}, itself central to GenAI, especially GPT and LLM. My new methodology, abbreviated as pANN, has many other applications: clustering, classification, measuring the similarity between two datasets (images, soundtracks, time series, and so on), tabular data synthetization (improving poor synthetizations), model evaluation, and even detecting extreme observations.

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.64\textwidth]{ann-cf.png}
\caption{Average NN distance over time, with probabilistic ANN}
\label{fig:gmlp9ut}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

Just to give an example, you could use it to categorize all time series without statistical theory. Parametric statistical models are 
subject to \textcolor{index}{identifiability}\index{identifiability (statistics)} issues (redundancy) and less explainable, leading to definitions less useful to developers, and math-heavy.  \textcolor{index}{pANN}\index{nearest neighbors!approximate (ANN)!probabilistic (pANN)} avoids that.
Fast and simple, pANN (for Probabilistic ANN) does not involve training or neural networks, and it is essentially math-free. Its versatility comes from four features:
\vspace{1ex}
\begin{itemize}
\item Most algorithms aim at minimizing a loss function. Here I also explore what you can achieve by maximizing the loss.
\item Rather than focusing on one set of datasets, I use two sets $S$ and $T$. For instance, 
\textcolor{index}{$K$-NN}\index{nearest neighbors!$K$-NN} looks for nearest neighbors within a set $S$. What about looking for nearest neighbors in $T$, to observations in $S$? This leads to far more applications than the one-set approach.
\item Some algorithms are very slow and may never converge. No one looks at them. But what if the loss function drops very fast at the beginning, fast enough that you get better results in a fraction of the time, by stopping early, compared to using the ``best" method?
\item In many contexts, a good approximate solution obtained in little time from an otherwise non-converging algorithm, may be as good for practical purposes as a more accurate solution obtained after far more steps using a more sophisticated algorithm.
\end{itemize}
\vspace{1ex}
\noindent Figure~\ref{fig:gmlp9ut} shows how quickly the loss function drops at the beginning. In this case, the loss represents the average distance to the approximate nearest neighbor, obtained so far in the iterative algorithm. The X-axis represents the iteration number. Note the excellent curve fitting (in orange) to the loss function, allowing you to predict its baseline (minimum loss, or optimum) even after a small number of iterations. To see what happens if you maximize the loss instead, read the full technical document. Another example of non-converging algorithm doing better than any kind of \textcolor{index}{gradient descent}\index{gradient descent} is discussed in chapter 13 in~\cite{vgelsevier}.

\subsection{Motivation and architecture}

With the increasing popularity of RAG, LLM, and ANN-based fast vector search including in real time, it was time for me 
to figure out how I could improve the existing technology. In this context, ANN stands for 
\textcolor{index}{approximated nearest neighbors}\index{approximated nearest neighbors},
 a better alternative to $K$-NN.

What I came up with covers a large number of applications: matching embeddings to prompts or user queries, data synthetization, GenAI model evaluation, measuring the similarity or distance between two datasets, detection of extremes in arbitrary 
 dimensions,  finding the envelop of a dataset, or classification (supervised and unsupervised).  The technique
 was first introduced in the context of NoGAN tabular data synthetization, 
see chapter 7 in~\cite{vgmloptim}. The goal is to find a good approximation to the solution, as quickly as possible. You can think of it~as a gradient descent method, with very steep descent at the beginning, leading to a satisfactory solution in a fraction of the time 
 most sophisticated algorithms require. Speed is further increased by using absolute differences rather than square roots of sums of squares. Also, there is no gradient and no neural networks: thus, no math beyond
 random numbers. 

The following architecture and algorithm are common to all applications. You have two sets of multivariate vectors,
 $S$ and $T$ respectively with $n$ and $m$ elements, each elements being a vector with $d$ components:
\begin{align}
S & =\{x_1,\dots, x_n\} \nonumber\\
T & = \{y_1,\dots, y_m\} \nonumber.
\end{align}
For each element $x_k$ in $S$, you want to find the closest neighbor $y_{\sigma(k)}$ in $T$. 
 Thus, the problem consists of finding 
the function $\sigma_0$ that minimizes the \textcolor{index}{loss function}\index{loss function}
 $L(\sigma)$ defined by
\begin{equation}
 L(\sigma) = \sum_{k=1}^n \|x_k - y_{\sigma(k)}\|. \label{lossder}
\end{equation}
The minimum is over all integer functions $\sigma$ defined on $\{1, \dots, n\}$ with
 values in $\{1, \dots, m\}$. There are $m^n$ such functions. The one minimizing $L(\sigma)$ is
 denoted as $\sigma_0$. It might not be unique, but this is unimportant. 
In some cases, we are interested in maximizing $L(\sigma)$ instead, which is identical to
minimizing $-L(\sigma)$. 
And frequently, to be admissible as a solution,
 a function $\sigma$ must satisfy $x_k \neq y_{\sigma(k)}$ for $1\leq k\leq n$. 

The oldest application in recent times, also the origin for the  abbreviation ANN, is the 
$K$-NN problem, or \textcolor{index}{$K$ nearest neighbors}\index{$k$-NN}. 
In this case, $S$ consists of $K$ copies of $T$. As we shall see, my algorithm results in a different solution,
 with a variable number of neighbors per observation, rather than the fixed value $K$. Also, when $K=1$,
 the trivial solution is $\sigma(k) = k$ for $1\leq k \leq n$. That is, the closest neighbor to $x_k$ is $x_k$ itself. 
Thus the aforementioned constraint
 $x_k \neq y_{\sigma(k)}$ to eliminate this solution. 

An ancient version dating back to 1890 is the assignment problem. It was solved in polynomial time in~1957, 
 using the \textcolor{index}{Hungarian algorithm}\index{Hungarian algorithm} [\href{https://en.wikipedia.org/wiki/Hungarian_algorithm}{Wiki}]. These days, we want something much faster than even quadratic time.
My method will provide a good approximation much faster than quadratic if you stop early. Brute force would solve this problem in $n\times m$ steps, by finding the closest $y_{\sigma(k)}$ to each $x_k$ separately. Note that 
unlike~in~the original assignment problem, here the function $\sigma$ does not need to be a permutation, 
 allowing for faster,  one-to-many neighbor allocation.





The solution can be an excellent starting point for an exact search, or used as a final, good enough result. 
The algorithm processes the data set $S$ a number of times. Each completed visit of $S$ is called   
an \textcolor{index}{epoch}\index{epoch (neural networks)}. In a given epoch, for each observation
 $x_k$ (with $1\leq k \leq n$), a potential new neighbor $y_{\sigma'(k)}$ is randomly selected. 
If 
$$
\| x_k - y_{\sigma'(k)}\|  < (1-\tau)\cdot \| x_k - y_{\sigma(k)}\|,
$$
then $y_{\sigma'(k)}$ becomes the new, closest neighbor to $x_k$, replacing the old neighbor 
$y_{\sigma(k)}$. In this case,  $\sigma(k) \leftarrow \sigma'(k)$. Otherwise, $\sigma(k)$ is unchanged, but $y_{\sigma'(k)}$ is flagged as unsuitable neighbor 
 in the list of potential neighbors to $x_k$. For each $x_k$, the list of unsuitable neighbors starts empty and grows very slowly, at least at the beginning. The parameter $\tau$ is called the temperature. The default value is zero,
 but positive values that decay over~time~may lead to an accelerated schedule. Negative values always underperform, 
 but it makes the loss function goes~up~and down, with oscillations of decreasing amplitude over time, behaving very much like the loss function in stochastic gradient descent and deep neural networks. 

Another mechanism to accelerate the convergence at the beginning (what we are interested in) is as follows. At the start of each epoch, sort $S$ in reverse order based on distance to nearest neighbors in $T$, obtained so far. In a given epoch, do not process all
 observations $x_k$, but only a fraction of them, for instance the top 50\% with the largest NN distances. 

Figure~\ref{fig:gmlp9ut} illustrates the convergence. The power function $\varphi(t) = \alpha + \beta t^{-\gamma}$ provides an excellent fit. Here $\varphi(t)$ is the average nearest neighbor distance at time $t$. The time represents the number of steps performed so far, on a dataset with $n=m=200$. Interestingly, $\gamma \approx 0.50$, but on some datasets, I was able to get faster convergence, with $\gamma\approx 0.80$. The coefficient $\alpha$ represents the 
 average NN distance at the limit, if you were to do an exact search. In other words,
$\alpha \approx L(\sigma_0)/n$. If you are only interested in $\alpha$, you can get a good approximation
 in a fraction of the time it takes to compute the exact NN distances. To do it even faster by 
 interpolating the curve fitting function based on the first few hundred measurements only, see
 Figure~\ref{fig:lollog1xx} and section~\ref{tuasbu5}.



%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.72\textwidth]{ann.png}
\caption{Approximate NNs from $T$ (blue) to points in $S$ (red) after a few thousand steps}
\label{fig:wasfmlp9ut}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

Figure~\ref{fig:wasfmlp9ut} shows the dataset used in Figure~\ref{fig:gmlp9ut}, with red segments linking points in $S$ (red) to their closest neighbor in $T$ (blue) obtained at the current iteration. View video 
\href{https://youtu.be/ORNIx_rBTrQ}{here}  showing
 how the approximate nearest neighbors get more and more accurate over time,  from beginning to end.

\subsection{Applications}

The methodology presented here is useful in several contexts. Now, I describe how to leverage my algorithm in various applications, ranging from traditional to GenAI and LLM. 

\subsubsection{Embeddings and large language models}\label{porctupues}

In large language models, embeddings are lists of tokens attached to a keyword.  For instance, Table~\ref{de2pufd1re} is based on my specialized xLLM that answers research questions about statistics and probability: see section~\ref{sxllm5}. The table features embeddings, for 7 one-token keywords. For each keyword, the top row 
 shows the top 10 tokens. The bottom one shows the importance of each token: the numbers represent the 
normalized \textcolor{index}{pointwise mutual information}\index{pointwise mutual information (PMI)} (PMI) between a token and a keyword. This metric measures the strength of the association between a token and a keyword. Here, I use 
\textcolor{index}{variable-length embeddings}\index{embeddings!variable length}~\cite{vle23} to reduce the size of the embedding tables.

To measure the similarity between two words, first compute the \textcolor{index}{dot product}\index{dot product} 
(the $\bullet$ product)
[\href{https://en.wikipedia.org/wiki/Dot_product}{Wiki}] between the two word embeddings. Tokens with a PMI of zero for either word (that is, absent for one of the two words) are ignored in the computations.
 Then, compute the norm $\|\cdot\|$ of each word. The norm is the square root of the sum of squared PMIs. For instance,
 based on Table~\ref{de2pufd1re}:
\begin{align}
\text{normal} \bullet  \text{Gaussian} & = 0.43903 \times 0.00858 + 0.05885 \times 0.01164 =  0.004452 \nonumber \\
\text{binomial} \bullet\text{Gaussian} & = 0.11796 \times 0.00858 +  0.01117 \times 0.01164 = 0.001142 \nonumber
\end{align}
$$
\| \text{normal} \| = 0.49678, \quad \| \text{Gaussian} \| =0.05853, \quad \| \text{binomial} \| = 0.13998.
$$
There are many different ways to define the similarity between two words. The
\textcolor{index}{cosine similarity}\index{cosine similarity}  [\href{https://en.wikipedia.org/wiki/Cosine_similarity}{Wiki}]
 is one~of them. It normalizes the dot products, but does not capture magintudes. It is computed as follows:
\begin{align}
\rho(\text{normal},  \text{Gaussian}) & = 
\frac{\text{normal} \bullet  \text{Gaussian}}{\| \text{normal} \| \cdot \| \text{Gaussian} \|}  
= 0.15311, \nonumber \\[0.5ex]
   \rho(\text{binomial}, \text{Gaussian}) & =
 \frac{\text{binomial} \bullet  \text{Gaussian}}{\| \text{normal} \| \cdot \| \text{Gaussian} \|}  
 =  0.13940. \nonumber
\end{align}

Whether using the dot product or cosine similarity, ``normal" is closer to ``Gaussian" than ``binomial". The distance may then be 
defined as $1 - \rho$. The goal, given two sets of embeddings $S$ and $T$, is to find, for each embedding in $S$, its closest neighbor in $T$. For instance, $S$ may consist of the top 1000 standardized user queries with associated embeddings (stored in cache for fast real-time retrieval), and $T$ maybe the full list of embeddings based on crawling and/or parsing your entire repository. 

\begin{table}[H]
%\[\
\begin{center}
\scalebox{0.77}{
\begin{tabular}{l|p{1.7cm}p{1.9cm}p{2.1cm}p{1.7cm}p{1.7cm}p{1.7cm}p{1.7cm}p{1.7cm}p{1.7cm}p{1.3cm}}    %  |r|r|r|r|r|r|r|r|r}
\hline
  word & token 1 & token 2 & token 3  & token 4 & token 5 & token 6 & token 7 & token 8 & token 9 & token 10\\
\hline
\hline
hypothesis 	&	alternative	&	null	&	statistical	&	false	&	test	&	nested	&	testing	&	type	&	bourget	&	chinese	\\
	&	0.05070	&	0.03925	&	0.03539	&	0.03177	&	0.01885	&	0.01661	&	0.01358	&	0.01056	&	0.01011	&	0.01011	\\
\hline
test & statistical	&	wilcoxon	&	negative	&	alternative	&	alpha	&	fisher	&	kolmogorov  & contingency & type & false\\
& 0.09546	&	0.05842	&	0.03206	&	0.02700	&	0.02519	&	0.02456	&	0.02224	&	0.02099	&	0.02066	&	0.01924 \\
\hline 
normal	&	distribution	&	bivariate	&	standard	&	log	&	multivariate	&	variate	&	ratio	&	trivariate	&	sum	&	difference	\\
	&	0.43903	&	0.15486	&	0.10019	&	0.09719	&	0.05885	&	0.05204	&	0.03569	&	0.03368	&	0.03240	&	0.03074	\\
\hline
Gaussian	&	inverse	&	joint	&	increment	&	multivariate	&	physicists	&	spectrum	&	noisy	&	distribution	&	board	&	polygon	\\
	&	0.04340	&	0.02718	&	0.01164	&	0.01164	&	0.01164	&	0.01006	&	0.00964	&	0.00858	&	0.00832	&	0.00774	\\
\hline	
walk	&	random	&	self-avoiding	&	wiener	&	connective	&	polya	&	levy	&	two-dim	&	lattice	&	trajectories	&	confined	\\
	&	0.16104	&	0.10019	&	0.04138	&	0.02888	&	0.01691	&	0.01491	&	0.01447	&	0.01344	&	0.01004	&	0.01004	\\
\hline
random	&	walk	&	variable	&	number	&	sequence	&	independent	&	set	&	constant	&	polya	&	one-dim	&	process	\\
	&	0.16104	&	0.10245	&	0.08385	&	0.06631	&	0.05068	&	0.03509	&	0.03230	&	0.03028	&	0.02939	&	0.02844	\\
\hline
binomial	&	distribution	&	negative	&	approximation	&	integer	&	beta	&	multivariate	&	discrete	&	trial	&	rise	&	infinity	\\
	&	0.11796	&	0.06830	&	0.01455	&	0.01327	&	0.01133	&	0.01117	&	0.01039	&	0.00990	&	0.00944	&	0.00886	\\
\hline
\end{tabular}
}
%\]
\caption{\label{de2pufd1re} Embeddings (one per word) with normalized PMI score attached to each token}
\end{center}
\end{table}


When the goal is to compute all nearest neighbors 
withing $T$ (in this case, $S=T$), the xLLM architecture is especially efficient.
It uses a separate embedding table for each top category. Assuming $q$ tables respectively with $N_1,\dots,N_q$ embeddings, 
standard $k$-NN  over categories bundled together is 
$O(N^2)$  with $N=N_1+\dots+N_q$, versus the much lower $O(N_1^2+\cdots+N_q^2)$ when the $q$ categories are 
treated separately.
 With the ANN algorithm described in section~\ref{f8v2koyuy}, these computing times are significantly reduced.  
However, with $q$ categories, you must add a little overhead time and memory as there is a top layer for
 cross-category management.  When a category
 has more than (say) $\num{5000}$ embeddings, further acceleration is achieved by splitting its table into
 smaller batches, and compute nearest neighbors on each batch separately. The solid gain in speed
 usually outweighs the small loss in accuracy.  For 
\textcolor{index}{prompt compression}\index{prompt compression} to reduce the size of the input user queries,
 see~\cite{breze24}. 




\subsubsection{Generating and evaluating synthetic data}\label{geasds}

My first use of \textcolor{index}{probabilistic ANN}\index{ANN (approximate nearest neighbors)}\index{ANN (approximate nearest neighbors)!probabilistic ANN (pANN)} (pANN) was for synthesizing tabular data, see chapter 7 in~\cite{vgmloptim}. 
It led to a faster and better alternative to GAN 
(\textcolor{index}{generative adversarial networks}\index{generative adversarial network}), and was actually called 
\textcolor{index}{NoGAN}\index{NoGAN} as it does not require neural networks. 
But it also helps with various related GenAI problems. For instance:
\vspace{1ex}
\begin{itemize}
\item {\bf Improving poor synthetic data}. Say you have a real dataset $T$, and and you created a synthetic version of it, the $S$ set. 
You can generate much more observations than needed in your synthetic data, then only keep the best ones. To do this, only keep in 
 $S$ observations with a nearest neighbor in $T$ that is close enough. In short, you discard synthetic observations that are too far  away from any real observation. This simple trick will improve the quality of your synthetic data, if the goal is good enough replication of the underlying distribution in the real data. pANN is particularly handy to solve this problem. \vspace{1ex}

\item {\bf Evaluating the quality of synthetic data}. The best metrics to evaluate the faithfulness of synthetic data are 
 typically based on the
 multivariate \textcolor{index}{empirical cumulative distributions}\index{empirical distribution!multivariate} (ECDF), see section~\ref{genaiyert}.
 The ECDF is evaluated at various locations $z$ in the feature space, computed both on the synthetic
 data $S$, and the real data $T$. In particular, the \textcolor{index}{Kolmogorov-Smirnov distance}\index{Kolmogorov-Smirnov distance} is defined as
$$
\text{KS}(S, T)  =\sup_z | F_\text{s}(z)-F_\text{r}(z)|, 
$$
where $F_\text{s}, F_\text{r}$ are the ECDFs, respectively for the synthetic and real data.
It involves finding the closest neighbors to each $z$, both in $S$ and $T$. Again, the pANN algorithm can help accelerate the computations. 
\end{itemize}\vspace{1ex}
\noindent For an alternative to pANN, based on 
 \textcolor{index}{interpolated binary search}\index{binary search! interpolated} and 
\textcolor{index}{radix encoding}\index{radix search},  see section~\ref{radixdw8}.
Several nearest neighbor search methods are discussed in the
 article ``Comprehensive Guide To Approximate Nearest Neighbors Algorithms"~\cite{eyal20}. 


%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.76\textwidth]{ann-max.png}
\caption{Extreme points in blue ($S=T$) obtained by maximizing the loss $L(\sigma)$}
\label{fig:z0m4rlp9ut}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

\subsubsection{Clustering, dataset comparisons, outlier detection}\label{bodong}

Nearest neighbor (NN) methods were first used in classification algorithms, with $S=T$: the ancestor of pANN is $K$-NN. 
To adapt pANN to $K>1$, proceed as follows. If you want the three approximate nearest neighbors ($K=3$) to observation 
$x_{k_1}$ in $S$, keep two extra copies of $x_{k_1}$,
 say $x_{k_2}$ and $x_{k_3}$, in $S$. At any time in the iterative algorithm, flag any nearest neighbor assigned to one of the copies,  say $x_{k_2}$, as nonassignable to the two other copies, in this case $x_{k_1}$ and $x_{k_3}$. To do so, 
 add the nearest neighbor in question (its index in $T$) 
to the lists \texttt{hask[$k_1$]} and \texttt{hash[$k_3]$} in line \textcolor{gray}{138} in the Python code
 in section~\ref{pyrhic}. You also need to set \texttt{optimize=\textquotesingle speed\textquotesingle} in line \textcolor{gray}{117} so that \texttt{hash} is active. 
In the code, the nearest neighbor to $x_{k_1}$ in $T$, is $y_j$ with $j=$\texttt{arr\_NN[$k_1$]}. Its index is $j$.

For classification purposes, a new point $x_{k_1}$ in $S$, outside the training set $T$, gets assigned using majority vote, by looking at the class assigned to its nearest neighbors in $T$. For clustering (unsupervised classification) the same rule applies, but there is no class label: the end result is a clustering structure that groups points in unlabeled clusters based on proximity. 

Beyond classification, pANN is also helpful to find similar datasets in a database. For example, images 
 or soundtracks. Each dataset hast its feature vector, consisting not necessarily of pixels or waves, but  instead, where 
vector components are summary statistics about the image or soundtrack. 
 This is an extension of what I discussed in section~\ref{geasds}. It also applies to comparing time series, where 
vectors consist of autocorrelations of various lags, with one vector per time series.   
Finally, if you maximize rather than minimize the loss function, you can detect extreme points as opposed to nearest neighbors: see Figure~\ref{fig:z0m4rlp9ut}.

\subsection{Project and solution}

Rather than asking you to write an algorithm and Python code from scratch, the goal is to understand my methodology, 
simplify and test the code in section~\ref{pyrhic}, add features, and investigate particular cases. 

\noindent The project consists of the following steps.
\vspace{1ex}
\begin{itemize}
\item[] {\bf Step 1:  Accelerating speed}. Simplify the code by removing the acceleration mechanisms: get rid~of the overhead attached to \texttt{optimize=\textquotesingle speed\textquotesingle} 
 (for instance, \texttt{hash}) and set \texttt{optimize=\textquotesingle memory\textquotesingle}. Also,  comment out line \textcolor{gray}{80}, 
 lines \textcolor{gray}{170--171}, and replace line \textcolor{gray}{130} by \texttt{k=iter\%N}. Then you don't need the
 function \texttt{sort\_x\_by\_NNdist\_to\_y}. Finally, set \texttt{decay=0}. Also identify and remove all the code linked to producing the video,
 for instance \texttt{flist}, \texttt{frame}, the \texttt{save\_image} function, and lines \textcolor{gray}{158-168}.
\vspace{1ex}
\item[] {\bf Step 2:  Performance of accelerators}. Understand the difference between \texttt{iter}, \texttt{steps}, and \texttt{swaps} in the code. 
Play with the different accelerators. How would you assess the performance of each accelerator? Finally, allowing the loss function to go up and down with
 decaying oscillations and downward trend, that is, 
\textcolor{index}{stochastic descent}\index{gradient descent!stochastic} similar to the orange curve in Figure~\ref{fig:gretcocol}, is always worse than going straight down, or \textcolor{index}{steepest descent}\index{gradient descent!steepest}. Explain why, since for neural networks, the opposite is true. Here, stochastic descent is emulated with negative \texttt{decay} in line \textcolor{gray}{119}.
\vspace{1ex}.
 \item[] {\bf Step 3:  Evaluation}. The key metric $\Delta_t$ linked to the 
\textcolor{index}{loss function}\index{loss function} (Formula~\ref{lossder}) is the average nearest neighbor distance at any given iteration $t$, 
starting with a rather large value and decreasing over time:
 the nearest neighbors in $T$ to points in $S$ become more and more accurate as $t$ increases, starting with random locations. However,  
$\Delta_t$ depends on $m$, the number of points in $T$: intuitively, the larger $m$, the smaller $\Delta_t$. How would you adjust $\Delta_t$ to make it independent of $m$? 
\end{itemize}
\vspace{1ex}

\noindent To answer the second part of \textcolor{red}{Step 2}, in deep neural networks, the loss function is a proxy to the performance or quality metric.
To the contrary, here the loss function and model evaluation metric are identical. Also, there is no risk in strictly decreasing the loss function at each iteration: eventually the algorithm must reach a global minimum. This is not true in deep neural networks, where you can get stuck in a local minimum if you don't allow the loss function to go up and down.   As for the terminology, a \texttt{swap} is when a change (nearest neighbor re-assignment) actually occurs during an iteration. Swaps become rarer and rarer over time. A \texttt{step} within an iteration is when a nearest neighbor candidate is not accepted (for instance, because it has already been rejected in the past), forcing the
 algorithm to choose another candidate. Steps are more numerous towards the end, and used only when \texttt{optimize} is set to \texttt{\textquotesingle speed\textquotesingle}. Otherwise steps
 and iterations are the same. 

Regarding \textcolor{red}{Step 3}, if the points are independently and uniformly distributed in a $d$-dimensional feature space and $S = T$, then $\Delta_t$ is
 almost proportional to $m^{-1/d}$ when $t$ is large enough. Thus the adjusted $\Delta'_t = m^{1/d}\Delta_t$ is nearly independent of $m$. The
 factor $m^{-1/d}$ can be obtained via simulation and curve fitting. However, there is a theoretical explanation.  Let $S = T$ and let us assume that the points follow 
 a \textcolor{index}{Poisson process}\index{Poisson process} of intensity $\lambda$ in $d$ dimensions. The probability that there is no point within a distance $R$ to
 a given, arbitrary location is $G(R) =\exp(-\lambda \nu R^d)$ where $\nu$ is the volume of the $d$-dimensional \textcolor{index}{unit ball}\index{unit ball} 
[\href{https://en.wikipedia.org/wiki/Volume_of_an_n-ball}{Wiki}].  Thus, the CDF (cumulative distribution) for the distance $R$ to the
 nearest neighbor is $F_R(r) = 1 - G(r)$, for $r\geq 0$. 

So, $R$ has a \textcolor{index}{Weibull distribution}\index{Weibull distribution}. Its expectation $\text{E}(R)$ is proportional to
 $\lambda^{-1/d}$, that is, to $m^{-1/d}$ since the intensity $\lambda$ is the expected number of points per unit volume (or per unit area in 2D).
The peculiarity here is that I use the 
\textcolor{index}{taxicab distance}\index{taxicab distance} [\href{https://en.wikipedia.org/wiki/Taxicab_geometry}{Wiki}] 
rather than the traditional Euclidean norm: see line \textcolor{gray}{145} in Python code in section~\ref{pyrhic}. The reason is for faster computations; the choice of the distance has very little impact. Then, the volume of the unit ball is $\nu=2^d / d!$

 
%xxxxyyy 
 %      MLT: how does avg NN distance varies a a function of number of points in 2-3 dim
  %          Poisson process [link to exercise in my first book on PB processes]
%takeaway: use it to improve quality of synthetic data even from poor synthesizer 
%project:
%start with base code, see if you can further improve
%adjust NN distance based on number of obs in S make avg dist size-dependent by randomly removing points vs keeping NN nearest neighbors
%improve gretel
%add pANN NoGAN XLLM to index
%xxxyyyy
%figure : niter = 60,000; x = y, 200 obs, K = N

\subsection{Python code}\label{pyrhic}

See section~\ref{bodong} for details about \texttt{hash}, \texttt{arr\_NN} and the parameter \texttt{optimize}.
The \texttt{sort\_x\_by\_NNdist\_to\_y} function is used together with \texttt{K=int(0.5*N)} 
in line \textcolor{gray}{80} to accelerate the speed of the algorithm, by processing only the top \texttt{K} observations in
 $S$ at each epoch, sorted by nearest distance to $T$. An \textcolor{index}{epoch}\index{epoch (neural networks)} (same meaning as in neural networks) is a full run of $S$. However here, only \texttt{K}
 observations out of \texttt{N} are processed during an epoch. Yet, due to re-sorting $S$ at the end of each epoch, the \texttt{K} observations change at each epoch. Thus over time, all observations are used. To not use this feature, set \texttt{K=N}.

The parameter \texttt{decay} in line \textcolor{gray}{151} offers a different acceleration mechanism. The default value is zero. A positive value provides a boost at the beginning, where it is most needed. A negative value always yields lower performance, yet 
 the resulting loss function goes up and down (rather than only down), in a way similar to 
\textcolor{index}{stochastic gradient  descent}\index{gradient descent!stochastic} in deep neural networks: there is no benefit to it, other than educational. 

Acceleration mechanisms offer a modest boost, about 10\% in my tests. But I haven't investigated them thoroughly. If you remove them, it will reduce the length of the code and make it easier to understand. There is also a significant amount of code to produce the visualizations and the video. If you remove this, the code will be significantly shorter.
The code is also on GitHub, \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/ann_fast.py}{here}, with a shorter version \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/ann.py}{here}.  
\vspace{1ex}





\begin{lstlisting}[numbers=left]
# Probabilistic ANN, can be used for clustering / classification

import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit
import matplotlib as mpl
from PIL import Image
import moviepy.video.io.ImageSequenceClip


#--- [1] Parameters and functions for visualizations

def save_image(fname,frame):

    # back-up function in case of problems with plt.savefig
    global fixedSize

    plt.savefig(fname, bbox_inches='tight')    
    # make sure each image has same size and size is multiple of 2
    # required to produce a viewable video   
    im = Image.open(fname)
    if frame == 0:  
        # fixedSize determined once for all in the first frame
        width, height = im.size
        width=2*int(width/2)
        height=2*int(height/2)
        fixedSize=(width,height)
    im = im.resize(fixedSize) 
    im.save(fname,"PNG")
    return()

def plot_frame():

    plt.scatter(x[:,0], x[:,1], color='red', s = 2.5) 
    z = []

    for k in range(N):

        neighbor = arr_NN[k]
        x_values = (x[k,0], y[neighbor,0]) 
        y_values = (x[k,1], y[neighbor,1]) 
        plt.plot(x_values,y_values,color='red',linewidth=0.1,marker=".",markersize=0.1) 
        z_obs = (y[neighbor,0], y[neighbor,1])
        z.append(z_obs)

    z = np.array(z)
    plt.scatter(y[:,0], y[:,1], s=10,  marker = '+', linewidths=0.5, color='green') 
    plt.scatter(z[:,0], z[:,1], s=10,  marker = '+', linewidths=0.5, color='blue')  
    return()

mpl.rcParams['axes.linewidth'] = 0.5
plt.rcParams['xtick.labelsize'] = 7
plt.rcParams['ytick.labelsize'] = 7


#--- [2] Create data, initial list of NN, and hash

def sort_x_by_NNdist_to_y(x, y, arr_NN):

    NNdist = {}
    x_tmp = np.copy(x)
    arr_NN_tmp = np.copy(arr_NN)
    for k in range(N):
        neighbor = arr_NN_tmp[k]
        NNdist[k] = np.sum(abs(x_tmp[k] - y[neighbor]))
    NNdist = dict(sorted(NNdist.items(), key=lambda item: item[1],reverse=True ))

    k = 0
    for key in NNdist:
        arr_NN[k] = arr_NN_tmp[key]
        x[k] = x_tmp[key]
        k += 1
    return(x, arr_NN)

seed = 57
np.random.seed(seed)
eps = 0.00000000001

N = 200            # number of points in x[]
K = int(0.5 * N)   # sort x[] by NN distance every K iterations
M = 200            # number of points in y[]

niter =  10000 
mean = [0, 0]
cov = [(0.1, 0),(0, 0.1)]
x = np.random.multivariate_normal(mean, cov, size=N)
y = np.random.multivariate_normal(mean, cov, size=M)
# y = np.copy(x) 
np.random.shuffle(x)
np.random.shuffle(y)

arr_NN = np.zeros(N)
arr_NN = arr_NN.astype(int)
hash = {}
sum_dist = 0

for k in range(N):

    # nearest neighbor to x[k] can't be identical to x[k]
    dist = 0

    while dist < eps:
       neighbor = int(np.random.randint(0, M))
       dist = np.sum(abs(x[k] - y[neighbor]))

    arr_NN[k] = neighbor
    sum_dist += np.sum(abs(x[k] - y[neighbor]))
    hash[k] = (-1,)

x, arr_NN = sort_x_by_NNdist_to_y(x, y, arr_NN)
low = sum_dist


#--- [3] Main part

mode     = 'minDist'  # options: 'minDist'  or 'maxDist'
optimize = 'speed'    # options: 'speed' or 'memory'
video    = False      # True if you want to produce a video
decay    = 0.0

history_val = []
history_arg = []
flist = []
swaps = 0
steps = 0
frame = 0

for iter in range(niter):

    k = iter % K 
    j = -1
    while j in hash[k] and len(hash[k]) <= N: 
        # if optimized for memory, there is always only one iter in this loop
        steps += 1
        j = np.random.randint(0, M) # potential new neighbor y[j], to x[k]

    if optimize == 'speed':
        hash[k] = (*hash[k], j) 

    if len(hash[k]) <= N:

        # if optimized for memory, then len(hash[k]) <= N, always
        old_neighbor = arr_NN[k]
        new_neighbor = j
        old_dist = np.sum(abs(x[k] - y[old_neighbor]))
        new_dist = np.sum(abs(x[k] - y[new_neighbor]))
        if mode == 'minDist':
            ratio = new_dist/(old_dist + eps)
        else:
            ratio = old_dist/(new_dist + eps)
        if ratio < 1-decay/np.log(2+iter) and new_dist > eps: 
            swaps += 1
            arr_NN[k] = new_neighbor
            sum_dist += new_dist - old_dist
            if sum_dist < low:
                low = sum_dist

            if video and swaps % 4 == 0:

                fname='ann_frame'+str(frame)+'.png'
                flist.append(fname)
                plot_frame() 

                # save image: width must be a multiple of 2 pixels, all with same size
                # use save_image(fname,frame) in case of problems with plt.savefig
                plt.savefig(fname, dpi = 200)
                plt.close() 
                frame += 1

    if iter % K == K-1:
        x, arr_NN = sort_x_by_NNdist_to_y(x, y, arr_NN)

    if iter % 100 == 0:
        print("%6d %6d %6d %8.4f %8.4f" 
                 % (iter, swaps, steps, low/N, sum_dist/N))
        history_val.append(sum_dist/N)
        history_arg.append(steps) # try replacing steps by iter 


history_val = np.array(history_val)
history_arg = np.array(history_arg)

if video:
    clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(flist, fps=6)
    clip.write_videofile('ann.mp4')


#--- [4] Visualizations (other than the video)

plot_frame()
plt.show()

#- curve fitting for average NN distance (Y-axis) over time (X-axis)

# works only with mode == 'minDist'

def objective(x, a, b, c):
    return(a + b*(x**c)) 

# ignore first offset iterations, where fitting is poor
offset = 5

x = history_arg[offset:]
y = history_val[offset:]

# param_bounds to set bounts on curve fitting parameters
if mode == 'minDist':
    param_bounds=([0,0,-1],[np.inf,np.infty,0])  
else: 
    param_bounds=([0,0,0],[np.inf,np.infty,1])  

param, cov = curve_fit(objective, x, y, bounds = param_bounds)   
a, b, c = param
# is c = -1/2 the theoretical value, assuming a = 0?
print("\n",a, b, c)  

y_fit = objective(x, a, b, c)
## plt.plot(x, y, linewidth=0.4)
plt.plot(history_arg, history_val, linewidth=0.4)
plt.plot(x, y_fit, linewidth=0.4)
plt.legend(['Avg NN distance','Curve fitting'],fontsize = 7)
plt.show()
\end{lstlisting}



%------------------------------

\section{Building and evaluating a taxonomy-enriched LLM}\label{sec822}

Section~\ref{sxllm5} and appendix~\ref{aasdaaqw} discuss \textcolor{index}{xLLM}\index{xLLM}, my multi-LLM architecture to deal with knowledge retrieval, referencing,  and summarization. It heavily relies on structures found in the corpus,
 such as taxonomies. I tested it on the Wolfram website, to create a specialized sub-LLM for the ``Probability \& Statistics" section. The  root directory is accessible  
\href{https://mathworld.wolfram.com/topics/ProbabilityandStatistics.html}{here} on Wolfram. This is the starting point for crawling all the content. 
Note that the strength of xLLM comes from using structured text such as taxonomies, consisting of concise, high-value text elements, 
 superior to standard embeddings and tokens based on raw content only.


\begin{table}[H]
\small
\[
\begin{tabular}{|l|lll|}
\hline
 Wolfram top categories &  \multicolumn{3}{c|}{Top categories based on parsed content}  \\
\hline
\hline
Bayesian Analysis	&	$L_1$ Methods	&	Proportions	&	Rank Statistics	\\
Descriptive Statistics  	&	Analysis of Variance	&	Gaming Theory	&	Standardization	\\
Error Analysis  	&	Bias	&	Indices	&	Sums of Squares	\\
Estimators 	&	Bivariate Methods	&	Least Squares	&	Sampling Methods	\\
Markov Processes 	&	Central Limit Theorem	&	Linear Algebra	&	Small Data	\\
Moments	&	Central Tendency Metrics	&	Markov Chains	&	Statistical Laws	\\
Multivariate Statistics 	&	Characteristic Function	&	Maximum Likehood	&	Statistical Moments	\\
Nonparametric Statistics  	&	Conditional Probas, Bayes	&	Measure Theory	&	Statistical Tests	\\
Probability	&	Confidence Intervals	&	Measures of Spread	&	Stepwise Methods	\\
Random Numbers	&	Continuous Distributions	&	Model Fitting	&	Tabular Data	\\
Random Walks  	&	Correlations Analysis	&	Multivariate Methods	&	Tests of Hypothesis	\\
Rank Statistics  	&	Curve Fitting	&	Normal Distribution	&	Time Series	\\
Regression  	&	Data	&	Optimization	&	Trials	\\
Runs	&	Degrees of Freedom	&	Outliers	&	Plots	\\
Statistical Asymptotic  	&	Density Functions	&	Parametric Estimation	& Regression		\\
Statistical Distributions	&	Descriptive Statistics	&	Point Estimation	&		\\
Statistical Indices 	&	Discrete Distributions	&	Poisson Processes	&		\\
Statistical Plots  	&	Error Analysis	&	Probability Theory	&		\\
Statistical Tests  	&	Estimation Theory	&	Quantiles \& Simulation	&		\\
Time-Series Analysis  	&	Exponential Family	&	Random Numbers	&		\\
Trials	&	Extreme Value Theory	&	Random Walks	&		\\

\hline
\end{tabular}
\]
\caption{\label{tabtax1} Top categories, Wolfram taxonomy (leftmost column) vs content-based}
\end{table}

Words found in high-quality taxonomies, glossaries, titles, synonyms dictionaries or indexes, are well-formed and important. They should be assigned higher weights than words found in ordinary content, and contribute to the quality and conciseness of the output returned to user prompts. The quality of the input sources is also very important. Here the goal is to build a faithful 
\textcolor{index}{taxonomy}\index{taxonomy creation}, one that
 matches the crawled data. In the case of Wolfram, the content has a strong emphasis on mathematical statistics. To build a specialized 
\textcolor{index}{LLM}\index{large language models (LLM)} more focused on 
 applied statistics, you need content augmentation with (say) Wikipedia, or design a separate LLM based on different sources. Here I stick with the 
 original Wolfram content. To avoid hallucinations, the resulting sub-LLM provides answers only if the relevant information exists in the specialized corpus. 



\begin{table}[H]
\small
\[
\begin{tabular}{|l|l|l|}
\hline
  \multicolumn{3}{|c|}{``Sampling Methods" sub-categories}  \\
\hline
\hline
Aggregated, Grouped Data	&	Estimation	&	Statistical Summaries		\\
Bernoulli Trials	&	Large, Small Samples	&	Sample Types		\\
Central Limit Theorem	&	Moments	&	Random Sampling		\\
Combinatorics	&	Normalization	&	Sampling Error		\\
Confidence Intervals	&	Order Statistics, Ranks	&	Variance Sources		\\
Data Gathering Process	&	Population	&	Time Series Sampling		\\
Empirical Distribution	&	Sample Bias	&	Observations	 Types	\\
Equal / Unequal Samples	&	Sample Size	&	Outliers		\\
\hline
\end{tabular}
\]
\caption{\label{tabtax1sub} ``Sampling Methods" sub-categories based on parsed content}
\end{table}

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\fbox{\includegraphics[width=0.96\textwidth]{detectedCategories.png}}  
\caption{Wolfram true vs re-assigned categories based on content (extract from full table)}
\label{fig:lot89c29}
\end{figure}
%imgpy9979_2and3.PNG
%------------------------- 

\noindent The Wolfram ``Probability \& Statistics" ontology has about 600 categories, subcategories and so on. Here we want to accomplish three goals:
\vspace{1ex}
\begin{itemize}
\item Pretend that we crawled the Wolfram website but did not retrieve the taxonomy. Build one from scratch, based on the crawled content.
Again, here we are only interested in the ``Probability \& Statistics" section. 
\item Pretend that we have some external taxonomy that covers the Wolfram crawl. For instance, the 
machine learning taxonomy from Wikipedia (see \href{https://en.wikipedia.org/wiki/Category:Machine_learning}{here}). Use that taxonomy to categorize each 
 Wolfram webpage and top keyword (consisting of multiple tokens) found in the crawled content. Of course, not all keywords can be uniquely categorized. 
\item Pretend that the external taxonomy is actually that from Wolfram, but we are not aware of it. Use that taxonomy as it if was an external source, to categorize 
 each Wolfram webpage. Then compare with the actual category assigned by Wolfram.  Then compare the two taxonomies: original versus reconstructed. 
This last step allows you to evaluate the quality of your LLM. 
\end{itemize}

\subsection{Project and solution}

%mention the code / excel spreadsheet / semi manual

The goal is not to write code from scratch to solve the problem, but to look at my implementation in section~\ref{pyucd}, understand the different tables and components,
 test it, and see how it works. It is a bonus if you can improve my algorithms. 



My code is significantly shorter than it appears at first glance. First, lines \textcolor{gray}{1--105} reads all the tables produced by 
 \texttt{xLLM6.py}, described in appendix~\ref{xllm6tr} and in the solution to Step 1 in project~\ref{beurreblanc}. You won't need all~of them, but they are there for convenience and
 consistency with the main xLLM architecture.  Then, lines \textcolor{gray}{240--352} allows you to understand and play with some of the new tables built to facilitate 
 the creation of the taxonomy.  The core of the code is split in two sections. First, lines  \textcolor{gray}{108--237} to create the new tables needed to create a taxonomy from scratch: see description in the 
 comments in lines  \textcolor{gray}{116-124}. Once produced, you can load them in memory next time you run the code, to boost speed. 
Then, lines \textcolor{gray}{355--432} deals with integrating an external taxonomy into xLLM. All these tables are also on GitHub, 
\href{https://github.com/VincentGranville/Large-Language-Models/tree/main/xllm6/build-taxonomy}{here}. 
The code also requires the \texttt{xllm6\_util.py} library, available \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm6/xllm6_util.py}{here}. 
\vspace{1ex}
The project consists of the following steps:

\begin{itemize}
\item[] {\bf Step 1:  Build taxonomy from scratch}. Using only the \texttt{dictionary} table in the code in section~\ref{pyucd},
 see how I build the \texttt{topWords} table, and then \texttt{connectedTopWords}, in lines \textcolor{gray}{126--168}.
 How can you use \texttt{topWords} to create categories, and \texttt{connectedTopWords} to create subcategories? The solution requires
 human intelligence to assign a meaningful label or name to each category and subcategory, and to discard non-valid entries. Thus, the process is 
semi-automated.  

\noindent Do not use the Wolfram category. That is, do no use the \texttt{hash\_category} table featuring the Wolfram taxonomy. The goal here is to create an alternate taxonomy from scratch.
See what I came up with in Tables~\ref{tabtax1} and~\ref{tabtax1sub}. Your solution will be different.
 \vspace{1ex}
\item[] {\bf Step 2:  Leverage external taxonomy}. In line \textcolor{gray}{402}, I retrieve a list of categories from \texttt{hash\_category}. 
While these are actually Wolfram categories, let's pretend that they are external. Now,  in lines \textcolor{gray}{407--427}, see how I assign a category from that list, to
 each word found in \texttt{dictionary}. For that purpose, I use a similarity metric  that compares two text elements: a dictionary word on one side, and a category entry on the other 
side; see lines \textcolor{gray}{380--400}.
 Try with different similarity metrics, or allow for multiple categories per word.   
My results are stored in the \texttt{assignedCategories} table, available 
\href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm6/build-taxonomy/xllm6_assignedCategories.txt}{here}. 
The relevancy score is included. Note that some words are uncategorized.
\vspace{1ex}

\item[] {\bf Step 3:  LLM evaluation}. Assign a category to each crawled webpage using \texttt{assignedCategories} built in \textcolor{red}{Step 2}. This table
 available 
\href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm6/build-taxonomy/xllm6_assignedCategories.txt}{here}
plays the role of an ontology based on the crawled content augmented with the Wolfram taxonomy, 
while ignoring how categories are assigned to webpages by Wolfram. 
Then, look at the Wolfram categories assigned to webpages, 
 using \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/list_final_URLs_stats.txt}{this table}. Note that in this table, each URL is 
 linked to two categories:  the actual category on the left in the parentheses, and the parent category on the right (one level above). In the rare instances where 
a category has multiple parent categories, only one is kept. Each webpage is assigned to just one category.   

\noindent You will need: the \texttt{arr\_url} table (\href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm6/xllm6_arr_url.txt}{here}) mapping URL IDs to URLs, and also the \texttt{url\_map} table 
(\href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm6/xllm6_url_map.txt}{here}) indexed by \texttt{dictionary} words. 
The latter helps you retrieve the list of webpages (URL IDs) containing any word in the dictionary. By inverting it, you can retrieve, for each URL, all the 
 dictionary words that it contains. Note that the 
Wolfram category table (\href{https://github.com/VincentGranville/Large-Language-Models/blob/main/list_final_URLs_stats.txt}{here}) is indexed by URL, while
\texttt{assignedCategories} 
is  indexed by words from the \texttt{dictionary} table (both use the same ``word" index). To evaluate the~LLM, for each webpage, compare the two 
category assignments: one coming from 
\href{https://github.com/VincentGranville/Large-Language-Models/blob/main/list_final_URLs_stats.txt}{here} versus the other one derived from \texttt{assignedCategories}. The former is supposed to be the correct one. Quantify the~amount of mismatch. 

\noindent In the end,  content-based categories found in \texttt{assignedCategories} also come from Wolfram (indirectly as if using an external taxonomy). But they may be assigned to webpages differently, compared to the real Wolfram taxonomy. The fewer mismatches
 between the two assignments (across all the webpages),  the better your algorithm. In short, this step reallocates Wolfram categories to webpages, not knowing 
how Wolfram does the allocation.
\vspace{1ex}
\item[] {\bf Step 4: Taxonomy augmentation}. How would you create an augmented taxonomy, by blending two different ones? For instance, the Wolfram taxonomy with
 the one obtained in \textcolor{red}{Step 1}.
\vspace{1ex}
\end{itemize}

\noindent In addition to the code in section~\ref{pyucd}, the Excel spreadsheet \texttt{Wolfram-stats2.xlsx}, available 
\href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm6/build-taxonomy/Wolfram-stats2.xlsx}{here}, illustrates some of the steps. 
The \texttt{TopWords} and \texttt{TopWord Pairs} tabs are related to \textcolor{red}{Step 1}. Then, I also use~a~list of words to ignore when
 building the fundamental \texttt{TopWords} table: see the man-made \texttt{ignoredWords} list in lines \textcolor{gray}{110--112} in the Python code.

To answer \textcolor{red}{Step 2} and \textcolor{red}{Step 3}, I produced a separate piece of code \texttt{reallocate.py}, available on GitHub, 
\href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm6/build-taxonomy/reallocate.py}{here}. 
Figure~\ref{fig:lot89c29} shows an extract from the output. The full table \texttt{detectedCategories.txt} is on GitHub,
\href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm6/build-taxonomy/detectedCategories.txt}{here}.
In essence, this Python script uses the Wolfram taxonomy as external data to the crawled content, and assigns a category to each URL based on words
 found on the webpage in question. 

Actually, for each page, the script assigns multiple categories, each with its relevancy score. In the end,
the candidate category with the highest score is selected as the target category. Two different scoring mechanisms are offered, 
 determined by the tuning parameter \texttt{mode}. The best fit is obtained with \texttt{mode=\textquotesingle depth\textquotesingle}, favoring deeper over general sub-categories 
 to better mimic the way Wolfram assigns categories to pages.  Even then, only 141 URLs are perfectly
 matched to their true Wolfram category. However, mismatch does not mean error: it means a close but not perfect match. Indeed, the
 reconstructed taxonomy may be superior to the true one, in the sense that the Wolfram page categorization is too granular.
It also illustrates how evaluation metrics are subject to argumentation.

A better evaluation metric would take into account the distance or similarity between the actual Wolfram category, and the reconstructed one. If the
 reconstructed category is the parent (one level above) of the Wolfram category, the penalty for not having a perfect match, should be small. 

 




% title: knowledge organization with RAG, taxonomy-based LLMs, and model evaluation




% evaluation: faifthfulness to content [wolfram] or to the topic [proba / stats]
% boost with wilipedia taxonomy / my glossary, indexes [bigger weights given to these entries]
% improve results by using a real dictionary: words found in crawled page, also in real dictionary, get double weight
% use synonyms for linkage

\subsection{Python code}\label{pyucd}

The code is also on GitHub, \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm6/build-taxonomy/taxonomy.py}{here}.
\vspace{1ex}

\begin{lstlisting}[numbers=left]
# taxonomy.py: vincentg@mltechniques.com

import requests

# Unlike xllm6.py, xllm6_short.py does not process the (huge) crawled data.
# Instead, it uses the much smaller summary tables produced by xllm6.py


#--- [1] get tables if not present already

# First, get xllm6_util.py from GitHub and save it locally as xllm6_util.py
#     note: this python code does that automatically for  you
# Then import everything from that library with 'from xllm6_util import *'
# Now you can call the read_xxx() functions from that library
# In addition, the tables stopwords and utf_map are also loaded
#
# Notes:
#    - On first use, dowload all locally with overwrite = True
#    - On next uses, please use local copies: set overwrite = False 

# Table description: 
#
# unless otherwise specified, a word consists of 1, 2, 3, or 4 tokens
# word_pairs is used in xllm6.py, not in xllm6_short.py
#
# dictionary = {}      words with counts: core (central) table
# word_pairs = {}      pairs of 1-token words found in same word, with count
# word2_pairs = {}     pairs of multi-token words found on same URL, with count 
# url_map = {}         URL IDs attached to words in dictionary
# arr_url = []         maps URL IDs to URLs (one-to-one)
# hash_category = {}   categories attached to a word
# hash_related = {}    related topics attached to a word
# hash_see = {}        topics from "see also" section, attached to word
# ngrams_table = {}    ngrams of word found when crawling
# compressed_ngrams_table = {}     only keep ngram with highest count
# utf_map = {}         map accented characters to non-accented version
# stopwords = ()       words (1 or more tokens) not accepted in dictionary
# word_hash = {}       list of 1-token words associated to a 1-token word 
# word2_hash = {}      list of multi-token words associated to a multi-token word 
# compressed_word2_hash = {}      shorter version of word2_hash 
# embeddings = {}      key is a 1-token word; value is hash of 1-token:weight
# embeddings2 = {}     key is a word; value is hash of word:weight
  

path = "https://raw.githubusercontent.com/VincentGranville/Large-Language-Models/main/xllm6/"

overwrite = False   # if True, get tables from GitHub, otherwise use local copy

if overwrite:

    response = requests.get(path + "xllm6_util.py")
    python_code = response.text

    local_copy = "xllm6_util"
    file = open(local_copy + ".py", "w")
    file.write(python_code)
    file.close()

    # get local copy of tables

    files = [ 'xllm6_arr_url.txt', 
              'xllm6_compressed_ngrams_table.txt',
              'xllm6_compressed_word2_hash.txt',
              'xllm6_dictionary.txt',
              'xllm6_embeddings.txt',
              'xllm6_embeddings2.txt',
              'xllm6_hash_related.txt',
              'xllm6_hash_category.txt',
              'xllm6_hash_see.txt',
              'xllm6_url_map.txt',
              'xllm6_word2_pairs',
              'stopwords.txt'
            ]

    for name in files:
        response = requests.get(path + name)
        content = response.text
        file = open(name, "w")
        file.write(content)
        file.close()  

import xllm6_util as llm6

# if path argument absent in read_xxx(), read from GitHub
# otherwise, read from copy found in path

arr_url       = llm6.read_arr_url("xllm6_arr_url.txt",       path="")
dictionary    = llm6.read_dictionary("xllm6_dictionary.txt", path="")
stopwords     = llm6.read_stopwords("stopwords.txt",         path="")

compressed_ngrams_table = llm6.read_table("xllm6_compressed_ngrams_table.txt", 
                                                           type="list", path="")
compressed_word2_hash   = llm6.read_table("xllm6_compressed_word2_hash.txt", 
                                                           type="hash", path="")
embeddings    = llm6.read_table("xllm6_embeddings.txt",    type="hash", path="", 
                                                                 format="float") 
embeddings2   = llm6.read_table("xllm6_embeddings2.txt",   type="hash", path="", 
                                                                 format="float") 
hash_related  = llm6.read_table("xllm6_hash_related.txt",  type="hash", path="")
hash_see      = llm6.read_table("xllm6_hash_see.txt",      type="hash", path="")
hash_category = llm6.read_table("xllm6_hash_category.txt", type="hash", path="")
url_map       = llm6.read_table("xllm6_url_map.txt",       type="hash", path="")
word2_pairs   = llm6.read_table("xllm6_word2_pairs.txt",   type="list", path="")


#--- [2] Create/save taxonomy tables if overwrite = True, otherwise read them

from collections import OrderedDict

ignoreWords = { "term", "th", "form", "term", "two", "number", "meaning", "normally", 
                "summarizes", "assumed", "assumes", "p", "s", "et", "possible", 
                "&#9671;", ";", "denoted", "denotes", "computed", "other"}

def create_taxonomy_tables(threshold, thresh2, ignoreWords, dictionary): 

    topWords = {}            # words with highest counts, from dictionary
    wordGroups = {}          # hash of hash: key = topWord; value = hash of words 
                             #        containing topWord (can be empty)
    connectedTopWords = {}   # key = (wordA, wordB) where wordA and wordB contains 
                             #         a topWord; value = occurrences count
    smallDictionary = {}     # dictionary entries (words) containing a topWord
    connectedByTopWord = {}  # same as connectedTopWords, but in flattened hash format; 
                             #         key = topWord
    missingConnections = {}  # if this table is not empty, reduce threshold and/or thresh2

    for word in dictionary:
        n = dictionary[word]     # word count 
        tokens = word.count('~')
        if n > threshold and word not in ignoreWords:    # or tokens > 1 and n > 1: 
            topWords[word] = n  

    for topWord in topWords:
        n1 = dictionary[topWord] 
        hash = {}
        for word in dictionary:
            n2 = dictionary[word]
            if topWord in word and n2 > thresh2 and word != topWord: 
                hash[word] = n2
        if hash:
            hash = dict(sorted(hash.items(), key=lambda item: item[1], reverse=True))
        else: 
            missingConnections[topWord] = 1
        wordGroups[topWord] = hash  

    for topWord in topWords:
        for word in dictionary:
            if topWord in word:
                smallDictionary[word] = dictionary[word]

    counter = 0    
    for topWordA in topWords:
        if counter % 10 == 0:
            print("Create connectedTopWords: ", counter, "/", len(topWords))
        counter += 1
        hash = {}
        for topWordB in topWords:
            key = (topWordA, topWordB)
            if topWordA != topWordB:
                connectedTopWords[key] = 0
                for word in smallDictionary:
                    if topWordA in word and topWordB in word:
                        connectedTopWords[key] += 1
                        if topWordB in hash:
                            hash[topWordB] += 1
                        else:
                            hash[topWordB] = 1
        hash = dict(sorted(hash.items(), key=lambda item: item[1], reverse=True))
        connectedByTopWord[topWordA] = hash

    taxonomy_tables = [topWords, wordGroups, connectedTopWords,
                       smallDictionary, connectedByTopWord, missingConnections]
    return(taxonomy_tables)


def save_taxonomy_tables(): 
   
    list = { "topWords" : topWords,
             "wordGroups" : wordGroups,
             "smallDictionary" : smallDictionary,
             "connectedByTopWord" : connectedByTopWord, 
             "missingConnections" : missingConnections,
           }

    for table_name in list:
        file = open("xllm6_" + table_name + ".txt", "w")
        table = list[table_name]
        for word in table:
            file.write(word + "\t" + str(table[word]) + "\n")
        file.close()

    file = open("xllm6_connectedTopWords.txt", "w") 
    for key in connectedTopWords:
        file.write(str(key) + "\t" + str(connectedTopWords[key]) + "\n")
    file.close()

    return()


#--- Get taxonomy tables 

build_taxonomy_tables = True  # if True, create and save these tables locally (slow)

if build_taxonomy_tables:
 
    threshold = 30     # minimum word count to qualify as topWord 
    thresh2   = 2      # another word count threshold             

    taxonomy_tables    = create_taxonomy_tables(threshold, thresh2, ignoreWords, dictionary)
    topWords           = taxonomy_tables[0] 
    wordGroups         = taxonomy_tables[1] 
    connectedTopWords  = taxonomy_tables[2] 
    smallDictionary    = taxonomy_tables[3] 
    connectedByTopWord = taxonomy_tables[4] 
    missingConnections = taxonomy_tables[5]

    connectedTopWords  = dict(sorted(connectedTopWords.items(), 
                                     key=lambda item: item[1], reverse=True))

    save_taxonomy_tables()
    for topWord in missingConnections:
        print(topWord)
    print()

else:

    smallDictionary     = llm6.read_dictionary("xllm6_smallDictionary.txt", path="")
    topWords            = llm6.read_dictionary("xllm6_topWords.txt", path="")
    wordGroups          = llm6.read_table("xllm6_wordGroups.txt", type="hash", path="")
    connectedByTopWord  = llm6.read_table("xllm6_connectedByTopWord.txt", type="hash", path="")
    
    connectedTopWords = {}
    data = llm6.get_data("xllm6_connectedTopWords.txt", path="")
    for line in data:
        line = line.split('\t')
        count = int(line[1])
        key = llm6.text_to_list(line[0])
        connectedTopWords[key] = count


#--- [3] Play with taxonomy tables to get insights and improve them

topWords = dict(sorted(topWords.items(), key=lambda item: item[0]))

def show_menu(n, dict_mode):

    # option 'o' useful to check if topWordA, topWordB are connected or not
    # option 'c' shows all topWordB connected to topWordA = topWord

    print("Command line menu: \n") 
    print("<Enter>                 - exit")
    print("h                       - help: show menu options")
    print("a                       - show all top words")
    print("ds                      - select short dictionary")
    print("df                      - select full dictionary")
    print("n integer               - display entries with count >= integer")
    print("f string                - find string in dictionary")
    print("g topWord               - print groupWords[topWord]")
    print("c topWord               - print connectedByTopWord[topWord]")
    print("l topWordA topWordB     - (topWordA, topWordB) connections count\n")
    print("current settings: n = %3d, dictionary = %s" %(n, dict_mode))
    print()
    return()

topWords = dict(sorted(topWords.items(), key=lambda item: item[0]))

dict_mode = 'short'
dict  = smallDictionary
query = "o"
n = 0   # return entries with count >= n
show_menu(n, dict_mode)

while query != "":  

    query    = input("Enter command, ex: <c hypothesis> [h for help]: ") 
    queries  = query.split(' ')
    action   = queries[0]
    if len(queries) > 2:
        queries[1] = queries[1] + " " + queries[2]

    if action == 'h':
        show_menu(n, dict_mode)

    elif action == 'ds':
        dict = smallDictionary
        dict_mode = 'short'

    elif action == 'df':
        dict = dictionary
        dict_mode = 'full'

    elif action == 'a':
        for topWord in topWords:
            count = topWords[topWord]
            print(count, topWord)
        print()

    elif action in ('f', 'g', 'c', 'l', 'n') and len(queries) > 1:
        string = queries[1]  

        if action == 'n':
            n = int(string)

        elif action == 'f':
            for word in dict:
                count = dict[word]
                if string in word and count >= n:
                    print(count, string, word)
            print()

        elif action == 'g':
            topWord = string
            if topWord in wordGroups:
                hash = wordGroups[topWord]
                countA = dictionary[topWord]
                for word in hash:
                    countB  = dictionary[word]
                    if countB >= n:
                        print(countA, countB, topWord, word) 
            else:
                print("topWord not in wordGroups")
            print()

        elif action == 'c':
            topWord = string
            if topWord in connectedByTopWord:
                hash = connectedByTopWord[topWord]
                countA = dictionary[topWord]
                for word in hash:
                    countB = dictionary[word]
                    countAB = hash[word]
                    if countAB >= n:
                        print(countA, countB, countAB, topWord, word) 
            else:
                print("topWord not in wordGroups")
            print()

        elif action == 'l':
            astring = string.split(' ')
            if len(astring) == 1:
                print("needs 2 topWords, space-separated")
            else: 
                key = (astring[0], astring[1])
                if key in connectedTopWords:
                    count = connectedTopWords[key]
                else:
                    count = 0
                print(count, key) 

    elif action != '':
        print("Missing arguments")

print()


#--- [4] Build local taxomomy using external taxonomy

## extract categories from external category table... 
## assign category to sample page based on words in page
## stem/plural 

def get_external_taxonomy(hash_category):

    categories = {}
    parent_categories = {}

    for word in hash_category:
        for category_item in hash_category[word]:
            category_item = category_item.lower()
            category_item = category_item.replace('  ',' ').split(' | ')
            category1 = category_item[0].replace(' ', '~')
            category2 = category_item[1].replace(' ', '~')
            level1 = int(category_item[2])
            level2 = level1 - 1
            categories[category1] = level1
            categories[category2] = level2
            parent_categories[category1] = category2
    return(categories, parent_categories)


def compute_similarity(dictionary, word, category):

    tokensA = word.split("~")
    tokensB = category.split("~")
    normA = 0
    normB = 0
    for tokenA in tokensA:
        if tokenA in dictionary:
            normA += dictionary[tokenA]**0.50
    for tokenB in tokensB:
        if tokenB in dictionary:
            normB += dictionary[tokenB]**0.50

    similarity = 0
    for tokenA in tokensA:
        for tokenB in tokensB:
            if tokenA == tokenB and tokenA in dictionary and tokenB in dictionary:
                weight = dictionary[tokenA]
                similarity += weight**0.50
    similarity /= max(normA, normB) 
    return(similarity)

categories, parent_categories = get_external_taxonomy(hash_category)


#--- Main loop

assignedCategories = {}
counter = 0

print("Assign categories to dictionary words\n")

for word in dictionary:
    max_similarity = 0
    max_depth = 0
    NN_category = ""
    for category in categories: 
        depth = categories[category]
        similarity = compute_similarity(dictionary, word, category)
        if similarity > max_similarity:
            max_similarity = similarity
            max_depth = depth
            NN_category = category
    assignedCategories[word] = (NN_category, max_depth, max_similarity)
    if counter % 200 == 0:
        print("%5d / %5d: %d %4.2f %s | %s" 
          %(counter, len(dictionary), max_depth, max_similarity, word, NN_category)) 
    counter += 1

OUT = open("xllm6_assignedCategories.txt", "w")
for word in assignedCategories:
    OUT.write(word+"\t"+str(assignedCategories[word])+"\n")
OUT.close()


\end{lstlisting}

\section{Predicting article performance and clustering using LLMs}\label{predxllm}

The dataset consists of 4000 articles published between 2015 and 2020, on Data Science Central. For each article, the following information is available: 
title, publication date, author, URL, type of article (forum question or blog post), and the number of pageviews measured at the end of the time period in question. The goal
 is to identify patterns that lead to unusual pageview numbers, to automatically suggest great titles to contributing authors. The dataset does not contain the full 
articles, only the titles. It would be useful to assess conversion rates (new subscribers) or sales based on the wording found in the subject and content, and the match between both, to maximize pageviews and conversions simultaneously. 

Nevertheless, titles alone lead to very interesting results.
 I also use efficient techniques to predict pageview numbers given the title and category, and to cluster high performing articles  into meaningful overlapping groups.  
A category is defined as a combination of attributes: author if among the top 10, blog or forum question, and URL pointing either to Data Science Central or a satellite channel. The dataset is on GitHub, \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/Articles-Pageviews.txt}{here}.

The methodology relies heavily on text processing and may be extended to much larger datasets. In this case, to boost efficiency, I recommend splitting the data into
 multiple subsets treated separately: a subset may correspond to a specific channel. This approach is similar to the multi-LLM architecture discussed 
 in section~\ref{sec822}. 

\noindent The main concepts and notations are as follows:
\vspace{1ex}
\begin{itemize}
\item Instead of vector or graph databases, tables are structured as \textcolor{index}{nested hashes}\index{hash table!nested hashes}. These are
 key-value tables (dictionary in Python), where the value is also a hash. For instance, the 
\textcolor{index}{distance matrix}\index{distance matrix} used in standard clustering algorithms 
 is a nested hash transformed into a matrix. See lines \textcolor{gray}{314--330} in the code in section~\ref{vinigold}. Another useful operation is 
\textcolor{index}{hash inversion}\index{hash table!inversion}, 
see lines \textcolor{gray}{409--420} in the code.
Nested hashes are very efficient to represent sparse data; they can be compared to matrices where both rows and columns have a variable number of elements. 
Hash inversion is similar to matrix transposition. 
\vspace{1ex}

\item Tokens are replaced by \textcolor{index}{multi-tokens}\index{token!multi-token}~\cite{mtokens}, 
referred to as ``words" or 
\textcolor{index}{contextual tokens}\index{token!contextual}. For instance, `data', `science', 
 `data$\sim$science' and `data$^\wedge$science' are multi-tokens. Of course, the first two are also single tokens. In `data$^\wedge$science', the tokens `data' and `science' are not adjacent in the title; I use it to leverage large context, for titles that contain both tokens in arbitrary locations other than adjacent. But `data$\sim$science'  corresponds to the classical word
 with adjacent tokens.
\vspace{1ex}

\item The pageview function is denoted as \texttt{pv}. At the basic level, $\texttt{pv}(A)$ is the pageview number of article $A$, based on its title and categorization. 
It must be normalized, taking the logarithm: see lines \textcolor{gray}{122--123} in the code. Then, the most recent articles have a lower 
\texttt{pv} because they have not accumulated much traffic yet. To correct for this, see lines \textcolor{gray}{127--136} in the code.  From now on,
 \texttt{pv} refers to normalized pageview counts also adjusted for time. The pageview for a multi-token $t$ is then defined as
\begin{equation}
\texttt{pv}(t) = \frac{1}{|S(t)|}\cdot \sum_{A\in S(t)} \texttt{pv}(A), \label{mur28u}
\end{equation}
where $S(t)$ is the set of all article titles containing $t$, and $|\cdot|$ is the function that counts the number of elements in a set. Sometimes, two different
 tokens $t_1, t_2$ have $S(t_1)=S(t_2)$. In this case, to reduce the number of tokens, I only keep the longest one. This is done
 in lines \textcolor{gray}{193--206} in the code. 
\vspace{1ex}

\item Likewise, you can define $\texttt{pv}(C)$, the pageview count attached to a category $C$, by averaging \texttt{pv}'s over all articles assigned to that category. 
Finally, $T(A)$ denotes the set of multi-tokens attached to an article $A$. 
\end{itemize}
\vspace{1ex}

\noindent With the notations and terminology introduced so far, it is very easy to explain how to predict the pageview count 
$\texttt{pv}_0(A)$ for an article $A$ inside or outside the training set. The formula is
\begin{equation}
\texttt{pv}_0(A) =  \frac{1}{W_A}\cdot \sum_{t\in T(A)} w_t \cdot \texttt{pv}(t), \label{poxewdyt}
\end{equation}
with:
$$
W_A  = \sum_{t\in T(A)} w_t, \quad
w_t  = 0 \text{ if } |S(t)| \leq \alpha, \quad
w_t  = \frac{1}{|S(t)|^\beta} \text{ if } |S(t)| > \alpha.  
$$
Here $\alpha, \beta > 0$ are parameters. I use $\alpha = 1$ and $\beta = 2$.  The algorithm puts more weights on rare tokens, but a large value of
 $\beta$ or a small value of $\alpha$ leads to \textcolor{index}{overfitting}\index{overfitting}. Also, I use the notation
 $\texttt{pv}_0$ for an estimated value or prediction, and $\texttt{pv}$ for an observed value.  In some cases, $T(A)$ is empty and thus
 Formula~(\ref{poxewdyt}) is meaningless. The solution consists in replacing the predicted value by $\texttt{pv}_0(A) = \texttt{pv}(C_A)$, where
 $C_A$ is the category attached to article $A$. 

The prediction formula~(\ref{poxewdyt})  looks like a regression, but with no regression coefficients to estimate. Also, the dimension of 
 the feature vector is variable: it depends on $A$. Despite not being a  minimization problem, thus the absence of loss function, gradient descent, or neural networks, the predictions are remarkably accurate, with a fairly small bias. It illustrates the power of \textcolor{index}{explainable AI}. You can further improve
 the results using the recalibration technique in lines \textcolor{gray}{459--475} in the code. It now involves one parameter, and a \textcolor{index}{loss function}\index{loss function} that is a fast
 approximation to  the \textcolor{index}{Kolmogorov-Smirnov distance}\index{Kolmogorov-Smirnov distance} between 
two \textcolor{index}{empirical distributions}\index{empirical distribution} (ECDF): one based on observed \texttt{pv}, and the other one on predicted \texttt{pv}. 

I now describe the unsupervised clustering procedure, used to detect groups of articles that perform well.  First, you define a 
\textcolor{index}{similarity metric}\index{similarity metric} 
 $s(t_1, t_2)$ between
 two multi-tokens $t_1, t_2$. You use one of the clustering methods to group the multi-tokens into clusters, based on the similarity. Then, for each 
multi-token group $G$, you retrieve the list  $L(G)$ of articles belonging to $G$ with the formula
\begin{equation}
L(G) = \bigcup_{t\in G} S(t). \label{prozac4}
\end{equation}
The similarity metric is stored in the \texttt{hash\_pairs} table in the code, indexed by multi-token pairs: see lines \textcolor{gray}{276--301}. The hash structure is far more efficient than a matrix
 because most pairs $\{t_1, t_2\}$ have $s(t_1, t_2)=0$ and are not even stored in the hash. But standard clustering procedures in Python libraries use a
 distance matrix instead. I turned the similarity hash into a distance matrix with the formula $d(t_1, t_2) = 1- s(t_1, t_2)$. The resulting matrix 
is big and extremely sparse, resulting in memory problems in some cases. The workaround is to use a different clustering technique,
 such as my very fast \textcolor{index}{connected components}\index{connected components} algorithm (see section~\ref{pureas}), designed to work with hash tables rather than matrices. The similarity metric is defined as
\begin{equation}
s(t_1, t_2) = \frac{|S(t_1) \cap S(t_2)|}{|S(t_1) \cup S(t_2)|}  \in [0, 1].
\end{equation}
Note that the multi-token groups are not overlapping, but the article groups are. Finally, I tried two clustering methods: 
 \textcolor{index}{hierarchical clustering}\index{hierarchical clustering} (also called agglomerative) from the \textcolor{index}{Sklearn}\index{Python library!Sklearn} library,
 and \textcolor{index}{$k$-medoids}\index{$k$-medoids clustering}
 [\href{https://en.wikipedia.org/wiki/K-medoids}{Wiki}] from the
\textcolor{index}{Sklearn\_extra}\index{Python library!Sklearn\_extra} library. 
The latter is a variant of \textcolor{index}{$k$-means }\index{$k$-means clustering} that works with any distance matrix. The detected clusters for both methods are on
 GitHub, respectively \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/nlp_scoring_clusters_hierarchical.txt}{here} 
 and \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/nlp_scoring_clusters_medoids.txt}{here}. 

\subsection{Project and solution} 

The goal is not to write code from scratch to solve the problems discussed earlier. Instead, I invite you to read my code, understand it, and run it. Then identify and play with the parameters, the impact they have on various components, and avoid overfitting. In the end, you should be able to improve some of the algorithms, especially 
 by replacing Python libraries used for clustering, with methods that can handle sparse graphs efficiently, such as connected components. 
Other key points to consider: testing different loss functions, automating multi-feature category detection,  fine-tuning, sample size determination, confidence intervals for 
predicted \texttt{pv}, 
and trying other similarity metrics, tokens reduction and time-adjustment techniques. 
\vspace{1ex}

\noindent The project consists of the following steps:
\vspace{1ex}

\begin{itemize}
\item[] {\bf Step 1:  High performance keywords}. Download and explore the input dataset available \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/Articles-Pageviews.txt}{here}, run my code, and identify keywords found in article titles, with either above or below average pageview counts. Understand 
how I assign a category to a title. What are the best and worst performing categories? See lines \textcolor{gray}{224--243} in the code
 in section~\ref{vinigold}. How are categories used to predict title \texttt{pv}?  Does it make sense to work with more granular categories? 
 \vspace{1ex}
\item[] {\bf Step 2:  Clustering articles}. What are the top parameters influencing the clustering algorithms in the code, besides \texttt{param\_N}
 specifying the number of clusters? How do they influence the cluster structure? Note that one of the clusters is
 very large, no matter which clustering method you use: see cluster with label 0, \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/nlp_scoring_clusters_hierarchical.txt}{here}. How would you fix this problem? Finally, replace the Python libraries used for clustering, by a far more
 efficient procedure that efficiently handles sparse graphs. 
\vspace{1ex}
\item[] {\bf Step 3: Predictions, sample size, and evaluation}. How would you compute confidence intervals for each predicted \texttt{pv}, based on sample size?
Predicted \texttt{pv} for each article are stored in the \texttt{predicted} array, and computed in lines \textcolor{gray}{431--454} in the code. The observed values are stored in the \texttt{observed} array.
Finally, use \textcolor{index}{cross-validation}\index{cross-validation} to better assess the quality of predictions, and to detect when overfitting is present. 
\vspace{1ex}
\item[] {\bf Step 4: Fine-tuning}. Identify all the parameters that may have an impact on predicted \texttt{pv}: \texttt{param\_W1}, \texttt{param\_W2}, \texttt{param\_G} and so on. 
To normalize \texttt{pv}, I use a log transform,  see lines \textcolor{gray}{122--124} in the code. Why? Then, to take into account the impact of time on \texttt{pv}, that is, 
to work with time-adjusted \texttt{pv}, 
I use a square root transform of the time with two parameters \texttt{param\_T1} and \texttt{param\_T2}, see lines \textcolor{gray}{133--136}. Finally, I use a linear transform with one parameter \texttt{param\_Z} to remove the bias in
 predicted \texttt{pv}, see line \textcolor{gray}{474}. Try other parametric transforms. How would you find the best transforms and
 best parameters?  
\end{itemize}
\vspace{1ex}

\noindent Now, here is my answer to \textcolor{red}{Step 1}. Best performing keywords include: `cheat sheet', `free book', `learn Python', `machine learning algorithms', `R + Python',
 `explained in one picture', `explained in simple English',  and `great articles'. Among the worst performers: 
`data security', `IoT', 'Apache', `business data', `C++', `web scraping' and `big data'. Keep in mind that the data was collected in 2020.
As for categories, blog posts do a lot better than forum questions. 

The category table \texttt{nlp\_scoring\_categories.txt} is stored \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/nlp_scoring_categories.txt}{here} 
on GitHub, and the multi-tokens table
 \texttt{nlp\_scoring\_multi\_tokens.txt}, 
\href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/nlp_scoring_multi_tokens.txt}{here} in the same folder. Due to the small number
 of articles, I bundled all authors but the most prolific into one group. Also, it makes sense to further group small categories together.  
Categories span across multiple categorical features, and are jointly encoded using the 
\textcolor{index}{smart encoding}\index{encoding!smart encoding} technique described \href{https://docs.google.com/presentation/d/1kDlAhS8yh_-Yu19ICxFk0Hxfq3ZXc4iy/}{here}. 
How much granularity you can afford depends on the sample size: here, about 4000. You want to make sure each category has at least 50 titles so
 that its average \texttt{pv} is stable enough.  Categories with high \texttt{pv} variance should be split or re-arranged to keep variance low.  

To answer \textcolor{red}{Step 2}, there are several parameters influencing the final number of multi-tokens, which in turn also impacts the clusters. But the one
directly related to the clustering algorithm is \texttt{param\_S} in line  \textcolor{gray}{278}, taking a value between 0 and 1. The closer to 1, the more
 homogeneous the clusters, and the smaller the dimension of the distance matrix. Finally, to split the large cluster, you need to access the 
\textcolor{index}{dendrogram}\index{dendrogram} and linkage structure produced in line \textcolor{gray}{394}, or perform clustering on the large cluster to obtain subgroups, 
 or use my connected components algorithm for clustering: the last option gives you full control over the clustering procedure. 

Now moving to \textcolor{red}{Step 3}. A simple method to compute model-free 
\textcolor{index}{confidence intervals}\index{confidence intervals} (CI), in this case for \texttt{pv}, consists of selecting 100 
subsamples each with (say) $N=3000$ articles from the training set (out of 4000), and compute $\texttt{pv}_0(A)$ for each article $A$ in each subsample.
 This gives you a range of values for $\texttt{pv}_0(A)$. The minimum and maximum determines your CI at level
 $1 - 1/100 = 99\%$. This method is known as \textcolor{index}{bootstrapping}\index{bootstrapping} [\href{https://en.wikipedia.org/wiki/Bootstrapping_(statistics)}{Wiki}]. 
The length $L_N$ of your CI depends on $N$, and in general, $L_N$ is almost proportional to $1/\sqrt{N}$. Despite the numerous computations involved, in this case
 the procedure is very fast as there is no neural network involved. 

Finally, regarding \textcolor{red}{Step 4}, see lines \textcolor{gray}{402--404} for the parameters influencing the predicted \texttt{pv}. 
It is tempting to use a \textcolor{index}{gradient descent}\index{gradient descent} algorithm to optimize all parameters jointly by minimizing the \textcolor{index}{loss function}\index{loss function} defined in lines \textcolor{gray}{465-471}. However, this may lead to overfitting. It is better to decouple the problem:
 separately optimize the parameters attached to time adjustment (lines \textcolor{gray}{129--136}), categorization (line \textcolor{gray}{148}), \texttt{pv} normalization (lines \textcolor{gray}{122--124}), and predictions 
(see lines \textcolor{gray}{459--475}). Also, it is possible to replace the summation~(\ref{mur28u}) by a weighted sum as in~(\ref{poxewdyt}), for instance giving
 a different weight to each article based on its category or on the title length. 



\begin{table}[H]
%\[\
\begin{center}
\scalebox{0.9}{
\begin{tabular}{ll}
\hline
  \texttt{pv} & Title \\
\hline
 9.042 &  AI vs Deep Learning vs Machine Learning \\
 8.242 &  Machine Learning vs. Traditional Statistics: Different philosophies, Different Approaches \\
 9.422 &   Machine Learning vs. Traditional Statistics: Different philosophies, Different Approaches \\
 5.635 &   A Comparative Roundup: Artificial Intelligence vs. Machine Learning vs. Deep Learning \\
 7.168 &   Artificial Intelligence vs. Machine Learning vs. Deep Learning \\
 6.715 &   AI vs. Machine Learning vs. Deep Learning: What is the Difference? \\ 
 7.717  &  Machine Learning vs Statistics vs Statistical Learning in One Picture \\
 7.855 &   Supervised Learning vs Unsupervised \& Semi Supervised in One Pi...\\
 9.185  &  Python vs R: 4 Implementations of Same Machine Learning Technique\\
 6.907 &   MS Data Science vs MS Machine Learning / AI vs MS Analytics \\
\hline		
\end{tabular}
}
%\]
\caption{\label{fy76re09}Cluster linked to $\{$`Learning$\sim$vs', `Machine$^\wedge$vs', `Machine$\sim$Learning$\sim$vs'$\}$}
\end{center}
\end{table}

The remaining of this project is devoted to visualizations pertaining to the various components discussed earlier. 
Table~\ref{fy76re09} features one of the 20 article clusters based on hierarchical clustering of multi-tokens. 
In this case, the multi-token group, automatically detected, is
$\{$`Learning$\sim$vs', `Machine$^\wedge$vs', `Machine$\sim$Learning$\sim$vs'$\}$. It is a mix of standard and contextual tokens.
The corresponding article cluster was automatically retrieved using Formula~(\ref{prozac4}). Note that two titles are identical but have
 different \texttt{pv}'s. This is due to posting the same article twice; each has a different URL. 


%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.93\textwidth]{dendo.PNG}
\caption{Multi-tokens hierarchical clustering: dendrogram (20 groups, 104 multi-tokens)}
\label{fig:z0m4rgv098}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

\subsection{Visualizations and discussion}

Figure~\ref{fig:z0m4rgv098} shows the dendrogram associated to the clustering algorithm. 
The goal here was to cluster good performing articles, thus the number of multi-tokens is small (104) because I only
 used those with highest \texttt{pv}'s. The rightmost group in brown is unusually large and could be split into 5 subgroups,
 based on the dendrogram. Each multi-token -- a final node in the dendrogram tree -- has several articles connected to it: this is not shown in the picture, but see
 Table~\ref{fy76re09} for an example combining 3 multi-tokens, corresponding to one of the small clusters in the dendrogram. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.50\textwidth]{pv_scatter.png}
\caption{Scatterplot, observed vs predicted article \texttt{pv} (4000 articles)}
\label{fig:z0m4rgv098cx}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

Figure~\ref{fig:z0m4rgv098cx} shows predicted versus observed \texttt{pv}'s in a scatterplot. It is based on
 the initial version of the algorithm, without parameters nor the loss function. You can barely see a small bias, with large \texttt{pv}'s
 slightly underestimated, and smaller ones overestimated. I also observed the same bias in various tests. That was the reason
 why I introduced the parameter \texttt{param\_Z} and a loss function to recalibrate the empirical distribution of predicted \texttt{pv}'s.  
That said, the fit is pretty good even before the upgrade. Multi-tokens found in only one article were excluded to
 avoid overfitting, by setting \texttt{param\_W1=1}. 

In Figure~\ref{fig:z0m4rgv098cy}, the decline in pageviews over time is quite noticeable, but seems to taper off at the end. However, it is artificially created by the fact that the most recent articles haven't accumulated much traffic yet. Figure~\ref{fig:z0m4rgv098cz} shows the corrected numbers: it represents the reality more faithfully; the transform used here is non linear.  


%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.66\textwidth]{pv_history.PNG}
\caption{Article \texttt{pv} over time, before adjusting for time}
\label{fig:z0m4rgv098cy}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.665\textwidth]{pv_history_detrend.PNG}
\caption{Article \texttt{pv} over time, after time adjustment}
\label{fig:z0m4rgv098cz}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------



%xxxx include images + cluster example
% created discord community for xLLM , invite people to pay on OpenCollective
% title of MLT article: Breakthough: First RAG/LLM Making Meaningful Predictions and Clustering 
% answer to Q4: time-adjusted and log transform independent of loss function


%clustering python issue memory explosion, exercise: use connected components
%add cluster examples + link to github
%exercise1: identify bad and good titles, find "article series" and play with the parameters/identify good categories 
%exercise: split the large cluster
%Under construction. %move to special project; replace by taxomoy creation wolfram



 % include scoring content / hidden decision trees / KW correls / smart crawling

%If you need massive amounts of text data for text generation, DMOZ (see \href{https://dmoz-odp.org/}{here}) and Wikipedia are two starting points. I run a webcrawler against DMOZ using hundreds of seed keywords to eventually crawl dozens of millions of webpages over several months and detect keyword associations and more. How do do it efficiently is discussed in my book and in many places on the Internet. There are Python libraries that help with this. I also recursively crawled Google search results to complement my source of data -- in short acting as a human doing searches on Google, except that it was on a large scale and automated. Maybe a precursor to ChatGPT?

%Note that you can also search images, images similar to some image, search the news, videos and so on. 
%----------------------------------------------------------------------}
%xxxyyyy

\subsection{Python code}\label{vinigold}

%xxx github ... 
% 
The code is also on GitHub, \href{https://github.com/VincentGranville/Statistical-Optimization/blob/main/nlp_scoring.py}{here}. 
\vspace{1ex}

\begin{lstlisting}[numbers=left]
# nlp_scoring.py | vincentg@mltechniques.com

import pandas as pd
import numpy as np
import matplotlib as mpl
from matplotlib import pyplot as plt

mpl.rcParams['lines.linewidth'] = 0.3
mpl.rcParams['axes.linewidth'] = 0.5
plt.rcParams['xtick.labelsize'] = 7
plt.rcParams['ytick.labelsize'] = 7


#--- [1] Read data

# read data either from GitHub, or use local copy, depending on path

# path = "https://raw.githubusercontent.com/VincentGranville/Statistical-Optimization/main/"
path = ""
filename = "Articles-Pageviews.txt"
url  = path + filename

data = pd.read_csv(url, sep='\t', engine='python', encoding='cp1252')
header = ['Title', 'URL', 'Author', 'Page views', 'Creation date', 'Status']


#--- [2] Core functions and tables

# pv is aggregated normalized pageview; pv_rel is average pv for a specific group

arr_titles = data['Title']       # article titles
arr_status = data['Status']      # blog post, forum question, etc.
arr_pv     = data['Page views']  # article absolute (un-normalized) pageviews 
arr_url    = data['URL']         # article URL
arr_author = data['Author']      # article author
arr_categories = {}              # article category (combo of author, status, URL extracts) 

category_pv    = {}     # key: article category; value: average pv per category
category_count = {}     # key: article category; value: number of articles in that category
hash_words_count = {}   # key: word; value: number of occurrences across all articles 
hash_pv = {}            # key: word; value: aggregated pv across all those articles
hash_titles = {}        # key: word; value: hash of articles containing word (value: pv)
hash_authors_count = {} # key: author; 

def compress_status(status):
    status = status.lower()
    if 'forum' in status:
        status = 'forum_question'
    elif 'resource' in status:
        status = 'resource'
    else:
        status = 'blog'
    return(status)

def compress_url(url):
    url = url.lower()
    if 'datasciencecentral' in url:
        url = 'DSC'
    else:
        url = 'Other'
    return(url)

def  update_hash(titleID, word, hash_words_count, hash_pv, hash_titles):

    if word in hash_words_count:
        hash_words_count[word] += 1
        hash_pv[word] += pv    # what if words found twice in same title ??
        hash = hash_titles[word]
        hash[titleID] = pv
        hash_titles[word] = hash
    else:
        hash_words_count[word] = 1
        hash_pv[word] = pv
        hash_titles[word] = { titleID : pv }
    return(hash_words_count, hash_pv, hash_titles)

def  update_single_tokens(titleID, words, hash_words_count, hash_pv, hash_titles):

    # words is an array of words (found in title)

    for word in words:
        hash_words_count, hash_pv, hash_titles = update_hash(titleID, word, 
                                   hash_words_count, hash_pv, hash_titles)
    return(hash_words_count, hash_pv, hash_titles)

def  update_joint_tokens(titleID, words, hash_words_count, hash_pv, hash_titles): 

    # words is an array of words (found in title)
    # capturing adjacent tokens

    for idx in range(len(words)):
        word = words[idx]
        if idx < len(words)-1 and words[idx+1] != '':
            word += "~" + words[idx+1]
            hash_words_count, hash_pv, hash_titles = update_hash(titleID, word, 
                                       hash_words_count, hash_pv, hash_titles)
        if idx < len(words)-2 and words[idx+2] != '':
            word += "~" + words[idx+2]
            hash_words_count, hash_pv, hash_titles = update_hash(titleID, word, 
                                       hash_words_count, hash_pv, hash_titles)
    return(hash_words_count, hash_pv, hash_titles)

def update_disjoint_tokens(titleID, words, hash_words_count, hash_pv, hash_titles):
    
    # words is an array of words (found in title)
    # word1 and word2 cannot be adjacent: 
    #    if they were, this is already captured in the update_joint_tokens function

    param_D = 1   # word1 and word2 must be separated by at least param_D tokens

    for k in range(len(words)):
        for l in range(len(words)):
            word1 = words[k]
            word2 = words[l]
            distance = abs(k - l)
            if word1 < word2 and distance > param_D and word1 != '' and word2 != '':
                word12 = word1 + "^" + word2
                hash_words_count, hash_pv, hash_titles = update_hash(titleID, 
                                  word12, hash_words_count, hash_pv, hash_titles)
    return(hash_words_count, hash_pv, hash_titles)

def get_article_pv(titleID, arr_pv):  
    # using log: it gives a better normalization and fit than sqrt, for pv distribution
    return(np.log(float(arr_pv[titleID])))


#--- [3] De-trend pv 

param_T1 = 0.80
param_T2 = 0.11 
arr_pv_new = np.zeros(len(arr_pv)) 

for k in range(len(arr_pv)):
    energy_boost = param_T1 * np.sqrt(k + param_T2 * len(arr_pv))
    arr_pv_new[k] = arr_pv[k] * (1 + energy_boost) 
arr_pv = np.copy(arr_pv_new)    


#--- [4] Populate core tables 

for k in range(len(data)):
    author = arr_author[k]
    if author in hash_authors_count:
        hash_authors_count[author] +=1
    else:
        hash_authors_count[author] =1

param_A = 50   # authors with fewer than param_A articles are bundled together

for k in range(len(data)):

    pv = get_article_pv(k, arr_pv) 
    cstatus = compress_status(arr_status[k])
    curl = compress_url(arr_url[k])
    category = curl + "~" + cstatus
    author = arr_author[k]
    if hash_authors_count[author] > param_A:
        arr_categories[k] = category + "~" + author
    else:
        arr_categories[k] = category

    words = str(arr_titles[k]).replace(',',' ').replace(':',' ').replace('?', ' ')
    words = words.replace('.',' ').replace('(',' ').replace(')', ' ')
    words = words.replace('-',' ').replace('  ',' ').replace('\xa0', ' ') 
    # words = words.lower() 
    words = words.split(' ')

    if 'DSC~resource' in category or 'DSC~blog' in category: 
        hash_words_count, hash_pv, hash_titles = update_single_tokens(k, words, 
                                   hash_words_count, hash_pv, hash_titles)
        hash_words_count, hash_pv, hash_titles = update_joint_tokens(k, words, 
                                   hash_words_count, hash_pv, hash_titles)
        hash_words_count, hash_pv, hash_titles = update_disjoint_tokens(k, words, 
                                   hash_words_count, hash_pv, hash_titles)

mean_pv = sum(hash_pv.values()) / sum(hash_words_count.values())
print("Mean pv: %6.3f" % (mean_pv))             


#--- [5] Sort, normalize, and dedupe hash_pv

# Words with identical pv are all attached to the same set of titles
# We only keep one of them (the largest one) to reduce the number of words

eps = 0.000000000001
hash_pv_rel = {}

for word in hash_pv:
    hash_pv_rel[word] = hash_pv[word]/hash_words_count[word]

hash_pv_rel = dict(sorted(hash_pv_rel.items(), key=lambda item: item[1], reverse=True))

hash_pv_deduped = {}
old_pv = -1
old_word = ''
for word in hash_pv_rel:
    pv = hash_pv_rel[word]
    if abs(pv - old_pv) > eps:
        if old_pv != -1:
            hash_pv_deduped[old_word] = old_pv 
        old_word = word
        old_pv = pv
    else:
        if len(word) > len(old_word):
            old_word = word
hash_pv_deduped[old_word] = old_pv

print()
print("=== DEDUPED WORDS: titles count, relative pv (avg = 1.00), word\n")
input("> Press <Enter> to continue")

for word in hash_pv_deduped: 
    count = hash_words_count[word]
    if count > 20 or count > 5 and '~' in word:
        print("%6d %6.3f %s" %(count, hash_pv_rel[word]/mean_pv, word))


#--- [6] Compute average pv per category

# Needed to predict title pv, in addition to word pv's
# Surprisingly, word pv's have much more predictive power than category pv's
# For new titles with no decent word in historical tables, it is very useful

for k in range(len(data)):
    category = arr_categories[k]
    pv = get_article_pv(k, arr_pv)
    if category in category_count: 
        category_pv[category] += pv
        category_count[category] += 1
    else:
        category_pv[category] = pv
        category_count[category] = 1

print()
input("> Press <Enter> to continue")
print()
print("=== CATEGORIES: titles count, pv, category name\n")

for category in category_count:
    count = category_count[category]
    category_pv[category] /= count
    print("%5d %6.3f %s" %(count, category_pv[category], category)) 
print()


#--- [7] Create short list of frequent words with great performance

# This reduces the list of words for the word clustering algo in next step
# Goal: In next steps, we cluster groups of words with good pv to
#       categorize various sources of good performance

short_list = {}
keep = 'good'    # options: 'bad' or 'good'

param_G1 = 1.10  # must be above 1, large value to get titles with highest pv
parma_G2 = 0.90  # must be below 1, low value to get articles with lowest pv
param_C1 = 10    # single-token word with count <= param_C1 not included in short_list
param_C2 = 4     # multi-token words with count <= param_C2 not included in short_list

for word in hash_pv_deduped: 
    count = hash_words_count[word]
    pv = hash_pv[word]/count
    if keep == 'good':
        flag = bool(pv > param_G1 * mean_pv)
    elif keep == 'bad':
        flag = bool(pv < param_G2 * mean_pv)  
    if flag and (count > param_C1 or count > param_C2 and '~' in word): 
        short_list[word] = 1


#--- [8] compute similarity between words in short list, based on common titles

# Find list of articles S1 and S2, containing respectively word1 and word2
# word1 is similar to word2 if |S1 intersection S2| / |S1 union S2| is high

hash_pairs = {}
aux_list = {}
param_S = 0.20  # if similarity score about this threshold, word1 and word2 are linked

for word1 in short_list:
    for word2 in short_list:

        set1 = set()
        for titleID1 in hash_titles[word1]:
            set1.add(titleID1)
        set2 = set()
        for titleID2 in hash_titles[word2]:
            set2.add(titleID2)

        count1 = len(set1)
        count2 = len(set2)
        count12 = len(set.union(set1, set2))
        similarity = len(set.intersection(set1, set2)) / count12

        if similarity > param_S and word1 < word2: 
            hash_pairs[(word1, word2)] = similarity
            hash_pairs[(word2, word1)] = similarity
            hash_pairs[(word1, word1)] = 1.00
            hash_pairs[(word2, word2)] = 1.00
            aux_list[word1] = 1
            aux_list[word2] = 1 


#--- [9] Turn hash_pairs{} into distance matrix dist_matrix, then perform clustering

# Keyword clustering based on similarity metric computed in previous step 
# Alternative to exploite sparse matrix: connected components algorithm on hash_pairs
# Connected components is much faster, works with big graphs
# In addition, not subject to deprecated parameters unlike Sklearn clustering
# Source code for connected components:  https://mltblog.com/3UDJ2tR  

param_N = 20  # prespecified number of clusters in word clustering
n = len(aux_list)
dist_matrix  = [[0 for x in range(n)] for y in range(n)] 
arr_word = []

i = 0
for word1 in aux_list:
    arr_word.append(word1)
    j = 0
    for word2 in aux_list:
        key = (word1, word2)
        if key in hash_pairs:
            # hash_pairs is based on similarity; dist_matrix = 1 - hash_pairs is distance
            dist_matrix[i][j] = 1 - hash_pairs[(word1, word2)]
        else:
            # assign maximum possible distance if i, j are not linked
            dist_matrix[i][j] = 1.00   # maximum possible distance
        j = j+1
    i = i+1

#- Clustering, two models: hierarchichal and k-medoids, based on distance matrix

from sklearn.cluster import AgglomerativeClustering
hierarch = AgglomerativeClustering(n_clusters=param_N,linkage='average').fit(dist_matrix)

# !pip install scikit-learn-extra 
from sklearn_extra.cluster import KMedoids
kmedoids = KMedoids(n_clusters=param_N,random_state=0).fit(dist_matrix)

#- Now showing the clusters obtained from each model 

def show_clusters(model, hash_titles, arr_titles, arr_pv):

    groups = model.labels_
    hash_group_words = {}
    for k in range(len(groups)):
        group = groups[k]
        word = arr_word[k]
        if group in hash_group_words:
            hash_group_words[group] = (*hash_group_words[group], word)
        else:
            hash_group_words[group] = (word,)

    hash_group_titles = {}
    for group in hash_group_words:
        words = hash_group_words[group]
        thash = {}
        for word in words:
            for titleID in hash_titles[word]: 
                thash[titleID] = 1
        hash_group_titles[group] = thash

    for group in hash_group_words:
        print("-------------------------------------------")
        print("Group", group)
        print()
        print("keywords:", hash_group_words[group])
        print()
        print("Titles with normalized pv on the left:")
        print()
        for titleID in hash_group_titles[group]:
            pv = get_article_pv(titleID, arr_pv)
            print("%6.3f %s" %(pv, arr_titles[titleID]))
        print("\n")
 
    return(hash_group_words, hash_group_titles)

print("\n\n=== CLUSTERS obtained via hierarchical clustering\n")
input("> Press <Enter> to continue")
show_clusters(hierarch, hash_titles, arr_titles, arr_pv)

print("\n\n=== CLUSTERS obtained via k-medoid clustering\n")
input("> Press <Enter> to continue")
show_clusters(kmedoids, hash_titles, arr_titles, arr_pv)

input(">Press <Enter> to continue")

#- plot dendogram related to dist_matrix

from scipy.cluster.hierarchy import dendrogram, linkage
# Doc for Scipy dendograms: https://mltblog.com/3UG0C08

Z = linkage(dist_matrix)    # Exercise: use Z to split the large group
dendrogram(Z)  
plt.show()


#--- [10] Predicting pv 

# Need to do cross-validation in the future
# Influenced by: param_W1, param_W2, param_G1, param_C1, param_C2, param_A, param_D
#                param_T1, param_T2
# Not influenced by: param_N, param_S
# Large param_W2 combined with small param_W1 may lead to overfitting

# We need the build inverted ("transposed") hash_titles, named reversed_hash_titles

reversed_hash_titles = {}

for word in hash_titles:
    pv_rel = hash_pv_rel[word]
    hash = hash_titles[word]
    for  titleID in hash:
        if titleID in reversed_hash_titles:
            rhash = reversed_hash_titles[titleID]
        else:
            rhash ={}
        rhash[word] = pv_rel
        reversed_hash_titles[titleID] = rhash

# Now predicting pv

observed = []
predicted = []
missed = 0
n = 0
param_W1 = 1     # low value increases overfitting, should be >= 1
param_W2 = 2.00  # chosen to minimize error between pv and estimated_pv 

for titleID in reversed_hash_titles:
    pv = get_article_pv(titleID, arr_pv)
    rhash = reversed_hash_titles[titleID]
    n += 1
    count = 0
    sum = 0
    for word in rhash:
        weight = hash_words_count[word]
        booster = 1.00
        if '~' in word:
            booster = 1.00  # test a different value
        elif '^' in word:
            booster = 1.00  # test a different value
        if weight > param_W1:
            count += booster * (1/weight)**param_W2
            sum += booster * (1/weight)**param_W2 * rhash[word]
    if count > 0:
        estimated_pv = sum / count
    else:
        missed += 1
        category = arr_categories[titleID]
        estimated_pv = category_pv[category]
    observed.append(pv)
    predicted.append(estimated_pv)
    
observed = np.array(observed)
predicted = np.array(predicted)
mean_pv =  np.mean(observed)
min_loss = 999999999.99
param_Z = 0.00

for test_param_Z in np.arange(-0.50, 0.50, 0.05):

    scaled_predicted = predicted + test_param_Z * (predicted - mean_pv)
    loss = 0
    for q in (.10, .25, .50, .75, .90):
        delta_ecdf = abs(np.quantile(observed,q)-np.quantile(scaled_predicted,q))
        if delta_ecdf > loss:
            loss = delta_ecdf
    if loss < min_loss:
        min_loss = loss
        param_Z = test_param_Z

predicted = predicted + param_Z * (predicted - mean_pv)
loss = min_loss    
mean_estimated_pv = np.mean(predicted)
mean_error = np.mean(np.abs(observed-predicted))
correl = np.corrcoef(observed, predicted)

plt.axline((min(observed),min(observed)),(max(observed),max(observed)),c='red')
plt.scatter(predicted, observed, s=0.2, c ="lightgray", alpha = 1.0)

plt.show()

print()
print("=== PREDICTIONS\n")
print("Predicted vs observed pageviews, for 4000 articles\n")
print("Loss: %6.3f" %(loss))
print("Missed titles [with pv estimared via category]: ",missed, "out of", n)
print("Mean pv (observed) : %8.3f" %(mean_pv)) 
print("Mean pv (estimated): %8.3f" %(mean_estimated_pv)) 
print("Mean absolute error: %8.3f" %(mean_error)) 
print("Correl b/w observed and estimated pv: %8.3f" %(correl[0][1]))
print()
print("Observed quantiles (left) vs prediction-based (right)")
print("P.10: %8.3f %8.3f"  %(np.quantile(observed, .10), np.quantile(predicted, .10)))
print("P.25: %8.3f %8.3f"  %(np.quantile(observed, .25), np.quantile(predicted, .25)))
print("P.50: %8.3f %8.3f"  %(np.quantile(observed, .50), np.quantile(predicted, .50)))
print("P.75: %8.3f %8.3f"  %(np.quantile(observed, .75), np.quantile(predicted, .75)))
print("P.90: %8.3f %8.3f"  %(np.quantile(observed, .90), np.quantile(predicted, .90)))

#- Plot normalized pv of articles over time

y = np.zeros(len(arr_pv))
for k in range(len(arr_pv)):
   y[k] = get_article_pv(k, arr_pv)

z = np.zeros(len(arr_pv))
window = 120  # for moving average
for k in range(len(arr_pv)):
   if k-window < 0:
       z[k] = np.mean(y[0:k+window])
   elif k+window > len(arr_pv):
       z[k] = np.mean(y[k-window:len(arr_pv)-1])
   else:
       z[k] = np.mean(y[k-window:k+window])
   
plt.plot(range(len(arr_pv)), y, linewidth = 0.2, alpha = 0.5)
plt.plot(range(len(arr_pv)), z, linewidth = 0.8, c='red', alpha = 1.0)

plt.xlim(0, len(arr_pv))
#plt.ylim(7, 15)
plt.grid(color='red', linewidth = 0.2, linestyle='--')
plt.show()

\end{lstlisting}



%---------------------------------------------------------
\appendix
\chapter{Glossary: GAN and Tabular Data Synthetization}\label{aasdaaqw}

The following list features the most important concepts related to tabular data synthetization and evaluation methods, with a focus on generative adversarial networks. %\vspace{1ex} \\

% MLT article: Generative AI Glossary: Part 1

% I started to create a glossary featuring well-known concepts, keywords and abbreviations. This part with 51 entries focuses on generative adversarial networks and tabular data synthetization. It was initially included in <this article | project textbook>.

%gen ai glossary: tabular data synthetization / transform
%add to project textbook (appendix) and stats optim book

\DefTblrTemplate{caption}{default}{}
\DefTblrTemplate{middlehead,lasthead}{default}{}
\DefTblrTemplate{firstfoot,middlefoot}{default}{}

\begin{center}
\begin{longtblr}{p{\dimexpr3.5cm-2\tabcolsep}p{\dimexpr12cm-2\tabcolsep}}
\hline activation function \index{activation function} & Function transforming values from the last layer of a deep neural network such as GAN, into actual output. For \textcolor{index}{dummy variables}\index{dummy variables}, it is customary to use \textcolor{index}{softmax}\index{softmax function}.  \\
\hline algorithmic bias \index{algorithmic bias} & Algorithm are designed by architects with their own biases, train on data reflecting these biases (for instance, pictures of mostly white people) and decision from blackbox systems (who gets a loan) impacted by these biases. Synthetic data can help address this issue.\\
\hline base distance & When evaluating generated data, you compare your synthetic data with the validation data, a subset of the real data not use for training. The base distance is the distance between the part of the real data not used for training (the validation set), and the part of the real data actually used for training. \\
\hline batch & In GAN implementations, during each epoch (a full run of the dataset), you synthetize small batches of data and evaluate these batches separately one at a time, as it is a lot faster than doing it on the whole data at once.\\
\hline binning & Many algorithms such as \textcolor{index}{XGboost}\index{XGboost} work on \textcolor{index}{binned data}\index{binning}, where feature values - either jointly or separately - are aggregated into buckets called bins of \textcolor{index}{flag vectors}\index{flag vector}. Bin counts also work well with categorical data.\\
\hline categorical feature & A non numerical feature sometimes represented by dummy variables, one per category value, such as disease type or keyword. It can lead to a large number of features, artificially increasing the dimension of the problem. Grouping and aggregation techniques can reduce
 the dimensionality. \\
\hline copula \index{copula} & Data synthetization technique based on \textcolor{index}{empirical quantiles}\index{quantile!empirical} and the feature 
\textcolor{index}{correlation matrix}\index{correlation matrix}, generalizing the 
\textcolor{index}{inverse transform  sampling}\index{inverse transform  sampling} method to multivariate data. \\
\hline correlation matrix & The distance between two correlation matrices, one computed on the real data and the other one on the synthetic data, is a fundamental evaluation metric to measure the quality of the generated data.\\

\hline Cramer's V & A generalization of the correlation coefficient to measure the association between categorical features, or between a categorical and numerical feature. The value is between 0 (no association) and 1 (strong association).\\
\hline data augmentation \index{augmented data} & The method consists of adding synthetic observations to your training set, to produce more robust predictions or classifications. By enriching the training set, your algorithm will be better trained to deal with future real data not in the training set.\\
\hline data cleaning & Required step before using any modeling technique, to detect outliers, missing values, duplicates,
 wrong formatting, and so on. Can be automated to a large extent.\\
\hline dummy variable & Binary feature with two values (0 and 1) to represent categorical information, for instance California = 0/1 to indicate
  whether the location is in California or not. In this case, you may have 50 dummy variables, one for each state. It allows you to use numerical algorithms on categorical data.\\
\hline EDA & Exploratory data analysis.  Used to detect outliers, unique values with count and frequency (for each feature), percentiles, duplicated and missing values,
 correlation between features, and empirical distributions. Also used to bin the data.\\
\hline ECDF &  \textcolor{index}{Empirical cumulative distribution function}\index{ECDF (empirical distribution)} uniquely characterizing the underlying distribution in a dataset. Works with numerical and categorical features. The one-dimensional version is computed for each feature separately. \\
\hline EPDF & \textcolor{index}{Empirical probability density function}\index{EPDF (empirical probability density function)}\index{empirical probability function}. The discrete derivative of the ECDF, and more difficult to handle than ECDF. For discrete variables, there is a one-to-one mapping between ECDF and EPDF.\\
\hline epoch & Also called iteration. One full run of your real data when training a \textcolor{index}{GAN}\index{GAN (generative adversarial network)} model. 
The \textcolor{index}{loss functions}\index{loss function} (generator and discriminator) are
 computed at each \textcolor{index}{epoch}\index{epoch (neural networks)} and should stabilize to low values after thousands of epochs, depending on the \textcolor{index}{hyperparameters}\index{hyperparameter}. \\
\hline explainable AI \index{explainable AI} & Set of methods leading to easy interpretation, with simple explanations whenever the 
 blackbox system makes a decision. Explainability can be increased using feature importance scores.
 Some algorithms such as NoGAN are fully explainable by design. \\
\hline faithfulness & One of the goals of synthetization is to correctly mimic the statistical distributions and patterns found in the real data. 
 Faithfulness metrics
 such as KS distance measure how well this is accomplished. Metrics measuring the quality of predictions (via training set augmentation and cross-validation), are called utility metrics. Security metrics measure how well personal information has been transformed.\\
\hline
GAN & \textcolor{index}{Generative adversarial network}\index{generative adversarial network}. Data synthetization technique based on 3 deep neural networks: the generator to generate synthetic observations, the discriminator to distinguish between fake and real data (competing with the generator), and the full model. \\
\hline gradient descent & Most machine learning algorithms including GAN, aim to minimize a loss function, or equivalently, 
maximize model fitting to data. \textcolor{index}{Gradient descent}\index{gradient descent} performs this task. It may or may not succeed depending on parameters such as \textcolor{index}{learning rate}\index{learning rate}. Neural networks use \textcolor{index}{stochastic gradient descent}\index{gradient descent!stochastic}.
 Discretized versions are available. \\
\hline Hellinger distance\index{Hellinger distance} & Metric to evaluate the quality of synthetizations. Based on the probability density functions (EPDF), computed on the real and synthetic data, then compared. Typically performed on each feature separately.\\
\hline hexagonal bin & \textcolor{index}{Hexagonal bin}\index{binning!hexagonal bins} plots are scatterplots where each dot is replaced by a fixed-size bin containing a variable number of observations. The color intensity represents the number of observations in each bin. Each bin is hexagonal: this is the optimum shape. The hexagons  are arranged in an  \textcolor{index}{tessellation}\index{tessellation} with underlying \textcolor{index}{hexagonal lattice}\index{lattice!hexagonal}. \\
\hline holdout & The \textcolor{index}{holdout method}\index{holdout method} consists of using a portion of your real data (called training set) to train a synthesizer, and the remaining (called \textcolor{index}{validation set}\index{validation set}) to evaluate the quality of the generated data. \\
\hline hyperparameter & In neural networks, parameters are the weights attached to the synapses connecting the neurons. Hyperparameters
 control the behavior of the whole system, and specify its architecture. For instance: number of epochs, batch size, loss functions, activation functions, 
 learning rate, type of gradient descent, number and type of layers (dense or sparse), and so on.\\
\hline imbalance & In a dataset, segments with few observations (for instance, fraudulent transactions) cause \textcolor{index}{imbalance}\index{imbalanced dataset}. Synthetization allows you to generate more observations for these segments, to balance the dataset to improve the performance of some algorithms.\\
\hline KS distance & \textcolor{index}{Kolmogorov-Smirnov distance}\index{Kolmogorov-Smirnov distance}. To evaluate the quality of synthesized data. While the Hellinger distance is based
 on the density (EPDF) and averaged deviations, KS is based on the maximum deviation between the two ECDFs: real versus synthetic. It is
 more robust than Hellinger.\\
\hline latent variable\index{latent variable} & In GANs, feature values that we cannot interpret directly, but which encode a meaningful internal representation of externally observed events.\\
\hline learning rate & Parameter that governs increments in gradient descent algorithms. Small values means slow convergence and possibly getting stuck around a local minimum. Large values
 may lead to missing the optimum or lack of convergence.\\
\hline loss function & The function to minimize in a gradient descent algorithm. For instance, the maximum KS distance between the generated and real data, in a synthetization problem.\\
\hline
metadata \index{metadata}& Information attached to a tabular dataset, specifying the type of data for each column: categorical, ordinal (integer), text, timestamp, 
 continuous feature, and so on.  \\
\hline missing values & Can be encoded as NaN, a blank cell, the empty string `', a large integer, or zero. NoGAN easily handles them. Techniques to retrieve missing values are called imputation methods.\\
\hline mode collapse & In GANs, \textcolor{index}{mode collapse}\index{mode collapse} happens when the generator can only produce a single type of output or a small set of outputs. This may happen due to problems in training, such as the generator finding a type of data that can easily fools the discriminator and thus keeps generating that one type.\\
\hline multivariate ECDF & Same as ECDF but in this case computed jointly for multiple features, rather than separately for each feature. 
 The computation is not straightforward.\\
\hline NoGAN\index{NoGAN} & Synthesizer not based on GAN or neural networks. A very efficient one, both in terms of speed and quality of the output, sharing some similarities with XGboost, is described 
in~\cite{vgnogan}. The copula and interpolation methods also fall in that category.\\
\hline overfitting & Synthetic data that looks too good to be true, could be the result of \textcolor{index}{overfitting}\index{overfitting}. This can happen when fine-tuning the hyperparameters to work on one particular dataset. To reduce overfitting, evaluate the quality of a synthetization on a validation set using the 
\textcolor{index}{holdout method}\index{holdout method}. Or assess performance of predictions based on augmented data, 
using \textcolor{index}{cross-validation}\index{cross-validation}.\\
\hline oversampling \index{oversampling} & Consists of producing a larger proportion of synthetic observations for underrepresented segments in the real data (for instance fraudulent transactions),  
 to fix the imbalance problem.  \\
\hline PCA & \textcolor{index}{Principal component analysis}\index{PCA (principal component analysis)}. Used as a transform to decorrelate the features in the real data, prior to training GAN, as this can improve synthetizations. The correct correlation structure is then put back into the synthetization, using the inverse PCA transform, after running GAN.\\
\hline quantile & The \textcolor{index}{empirical quantile function}\index{quantile!quantile function} is the inverse of the ECDF. It generalizes percentiles.\\
\hline reinforcement learning & Machine learning classification technique where correct allocation of future observations (outside the training set) is rewarded,
enabling the system to self-learn via trial and error.\\
\hline replicability & A \textcolor{index}{replicable}\index{replicability} neural network is one that can produce the exact same results when run multiple times on the same data, regardless of the platform. Usually controlled by a seed parameter: using the same seed leads to the same results. \\
\hline scaling\index{scaling factor} & A transformation that keeps the values of each feature within the same range, or with the same variance in the real data, before using GAN. 
  A measurement, whether in yards or miles, will be scale-free after the transformation. It can dramatically improve the
 quality of the generated data. Inverse scaling is then applied to the generated data, after the GAN synthetization.\\
\hline seed \index{seed (random number generators)}& Parameter used to initialize the various random number generators involved in the GAN architecture, typically one for each Python library 
 that generates random numbers. It produces replicable results, at least with CPU implementations. In GPU, the problem is different. \\
\hline stopping rule \index{stopping rule}& A criterion to decide when to stop training a GAN, typically when an epoch produces an unusually good synthetization, 
 based on quality evaluation metrics such as the KS distance. It produces much better results than stopping after a fixed number of epochs.\\
\hline synthetization & Production of generated observations, also called \textcolor{index}{synthetic data}\index{synthetic data}, with statistical properties  
 mimicking those computed on a pre-specified real data set.\\
\hline tabular data & Data arranged in tables, where columns represent features, and rows represent observations. Typically used
 for transactional data. Time series are treated with specific algorithms. \\
\hline training set & The portion of your real data used to train your synthesizer. The other part is called the validation set, and used to evaluate the quality of the synthetic data (how well it mimics real data). This setting known as holdout allows you to test you synthetisizer on future data and avoid overfitting.\\
\hline transform \index{transform (mapping)}& Similar to transformers in large language models. Consists of using an invertible transform on your real data prior to GAN
 processing, to improve GAN performance. You need to apply the inverse transform on the generated data, after GAN. Example of transforms: scaling, PCA, standardization (transformed features having the same variance and zero mean), and normalization (to eliminate skewness).\\
\hline validation set & See training set. \\
\hline vanishing gradient \index{gradient descent!vanishing gradient}& When the gradient gets close to zero in a gradient descent algorithm, it can prevent further progress towards locating the optimum. In the worst case, this may completely stop the neural network from further training. \\
\hline Wasserstein loss & The GAN \textcolor{index}{Wasserstein loss function}\index{loss function!Wasserstein} seeks to increase the gap between the scores for real and generated data. It is one of the many loss functions to improve the gradient descent algorithm, avoiding mode collapse and similar problems in some synthetizations.\\
\hline WGAN & Wasserstein GAN, based on the Wasserstein loss function.\\
\hline
\end{longtblr}
\end{center}

%-----------

\chapter{Glossary: GenAI and LLMs}\label{aasdaaq32}

\begin{center}
\begin{longtblr}{p{\dimexpr3.5cm-2\tabcolsep}p{\dimexpr12cm-2\tabcolsep}}
\hline ANN & Approximate nearest neighbor. Similar to the \textcolor{index}{$K$-NN}\index{$k$-NN} algorithm used in supervised classification, but faster and applied to retrieving information in vector databases, such as LLM embeddings stored as vectors. I designed a probabilistic version called \textcolor{index}{pANN}\index{ANN (approximate nearest neighbors)!probabilistic ANN (pANN)}, especially useful for model evaluation and improvement, with applications to GenAI, synthetic data, and LLMs. See section~\ref{f8v2koyuy}. \\
\hline
diffusion & \textcolor{index}{Diffusion models}\index{diffusion} 
use a Markov chain with diffusion steps to slowly add random noise to data and then learn to reverse the diffusion process to construct desired data samples from the noise. The output is usually a dataset or image similar but different from the original ones. Unlike 
variational \textcolor{index}{auto-encoders}\index{auto-encoder}, diffusion models have high dimensionality in the latent space (latent variables): the same dimension as the original data. Very popular in computer vision and image generation.\\
\hline
embedding & In LLMs, \textcolor{index}{embeddings}\index{embeddings} are typically attached to a keyword, paragraph, or element of text; they consist of tokens. The concept has been extended to computer vision, where images are summarized in small dimensions by a number of numerical features (far smaller than the number of pixels). Likewise, in LLMs, tokens are treated as the features in your dataset, especially when embeddings are represented by fixed-size vectors. The dimension is the number of tokens per embedding. See Figure~\ref{de2pufd1re} and the \textcolor{index}{token}\index{token} entry.\\
\hline
encoder & An auto-encoder is (typically) a neural network to compress and reconstruct unlabeled data. It has two parts: an encoder that compacts the input, and a decoder that reverses the transformation. The original transformer model was an auto-encoder with both encoder and decoder. However, OpenAI (GPT) uses only a decoder. \textcolor{index}{Variational auto-encoders}\index{variational auto-encoder} (VAE) are very popular.\\
\hline
GAN & \textcolor{index}{Generative adversarial network}\index{generative adversarial network}. One of the many types of 
DNN (\textcolor{index}{deep neural network}\index{deep neural network (DNN)}) architecture. It consists of two DNNs: the generator and the discriminator, competing against each other until reaching an equilibrium. Good at generating synthetic images similar to those in your training set (computer vision). Key components include a loss function, a stochastic gradient descent algorithm such as \textcolor{index}{Adam}\index{Adam (stochastic gradient descent)} to find a local minimum to the loss function, and hyperparameters to fine-tune the results. Not good at synthesizing tabular data, thus the reason I created \textcolor{index}{NoGAN}\index{NoGAN}: see section~\ref{genaiyert}.\\
\hline 
GPT & In case you did not know, \textcolor{index}{GPT}\index{GPT} stands for Generative Pre-trained Transformer. The main application is LLMs. See \textcolor{index}{transformer}\index{transformer}.
\\
\hline
graph database & My LLMs rely on taxonomies attached to the crawled content. Taxonomies consist of categories, subcategories and so on. When each subcategory has exactly one parent category, you use a tree to represent the structure. Otherwise, you use a \textcolor{index}{graph database}\index{graph database}.
\\
\hline
key-value database & Also known as hash table or dictionary in Python. In my LLMs, embeddings have variable size. I store them as short 
\textcolor{index}{key-value tables}\index{key-value database} rather than long vectors. Keys are tokens, and a value is the association between a token, and the word attached to the parent embedding.
\\
\hline 
LangChain\index{LangChain} & Available as a Python library or API, it helps you build applications that read data from internal documents and summarize them. It allows you to build customized GPTs, and blend results to user queries or prompts with local information retrieved from your environment, such as internal documentation or PDFs.
\\
\hline
LLaMA\index{LLaMA} & An LLM model that predicts the next word in a word sequence, given previous words. See   how I use them to predict the next DNA subsequence in DNA sequencing, in section~\ref{dnalove}. Typically associated to \textcolor{index}{auto-regressive models}\index{auto-regressive model} or Markov chains.
\\
\hline %---
LLM & Large language model. Modern version of \textcolor{index}{NLP}\index{NLP (natural language processing)} (natural language processing) 
and \textcolor{index}{NLG}\index{NLG (natural language generation)} (natural language generation). Applications include chatbots, sentiment analysis, text summarization, search, and translation.
\\
\hline
multi-agent system\index{multi-agent system} & LLM architecture with multiple specialized LLMs. The input data (a vast repository) is broken down into top categories. Each one has its own LLM, that is, its own embeddings, dictionary, and related tables. Each specialized LLM is sometimes called a simple LLM. 
See my own version named \textcolor{index}{xLLM} \index{xLLM}, in section~\ref{bernoc}. 
\\
\hline 
multimodal\index{multimodal system} & Any architecture that blends multiple data types: text, videos, sound files, and images. The emphasis is on processing user queries in real-time, to return blended text, images, and so on. For instance, turning text into streaming videos.
\\
\hline
normalization & Many \textcolor{index}{evaluation metrics}\index{evaluation (GenAI models)} take values between 0 and 1 after proper scaling. Likewise, weights attached to tokens in LLM embeddings have a value between -1 and +1. In many algorithms and \textcolor{index}{feature engineering}\index{feature engineering}, the input data is usually transformed first (so that each feature has same variance and zero mean), then processed, and finally you apply the inverse transform to the output. These transforms or scaling operations are known as \textcolor{index}{normalization}\index{normalization}.
\\
\hline
parameter & This word is mostly used to represent the weights attached to neuron connections in DNNs. Different from hyperparameters. The latter are knobs to fine-tune models. Also different from the concept of \textcolor{index}{parameter}\index{parameter (neural networks)} in statistical models despite the same spelling.
\\
\hline
RAG & \textcolor{index}{Retrieval-augmentation-generation}\index{RAG}\index{retrieval-augmentation-generation (RAG)}. In LLMs, retrieving data from summary tables (embeddings) to answer a prompt, using additional sources to augment your training set and the summary tables, and then generating output. Generation focuses on answering a user query (prompt), on summarizing a document, or producing some content such as synthesized videos.
\\
\hline 
regularization \index{regularization} & Turning a standard optimization problem or DNN into constrained optimization, by adding constraints and corresponding Lagrange multipliers to the loss function. Potential goals: to obtain more robust results, or to deal with over-parameterized statistical models and ill-conditioned problems. Example: Lasso regression. Different from normalization.
\\
\hline
reinforcement learning\index{reinforcement learning} & A semi-supervised machine learning technique to refine predictive or classification algorithms by rewarding good decisions and penalizing bad ones. Good decisions improve future predictions; you achieve this goal by adding new data to your training set, with labels that work best in cross-validation testing. In my LLMs, I let the user choose the parameters that best suit his needs. This technique leads to 
\textcolor{index}{self-tuning}\index{self-tuning} and/or customized models: the default parameters come from usage.
\\
\hline
Synthetic data & Artificial tabular data with statistical properties (correlations, joint empirical distribution) that mimic those of a real dataset. You use it to augment, balance or anonymize data. Few methods can synthesize outside the range observed in the real data (your training set). I describe how to do it in section 10.4 in~\cite{vgmloptim}. 
A good metric to assess the quality of synthetic data is the full, multivariate 
\textcolor{index}{Kolmogorov-Smirnov distance}\index{Kolmogorov-Smirnov distance!multivariate}, based 
on the \textcolor{index}{joint empirical distribution}\index{empirical distribution!multivariate}\index{ECDF (empirical distribution)} (ECDF) computed both on the real and generated observations. It works both with categorical and numerical features. The word \textcolor{index}{synthetic data}\index{synthetic data} is also used for generated (artificial) time series, graphs, images, videos and soundtracks in multimodal applications.
\\
\hline
token & In LLMs or NLP, a \textcolor{index}{token}\index{token} is a single word; embeddings are vectors, with each component being a token. A word such as ``San Francisco" is a single token, not two. In my LLMs, I use double tokens, such as ``Gaussian distribution" for terms that are frequently found together. I treat them as ordinary (single) tokens. Also, the value attached to a token is its ``correlation" (\textcolor{index}{pointwise mutual information}\index{pointwise mutual information (PMI)}) to the word representing its parent embedding, 
see Figure~\ref{de2pufd1re}. But in traditional LLMs, the value is simply the \textcolor{index}{normalized}\index{normalization} token frequency computed on some text repository.
\\
\hline %---
transformer & A \textcolor{index}{transformer model}\index{transformer} is an algorithm that looks for relationships in sequential data, for instance, words in LLM applications. Sometimes the words are not close to each other, allowing you to detect long-range correlations. It transforms original text into a more compact form and relationships, to facilitate further processing. Embeddings and transformers go together.
\\
\hline
vector search \index{vector database} & A technique combined with 
\textcolor{index}{feature encoding}\index{feature encoding} to quickly retrieve embeddings in LLM summary tables, most similar to prompt-derived embeddings attached to a user query in GPT-like applications. Similar to multivariate ``vlookup" in Excel. A popular metric to measure the proximity between two embeddings is the 
\textcolor{index}{cosine similarity}\index{cosine similarity}. To accelerate \textcolor{index}{vector search}\index{vector search}, especially in real-time, you can 
\textcolor{index}{cache}\index{caching} popular embeddings and/or use approximate search such as \textcolor{index}{ANN}\index{ANN (approximate nearest neighbors)}.\\
\hline
\end{longtblr}
\end{center}

%---------------------------------------

\chapter{Introduction to Extreme LLM and Customized GPT}\label{aasdaaqw}

In this chapter, I discuss elements of architecture related to the \textcolor{index}{large language models}\index{large language models (LLM)} featured in section~\ref{sxllm5}. The goal is to crawl some large websites, and create an application that returns specialized results to user queries or prompts. 
Named \textcolor{index}{xLLM}\index{xLLM}, it involves the following steps.
\vspace{1ex}

\noindent {\bf xLLM architecture: main steps}

%\vspace{1ex}
\begin{itemize}
\item Crawl specialized websites: Wolfram or a major category in Wikipedia. Focus on one top category.
\item Reconstruct the taxonomy and create word associations and keyword \textcolor{index}{embeddings}\index{embeddings}.
\item Parse user queries, retrieve the information, and return results, based on embeddings and other tables. 
\item Augment your data by adding other sources, such as parsed books.
\item Add more top categories, each one with its separate crawling / sources, set of embeddings, and tables.
\item Get the system to self-tune itself based on user feedback (favorite parameter values selected by the users). This leads to user-customized results.
\end{itemize}
\vspace{1ex}
%---

\noindent The keyword used to describe this type of system is \textcolor{index}{RAG}\index{RAG}: retrieve, augment, and generate. The xLLM project~is broken down into major components, with separate pieces of code. In particular:
\vspace{1ex}

\noindent {\bf Python code used in the xLLM project}

\begin{itemize}
\item To read the embedding and other tables, see \texttt{xllm5\_util.py}, is in section~\ref{xllm5util}.
Also on GitHub, \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm5/xllm5_util.py}{here}.

\item The program \texttt{xllm5\_short.py}  reads the tables, process the user queries, and return the results.  It is used in section
 in section~\ref{beurreblanc}, and available on GitHub, \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm5/xllm5_short.py}{here}. 

\item The program \texttt{xllm5.py} reads the crawled data and produces the input tables for 
\texttt{xllm5\_short.py}. It is on GitHub, \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm5/xllm5.py}{here}.  This is the main code, for developers, and discussed in section~\ref{bernoc}.

\item Crawling is done with 
\texttt{crawl\_directory.py}, available \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/crawl_directory.py}{here} and used in 
section~\ref{oudopre}. 
\end{itemize}
\vspace{1ex}

\noindent Section~\ref{mnbrd} is an introduction to the topic. Figures~\ref{fig:log10trmt} -- \ref{fig:log109mt} show how xLLM compares to the Wolfram search box, even though both are based on the exact same content (the Wolfram website). Google is not better than Wolfram search, displaying rudimentary output only, even
 if you ask Google to search Wolfram exclusively. And OpenAI / GPT pictured in Figure~\ref{fig:log109mt65} is not better either. 


% built taxonomy if there is none
% n-grams table + process user queries [last section of the project?]

\section{Best practices}\label{mnbrd}

I explain how to improve the performance of GPT-like apps, both in terms of quality and speed (thus costs), using specialized tables, one set per top category. 
 For instance, if you want to cover AI, machine learning, statistics and mathematics, some of your top categories include machine learning, probability and statistics, discrete mathematics, and so on. Some category overlap is expected. The components below dramatically improve the results.   

\vspace{1ex}
\noindent{\bf Customized Embeddings}. Break down your information database (what you crawled for instance) in 20 or so top categories. Have a separate embedding table for each category. In my case, it’s not just embeddings but other tables such as local taxonomies. Have a separate one for each top category, allow for overlap. In order words, avoid silos: they will dilute the quality of your output. Twenty embedding tables, each with 50k keywords, is better than a single one with 1 million keywords.


%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{clt-xllm.png}  
\caption{Home-made xLLM: results for query ``central limit theorem"}
\label{fig:log10trmt}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\fbox{\includegraphics[width=0.77\textwidth]{clt-wolfram.png}}  
\caption{Wolfram search box: results for query ``central limit theorem"}
\label{fig:log109mt}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\fbox{\includegraphics[width=0.67\textwidth]{clt-gpt.png}}  
\caption{OpenAI GPT: results for query ``central limit theorem"}
\label{fig:log109mt65}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

\vspace{1ex}
\noindent {\bf Variable-length embeddings}. Abbreviated as \textcolor{index}{VLE}\index{embeddings!variable length}~\cite{vle23}. Many systems have a fixed but very large number of tokens per embedding. 
A fixed size may help with \textcolor{index}{KNN vector search}\index{vector search}. But if you only keep the most relevant tokens for each embedding, the size of your tables will decrease
 significantly, boosting scalability. You can still achieve very efficient search: for instance, using the \textcolor{index}{radix search}\index{radix search} algorithm described in section~\ref{radixdw8}. 
 Also, the quantities attached to each token -- the relevancy metric in particular -- do not need to be a frequency  between 0 and 1, or a value between -1 to +1. 
 I use \textcolor{index}{pointwise mutual information}\index{pointwise mutual information (PMI)} instead. It is easier to interpret and compare, especially when you have multiple embedding tables.


\vspace{1ex}
\noindent{\bf High-quality taxonomy}. Creating or relying on a good taxonomy helps you create better embeddings and better results. The words found in category and sub-category titles should be added to the embeddings, with a higher weight. Category titles are cleaner than raw text found on web pages. In case of parsing books, sections and subsection titles could carry a higher weight than raw text. When I crawled Wolfram, I retrieve the full taxonomy with 5000+ entries, all man-made by experts. It is one of the main contributors to the output quality.


\vspace{1ex}
\noindent{\bf Self-tuning}. All GPT-like apps have several parameters, transparent to the user. For instance, the user can’t choose which thresholds to apply to the embeddings. Allow the user to set all the parameters to her liking. This way, you can collect the most popular choices for your parameters, based on user feedback. Of course, this is done automatically on a permanent basis. In the end, you come up with optimum parameters. Trained in real-time by human beings! (this is what I meant by no algorithmic training; it is replaced by humans)

Even better: offer the user the ability to keep his favorite, self-customized set of parameter values. In the end, there is no one-size-fits-all evaluation metric. My xLLM is terrible for the novice looking for basic definitions, while GPT is a lot better. Conversely, for professional users looking for research results or serious references, the opposite is true.

\vspace{1ex}
\noindent{\bf Offer two prompt boxes}. One for the standard query. And one where the user can suggest a category (or two) of his own. You can offer a selection of 10 or 20 pre-selected categories. But you might as well let the user enter a category himself, and process that information as a standard text string. Then match it to existing categories in your system. And then, process the user query or prompt, and match it to the right category to return the most relevant results. Remember, each top category has its own embeddings! You want to use the correct embedding table(s) before returning results.

\vspace{1ex}
\noindent{\bf Multi-agent system}. This is becoming a hot topic! Some say 2024 will be the year of the customized GPT. A 
\textcolor{index}{multi-agent system}\index{multi-agent system} is simply a top layer in your system, controlling all the top categories and embedding tables. In short, it glues the various customized embeddings together, allowing them to “communicate” with each other. In other words, it controls the cross-interactions. It is similar to multimodal (blending images and text) but for text only (blending multiple top categories). Related to \textcolor{index}{mixture of experts}\index{mixture of experts (LLMs)}~\cite{mexperts}. 



\vspace{1ex}
\noindent{\bf Weighted sources}. Your app will blend multiple sources together. Say one of your top categories is statistical science, and it has one specialized embedding table. The content may consist of crawled books and crawled sub-categories both in Wolfram and Wikipedia. Not all sources carry the same weight. You want a well-balanced embedding table. If (say) Wikipedia has more stuff, typically of lower quality, you want to weight that source appropriately.

\vspace{1ex}
\noindent{\bf Find structured data}. The Internet and most websites are considerably more structured than most people think. You just have to find where the structure is hiding. In the case of Wolfram, it comes with a nice taxonomy, among other structures. Wikipedia has its own too. If you crawl books, look for indexes or glossaries and match index terms back to entries in the text. Indexes also have sub-entries and cross-links between entries, that you can leverage.

Even better: each entry (index term) is in some sections or subsections, in addition to being in sentences. Use the table of content and the sectioning as a pseudo-taxonomy. Associate section keywords with index terms found in it. And voila! Now you have strong keyword associations, in addition to the loose associations when focusing on raw (unstructured) text only.



 %%% examples

\section{Python utility for xLLM}\label{xllm5util}

This library contains the functions to read, create, or update embeddings and related tables needed to retrieve data 
 when processing a user query. These tables are stored as dictionaries, also known as hash or key-value databases. The key is either a word consisting of tilde-separated tokens (up to 4 tokens), or a single token. 
The value is either a count, a list or a dictionary itself. 
The tables in question are on GitHub, \href{https://github.com/VincentGranville/Large-Language-Models/tree/main/xllm5}{here}.
The library also contains NLP and transform functions, for instance to turn a list into a dictionary. 
The Python code is also on GitHub, \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm5/xllm5_util.py}{here}.
The library is named \texttt{xllm5\_util.py}.
\vspace{1ex}

\begin{lstlisting}[numbers=left]
import numpy as np
import requests
from autocorrect import Speller
from pattern.text.en import singularize
spell = Speller(lang='en')

#--- [1] functions to read core tables (if not produced by you script)

pwd = "https://raw.githubusercontent.com/VincentGranville/Large-Language-Models/main/llm5/"

#--- [1.1] auxiliary functions

def text_to_hash(string, format = "int"): 
    string = string.replace("'","").split(', ')
    hash = {}
    for word in string:
        word = word.replace("{","").replace("}","")
        if word != "":
            word = word.split(": ")
            value = word[1]
            if format == "int":
                value = int(value)
            elif format == "float":
                value = float(value)
            hash[word[0]] = value
    return(hash)


def text_to_list(string):
    if ', ' in string:
        string = string.replace("'","").split(', ')
    else:
        string = string.replace("'","").split(',')
    list = ()
    for word in string:
        word = word.replace("(","").replace(")","")
        if word != "":
            list = (*list, word)
    return(list)


def get_data(filename, path):
    if 'http' in path: 
        response = requests.get(path + filename)
        data = (response.text).replace('\r','').split("\n")
    else:
        file = open(filename, "r")
        data = [line.rstrip() for line in file.readlines()] 
        file.close()
    return(data)


#--- [1.2] functions to read the tables

def read_table(filename, type, format = "int", path = pwd): 
    table = {}
    data = get_data(filename, path)
    for line in data:
        line = line.split('\t')
        if len(line) > 1:
          if type == "hash":
              table[line[0]] = text_to_hash(line[1], format)
          elif type == "list": 
              table[line[0]] = text_to_list(line[1])
    return(table)


def read_arr_url(filename, path = pwd):
    arr_url = []
    data = get_data(filename, path)
    for line in data:
        line = line.split('\t')
        if len(line) > 1:
            arr_url.append(line[1])
    return(arr_url)


def read_stopwords(filename, path = pwd):
    data = get_data(filename, path)
    stopwords = text_to_list(data[0])
    return(stopwords)


def read_dictionary(filename, path = pwd):
    dictionary = {}
    data = get_data(filename, path)
    for line in data:
        line = line.split('\t')
        if len(line) > 1:
            dictionary[line[0]] = int(line[1]) 
    return(dictionary)


#--- [2] core function to create/update dictionary and satellite tables

def trim(word):
    return(word.replace(".", "").replace(",",""))


def reject(word, stopwords):

    # words can not contain any of these
    # note: "&" and ";" used in utf processing, we keep them 
    flaglist = ( "=", "\"", "(", ")", "<", ">", "}", "|", "&quot;", 
                 "{", "[", "]", "^", "/", "%", ":", "_", 
                )

    # words can not start with any of these chars
    bad_start = ("-",)

    rejected = False
    for string in flaglist:
        if string in word:
            rejected = True
    if len(word) == 0:
        rejected = True
    elif word[0].isdigit() or word[0] in bad_start:
        rejected = True
    if word.lower() in stopwords:
        rejected = True
    return(rejected)


def create_hash(list): 
    hash = {}
    for item in list: 
        if item in hash:
            hash[item] += 1
        elif item != "":
            hash[item] = 1
    return(hash)


def update_hash(word, hash_table, list):
    if list != "":
        hash = hash_table[word]
        for item in list:
            if item in hash:
                hash[item] += 1
            elif item != "":
                hash[item] = 1
        hash_table[word] = hash
    return(hash_table)  


def add_word(word, url_ID, category, dictionary, url_map, hash_category, 
             hash_related, hash_see, related, see, word_pairs, word_list):

    # word is either 1-token, or multiple tokens separated by ~

    urllist = (str(url_ID),) 

    if word in dictionary:

        dictionary[word] += 1
        url_map = update_hash(word, url_map, urllist) 
        hash_category = update_hash(word, hash_category, category) 
        hash_related = update_hash(word, hash_related, related) 
        hash_see = update_hash(word, hash_see, see) 

    else: 

        dictionary[word] = 1 
        urlist = (url_ID,)
        url_map[word] = create_hash(urllist)  
        hash_category[word] = create_hash(category)
        hash_related[word] = create_hash(related)   
        hash_see[word] = create_hash(see)

    # generate association between 2 tokens of a 2-token word 
    # this is the starting point to create word embeddings
 
    if word.count('~') == 1:

        # word consists of 2 tokens word1 and word2
        string = word.split('~')
        word1 = string[0]
        word2 = string[1]

        pair = (word1, word2)
        if pair in word_pairs:
            word_pairs[pair] += 1
        else:
            word_pairs[pair] = 1
        pair = (word2, word1)
        if pair in word_pairs:
            word_pairs[pair] += 1
        else:
            word_pairs[pair] = 1

        if word1 in word_list:
            word_list[word1] = (*word_list[word1], word2)
        else:
            word_list[word1] = (word2,)
        if word2 in word_list:
            word_list[word2] = (*word_list[word2], word1)
        else:
            word_list[word2] = (word1,)

    return()


def stem_data(data, stopwords, dictionary, mode = 'Internal'):

    # input: raw page (array containing the 1-token words)
    # output: words found both in singular and plural: we only keep the former
    # if mode = 'Singularize', use singularize library
    # if mode = 'Internal', use home-made (better)

    stem_table = {}
    temp_dictionary = {}

    for word in data:
        if not reject(word, stopwords):
            trim_word = trim(word)  
            temp_dictionary[trim_word] = 1

    for word in temp_dictionary:
        if mode == 'Internal': 
            n = len(word)
            if n > 2 and "~" not in word and \
                  word[0:n-1] in dictionary and word[n-1] == "s":
                stem_table[word] = word[0:n-1]
            else:
                stem_table[word] = word
        else:
            # the instruction below changes 'hypothesis' to 'hypothesi'
            word = singularize(word)

            # the instruction below changes 'hypothesi' back to 'hypothesis'
            # however it changes 'feller' to 'seller' 
            # solution: create 'do not singularize' and 'do not autocorrect' lists
            stem_table[word] = spell(word) 

    return(stem_table)


def update_core_tables(data, dictionary, url_map, arr_url, hash_category, hash_related, 
                       hash_see, stem_table, category, url, url_ID, stopwords, related, 
                       see, word_pairs, word_list):

    # data is a word array built on crawled data (one webpage, the url)
    # url_ID is incremented at each call of update_core_tables(xx)
    # I/O: dictionary, url_map, word_list, word_pairs, 
    #       hash_see, hash_related, hash_category
    # these tables are updated when calling add_word(xxx)
    
    arr_word = []  # list of words (1 to 4 tokens) found on this page, local array
    k = 0

    for word in data:

        if not reject(word, stopwords):

            raw_word = word
            trim_word = trim(word) 
            trim_word = stem_table[trim_word]

            if not reject(trim_word, stopwords):

                arr_word.append(trim_word)  
                add_word(trim_word, url_ID, category, dictionary, url_map, hash_category, 
                         hash_related, hash_see, related, see, word_pairs, word_list)

                if k > 0 and trim_word == raw_word:
                    # 2-token word
                    if arr_word[k-1] not in trim_word:
                        word = arr_word[k-1] + "~" + trim_word
                        add_word(word, url_ID, category, dictionary, url_map, hash_category, 
                                 hash_related, hash_see, related, see, word_pairs, word_list)

                if k > 1  and trim_word == raw_word:
                    # 3-token word
                    if arr_word[k-2] not in word:
                        word = arr_word[k-2] + "~" + word
                        add_word(word, url_ID, category, dictionary, url_map, hash_category, 
                                 hash_related, hash_see, related, see, word_pairs, word_list)

                if k > 2  and trim_word == raw_word:
                    # 4-token word
                    if arr_word[k-3] not in word:
                        word = arr_word[k-3] + "~" + word      
                        add_word(word, url_ID, category, dictionary, url_map, hash_category, 
                                 hash_related, hash_see, related, see, word_pairs, word_list)
                k += 1

    arr_url.append(url)
    url_ID += 1   
    return(url_ID)


#--- [3] simple text processsing

def collapse_list(list):  
    # group by item and get count for each item
    clist = {}
    for item in list:
        if item in clist:
            clist[item] += 1
        elif item != '': 
            clist[item] = 1
    return(clist)


#--- [4] create embeddings and ngrams tables, once all sources are parsed

def create_pmi_table(word_pairs, dictionary):

    pmi_table  = {}     # pointwise mutual information 
    exponent = 1.0

    for pair in word_pairs:

        word1 = pair[0]
        word2 = pair[1]
        f1 = dictionary[word1] / len(dictionary)
        f2 = dictionary[word2] / len(dictionary)
        f12 = word_pairs[pair] / len(word_pairs) 
        pmi = np.log2(f12 / (f1 * f2)**exponent) 
        word2_weight =  word_pairs[pair] / dictionary[word1]
        pmi_table[pair] = pmi 

    return(pmi_table)


def create_embeddings(word_list, pmi_table): 

    embeddings = {}

    for word in word_list:

        list = word_list[word]
        clist = collapse_list(list)
        embedding_list = {}

        for word2 in clist:
            count = clist[word2] 
            pair =  (word, word2)

            if pair in pmi_table:

                pmi = pmi_table[pair]
                embedding_list[word2] = pmi

        embeddings[word] = embedding_list

    return(embeddings)


def build_ngrams(dictionary):

    ngrams_table = {}
    for word in dictionary:
        tokens = word.split("~")
        tokens.sort()
        sorted_word = tokens[0]
        for k in range(1, len(tokens)):
            sorted_word += "~" + tokens[k] 
        if sorted_word in ngrams_table:
            ngrams_table[sorted_word] = (*ngrams_table[sorted_word], word,)
        else:
            ngrams_table[sorted_word] = (word,) 
    return(ngrams_table)


def compress_ngrams(dictionary, ngrams_table):
    # for each sorted_word, keep most popular ngram only

    compressed_ngrams_table = {}
    for sorted_word in ngrams_table:
        ngrams = ngrams_table[sorted_word]
        max_count = 0
        for ngram in ngrams:
            if dictionary[ngram] > max_count:
                max_count = dictionary[ngram]
                best_ngram = ngram
        compressed_ngrams_table[sorted_word] = (best_ngram, )
    return(compressed_ngrams_table) 

\end{lstlisting}

\section{Comparing xLLM with standard LLMs}\label{ruipof}

The xLLM architecture is so different in so many foundational respects that it is almost a different animal. Here, I highlight some important differences. 
Figure~\ref{fig:log1frdiag} shows that embeddings are just one of the many summary tables needed  to generate answers to user prompts. Each table is unique to each sub-LLM, though redundancy is allowed. Even for embeddings, the storage is non-standard, based on variable-length and key-values or graph databases, rather than vector databases.   Not the least, xLLM is energy friendly, as it does not need GPU, neural networks, training (self-tuned instead), or trillions of weights.


\begin{figure}[H]
\centering
\includegraphics[width=0.77\textwidth]{xLLM-diagram.png}
\caption{xLLM: Satellite tables and variable-length embeddings (one set per sub-LLM)}
\label{fig:log1frdiag}
\end{figure}

%xxx 
%---

\begin{center}
\begin{longtblr}{|p{\dimexpr8.3cm-2\tabcolsep}|p{\dimexpr8.3cm-2\tabcolsep}|}
\hline 
xLLM
& 
Standard LLM
\\
\hline
\hline
Multiple specialized LLMs: one per category to increase accuracy and relevance as well as latency while mitigating hallucination and therefore liability &
One monolithic LLM, all users get same output; prompt engineering (front-end) needed to fix the resulting back-end problems
\\
\hline
Raw input + reconstructed (or discoverable) taxonomy to potentially provide a knowledge graph; UI navigation feature to further increase accuracy and relevancy &
Based on raw input only
\\
\hline
Embeddings is just one of the main summary tables to increase accuracy and relevancy while reducing latency and cost &
Embeddings is the core summary table; no effort to discover or leverage internal structure of the corpus (input sources)
\\
\hline
User enters prompt and selected categories to increase accuracy and relevancy &
User enters prompt only; results in slower response (latency) as data retrieval is not targeted to specific tables
\\
\hline
Internal relevancy score attached to each item displayed to the user &
No relevancy score provided to the user
\\
\hline
Output broken down into sections, links, categories, related concepts, internal versus external sources to reduce liability, increase explainability and observability while mitigating hallucinations &
In many instances, links not provided (input sources kept secret resulting in liability issues)

\\
\hline
User can customize hyperparameters and relevancy metrics to provide customized calibration &
Hyperparameters not accessible to user resulting in training issues

\\
\hline
Fast and variable-length embeddings to reduce latency, mitigate hallucination and reduce liability &
Massive tables, trillions of weights, fixed-length embeddings slowing down vector search, delicate and time consuming training with deep neural networks

\\
\hline
Inexpensive, lightweight, energy-efficient, ideal for implementation to increase user customization and diverse user scalability &
Needs lots of GPU and cloud time, thus expensive especially if hosted externally; not eco-friendly

\\
\hline
Self-tuned with reinforcement learning based on user preferences &
There is no universal evaluation metric because satisfaction depends on type of user

\\
\hline
Explainable AI (meaningful parameters) &
Neural networks are blockboxes, notorious for lack of explainability

\\
\hline
Consise output targeted to professional users, faithful representation of input text elements &
Excessive stemming and glitches in standard libraries (autocorrect, singularize, stopwords) leads to hallucinations, prompt engineering needed tofix issues

\\
\hline
Ability to orchestrate outcomes employing a multi-agent architecture &
One LLM: no top layer to manage multiple LLMs

\\
\hline
Ability to process outcomes from multiple prompts in parallel or sequentially &
Bulk processing only available in paid version via API


\\
\hline
Secure, reduced risk of hallucination or data leakage, especially in local implementations &
Hallucinations may lead to errors and liability; confidential input data in prompts may not be protected
\\
\hline
\end{longtblr}
\end{center}

\section{Comparing xLLM5 with xLLM6}\label{xllm6tr}

Unless otherwise specified, all LLM projects in this textbook -- \ref{bernoc} and \ref{beurreblanc} in particular -- are based on xLLM5. The code and datasets are also on GitHub, 
\href{https://github.com/VincentGranville/Large-Language-Models/tree/main/xllm5}{here}. 
A new version, namely xLLM6, is now available and still 
being further developed. I did not include the code in this book, but you can find it on GitHub, 
\href{https://github.com/VincentGranville/Large-Language-Models/tree/main/xllm6}{here}.  The overall architecture is similar, with the following main differences:
\vspace{1ex}

\begin{itemize}
\item The introduction of multi-token words in embedding tables. The new embeddings, called
\textcolor{index}{x-embeddings}\index{embeddings!x-embeddings}, are stored in a key-value table named \texttt{embeddings2} in the Python code.
The key is a \textcolor{index}{multi-token}\index{token!multi-token} word, and the value is a key-value table itself. 
The old embeddings are still stored in the
 \texttt{embeddings} table with the same data structure; they may be discarded moving forward. 
See the x-embeddings on GitHub, \href{https://raw.githubusercontent.com/VincentGranville/Large-Language-Models/main/xllm6/xllm6_embeddings2.txt}{here}, compared to the old embeddings, \href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm6/xllm6_embeddings2.txt}{here}. 
\vspace{1ex}
\item A new counter named \texttt{paragraph} in the function \texttt{update\_core\_tables2} in \texttt{xllm6\_util.py}. This new function replaces the old version
\texttt{update\_core\_tables} in xLLM5. Most of the main upgrades are implemented there, including a call to new functions 
\texttt{update\_word2\_hash} and 
\texttt{update\_word2\_pairs}.

\noindent Words need to be in a same paragraph to appear jointly in x-embeddings. Long-range dependencies are still included, via categories,
 breadcrumbs, metadata, and other navigation features. The \texttt{word2\_hash} table, a precursor to the x-embeddings, replaces the old 
\texttt{world\_list} table: not only the format is streamlined (key-value hash rather than list), but it now handles multi-token words. Due to the
 potential explosion in size, the shorter version \texttt{compressed\_word2\_hash} is the central table. 
The \texttt{word2\_pairs} table contains similar content, but organized in a different way to facilitate taxonomy creation. All these tables are local to a
 specific top category, in other words, to a sub-LLM in a multi-LLM architecture. 

\noindent See on GitHub the \texttt{compressed\_word2\_hash}, 
\href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm6/xllm6_compressed_word2_hash.txt}{here}, and the  \texttt{word2\_pairs}, 
\href{https://github.com/VincentGranville/Large-Language-Models/blob/main/xllm6/xllm6_word2_pairs.txt}{here}.
\vspace{1ex}

\item I also added two sections to the output results delivered to user prompts: ``x-Embeddings", and ``Linked Words". The former is based
 on \textcolor{index}{pmi}\index{pointwise mutual information (pmi)} (pointwise mutual information) just like in the old ``Embeddings" section, and the latter on raw word counts.  
\vspace{1ex}
\item The stopwords list, again specific to each sub-LLM, can now handle multiple-token words, with tokens separated by the tilde character ``$\sim$" within a same word.
These lists are built by looking at meaningless or useless words with highest counts in the \texttt{dictionary} table. They significantly contribute to
 improving the quality of embeddings, and need to be built from scratch. Do not use stopwords from Python libraries unless you blend them with
 customized do-not-exclude lists. 

\vspace{1ex}
\end{itemize}



\subsection{Conclusion}

The x-embeddings, made up of multi-token words, is a significant improvement over standard single-token embeddings.
They can dramatically increase the number of weights, from $\num{5000}$ to over 1 million for a specialized sub-LLM
 corresponding to a top category. Applied to the whole human knowledge, it would result in a few trillion weights.   

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{xembeddings2.png}
\caption{xLLM6, top results for ``hypothesis"}
\label{fig:xembeddings}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------


However, much of it is garbage and never fetched during similarity search 
to match front-end user prompts with back-end x-embeddings built on crawls. In short, there is a waste of space, computer time,  and bandwidth. In addition, it can result in hallucinations in standard LLMs (OpenAI and the likes). Compressed tables extract the essence and are much smaller. Compression is
 achieved via stopwords and by not linking words together unless they are found in a same, short paragraph. Of course, besides x-embeddings,
 xLLM uses many other small tables that take care of long-range word dependencies, such as taxonomies. Much of the compression results from using 
 a separate LLM for each top category, allowing the user to select categories. 

While Figure~\ref{fig:xembeddings} mostly shows 1-token words in the x-embeddings, you can change the parameters to favor multi-token words. For instance, ignoring single-token words, doubling the count (weight) attached to multi-token words, or doubling the count when a word is found in the title, taxonomy, or metadata as opposed to the main text. The latter approach also favors natural \textcolor{index}{$n$-grams}\index{$n$-gram}: those with highest occurrences among all permutations of a same multi-token word. For instance, ``statistical hypothesis" favored over ``hypothesis statistical". To further boost natural $n$-grams, consider the following two options: 
\vspace{1ex}
\begin{itemize}
\item Return to the front-end user prompt the most common $n$-gram permutation attached to any multi-token word, based on counts stored in the \texttt{dictionary} table. 
\item Or  normalize $n$-grams to their most common permutation, in the various back-end tables. To perform this task, the hash table
 \texttt{compressed\_ngrams\_table} is very useful. Its key is a sorted $n$-gram, while the corresponding value is a vector 
 featuring the permutations of the $n$-gram in question, each with its own count. Normalized $n$-grams further reduces the size 
of the largest summary tables such as \texttt{word2\_pairs}. 
\end{itemize}
\vspace{1ex}
Finally, you can use the \texttt{dictionary} and \texttt{word2\_pairs} tables to create a \textcolor{index}{taxonomy}\index{taxonomy creation}, if you don't already have one. Top multi-token dictionary entries, say \texttt{word$_1$} and \texttt{word$_2$},  where \texttt{(word$_1$, word$_2$)} is not a key of \texttt{word2\_pairs},  correspond to different categories. To the contrary,
if \texttt{word2\_pairs[(word$_1$, word$_2$)]} is a large integer (counting the simultaneous occurrences of both words across many paragraphs), it means that
\texttt{word$_1$} and \texttt{word$_2$} belong to the same category. Based on this, you can reconstruct the underlying taxonomy. The first step is to ignore top
words not representing any category. Also, words found in titles should be given extra weight. 

It is interesting to note that despite not using a synonyms or abbreviations dictionary yet, some of the issues in xLLM5, such 
as ``ANOVA" and
``analysis of variance", or ``Bayes" and ``Bayesian" not returning similar results, are not present in xLLM6. 
Other improvements not yet implemented consist of handling capital letters separately, and treating words such as
``Saint Petersburg" as one token. To conclude, Figure~\ref{fig:xembeddings} displays only the top 10 entries in each section,
ordered by relevancy. Some entries not shown because not in the top 10, are quite relevant. Fine-tuning the
 \textcolor{index}{hyperparameters}\index{hyperparameter} may result in a different ordering, better or worse depending on the user.







%------------------------------------------------------------------
\begin{comment}

\chapter{Random Walks, Brownian Motions, and Related Stochastic Processes}\label{ch1}



\begin{Exercise} {\em Statistical estimations} -- Simulate $10^6$ realizations of a Brownian motion with $0\leq t \leq 1$, using the random walk construction described previously. Study the distribution of the following quantities, using computations averaged across all your simulations. In particular, what is the mean and variance, for the following quantities:
\begin{itemize}
	\item Extreme values: $\min Z_t$, $\max Z_t$, 
	\item Proportion of the time when $Z_t > 0$ (note that $Z_0 = 0$),
	\item Number of times when the sign of $Z_t$ changes.
\end{itemize}
Keep in mind that the $Z_t$'s are auto-correlated. Given a particular realization of a stochastic process, these statistics can be used to check if it is a Brownian motion or not. Another interesting exercise is to study the process in question if the variable $U_1$ does not have a variance, for instance if $U_1$ has a Cauchy distribution. 
\end{Exercise}


\section{Integration, differentiation, moving averages}\label{movbc}

Let's use the construction scheme in section~\ref{bmrwp} to build a Brownian motion $\{Z_t\}$.
The underlying time-discrete random walk $\{X_k\}$ is referred to as the base process. 
I introduce two transformations:\vspace{1ex}
\begin{itemize}
	\item The cumulative or \textcolor{index}{integrated process}\index{integrated process} $\{ S_t \}$ derived from $\{ Z_t \}$,
	\item The theoretical \textcolor{index}{moving average process}\index{moving average process} $\{ M_t \}$ derived from $\{ Z_t \}$.
\end{itemize}\vspace{1ex}
The inverse of integration is differentiation: the \textcolor{index}{differentiated}\index{differentiated process} $\{S_t\}$ is $\{Z_t\}$. In practice, the smoother process (integrated or moving average) is easier to study and sometimes displays patterns that can't be identified in the original process. 
To build the processes in question, proceeed as follows.
In the construction of the Brownian motion described in section~\ref{bmrwp}, replace 
$X_n = U_1 + \cdots + U_n$ by 
$X_n = V_1 + \cdots + V_n$, where $V_k$ is described below for each transformation. \vspace{1ex}
\begin{itemize}
	\item Integration: $V_k = U_1 + \cdots + U_k$.
	\item Differention: $V_k = U_{k+1} - U_k$. If $\{ Z_t \}$ is a Brownian motion  then the resulting process is a white noise: nowhere continuous, nowhere differentiable.
	\item Moving average: $V_k = U_k + U_{k + 1} + \cdots + U_{k + h(k)}$ where $h(k)$ is as small as possible to make the resulting process continuous and differentiable everywhere.  
\end{itemize}\vspace{1ex}
For moving averages applied to a Brownian motion, $h(k) = \lfloor \sqrt{k}\rfloor$ works. Here $\lfloor \cdot \rfloor$ stands for the integer part function. Does $h(k) = \lfloor \log k \rfloor$ work? This would make the resulting process far more similar to the original one, but maybe barely (if at all) continuous -- in other words, more chaotic than with $h(k) =  \lfloor\sqrt{k} \rfloor$.

\begin{figure}[H]
\centering
\includegraphics[width=0.78\textwidth]{ma2.PNG} %0.86
\caption{Brownian motion (green), integrated (orange) and moving average (red)}
\label{fig:trc}
\end{figure}

As in section~\ref{bmrwp}, you need to use the correct rescaling of the vertical axis to obtain meaningful variances that do not depend on $n$.
 I illustrate this for the integrated process. In this case we have:
$$
X_n = \sum_{k=1}^n V_k =\sum_{k=1}^n k\cdot U_{n-k+1},
$$
thus
$$
\text{Var}[X_n] = \sum_{k=1}^n k^2 \text{Var}[U_1] =\frac{n(n+1)(2n+1)}{6}\text{Var}[U_1]\sim \frac{n^3}{3}\text{Var}[U_1].
$$
The rescaling of the horizontal time axis is the same. Thus, the proper rescaling factor for the vertical axis, as $n$ tends to infinity, is  
$X_n / \sqrt{n^3/3}$. This leads to 
\begin{equation}
\text{Var}[S_t] = t^3 \cdot \text{Var}[U_1].\label{eq2}
\end{equation}

The same logic applies to compute $\text{Var}[M(t)]$. The details are left as an exercise. A more complicated exercise consists of computing the covariance between $S_t$ and $S_{t + s}$ for $s > 0$, and proving that $\{S_t\}$ is {\em not} a Brownian motion itself (being differentiable everywhere unlike Brownian motions.) 

Figure~\ref{fig:trc} was produced with the Python code in this section. It represents one realization of a Brownian motion $\{Z_t\}$, together with its integration $\{S_t\}$ and moving average $\{M_t\}$. The time period is $0\leq t \leq 5$. The Python program \texttt{brownian.py} is also on my
GitHub directory, \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/Brownian.py}{here}. 
The Brownian motion, integrated Brownian and moving average are denoted respectively as \texttt{X}, \texttt{S} and \texttt{M} in the code,
 while \texttt{T} is the time. 

Different metrics exist to characterize the smoothness stochastic process is. The 
\textcolor{index}{Hurst exponent}\index{Hurst exponent} [\href{https://en.wikipedia.org/wiki/Hurst_exponent}{Wiki}] is a well known one, measuring the amount of long-term memory in your process. It is equal to $\frac{1}{2}$ for a Brownian motion or 
 \textcolor{index}{Brown noise}\index{Brown noise} [\href{https://en.wikipedia.org/wiki/Brownian_noise}{Wiki}], and to 0
 for a \textcolor{index}{pink noise}\index{pink noise} [\href{https://en.wikipedia.org/wiki/Pink_noise}{Wiki}] and for a 
one-dimensional \textcolor{index}{white noise}\index{white noise} [\href{https://en.wikipedia.org/wiki/White_noise}{Wiki}]. The higher the value, the smoother the process.  When the parameter \texttt{smooth} in the Python code is increased,
 the resulting simulation is smoother.  Another methodology to generate integrated (smooth) processes is discussed in my book on synthetic data~\cite{vgsynthetic}, in the chapter dealing with linear algebra and auto-regressive time series. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.9\textwidth]{linear.png}  
\caption{Integrated Brownian (top left), Brownian (top right) and nowhere continuous (bottom)}
\label{fig:lollog1xx}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

There are many different ways to simulate Brownian motions and related processes. Figure~\ref{fig:lollog1xx} is based on simple 
\textcolor{index}{auto-regressive time series}\index{autoregressive models} [\href{https://en.wikipedia.org/wiki/Autoregressive_model}{Wiki}], properly scaled to make them time-continuous. The technique is described in the chapter ``Gentle Introduction to Linear Algebra -- Synthetic Time Series" in my book on synthetic data~\cite{vgsynthetic}. The originality here consists in choosing auto-regressive models where  some of the roots of the \textcolor{index}{characteristic polynomial}\index{characteristic polynomial} 
 [\href{https://en.wikipedia.org/wiki/Characteristic_polynomial}{Wiki}] are multiple, in particular roots with 
modulus equal to 1 (the ``largest" roots). The processes in the top part of Figure~\ref{fig:lollog1xx} are standard: roots with multiplicity 1 yield Brownian motions, multiplicity 2 yields integrated Brownian, multiplicity 3 yields doubly integrated Brownian and so on. In the bottom part, the underlying time series used in the simulation has $X_n$ depends on $X_{n-2}$, $X_{n-3}$ and so on, but not
 on $X_{n-1}$. The resulting processes are non-Brownian. Indeed they look like a ``derivative" of a Brownian: a non-standard function that densely fills some peculiar domain, resulting in a shape with a \textcolor{index}{fractal dimension}\index{fractal dimension}
 [\href{https://en.wikipedia.org/wiki/Fractal_dimension}{Wiki}] between 1 and 2: something intermediate between a curve and a surface.

The following Python script \texttt{Brownian.py} is used to produce Figure~\ref{fig:trc}.  It is on 
GitHub, \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/Brownian.py}{here}. \vspace{1ex}

%https://www.youtube.com/watch?v=W9jktqV3_Mc brownian wolfram alpha look for api

\begin{lstlisting}
import numpy as np
import matplotlib.pyplot as plt

n = 10000
m = 5*n
T = []
X = []
T.append(0.0)
X.append(0.0)
np.random.seed(1979)
for k in range(1,m):
    u = np.random.uniform(0,1)
    if u < 0.5:
        X.append(X[k-1]-1)
    else:
        X.append(X[k-1]+1)
    T.append(T[k-1] + 1/n)

S = []
S.append(0.0)
for k in range(1,m):
    S.append(X[k]+S[k-1])

M = []
smooth = 2.5  # the larger, the smoother the moving average
M.append(0.0)
hn = int(smooth*np.sqrt(n))
for k in range(1,m):
    sum = 0.0
    for h in np.arange(-hn, hn+1):
        idx = k + h
        if idx >= m:  # fix for index outside the array range
            idx = m - 1 - (idx % n)
        elif idx < 0: # fix for index outside the array range
            idx = -idx
        sum += X[idx]
    sum /= (2*hn + 1)
    M.append(sum)

for k in range(1,m):
    X[k] = X[k]/(n**0.5)
    S[k] = S[k]/(n**1.5)
    M[k] = M[k]/(n**0.5)

axes = plt.axes()
axes.tick_params(axis='both', which='major', labelsize=8)
axes.tick_params(axis='both', which='minor', labelsize=8)
for axis in ['top','bottom','left','right']:
    axes.spines[axis].set_linewidth(0.5) 
plt.plot(T, X, linewidth = 0.4, color = 'green', alpha = 0.2)   # Brownian motion
plt.plot(T, S, linewidth = 0.8, color = 'orange', alpha = 0.8)  # integrated Brownian motion
plt.plot(T, M, linewidth = 0.8, color = 'red', alpha = 1.0)     # moving average process
plt.axhline(y = 0.0, color = 'grey', linestyle = '--', linewidth = 0.4)
plt.show()
\end{lstlisting}

\section{Reflected random walks}\label{rflectr}

The goal here is to introduce the reader to a bounded process, where moves up or down are allowed or not based on the current state. It mimics an environment where constraints prevent the system from going too far up too quickly, or the other way around. Thus such processes do not exhibit massive explosions or implosions. 

\begin{figure}%[H]
\centering
\includegraphics[width=0.85\textwidth]{ma4.PNG} %0.86
\caption{Reflected random walk with $a=b=\frac{1}{2}$}
\label{fig:ivf}
\end{figure}



Our \textcolor{index}{reflected random walk}\index{reflected random walk} [\href{https://en.wikipedia.org/wiki/Reflected_Brownian_motion}{Wiki}] is defined as follow. Start with $X_0=0$ and then

\begin{equation}
X_k = \begin{cases}X_{k-1} + U_k\cdot k^{-a} & \text{if } X_{k-1} < 0, \\
                      X_{k-1} - U_k\cdot k^{-a}                                   & \text{if }   X_{k-1} \geq 0,%
        \end{cases}\label{poputres}
\end{equation}
%\begin{align}
%X(k)   & =  X(k-1) + \frac{U(k)}{k^a} \quad \text{ if } X(k-1) < 0,  \nonumber \\
% X(k)  & = X(k-1) - \frac{U(k)}{k^a} \quad \text{ if } X(k-1) \geq 0, \nonumber 
%\end{align}
where $U_k = V^b_k$, the $V_k$'s are uniform independent deviates on $[0, 1]$ and $a\geq 0, b>0$ are two parameters.
Also define $Z_k=k^a X_k$.
The distribution of $Z_k = k^a X_k$ is stable over time, in contrast to Brownian motions that lack stationarity unless normalized. For the reflected random walk, the limiting distribution of $Z_k$ -- called the \textcolor{index}{invariant measure}\index{invariant measure} [\href{https://en.wikipedia.org/wiki/Invariant_measure}{Wiki}] 
 or \textcolor{index}{attractor distribution}\index{attractor distribution} in dynamical systems -- is not Gaussian. This is another indication that the process is not Brownian. 

\begin{figure}[H]
\centering
\includegraphics[width=0.80\textwidth]{ma3.PNG} %0.86
\caption{Invariant measure (density function) of reflected random walk with $a=b=\frac{1}{2}$}
\label{fig:iv}
\end{figure}

However it satisfies
 the \textcolor{index}{stochastic integral equation}\index{stochastic integral equation}~(\ref{seqa}), which has an exact solution. For this type of
 \textcolor{index}{functional equations}\index{functional equation} [\href{https://en.wikipedia.org/wiki/Functional_equation}{Wiki}], the unknown is a probability distribution: here, the attactor distribution. The solution is also referred to as a 
\textcolor{index}{fixed point}\index{fixed point algorithm} [\href{https://en.wikipedia.org/wiki/Fixed_point_(mathematics)}{Wiki}], in this case of infinite dimension. It may not be unique.


 Let $Z$ be the limit of $Z_k$ as $k\rightarrow\infty$.  What is the
 distribution of $Z$? Let $F_Z$ and $f_Z$ denote respectively the CDF (cumulative distribution function) and density function attached to $Z$.
 Likewise, $F_U$ and $f_U$ is the common CDF and density attached to the $U_k$'s. To find $f_Z$ you need to solve the following integral equation where $f_Z$ is the unknown:
\begin{equation}
f_Z(z) = \int_0^1 \Big[f_U(x+z) + f_U(x-z)\Big]\cdot f_Z(x) dz. \label{seqa}
\end{equation}

Figure~\ref{fig:iv} shows an approximation to the solution computed on $2\times 10^6$ iterates of Formula~(\ref{poputres}) with 
$a=b=\frac{1}{2}$. In this case, the exact density function is known and equal to 
$$
f_Z(z) = \frac{b+1}{2}\cdot \Big(1-|z|^{1/b}\Big) = \frac{1-F_U(z)}{2 \text{E}[U]}, \quad -1\leq z \leq 1.
$$
This is the correct solution if $b=1,b=\frac{1}{2}$, or $b\rightarrow 0$, regardless of $0<a<1$. It can be verified by plugging this solution in Formula~(\ref{seqa}). I haven't checked if the formula is still valid for other values of $b$. It is easy to empirically obtain the following result, 
 based on observations over a very large time period:
$$
\text{Var}[Z] = \frac{b+1}{3(3b+1)}.
$$
Of course $\text{E}[Z]=0$. Figure~\ref{fig:ivf} shows one realization of a reflected random walk (the first $2000$ values of $Z_k$) when $a=b=\frac{1}{2}$. The Python code to produce Figures~\ref{fig:ivf} and ~\ref{fig:iv} is on my GitHub repository 
 \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/brownian_reflective.py}{here}, and also in 
 section~\ref{dupuis}.

\subsection{Exercises}

The purpose of this section is to explore particular cases of reflective random walks, corresponding to specific values of $a$ and $b$. 
In Exercise~\ref{knorr}, the formula for the integral equation is established. 

\begin{Exercise} {\em Reflective random walks: special cases} -- Prove that if $0<a < 1$, then  $X_k\rightarrow 0$ as
$k\rightarrow\infty$. Under the same condition, prove that the limiting distribution of $Z$
\begin{itemize}
\item	always exists and its support domain is $[-1, 1]$,
\item is symmetric, with mean and median equal to 0,
\item does not depend on $a$, but only on $b$.
\end{itemize}
For instance, if $b =1$, the distribution of $Z$ is triangular regardless of $a$. If $a < 1$ and $b = 0$, (the non-stochastic case) prove that 
$Z$ can only take on 3 values: $-1,+1$ and $0$ respectively with probability $\frac{1}{4},\frac{1}{4}$ and $\frac{1}{2}$.
Also show that when $b\rightarrow 0^{+}$,  the distribution of Z converges to a uniform distribution on $[-1, 1]$. 
When $b=0$ (the non-stochastic case), we also have the following cases:
\begin{itemize}
\item if $a=1$, $X_k\rightarrow 0$,
\item 	if $a = 3$, $X_k\rightarrow \zeta(3) -\frac{5}{4}\approx -0.048$,   
\item	if $a = 4$, $X_k\rightarrow \zeta(4) -\frac{9}{8}\approx -0.043$.
\end{itemize}
Here $\zeta(\cdot)$ is the \textcolor{index}{Riemann zeta function}\index{Riemann zeta function} [\href{https://en.wikipedia.org/wiki/Riemann_zeta_function}{Wiki}]. 
\end{Exercise}

\begin{Exercise}\label{knorr} {\em Establishing the integral equation} -- Prove that the density $f_Z(z)$ attached to the limiting distribution must
satisfy~(\ref{seqa}). \vspace{1ex} \\ 
{\bf Solution} \\
Based on Formula~(\ref{poputres}), we have
$$
P(X_k<z) = \int_{-1}^1 P(X_k < z | X_{k-1} = x) f_{X_{k-1}}(x) dx.
$$
Let $U$ be a random variable with the same distribution as any $U_k$, and let $k\rightarrow\infty$. Separating the cases $x<0$ and $x\geq 0$, we have:
$$
P(Z<z) = \int_0^1 P(U > x-z )f_Z(x)dx + \int_{-1}^0 P(U < z-x )f_Z(x)dx.
$$  
Taking advantage of the symmetries of the problem, this can be further simplified to
$$
F_Z(z) = \frac{1}{2} + \int_0^1 \Big[F_U(x+z)-F_U(x-z)\Big] f_Z(x)dx.
$$
Finally, taking the derivative with respect to $z$ on both sides of the previous equality, we obtain the desired integral equation for $f_z$. \qed
\end{Exercise}


%------

\begin{Exercise} \label{exensd}{\em Numerical instability of chaotic dynamical systems} --- As discussed in section~\ref{thorium}, the computation
 of $X_k$ using the iterative map $X_{k+1}=g(X_k)$ quickly results in total loss of accuracy after as little as 45 iterations, as rounding errors propagate exponentially fast. Indeed, the problem is similar to computing the first $1000$ binary digits of $\sqrt{2}$ if your machine precision is limited to 32 bits. How would you test the accuracy of your computations, especially when working with very high (yet finite) precision? 
\vspace{1ex}

\noindent {\bf Solution} \\
One way assess \textcolor{index}{numerical instability}\index{numerical stability} is to pretend -- by truncating and rounding the values of $X_k$ -- that your machine has a precision of only 3 digits. Then do the same, pretending the precision is 6 digits, then 12 digits. Comparing the results obtained with 3-digit versus 6-digit or 12-digit precision, you can tell how far (that is, how many iterations) you can go to preserve the desired accuracy in the sequence $\{X_k\}$. The idea is to incrementally increase the precision until you see no more improvements. This is the actual limit of your machine or library function that you use. In my case, I found in some older system that increasing the precision from 180 to 200 or 300 digits had no effect. This meant that no matter what is advertised in the documentation (or possibly due to some glitches of my own), the accuracy of my results was limited to 180 digits.
\end{Exercise}

\begin{Exercise} {\em Khinchin's constant} -- Let $a_1,a_2$ and so on be the coefficients or digits of the continued fraction expansion of any  
good seed $x_0\in [0,1]$. How do you determine the value of $(a_1 a_2\cdots a_k)^{1/k}$ as $k\rightarrow\infty$?
 \vspace{1ex}

\noindent {\bf Solution} \\
The existence and determination of \textcolor{index}{Khinchin's constant}\index{Khinchin's constant} [\href{https://en.wikipedia.org/wiki/Khinchin\%27s_constant}{Wiki}] is a well-know and fascinating problem. It is a painfully slow process to obtain an empirical estimation and assess its accuracy, but
 the theory helps solve this problem. Here $P(a_k = n)$ is easy to obtain from the invariant distribution, see~(\ref{tatar}). Let $K_0$ be the constant in question. We have:
$$\log K_0 = \lim_{k\rightarrow\infty} \frac{1}{k}\sum_{i=1}^k \text{E}\Big[\log a_i\Big] =\lim_{k\rightarrow\infty} 
\text{E}\Big[\log a_k\Big] = 
\sum_{n=1}^\infty (\log n) P(a_k=n).$$
From there, it is easy to obtain
$$K_0 = \prod_{n=1}^\infty \Bigg(1+\frac{1}{n(n+2)}\Bigg)^{\log_2 n} \approx 2.6854520010.$$

\noindent The values of $\log a_k = \log \lfloor p(X_k)\rfloor$ for $k$ between 1 and $\num{20000}$ are plotted in Figure~\ref{fig:picf},
 for $X_0 =\pi$. You can download these values from the
``Online Encyclopedia of Integer Sequences" (OEIS), \href{https://oeis.org/A001203}{here}.  The first few ones are shown at the very beginning of section~\ref{oifvcd}.
\end{Exercise}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{picf.png}  
\caption{First \num{20000} coefficients (their logarithm) in the continued fraction of $\pi$}
\label{fig:picf}
\end{figure}

\begin{Exercise} {\em Digits of generalized Gauss map} -- Let $X_{k+1} = 1/X_k^\alpha  -\lfloor 1/X_k^\alpha\rfloor$, and
$a_k = \lfloor 1/X_k^\alpha\rfloor$, with $\alpha>0$. When $\alpha=1$ (the Gauss map), $\text{E}[a_k]=\infty$. For which values
 of $\alpha$ is the expectation finite?
\end{Exercise}


\section{Measuring the amount of chaos}

Before discussing chaos, I want to emphasize the fact that many dynamical systems have a non-chaotic version depending on the parameter.
 For instance, for the \textcolor{index}{logistic map}\index{logistic map}\index{map!logistic} $g(x)=r x(1-x)$ discussed earlier, 
  there is no chaos in the sequence $\{X_k\}$ if $r<3$. Chaos starts to kick-in at $r\geq 3$. The sequence is fully chaotic when
 $r=4$. In this book, I only consider fully chaotic sequences. 

In the logistic map, when $r\geq 3$, the sequence exhibits a number of \textcolor{index}{bifurcations}\index{bifurcation} [\href{https://en.wikipedia.org/wiki/Bifurcation_theory}{Wiki}]:
 2 when $r=r_1=3$, then 4 starting at $r=r_2 \approx 3.449$, then 8 starting at $r=r_3 \approx 3.544$, 
 then 16 starting at $r =r_4 \approx  3.564$ and so on, up
 to an infinite number at $r=4$. Successive ratios $(r_{n-1}-r_{n-2})/(r_n - r_{n-1})$ tend to some constant when $n\rightarrow \infty$.
 That constant, also found in many other dynamical systems, is the universal constant of chaos. It is known
 as \textcolor{index}{Feigenbaum's constant}\index{Feigenbaum's constant} [\href{https://en.wikipedia.org/wiki/Feigenbaum_constants}{Wiki}], and its value is approximately $4.669$.

\subsection{Hurst exponent and related metrics}

In the context of \textcolor{index}{Brownian motions}\index{Brownian motion}, 
 \textcolor{index}{random walks}\index{random walk} and related chaotic processes discussed in chapter~\ref{ch1}, chaos is
 often measured using some kind of moving average. Here I illustrate some of these metrics applied to different types of stochastic processes  
 ranging from very smooth (barely chaotic) to highly unpredictable (very chaotic). These processes are built using the following method: 
\vspace{1ex}
\begin{itemize}
\item[]Step 1: Use an autocorrelated \textcolor{index}{stationary process}\index{stationarity} $\{X_k\}$, with $k=0,1$ and so on.
\item[]Step 2: The standardized $\{X_k\}$ is $\{Y_k\}$ with mean and variance equal to 0 and 1 respectively. 
\item[]Step 3: $\{Z_k\}$ is the final process with $Z_k = (Y_0 + \cdots + Y_{k})/\sqrt{k+1}$.
\end{itemize}\vspace{1ex}
In practice, we might be more interested in time-continuous processes than in the discrete version. So a time series featuring $\{Z_k\}$ 
for $k=0,1$ and so on up to $k=N$ may be re-scaled (say) with $t=k/750$ so that it looks time-continuous with the time now ranging from $t=0.00$ to $t=N/750$ (with time increments of $1/750$). This technique is described in section~\ref{bmrwp}. 

Now I provide 4 examples of such processes, each featuring a class of patterns in terms of chaotic behavior. Details about the construction is found in
 my spreadsheet \texttt{Chaos\_BrownianFractional.xlsx} available on GitHub, \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/chaos_BrownianFractional.xlsx}{here}. The spreadsheet illustrates the construction 
 of $\{X_k\}, \{Y_k\}, \{Z_k\}$ for series A, as well as the computation of smoothness metrics.
 I used it to produce Figure~\ref{fig:hurst}. In the spreadsheet, $N = \num{22212}$.\vspace{1ex}

\begin{itemize}
\item[] Series A: $X_{k+1} = b X_k - \lfloor bX_k \rfloor$ with $X_0 = \log 2$ and $b = (1 + \sqrt{5})/2$.
\item[] Series B: $X_k$ is either $0$ or $1$ with probability $\frac{1}{2}$. The $X_k$'s are independent.
\item[] Series C: $X_{k+1} = b + X_k - \lfloor b + X_k \rfloor$, with $X_0 = \log 2$, $b = (1+\sqrt{5})/2$.
\item[] Series D: for $\{X_k\}$, use $\{Z_k\}$ from series C.
\end{itemize} \vspace{1ex}
 Interestingly, in series A,
 $\lfloor b X_k \rfloor$ is the $k$-th digit of $X_0$ in the irrational 
\textcolor{index}{Golden ratio base}\index{golden ratio base}\index{base (numeration systems)!golden ratio base} [\href{https://en.wikipedia.org/wiki/Golden_ratio_base}{Wiki}], discussed later in this book.  The underlying 
 weakly autocorrelated dynamical system $\{X_k\}$ is in the same family as the
 \textcolor{index}{dyadic map}\index{dyadic map} corresponding to $b=2$, and related to the binary numeration system. Of course in series B, $\{X_k\}$ is not autocorrelated. Series C and D have long-range autocorrelations.

The operation that consists of transforming $\{X_k\}$ into $\{Z_k\}$ is known as integration. The resulting sequence $\{Z_k\}$ is smoother (less chaotic) than
 $\{Y_k\}$. The inverse operation is known as differentiation: it increases chaos. Note that $\{Z_k\}$ is a fully variance-normalized Brownian motion if
 you start with a white noise for $\{X_k\}$. So it is not really a Brownian motion, as oscillations stay within a standard range instead of growing in amplitude over time. It is related to 
\textcolor{index}{fractional Brownian motions}\index{fractional Brownian motion}\index{Brownian motion!fractional} 
[\href{https://en.wikipedia.org/wiki/Fractional_Brownian_motion}{Wiki}] when the original
 sequence $\{X_k\}$ exhibits long-range autocorrelations. 

Finally, Series A an B result in the same category of processes. 
Despite the fact that $\{X_k\}$ is autocorrelated in series A and not in series B, that autocorrelations are weak enough as to have no impact on $\{Z_k\}$ when turned into a time-continuous process. So I did not include series B in my illustrations. To the contrary,
 autocorrelations in series C and D are strong and long-range, and have a noticeable impact on the time-continuous version of
 $\{Z_k\}$.

\begin{figure}%[H]
\centering
\includegraphics[width=0.72\textwidth]{hurstACD.png}  %0.77
\caption{Series A, C, D (left) with corresponding chaos measurements (right)}
\label{fig:hurst}
\end{figure}

\subsubsection{Detrending moving averages and spreadsheet illustration}

The traditional metric to measure the smoothness of $\{Z_k\}$ is the 
\textcolor{index}{detrending moving average}\index{detrending moving average}, and it is abbreviated as DMA. It is the mean squared error between your observations and its various moving averages  of order $m = 1, 2, 3$, and so on. 
The exact definition can be found in ``Statistical test for fractional Brownian motion based on detrending moving 
average algorithm"~\cite{grz2018} and in many other articles. Other criteria are also used, such as FA and DFA. A comparison of these metrics can be found in ``Comparing the performance of FA, DFA and DMA using different synthetic long-range correlated time series"~\cite{ying2018}. In my spreadsheet, I use DMA, along with autocorrelations of various lags. The better notation DMA($m$)  emphasizes the fact that it depends on $m$. Now we have this well-known result:
$$
\text{DMA}(m) \sim C(h) \cdot m^{2H},
$$
where $C(\cdot)$ is a bounded function. 

This is an asymptotic result, meaning that it becomes more accurate as $m$ grows to infinity. 
The constant $H$ is known as the \textcolor{index}{Hurst exponent}\index{Hurst exponent} [\href{https://en.wikipedia.org/wiki/Hurst_exponent}{Wiki}]. It takes on values between 0 and 1, with $H = \frac{1}{2}$ corresponding to the Brownian motion. Higher values correspond to smoother time series, and lower ones to more chaotic data. In the spreadsheet, since I represent time-continuous
 processes where $750$ increments of discrete time correspond to one increment or unit of continuous time (say a second), I use large values 
$m=100,200,\dots, 500$. 

Also, in order to work with stabilized variances and avoid the volatility present at the beginning of $\{Z_k\}$ (due to the fact that we start at 0), I study the time series and perform all the computations assuming we start at time $t=4.00$, that is,
 after skipping the first $4\times 750$ observations in discrete time. This is noticeable in Figure~\ref{fig:hurst}, where the X-axis
 on the left plots represents the time, and starts at $t=4.00$.   

The main takeaway from my spreadsheet and analysis, summarized in Figure~\ref{fig:hurst}, is the fact that the curvature
 of the function $\text{DMA}(m)$ is a good indicator of the amount of chaos present in $\{Z_k\}$. If you look at the right plots in Figure~\ref{fig:hurst}, the curve with the yellow dots represent $\text{DMA}(m)$, and the X-axis represents $m$.




\begin{Exercise} {\em A very smooth time series} -- Create series E as follows: use $\{Z_k\}$ obtained in series D as your starting point $\{X_k\}$ to produce a new $\{Z_k\}$, smoother than that  in series D. \vspace{1ex} 

\noindent {\bf Solution} \\
Using my Excel spreadsheet, this can be done with one easy step: copy column $\{Z_k\}$ (the values only) onto column $\{X_k\}$ and all the summary
 statistics, the new $\{Z_k\}$, and the plot will automatically be updated. See solution in Figure~\ref{fig:hurst2}. Note that as the smoothness
 increases (or in other words, chaos and entropy is reduced), the curvature of  the curve with yellow dots on the right-hand side in Figure~\ref{fig:hurst} and~\ref{fig:hurst2} is also increasing. Other examples are provided in section~\ref{movbc}: see Figure~\ref{fig:lollog1xx}.
\end{Exercise}

\begin{figure}[H]
\centering
\includegraphics[width=0.72\textwidth]{hurstE.png}  %0.77
\caption{Series E (left) with corresponding chaos measurements (right)}
\label{fig:hurst2}
\end{figure}




\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{rescale.png}  
\caption{Logistic map: raw $(Y_k,Y_{k+20})$ on the left, versus rescaled on the right}
\label{fig:picfrsc}
\end{figure}




%-------------------------------------------------------------------------------------------------------------------------------------------

\subsection{Random numbers based on digits of irrational numbers}\label{puutrew}

I still use the notation $\{Y_k\}$ for the logistic map $Y_{k+1}=4Y_k(1-Y_k)$, and $\{X_k\}$ for the 
dyadic map $X_{k+1}=2X_k- \lfloor 2X_k\rfloor$. The homomorphism between the two maps
 (discussed in section~\ref{hohok}) makes it possible to link the digits of both systems. However, the mapping between the two is not one-to-one. More specifically, if $X_0 = (2\pi)^{-1}\arcsin \sqrt{Y_0}$ or $Y_0 = \sin^2(2\pi X_0)$ then $Y_k = \sin^2(2\pi X_k)$. But retrieving
 $X_k$ from $Y_k$ is not possible unless you have some information about $X_k$. 

\noindent The reverse map is as follows. Let $Z_k = (2\pi)^{-1}\arcsin\sqrt{Y_k}$, thus $0\leq Z_k\leq\frac{1}{4}$. We have four cases: \vspace{1ex}
\begin{itemize}
\item   If $X_k < \frac{1}{4}$  then $X_k=Z_k$,  % int(2x_k) = 0, int(2y_k) = 0 | let z_k = y_k
\item   If $X_k > \frac{3}{4}$  then $X_k = 1 - Z_k$,  %int(2x_k) = 1, int(2y_k) = 0 | let  z_k to 1-y_k
\item   If $\frac{1}{4} < X_k < \frac{1}{2}$ then $X_k  = \frac{1}{2} - Z_k$, %int(2x_k) =0, int(2y_k) = 0 | let z_k = 0.5 - y_k
\item   If $\frac{1}{2} < X_k < \frac{3}{4}$ then  $X_k=\frac{1}{2} + Z_k$. %  y_k  = x_k - 0.5, %int(2x_k) =1, int(2y_k) = 0 | let x_k = 0.5 + y_k
\end{itemize}\vspace{1ex}
Again, $a_k=\lfloor 2X_k\rfloor$ is the $k$-th binary digit of $X_0$. This makes the connection between random generators based on the logistic map, and binary digits of real numbers. 
Now I focus exclusively on the case where $Y_0=\frac{1}{3}$, resulting in 
$X_0=(2\pi)^{-1}\arcsin \sqrt{1/3}$ being an irrational number. These seeds
 are believed to be \textcolor{index}{good seeds}\index{seed (dynamical systems)!good seed} in their respective systems. 
 In particular, the sequence $\{Y_k\}$ has zero autocorrelation of any order, while the lag-$m$ autocorrelation in the sequence
 $\{X_k\}$ is $2^{-m}$. 
Thus the mapping $Y_k \mapsto X_k$ turns an autocorrelation-free sequence into an autocorrelated one,
 while the inverse mapping $X_k \mapsto Y_k$ (using the four cases in the bullet list) does the opposite. 
Also, $Y_0=\frac{1}{3}$ leads to all $Y_k$'s being rational numbers, even though
 the corresponding $X_k$'s are all irrational.

Starting with $Y_0=\frac{1}{3}$ leads to $Y_k = 2\cdot 4^k \cdot 3^{-2^k} Q_k$ where $Q_k$ is an integer if
 $k>0$. Now let $S_k = \sqrt{Q_k/Q_{k-1}}$. Using arguments similar to those in Exercise~\ref{tonnefer}, it is easy to see
 that $S_k$ is an integer if $k>0$. The sequence $\{S_k\}$ grows much more slowly than $\{Q_k\}$, albeit still dramatically fast. 
  The hope is that $\{S_k\}$ can shed some lights on the digit distribution of the logistic map with the seed $Y_0=\frac{1}{3}$. Any insight 
 might be valuable to check whether $\frac{1}{3}$ is a \textcolor{index}{good seed}\index{seed (dynamical systems)!good seed} or not. As of this writing, nobody knows, though it is
 conjectured to be a good seed. Obtaining a recurrence relation for $S_k$ is trivial and would help analyze the digit distribution.
  The first few values are $1, 7, 17, 5983, 28545857$. Also $S_k = 3^{2^{k-1}} 2^{-1}R_k$, where
 $R_k$ is defined in Exercise~\ref{tonnefer}.
\begin{Exercise} \label{tonnefer} {\em Transformation of the logistic map} -- Let $R_k = \sqrt{Y_{k+1}/Y_k}$. Prove that if $Y_0$ is a rational number,
  then $R_k$ is rational if $k>0$. Also prove that $R_k = |R_{k-1}^2 - 2|$ and $0\leq R_k\leq 2$.\vspace{1ex} \\
\noindent{\bf Solution} \\
\noindent We have $Y_{k+1}=4Y_k(1-Y_k) = 4Y_k(1-2Y_{k-1})^2$. Thus $R_k = 2|1-2Y_{k-1}|$ is rational if $k>0$ and
 $Y_0$ is rational (because then, all $Y_k$ are rational). And since $0\leq Y_{k-1}\leq 1$ for all $k>0$, we have $0\leq R_k\leq 2$.
Also, the relationship $Y_{k+1}=4Y_k(1-Y_k)$ leads to 
$$
\frac{1}{4}\frac{Y_{k+1}}{Y_k} - 1 = -Y_k, \quad \frac{1}{4}\frac{Y_{k}}{Y_{k-1}} - 1 = -Y_{k-1}\quad .
$$
Taking the ratio of the above expressions, one obtains 
$ (\frac{1}{4} R_k^2-1) / (\frac{1}{4} R_{k-1}^2-1) = R_{k-1}^2$.  The remaining of the proof consists of simple mathematical manipulations.\qed
\end{Exercise}




As for the invariant distribution $F(x)$, we have this remarkable
approximation: $F(x) \approx -2 +\sqrt{5x-1}$. It leads to a very good approximation for the digit frequencies, featured in
 Table~\ref{ttyttuchi} in the row labeled $p_n$. The quantity $p_n$ is the probability for a digit $a_k$ to be equal to $n$. It corresponds to the empirical frequencies if $x_0$ is a good seed, when computed over all the infinitely many digits. Here, $n=0,1$ or $2$. The remaining of
 this discussion is to show that the approximation of $F(x)$, although excellent, is not exact. This is not obvious, since you would
 need a tremendous amount of simulations, with a very good random number generator and high accuracy everywhere, to notice that there is a tiny error.

\begin{figure}[H]
\centering
\includegraphics[width=0.60\textwidth]{error1.PNG} %0.86
\caption{Error due to using an approximation for the invariant distribution}
\label{err32}
\end{figure}

The goal is to show that if $F(x)=P(X_k<x) = -2+\sqrt{5x-1}$, then the functional equation $P(X_k<x) = P(g(X_k)<x)$ is not satisfied.
I start with the obvious fact that
$$ P\Big[g(X_k)< x\Big] = P\Big[1<X_k<\sqrt{x}\Big] + P\Big[\sqrt{2}<X_k<\sqrt{x+1}\Big] + P\Big[\sqrt{3}<X_k<\sqrt{x+2}\Big].$$
Then proving that $F(x) \neq -2+\sqrt{5x-1}$ amounts to proving that in general, if $x\in [1, 2]$,
\begin{equation}
\sqrt{5x-1} \neq -\lambda + \sqrt{5\sqrt{x}-1} + \sqrt{5\sqrt{x+1}-1} + \sqrt{5\sqrt{x+2}-1},\label{popi}
\end{equation}
where $\lambda = \sqrt{5\sqrt{2}-1} +  \sqrt{5\sqrt{3}-1}$. Take for instance $x=1.29$, then
the left side of the non-equality in~(\ref{popi}) is $2.33452\dots$, and the right side is $2.33453\dots$. Note that 
 if $x=1$ or $x=2$, both sides are equal. The difference between the left and right side (the error) when $x\in[1,2]$ is pictured 
 in Figure~\ref{err32}.

For a comprehensive reference on nested radicals, also called continued square roots by analogy to continued fractions, see the 98-page article published in 2017 by 
Dixon~\cite{sccf17}, featuring a chronology of the research on this topic. 





\subsubsection{Bad seeds in the stochastic dyadic map}

Possibly the most simple example is the binary numeration system attached to the 
 \textcolor{index}{dyadic map}\index{dyadic map}. Using the same framework and notation as in section~\ref{gorexcres}, let
$$Z =\sum_{k=0}^\infty \frac{X_k}{2^{k+1}},$$ 

\noindent with  $P(X_k=1) = p, P(X_k=0) = 1-p .$
Here the $X_k$'s are independently and identically distributed Bernoulli random variables of parameter $p$. For good seeds,
 $p=\frac{1}{2}$. But what happens if $p\neq \frac{1}{2}$? For instance, if $p=\frac{3}{4}$, you end up with an invariant distribution 
 $F_Z(z) = P(Z<z)$ very similar in shape to 
 that discussed in Exercise~\ref{polcjonb} and plotted in Figure~\ref{fig:r1lkn2x}.
 It is nowhere differentiable: the corresponding density function $f_Z$ does not exist.

Now let us pretend that $f_Z$ exists, and let's compute the moments.
To see how this works, I start with the 
\textcolor{index}{functional equation}\index{functional equation}.  In section~\ref{gorexcres}, that equation
 was $F_Z=F_{X(1+Z)}$. Here it is  $F_Z=F_{(X+Z)/2}$, where $F_Z$ is the unknown. Thus we have $\text{E}[Z^n] = 2^{-n} \text{E}[(X+Z)^n]$ which leads to the following recursion formula for the moments:
$$\text{E}[Z^n]=\frac{p}{2^n-1 + p} 
\cdot\sum_{j=0}^{n} \binom{n}{j}\text{E}[Z^{n-j}].$$
This yields
$$\text{E}[Z] = p, \quad
\text{E}[Z^2] = \frac{p}{3}(1+2p), \quad
\text{E}[Z^3] = \frac{p}{7}(1+4p+2p^2),\quad
\text{E}[Z^4] = \frac{p}{105}(7+46p + 44p^2+8p^3).
$$
Simple simulations show that estimated (empirical) moments converge to the  theoretical values. In a nutshell, we could write integrals such 
 as 
$$
E[Z^4] = \int_0^1 z^4 f_Z(z) dz = \frac{p}{105}(7+46p + 44p^2+8p^3),
$$
even though $f_Z$ does not exist when $p\neq\frac{1}{2}$. Indeed, this can be extended to positive, non-integer values of $n$. In the 
 above example, $n=4$.

Interestingly, the set of (bad) seeds that have a binary expansion with the above invariant distribution together with $p\neq \frac{1}{2}$,
 has Lebesgue measure zero. However it is dense in $[0, 1]$ but full of microscopic holes. It consists of numbers that do not have the same 
 proportion of 0 and 1 in their base 2 representation. This type of sets fits in the
 same category as the \textcolor{index}{Cantor set}\index{Cantor set} [\href{https://en.wikipedia.org/wiki/Cantor_set}{Wiki}].
 Indeed, the Cantor sets consists of numbers in $[0, 1]$ that have no digit equal to 2 in their base 3 representation. That set
 also has macroscopic holes visible to the naked eye.

%[DONE] https://stats.stackexchange.com/questions/439528/parameter-estimation-when-the-likelihood-function-does-not-exist


\subsubsection{Nested radicals and fractal invariant distributions}

I now come back to the system with the functional equation  $F_{Z^2} = F_{X+Z}$, that is,
 $P(Z^2<z) = P(X+Z<z)$. This is a stochastic version of the system studied in section~\ref{prasoil}, since the seed $Z$ is now a
 random variable. It is defined by  
$$Z=\sqrt{X_0+\sqrt{X_1+\sqrt{X_2+\dots}}}$$
With the notation $p_n=P(X=n)$, let's use the following distribution for $X$ : 
\begin{equation}
p_0 = \frac{1}{2}, \quad p_1 = \frac{1}{1 + \sqrt{5}}, \quad p_2 = \frac{3 - \sqrt{5}}{4}. \label{pxz1}
\end{equation} 

The choice for the distribution is not arbitrary: it makes the system smoother and possibly easier to solve. To the contrary 
$p_0 = p_1= p_2 = \frac{1}{3}$ yields a far more chaotic system, and the 
case $p_0 = p_2 = \frac{1}{2}$, $p_1=0$ is so wild that the support domain of $Z$ has large visible gaps, making it strikingly  
similar to the \textcolor{index}{Cantor set}\index{Cantor set}.  Good seeds have yet a different distribution for the digits
 $X_k$, specified in row $p_n$ in Table~\ref{ttyttuchi} (look for the column labeled ``nested radicals").

Wherever the density $f_Z$ exists (the derivative of $F_Z$), we have:
\begin{equation}
f_Z(z)= 
\begin{cases}
 2p_0 z f_Z(z^2), & 1<z<\sqrt{2}      \nonumber    \\[2pt]  
 2p_1 z f_Z(z^2 -1), & \sqrt{2}<z<\sqrt{3}   \nonumber \\[2pt]  
 2p_2 z f_Z(z^2 -2), & \sqrt{3}<z<2    \nonumber \\ 
\end{cases}
\end{equation}

Thus, for $f_Z(z)$ to be properly defined at $z = (1+\sqrt{5})/2$, we must have $p_1=1/(1+\sqrt{5})$. Likewise, 
 a similar argument applied to the other extremities leads to~(\ref{pxz1}). The idea is to apply each of these 3 formulas
 backwards and recursively. The first case suggests that maybe $f_Z(z)=f(1)\cdot z^{-1}$ if the density existed. Hence trying
 $F_Z(z)=\log_2 z$ as the potential solution to the functional equation seems reasonable.  It 
turns out to be a pretty good approximation as seen in Figure~\ref{fig:botul}.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{nr.png}  
\caption{$F_Z(z)$ vs $\log_2 z$ on the left; approximation error on the right}
\label{fig:botul}
\end{figure}




There is more to the story. The invariant distribution, despite looking smooth on a macroscopic scale, is actually pretty chaotic.
The right plot in Figure~\ref{fig:botul} shows the approximation error $F_Z(z)-\log_2 z$ with $z\in [1,2]$. It behaves exactly like a fractal! 
The figure was produced with \texttt{invariant\_nested\_radical.py}, also available
 on GitHub,  \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/invariant_nested_radicals.py}{here}.\vspace{1ex}

\begin{lstlisting}
# invariant_nested_radical.py
import numpy as np
from matplotlib import pyplot as plt

seed = 453
np.random.seed(seed)
p0 = 1/2
p1 = 1/(1 + np.sqrt(5))
p2 = (3 - np.sqrt(5))/4
z_values = []

def my_plot_init():
    axes = plt.axes()
    [axx.set_linewidth(0.2) for axx in axes.spines.values()]
    axes.margins(x=0)
    axes.margins(y=0)
    axes.tick_params(axis='both', which='major', labelsize=7)
    axes.tick_params(axis='both', which='minor', labelsize=7)
    return()

for number in range(1000000):
    z = 0.0
    prod = 1.0
    for k in range(60):
        rnd = np.random.rand()
        if rnd < p0:
            x_k = 0
        elif rnd < p0 + p1:
            x_k = 1
        else:
            x_k = 2
        z = np.sqrt(x_k + z)
    z_values.append(z)

x = np.sort(z_values)
y = np.arange(len(x))/float(len(x))
approx = []
for arg in x:
    val = np.log(arg)/np.log(2)
    approx.append(val) 

my_plot_init()
plt.plot(x, y, linewidth = 0.4)
plt.plot(x, approx, linewidth = 0.4)
plt.show()

my_plot_init()
plt.plot(x, y-approx, linewidth = 0.4)
plt.show()
\end{lstlisting}

\subsubsection{Generalizations and characteristic function}
%---

In some systems,  the random variables $X$ and $Z$
are linked by the formula $Z_{k} = (X_k +Z_{k+1})^\alpha$, where $\alpha$ is a parameter, $\{X_k\}$ the digit sequence (random here)
 and $Z_0$ is the seed (also random). In my notation, $Z=Z_0$ and $X$ is a random variable with same distribution as any $X_k$.
Also, the $X_k$'s are independent. The \textcolor{index}{invariant distribution}\index{invariant distribution} is
 the limit distribution (if it exists) of $Z_k$ as 
$k\rightarrow\infty$, 
 or the distribution that the seed $Z=Z_0$ must have, if we want $Z_1$ to have the same distribution as $Z_0$, and by extension, the same distribution as all $Z_k$.

For the \textcolor{index}{nested radicals}\index{nested radicals}, $a=1/2$. For continued fractions 
(the \textcolor{index}{Gauss map}\index{Gauss map}), $\alpha = -1$. The additive block $X_k + Z_{k+1}$  of two independent components suggests
 that \textcolor{index}{characteristic functions}\index{characteristic function}
 [\href{https://en.wikipedia.org/wiki/Characteristic_function_(probability_theory)}{Wiki}] may help solve the problem (finding $F_Z$ when $F_X$ is known) or
 the inverse problem (finding $F_X$ when $F_Z$ is known). The latter usually has no solution unless 
 $F_Z$ is one of the very rare \textcolor{index}{attractor distributions}\index{attractor distribution} of the system.  
For nested radicals, we have $\varphi_{Z^2} = \varphi_X\cdot \varphi_Z$ where $\varphi$ denotes the characteristic function.
  For continued fractions,
 $\varphi_{1/Z} = \varphi_X\cdot \varphi_Z$. This formula may be useful to solve the inverse problem. 

For the numeration system in base $b$,  we have $Z_k = (X_k +Z_{k+1})/b$, resulting 
 in $\varphi_{bZ} = \varphi_X \cdot \varphi_Z$. This is true even if $b$ is not an integer, and regardless of the distribution 
 of $X_k$ as long as it takes integer values between $0$ and $b-1$ inclusive (the upper bound is $\lfloor b\rfloor$ if $b$ is not an integer). 
However, in most cases related to \textcolor{index}{bad seeds}\index{seed (dynamical systems)!bad seed}, $Z$ is singular and may not have a characteristic function, regardless of the system. 
For good seeds in the base $b$ system, $Z$ has a uniform distribution on $[0, 1]$ and $X$ has a discrete uniform distribution. 
The \textcolor{index}{dyadic map}\index{dyadic map} corresponds to $b=2$. 


%-----
% xxxxxxx xxx99 https://www.datasciencecentral.com/new-family-of-generalized-gaussian-distributions/
%   in [DSS recovered]
% xxx contact author of IEEE paper / conferences on dynamical systems
% xxxxxxxxxx MLT article: copula with parametric approach / gradient based and remove non-influential obs in dataset
% xx dynamical system to generate sequence of rgb colors: dyadic ; 3 successive iterates yiels one RGB color
%--------------------------
% xxx Contact author e-mail: avlad@racai.ro, adriana_vlad@yahoo.com
%The autocorrelation function of the logistic map chaotic signal in relation with the statistical independence issue
%   contact authors, contact special group on LinkedIn
%----------------------
%https://math.stackexchange.com/questions/3445421/limiting-distributions-attractors-associated-with-the-discrete-difference-oper
%xxx continued fraction with lower / higher exponent
% rabbit number


% https://math.stackexchange.com/questions/3330127/non-standard-solution-to-fx-frac12-bigf-fracx2-f-frac1x2


\subsection{Exercises}\label{jeporei}

The following original exercises complement the material discussed in sections~\ref{scrotew}
  and~\ref{nodensity}. They focus on important aspects of the methodology that could be skipped in a first reading. If you don't have time to solve them, you should at least read the solutions: they cover additional techniques
 and methodology that are fundamental for a full understanding of the subject, including Python implementations. 

The underlying system
 assumed in this section is that governed by~(\ref{digere}). Here $X$ is a random variable with the same
 distribution as any $X_k$, since  all the $X_k$'s are  
independently and identically distributed.

\pagebreak

\begin{Exercise}\label{paasweqasde} {\em Case when the invariant distribution is uniform} -- If 
 $X$ has a \textcolor{index}{Rademacher distribution}\index{Rademacher distribution} [\href{https://en.wikipedia.org/wiki/Rademacher_distribution}{Wiki}], that is $P(X=\frac{1}{2}) =
 P(X=-\frac{1}{2}) =\frac{1}{2}$, prove that $Z$ is uniform on $[-1, 1]$.
\vspace{1ex}

\noindent {\bf Solution} \\
Is it easy to compute the moments $\texttt{E}[Z^n]$ for $n=1,2$ and so on, using
Formula~(\ref{johndick}). The formula can be generalized to non-integer $n$. The moments of $Z$ coincide with those of a uniform distribution on $[-1, 1]$.
In certain cases (if $X$ and $Z$ are positive and continuous), 
you can also use the \textcolor{index}{Mellin transform}\index{Mellin transform} [\href{https://en.wikipedia.org/wiki/Mellin_transform}{Wiki}] 
to solve the functional equation $F_Z = F_{X(1+Z)}$, where the unknown is $F_Z$.
If $\text{M}(\cdot)$ denotes the Mellin transform, we have $\text{M}[Z] =\text{M}[X(1+Z)] = \text{M}[X]\cdot \text{M}[1+Z]$, which simplifies
 the computations. The use of Mellin transforms in this context dates back to 1948, see~\cite{mellin48}.
\end{Exercise}

\begin{Exercise}\label{p} {\em An almost perfect identity} -- This time, let us assume that the $X$ has a continuous distribution.
More specifically,  $X$ is distributed as $\sin\pi Y$ where $Y$ has a Gaussian distribution with zero mean and unit variance. 
Using simulations, compute $\text{Var}[Z]$. Does $Z$ also have a unit variance?
\vspace{1ex}

\noindent {\bf Solution} \\
We have $\text{Var}(Z) = 1.00000020\dots$. So it is different from 1. It is almost impossible to notice the discrepancy
 using simulations alone. Note that $\text{E}[X] = 0$, thus $\text{Var}[Z] = \text{E}[X^2] / (1 - \text{E}[X^2])$.  Then,
$$
\text{E}[X^2] =\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty x^2 \sin^2 \pi x dx
 = \frac{1+(4\pi^2 - 1)\exp(-2\pi^2)}{2}
= 0.500000051\dots
$$
See computations performed by Mathematica, \href{https://mltblog.com/3JqDagR}{here}. Thus the announced result.
\end{Exercise}

\begin{Exercise}\label{p} {\em A model fitting problem} --  You collected some data. The
 features in your dataset (also called predictors or independent variables) are $X_0, X_1,\dots$ and the
 response $Z$. You try to fit a model to your data, the two options being
$Z = X_0 + X_0X_1+X_0X_1 X_2+\dots$ and $Z = X_0 + X_1X_2+X_3X_4 X_5+\dots$, where the $X_k$ are independently and identically
 distributed in both cases. How to determine which model is a better fit?
\vspace{1ex}

\noindent {\bf Solution} \\
One easy way is to look at $\text{Var}[Z]$. The first and second models respectively have
$$
 \text{Var}[Z] = \frac{\text{Var}[X]}{(1-\text{E}[X^2])(1-\text{E}[X])^2}, 
\quad \text{Var}[Z]=\frac{\text{Var}[X]}{(1 - \text{E}[X^2]) (1 - \text{E}^2[X])}.
$$
The statistic for your test can be $T =  (1 - \text{E}[X^2]) (1 - \text{E}^2[X]) \text{Var}[Z]/ \text{Var}[X]$. It is expected
 to be equal to 1 (regardless of the distributions of $X$ and $Z$) if the second model is the correct one, assuming $0 < |\, \text{E}[X] \,| < 1$.
\end{Exercise}


\begin{Exercise}\label{polcjon} {\em Computing the digits} -- Given a seed $Z_0$, compute
  the digits $X_k$ defined by~(\ref{digere}). For simplicity, let us assume that $X_k$ can only take on two values: $a$ and $b$,
with $0<a<b$ and $a+b=1$.  \vspace{1ex}

\noindent {\bf Solution} \\
The following code \texttt{digits.py} computes the digit of $Z_0 = \log 2$ when $b = \sqrt{2}/2$. 
If $X_k=b$, the digit is shown as 1; if $X_k=a$, it is shown as 0. The variable \texttt{sum} computes the approximation
 to $Z_0$ when using 60 digits. The program is also on my GitHub repository, 
 \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/digits.py}{here}.\vspace{1ex}
\begin{lstlisting}
# digits.py
import numpy as np

b = np.sqrt(2)/2   
a = 1-b
z = np.log(2)  

prod = 1.0
beta = b/(1-a)
alpha = a/(1-b)

if z > beta: 
  sum   = b
  digit = 1
else:
  sum   = a
  digit = 0

prod = sum
print("digit %3d = %1d" % (0, digit))

for k in range(1, 60):
    beta  = sum + prod * b/(1-a)  
    alpha = sum + prod * a/(1-b)
    if alpha < z:
         prod  = prod * b
         digit = 1
    else: 
        prod  = prod * a
        digit = 0
    sum += prod 
    print("digit %3d = %1d" % (k, digit))
print ("sum = %14.13f, z = %14.13f" % (sum, z))
\end{lstlisting}
\end{Exercise}

\begin{Exercise}\label{polcjonb} {\em Computing the invariant distribution} -- 
Using the same system as in Exercise~\ref{polcjon}, let $P(X_k = b) = p$, and $P(X_k=a) = 1-p$. 
Plot the invariant distribution
  if $a=0.4$, $b=0.6$ and $p=0.8$. In which case (depending on $a,b,p$) is the invariant distribution uniform?\vspace{1ex}

\noindent {\bf Solution}

\noindent The random variable $Z$ can only take values in $[\alpha, \beta]$ with $\alpha = a/(1-a)$ and $\beta=b/(1-b)$. The invariant
 distribution $F_Z(z) = P(Z<z)$ is uniform on $[\alpha,\beta]$ if and only if $b=p$, and thus $a=1-p$. In all other cases, the 
 invariant distribution is singular and nowhere differentiable. See plot in Figure~\ref{fig:r1lkn2x}, 
 featuring the \textcolor{index}{empirical distribution}\index{empirical distribution} based on $\num{10000}$ values of $Z$. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.63\textwidth]{ecdf.png}  
\caption{Invariant distribution when $a=0.4, b=0.6, p=0.8$}
\label{fig:r1lkn2x}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

\noindent Below is the code \texttt{code invariant\_cdf.py} to produce Figure~\ref{fig:r1lkn2x}. 
It is also on GitHub, \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/invariant_cdf.py}{here}. 
\vspace{1ex} 

\begin{lstlisting}
# invariant_cdf.py
import numpy as np
from matplotlib import pyplot as plt

seed = 453
np.random.seed(seed)
p = 0.8
a = 0.4
b = 0.6
z_values = []

for number in range(10000):
    z = 0.0
    prod = 1.0
    for k in range(60):
        rnd = np.random.rand()
        if rnd < p:
            x_k = b
        else:
            x_k = a
        prod = prod * x_k
        z += prod
    z_values.append(z)

x = np.sort(z_values)
y = np.arange(len(x))/float(len(x))

axes = plt.axes()
[axx.set_linewidth(0.2) for axx in axes.spines.values()]
axes.margins(x=0)
axes.margins(y=0)
axes.tick_params(axis='both', which='major', labelsize=7)
axes.tick_params(axis='both', which='minor', labelsize=7)
plt.plot(x, y, linewidth = 0.4)
plt.show()
\end{lstlisting}

\end{Exercise}

%-----------------------------
%[DONE] Variance, Attractors and Behavior of Chaotic Statistical Systems      [see DSC recovered], 
%[DONE]  see https://math.stackexchange.com/questions/3477718/new-numeration-system-mapping-to-binary-numeration-system
%[DONE]  see https://stats.stackexchange.com/questions/435049/variance-of-z-x-1-x-1-x-2-x-1-x-2-x-3-cdots make this an exercise:
%[DONE]      https://math.stackexchange.com/questions/3477718/new-numeration-system-mapping-to-binary-numeration-system
%---------------------------------


\renewcommand{\arraystretch}{1.0} %%%
\renewcommand{\arraystretch}{1.4} %%%


%-------------------------
\chapter{Application: Synthetic Stock Exchange}{}\label{pouti}

In this chapter, I describe an original number guessing game played with real money. It can also be used
 as a synthetic stock exchange, leveraging the mathematics discussed in chapter~\ref{chapterPRNG}.  In particular, it is based on the \textcolor{index}{dyatic map}\index{dyadic map} and efficient algorithms to 
compute the \textcolor{index}{digits}\index{digit} attached to
\textcolor{index}{good seeds}\index{seed (dynamical systems)!good seed} in the dynamical systems involved.


If properly implemented, this system  cannot be manipulated: formulas to win numbers  are made public. Also, it simulates a neutral, efficient stock market. In short, for the participants, there is nothing random: everything is deterministic and fixed in advance, and known to all users. Yet it behaves in a way that looks perfectly random, and public algorithms offered to compute winning numbers require so much computing power, that for all purposes, they are useless -- except to comply with gaming laws and to establish trustworthiness. Any attempt at predicting winning numbers based on historical data, using machine learning techniques, will fail: the sequence of winning numbers appear to be totally random, despite the fact that it is deterministic.

I use private algorithms to determine the winning numbers, and while they produce the exact same results as the public algorithms, they are incredibly more efficient, by many orders of magnitude. Also, I mathematically prove that the public and private algorithms produce the same results. To prove it on a real example may not possible: the public algorithm requires too much computing time. 

The goal is to turn this application into a real platform where participants play with real money. I also discuss how it can be done given the legal barriers. The operator generates revenue via charging a fee per transaction, just like stock exchanges, and unlike lotteries that retain a portion of the payments to finance themselves. In short, all the money used by participants to purchase numbers, goes back to the participants. 

\section{Introduction}

The application discussed here requires a very large number of pseudo-random digits generated in real-time, obtained as fast as possible. The digits must emulate randomness extremely well. I use the quadratic irrational random number
 generator discussed in section~\ref{pivizintrobvbc}. 

The idea consists of creating a virtual, market-neutral stock market where people buy and sell stocks with tokens. In short, a synthetic stock market where you play with synthetic money: tokens, themselves purchased with real money. Another description is a lottery or number guessing game. You pay a fee per transaction, and each time you make a correct guess, you are paid a specific amount, also in real money. The participant can select different strategies ranging from conservative and limited to small gains and low volatility, to aggressive with a very small chance to win a lot of money.

The algorithm that computes the winning numbers is public; it requires some input information, also publicly published (the equivalent of a public key in cryptography). So you can use it to compute the next winning number and be certain to win each time, which would very quickly result in bankruptcy for the operator. However the public algorithm necessitates so much computing time to obtain any winning number with certainty, to make it useless. But you can guess the winning number: your odds of winning by pure chance (in a particular example) is 1/256. 

The operator uses a private algorithm 
that very efficiently computes the next winning numbers. From the public algorithm, it is impossible to tell -- even if you are the greatest computer scientist or mathematician in the world -- that there is an alternative that could make the computations a lot faster: the private algorithm is the equivalent of a private key in cryptography. The public algorithm takes as much time as breaking an encryption key (comparable to factoring a product of two very large primes), while the private version is equivalent to decoding a message if you have the private key (comparable to finding the second factor in  the number in question if you know one of the two factors -- the private key).

Thus, technically, this application is not a game of chance but a mathematical competition. But one that no one can win 
other than by luck, due to tremendous computing power required to find the winning number with certainty. The 
digits used in the system must be uniformly distributed. Even a tiny bias will quickly lead to either the bankruptcy of the operator, or the operator enriching itself and being accused of lying about the neutrality of the simulated market.
  


\section{Winning, sequences and customized ROI tables}

Rather than trading stocks or other financial instruments, participants (the users) purchase numbers. Sequences of winning numbers are generated all the time, and if you can predict the next winning number in a given sequence, your return is maximum. If your prediction is not too far from a winning number, you still make money, but not as much. The system has the following features:
\vspace{1ex}

\begin{itemize}
\item	The algorithms to find the winning numbers are public and regularly updated. Winning is not a question of chance, since all future winning numbers are known in advance and can be computed using the public algorithm.
\item 	The public algorithm, though very simple in appearance, is not easy to implement efficiently. In fact, it is hard enough that mathematicians or computer scientists do not have advantages over the layman, to find winning numbers.
\item 	To each public algorithm, corresponds a private version that runs much, much faster. I use the private version to compute the winning numbers, but both versions produce the exact same numbers.
\item 	Reverse-engineering the system to discover any of the private algorithms, is as difficult as breaking strong encryption.
\item 	The exact return is known in advance and specified in public ROI tables. It is based on how close you are to a winning number, no matter what that winning number is. Thus, your gains or losses are not influenced by the transactions of other participants.
\item 	The system is not rigged and cannot be manipulated, since winning numbers are known in advance.
\item 	The system is fair: it simulates a perfectly neutral stock market.
\item 	Participants can cancel a transaction at any time, even 5 minutes before the winning number is announced.
\item 	Trading on margin is allowed, depending on model parameters.
\item 	The money played by the participants is not used to fund the company or pay employees or executives. It goes back, in its entirety, to the participants. Participants pay a fee to participate.
\end{itemize}\vspace{1ex}

%---------
\noindent Comprehensive tables of previous winning numbers are published, well before a new sequence -- based on these past numbers -- is offered to players. It entices participants to design or improve strategies to find winning numbers, even though arbitraging is technically not possible, unless there is an unknown flaw in my method. Actually, past winning numbers are part of the public data that is needed to compute the next winning numbers, both for participants and the platform operators.

Various ROI tables are offered to participants, and you can even design your own. If you are conservative, you can choose one with a return of 10\% for a perfect guess, a 54\% chance of winning on any submitted number, and a maximum potential loss of 4\%. This table is safe enough that you could ``trade" on margin. It mimics
 a neutral stock market. The return is what you make or lose in one day percentagewise, on one sequence. New winning numbers are issued every day for each live sequence, so your return -- negative or positive -- gets compounded if you play frequently.
 A sequence is the equivalent of a specific stock in the stock market.

Another interesting ROI table offers a return of 330\% for a perfect guess, with the same 54\% chance of winning on any transaction, and  a maximum potential loss also of 4\%. However, the payoff when your guess is close to but different from the winning number, is a lot lower than in the previous example. 
If you are a risk taker, you may like a table offering a maximum return of 500\%, a 68\% chance of winning on any transaction, and a maximum potential loss of 60\%. Or another table with a maximum return of 600\%, a 80\% chance of winning, but a maximum potential loss of 100\%.  I discuss ROI tables and sequences in details later in this chapter, along with examples. By winning, I mean that your guess is close enough to the winning number so that you get financially rewarded. The reward in question might be small depending on the ROI table, compared to correctly guessing the winning number. 

All the sequences currently implemented consist of 8-bit numbers: each winning number -- a new one per day per sequence -- is an integer between 0 and 255. I am working on a generalization to 16-bit numbers, offering payoffs of the same caliber as lottery jackpots. By design, all ROI tables including customized ones, are neutral, with an average zero gain. This beats all casinos and lotteries where the average gain is negative. It applies
 to all sequences. Indeed, sequences and ROI tables are independent. The participant can test various strategies: for instance:
\vspace{1ex}
%----
\begin{itemize}
\item 	Try various ROI tables.
\item 	Play every day until you experience your first win.
\item 	Play every day until you experience your first loss.
\item 	Play until you have achieved a prespecified goal, or exit if your losses reaches some threshold, whatever comes first. 
\item 	Increase or decrease how much you spend depending on your results.
\item 	Look if you can find patterns in the winning numbers, then exploit them. 
\end{itemize}\vspace{1ex}

\noindent Any pattern in the winning numbers is likely short-lived and coincidental, in the same way that any purely random 
 time series exhibits a number of patterns, since the number of potential patterns is infinite.

\section{Implementation details with example}

Here I show an example of a typical sequence. It illustrates how the winning numbers are computed for the sequence in question. The purpose is to explain the mechanics for one of the 8-bit systems. The 32-bit version offers more flexibility, as well as potential returns that can beat those of a state lottery jackpot. The sample 8-bit sequence is defined by the public algorithm below.


\subsection{Seeds, public and private algorithms}\label{porcinired}

I now describe the public algorithm. It works as follows. Start with initial values $y_0$ and $z_0$ that are positive integers 
called \textcolor{index}{seeds}\index{seed (dynamical systems)}, with $z_0>y_0$. Then for $t = 0, 1, 2$ and so on, compute $y_{t+1}$ and $z_{t+1}$ iteratively as follows:\vspace{1ex}

\noindent  \textcolor{white}{0000}{\bf If}  $ z_t  <2y_t$   {\bf then}   \\
  \textcolor{white}{000000}  $y_{t+1}=4y_t-2z_t$\\
 \textcolor{white}{000000} $z_{t+1}=2z_t+3$\\
\textcolor{white}{0000}{\bf else} \\  
\textcolor{white}{000000} $ y_{t+1}=4y_t$\\
\textcolor{white}{000000} $ z_{t+1}=2z_t-1.$
\vspace{1ex}

\noindent I discuss the seeds in section~\ref{sredsa}. In one version of the system, the seeds are public but are extremely large integers with millions of even billions of digits. In another version, I guarantee the existence of working seeds $y_0, z_0$ smaller than $10^3$, but I keep them secret. Thus the participant would have to test up to $10^3 \times 10^3 = 10^6$ pairs of seeds to find the winning numbers. And that's just the easy part of the problem.

In this example, the public algorithm computes big numbers linked to the successive binary digits of some quadratic irrational denoted as $x_0$,
 without actually computing the digits. It is part of a family of algorithms described in section~\ref{zw23}.
The quadratic irrational $x_0$ depends on the seeds $y_0, z_0$.
The player is unaware of this fact, and certainly does not know which $x_0$ is being used. It also guarantees that the digits -- and thus the winning numbers -- are uniformly distributed and uncorrelated. In short, they constitute a perfect random sequence. 

The operator, aware of these facts, can pre-compute billions of binary digits of $x_0$ using a very fast method, or better,
 obtain these digits from an external source such as \href{https://sagecell.sagemath.org/}{Sagemath}, and store them in some lookup table. This constitutes the private algorithm. 
In Sagemath, the command to get the first $10^4$ digits of (say) $\sqrt{2}$ is 
\texttt{N(sqrt(2),prec=10000).str(base=2)}. It takes less than one second to execute. It requires more and more time the more digits you want. The
 runtime is proportional to the square of the number of digits needed.  

Some methods do not need to compute previous digits to get those starting at a given location: see section~\ref{puosaw}. This can significantly increase performance. It allows the operator to quickly start at iteration (say) $t=10^8$ when creating a new set of winning numbers, skipping the previous iterations and saving a significant amount of computing time. To the contrary, the participant, having no clue as to when the winning numbers start, must go through all the iterations when using the public algorithm.


%If 4x(t) + 1 < 2y(t) Then 
%    y(t+1) = 4y(t) - 8x(t) - 2
 %   x(t+1) = 2x(t) + 1 
%Else 
%    x(t+1) = 2x(t) 
%    y(t+1) = 4y(t). 

\subsection{Winning numbers and public data}\label{bvp0z1}

% xxxx %
The iterations in the public algorithm  represent the time. 
All the winning numbers for a particular sequence are successive 8-bit blocks starting at a specific machine-generated iteration $T$ in the public algorithm.
No one knows the starting time $T$, not even the platform operator nor its software engineers. Typically, $T > 10^8$ and can automatically be specified
 by the system,  potentially using a random value such as the actual time (in milliseconds) when it was created by the algorithm in production mode. A different $T$ is used for each sequence, and $T$ can be periodically reset to increase the security of the system.

In the 8-bit system, winning numbers are always integers between 0 and 255. In the public algorithm, they occur only at iterations $t = T, T+8, T+16, T+24$ and so on. 
The winning number at iteration 
$t\geq T$ is defined as $w_t = (z_t - 256\cdot z_{t-8} + 255)/4$. It is a positive integer. It consists of 8 successive digits in the binary expansion of the underlying quadratic irrational $x_0$ used in the private algorithm.
The reason for skipping 7 out of 8 numbers is to make sure that winning numbers are not auto-correlated.

%---------
\subsubsection{Using the public algorithm to win}

Before any winning sequence is made available to the public and participants are allowed to play, the operator publishes the previous 2000 winning numbers  attached to the sequence in question. Using the public algorithm, the participants can identify when these 2000 numbers appear in the sequence $\{w_t\}$, though they must first find the right seeds among several candidates publicly listed. Once the past winning numbers are found with the public algorithm and correct seeds, they player knows that the next number is a winning number: he can purchase that number and win with 100\% certainty, thus getting the maximum payout. 

In case multiple seeds $(y_0, z_0)$ lead to the same 2000 winning numbers at some point within the prespecified number of iterations (say, one trillion), the operator must still pay the full prize to the participant, regardless of which number the participant purchased and which seed he used.  As long as the participant can show how he found the 2000 numbers in question, by sharing the seeds that he used. So the operator should make sure that
 the probability of two different seeds yielding 2000 identical winning numbers in the right order, in less than a trillion iterations, is essentially zero, see section~\ref{uitres}.    

Because winning numbers have a random behavior, the chance of such a ``collision" is essentially zero anyway. The chance of finding the 2000 winning numbers using the exact same seed as the operator (after testing a large number of them using the public algorithm on potential seeds listed by the operator),  is considerably higher by many orders of magnitude. It is 
 essentially 100\%. However the limitation here is of a different nature: it requires years of computing time, and sequences (that is, seeds) and even algorithms are updated frequently to essentially make it impossible to win other than by luck. Given that a winning number has only 8 bits, the chance of winning by luck is $1/256$.  And even if your guess is close to the winning number, you still get a payout, albeit smaller.
%-----------

\subsubsection{Python code}\label{bornase}

The code below is also available on GitHub, \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/lottery.py}{here}.
 It covers more than the public algorithm. In particular, it computes the successive digits $d(t)$ of
 the quadratic irrational $x_0$ determined by the seeds $y_0,z_0$, for $t=1,2$ and so on. 
 Using results obtained in section~\ref{puosaw}, the implementation can be accelerated by several orders of magnitude
 when $T$ is within some known range, for instance if $\tau < T < \tau +10^4$ and $T>10^9$.  
The operator typically knows $\tau$ but not $T$, while the player knows none of them.
\vspace{1ex}

\begin{lstlisting}
# w is an 8-bit winning number if t >= T and (t - T) % 8 == 0

t = 0
y = 2          # seed: 1st component, y0 (for t = 0)
z = 5          # seed: 2nd component, z0 (for t = 0)
T = 43         # must be >= 10
max = 200    # maximum for t, must be >= T
buffer = {}  # to store 9 previous vales of z
x0 = 0          # irrational number represented by the digits 

for t in range(1, max): 

    if z < 2*y:
        y = 4*y - 2*z
        z = 2*z + 3
        d = 1        # t-th binary digit of x0  
    else:
        y = 4*y
        z = 2*z - 1
        d = 0        # t-th binary digit of x0
  
    x0 += d * 1/2**t 

    if t >= T - 8:
      buffer[t % 9] = z
      if t >= T:
          w = (z - 256*buffer[(t-8) % 9] + 255) >> 2  

    if t >= T and (t - T) % 8 == 0:
        print(t, w, d, z) 

print("\nNumber x0:", x0)
\end{lstlisting}
\vspace{1ex}

\noindent At iteration $t$, the array \texttt{buffer}, once updated, contains the 
preceding 8 values $z_{t-1}, z_{t-2},\dots, z_{t-8}$ as well as $z_t$, in circular order. Actually, rather than keeping these huge values
 in a buffer, it is sufficient to just keep the corresponding digits instead, and compute $w_t$ with the convolution formula
$$
w_t = \sum_{k=0}^7 2^k d(t-k).
$$ 
Note that $d(t) = (z_t - 2z_{t-1} + 1)/4$ is equal to either 0 or 1. It is represented by the variable  \texttt{d} in the code. 
In the end, the whole system is justified based on the following theorem.

\begin{theorem}\label{vgfd2}
If $x_0$ is an irrational number, then the corresponding sequence  
 $w_t,w_{t+8}, w_{t+16},\dots$  is free of auto-correlations regardless of $t>0$, and these numbers
 are uniformly distributed on $\{0, 1,\dots, 255\}$. 
Assuming $d(t) = (z_t - 2z_{t-1} + 1)/4$ and $z_0 > y_0$, we also have
\begin{align}
x_0 & = \sum_{t=1}^\infty \frac{d(t)}{2^t} = \frac{-(z_0 -1) + \sqrt{(z_0-1)^2+8y_0}}{4},  \label{popot2i}\\
w_t & = \sum_{k=0}^7 2^k d(t-k) =  \frac{z_t - 256\cdot z_{t-8} + 255}{4}  \label{popot2}
\end{align}
Now let's consider two sequences: one with seeds $y_0, z_0$ and quadratic irrational $x_0$, and another one with seeds $y'_0, z'_0$ and 
quadratic irrational $x'_0$. If $x_0, x'_0$ are irrational numbers linearly independent over the set of rational numbers, then
 the two sequences $w_t,w_{t+8}, w_{t+16},\dots$ and $w'_t,w'_{t+8}, w'_{t+16},\dots$ are free of cross-correlations. 

%Finally if for some integer $k\geq 0$, the seeds of both sequences are related by the mapping 
%\begin{equation}
%y'_0 = 2^{2k}y_0,\quad 
%z'_0 = 2^k(z_0-1)+1, \label{piurews}
%\end{equation}
%with $z'_0 > y'_0$, then the corresponding quadratic irrationals satisfy $x'_0 = 2^k x_0$. Thus 
% for $t=1,2$ and so on, we have:
%\begin{equation}
%d'(t) = d(t+k), \quad w'_t = w_{t+k}. \label{vce32}
%\end{equation}

\end{theorem}

\begin{proof} 
\quad \\
Formula~(\ref{popot2i}) is proved \href{https://math.stackexchange.com/questions/3537637/limit-associated-with-a-recursion-connection-to-normality-of-quadratic-irration/3553816}{here}. To prove~(\ref{popot2}), recursively use the fact
 that $d(t) = (z_t - 2z_{t-1} + 1)/4$. That is:   
\begin{align}
d(t) + 2 d(t-1) & = (z_t - 4z_{t-2} + 3)/4, \nonumber \\
d(t) + 2 d(t-1) + 4d(t-2) & = (z_t - 8z_{t-3} + 7)/4,  \nonumber \\
d(t) + 2 d(t-1) + 4d(t-2) + 8d(t-3) & = (z_t - 16z_{t-4} + 15)/4, \nonumber \\
d(t) + 2 d(t-1) + 4d(t-2) + 8d(t-3) + 16d(t-4) & = (z_t - 32z_{t-5} + 31)/4, \nonumber
\end{align}
and so on. From this, it is clear that the sequence $w_t, w_{t+8},w_{t+16},\dots$  consists of successive blocks of 8 bits in the binary digit representation of the
 number $x_0$. Thus assuming these digits are a realization of independent Bernoulli trials with same chance of fail/win (thus
 assuming $x_0$ is a \textcolor{index}{good seed}\index{seed (dynamical systems)!good seed} of the 
\textcolor{index}{dyadic map}\index{dyadic map}),
 then the winning numbers are independent with a uniform distribution on $\{0,\dots,255\}$. The context here is about the empirical (observed) joint distribution of the first $n$ digits, its limit when $n\rightarrow\infty$,  and the conjecture that $x_0$ is a  \textcolor{index}{strongly normal}\index{normal number!strongly normal} number. The concept of strong normality is defined in section~\ref{vcfprng}. See also section~\ref{cxcdsojhg} where correlation is defined. Also note that $0<x_0<1$ since
$y_0 < z_0$. The last statement about the absence of cross-correlations is true if both $x_0$ and $x_0'$ are strongly normal. 
See \href{https://stats.stackexchange.com/questions/450922/cross-correlations-in-digit-distributions}{here}, and 
Exercise~\ref{pqcorr}. \qed
\end{proof}\vspace{1ex}

\noindent Table~\ref{tabtres} illustrates some of the quantities discussed here. It corresponds to the output of the Python code
 in section~\ref{bornase}, assuming the winning numbers $w_t$ start at iteration $t = T = 43$. In this case,
 $x_0 = -1 +\sqrt{2}$. The number $z_t$, needed to compute $w_t$, grows by a factor 2 at each iteration.
 In practice, $T$ is above~$10^9$.


%\renewcommand{\arraystretch}{0.99999} %%%
\begin{table}%[H]
\small
\setlength\extrarowheight{-2pt}
\[
\begin{array}{rrrc}
\hline
t	&  w_t & d(t) & z_t\\ 
\hline
  43 &  157 &  1 & 49758216191605 \\
  51 &  230 &  0 & 12738103345051545 \\
  59 &   72 &  0 & 3260954456333195553 \\
  67 &   69 &  1 & 834804340821298061589 \\
  75 &  151 &  1 & 213709911250252303767133 \\
  83 &  216 &  0 & 54709737280064589764386657 \\
  91 &  155 &  1 & 14005692743696534979682984557 \\
  99 &   55 &  1 & 3585457342386312954798844046557 \\
 107 &   84 &  0 & 917877079650896116428504075918673 \\
 115 &  171 &  1 & 234976532390629405805697043435180717 \\
 123 &  233 &  1 & 60153992292001127886258443119406264229 \\
 131 &  241 &  1 & 15399422026752288738882161438568003643333 \\
 139 &  214 &  0 & 3942252038848585917153833328273408932693849 \\
 147 &  246 &  0 & 1009216521945237994791381332037992686769626073 \\
 155 &   11 &  1 & 258359429617980926666593621001726127813024274477 \\
 163 &  168 &  0 & 66140013982203117226647966976441888720134214266529 \\
 171 &  147 &  1 & 16931843579443998010021879545969123512354358852231757 \\
 179 &  186 &  0 & 4334551956337663490565601163768095619162715866171330281 \\
 187 &  132 &  0 & 1109645300822441853584793897924632478505655261739860552209 \\
 195 &  206 &  0 & 284069197010545114517707237868705914497447747005404301366073 \\
\hline
\end{array}
\]
\caption{\label{tabtres} Winning number $w_t$, digit $d(t)$, and $z_t$ at iteration $t$}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%


\subsection{Optimizing computations}\label{puosaw}

Regardless of $k\geq 0$, the change of seeds $y'_0 =  2^{2k}y_0, z'_0 = 2^k(z_0-1)+1$ leads to the new quadratic irrational $x'_0 = 2^k x_0$, according to formula~(\ref{popot2i}). Thus, in theory, it allows you to  get the digits of $x_0$ starting at location $k+1$ without  computing the first $k$ digits. And consequently, the corresponding winning numbers -- no matter how far in the sequence -- with little computational efforts. However, this is true as long as $y'_0 < z'_0$. 
In practice, when $k$ is large, this is never the case. 

A workaround consists in finding seeds $y_0', z_0'$ leading to $x_0' = 2^kx_0 - \lfloor 2^k x_0\rfloor$ where
 the brackets represent the integer part function. It involves computing 
\textcolor{index}{integer square roots}\index{integer square root} [\href{https://en.wikipedia.org/wiki/Integer_square_root}{Wiki}], that is, the integer part of the square root of very large integers. There are very efficient methods to accomplish this,
 especially to get the binary digits. The \textcolor{index}{mpmath}\index{mpmath (Python library)} and 
\textcolor{index}{gmpy2}\index{gmpy2 (Python library)} Python libraries offer specific functions for these
 computations. Indeed you can even use the \texttt{isqrt} function from the math library. Here I illustrate how to
 do it with the \texttt{isqrt} function from the gmpy2 library: it is implemented in C and possibly the fastest of all. 

The starting point is to reverse the problem. Given two large positive integers $\alpha,\beta$, you want to find the seeds 
 $y_0, z_0$ for the quadratic irrational 
$x_0 = -\alpha + \sqrt{\beta}$. 
The choice of $\alpha,\beta$ must result in $0<x_0<1$ for the private algorithm to work.
By virtue of~(\ref{popot2i}), this leads
\begin{equation}
\alpha = \lfloor \sqrt{\beta} \rfloor, \quad y_0 = 2(\beta - \alpha^2),\quad z_0 = 1 + 4\alpha. \label{patron}
\end{equation}
To skip the first $k$ digits, you need to find seeds $y_0',z_0'$ such that $x'_0 = -\alpha' +\sqrt{\beta'}$ with 
 $\beta'= 2^{2k}\beta$ and $\alpha'=\lfloor \sqrt{\beta'} \rfloor$. This leads to the same solution
 as~(\ref{patron}), but this time with $y_0,z_0,\alpha,\beta$ replaced respectively by
$y_0',z_0',\alpha',\beta'$. The main challenge is the computation of $\alpha'$, that is, the integer square root of $2^{2k}\beta$.

I provide the code below if you want to use the gmpy2 library. Here \texttt{square} represents $\beta'$. For $k=10^9$ and $\beta=3$, it took about one minute on my laptop, excluding the time needed to print the result. The integer square root \texttt{isqrt} is computed in base 16, which is very handy since winning numbers are 8-bit long, thus easy to encode in base 16: each winning number consists of two consecutive digits in base 16.  It would be even better to use \texttt{base=256} in the code snippet. The maximum base allowed
 when I tested my script, was~60.

For much larger values of $k$, the code needs to be modified, as you will run out of memory and require a parallel architecture at some point. The time required seems to be a sublinear function of $k$ until you use  up all the memory. I suggest running it in GPU. \vspace{1ex}

\begin{lstlisting}
import gmpy2

beta = 3
base = 16
k = 10**9
square = beta * (2**(2*k))
isqrt = gmpy2.isqrt(square).digits(base)
print(isqrt)
\end{lstlisting}


\subsection{Seeds with billions of digits and enhanced system}\label{sredsa}

The methodology also works when the seeds are not integer numbers: Formula~(\ref{popot2i}) and~(\ref{popot2}) remain valid even
 if $y_0, z_0$ are irrational numbers, as long as $0< y_0< z_0$. In particular, the winning numbers $w_t$ are still 8-bit integers. The public algorithm still works, but now $y_t$ and $z_t$ are no longer integers. As a result, it will quickly lead to numeral inaccuracy and completely erroneous numbers unless you use special techniques to handle very high precision float, with billions of digits after the decimal point. Think about $y_0 = \log(415)$ and
 $z_0 = \exp(\sqrt{7\pi})$ just to give an example of the possibilities. This makes it a lot harder to win for the player. But 
it does not  make it much more difficult for the operator since it knows $x_0$, and the player does not.

When $y_0$ and $z_0$ are integers, the public algorithm -- if implemented in Python -- automatically takes care of very large integers. This no longer works when the seeds are not integers. In the above example, the seeds are private although some minimum information about them is provided to the player, to guarantee that the set of winning numbers is uniquely defined given the public information, and that the player only has to try a finite set of seeds.  Should the winning numbers be reachable via multiple sets of seeds, it may increase the player's chance of winning. Thus the operator might want to make sure that there is only one path to the winning numbers, given the public information.

\subsubsection{Towards maximum security}\label{tms20}

What if some player figures out the connection between the public algorithm and Formula~(\ref{popot2i}) and~(\ref{popot2})? For instance, by reading this chapter. He still has to try many seeds, but he could relatively quickly compute (possibly in a number of weeks or months) billions of digits of all the potential candidates for $x_0$, and then get an hedge to find the winning numbers.
In order to make the system robust against such hacks,  there are different possibilities, in addition to frequently changing the seeds or using non-integer seeds: \vspace{1ex}
\begin{itemize}
\item Reparametrize $y$ and $z$ in the public algorithm, using a mapping $(x, y) \mapsto (y',z')$
 such as a linear transform. The new public algorithm will be  based on $y', z'$, using the updated recursion. The player may not see the connection between the new recursion and the version published in this chapter.
\item Use an entirely different recursion attached to more general algebraic numbers rather than quadratic irrationals. This is a work in progress.
\item The easiest solution is to keep the recursion as is, but instead use (say) 5 different seeds $(y_{0,k},z_{0,k})$ with 
$0\leq k < 5$. It leads to 5 different sequences, and the winning number at iteration $t$ comes from the $k$-th
 sequence, with $k = t \bmod 5$.
\end{itemize}\vspace{1ex}
The last option is simple and offers flexibility. For instance, you could use 200 rather than 5 sequences.

\subsubsection{Real example}
 
Here I illustrate the method with a simple example. In this case, the public data consists of two pairs of seeds
 $(y_{0,1}, z_{0,1})$, $(y_{0,2}, z_{0,2})$ and the past 2000 winning numbers. Can you find the next 500 winning numbers with this information? One of the two seeds leads to the winning numbers, the other one does not. Also available in the public information: you need less than one trillion iterations to identify the 2000 winning numbers in question. 
You may use the
 public algorithm in section~\ref{porcinired} to answer this question, or better, any hack of your own. The
 public information is available at the following locations: \vspace{1ex}

\begin{itemize}
\item The seeds $y_{0,1}, z_{0,1}, y_{0,2}, z_{0,2}$ are on GitHub. The links to the files are
 \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/lottery_seed_y10000000_2.txt}{$y_{0,1}$}, 
\href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/lottery_seed_z10000000_2.txt}{$z_{0,1}$}, 
 \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/lottery_seed_y10000000_3.txt}{$y_{0,2}$}, 
 \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/lottery_seed_z10000000_3.txt}{$z_{0,2}$}.
Each of them is about 3 megabytes (uncompressed).
\item  The file with the 2000 past winning numbers can be found \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/lottery_winning_numbers_123903793_2.txt}{here}. 
It also contains the 500 future winning numbers. Ignore them: these are the numbers that you are supposed to find. I included them so you that can check against the winning numbers that you come up with. 
\end{itemize}\vspace{1ex}

\noindent Note that the filenames contain information about times and seeds chosen in this test. In the file with the winning numbers,
 the first column represents the time $t$. In practice, this information is not available or encrypted. Here you can retrieve
 the winning numbers in a matter of minutes, by using this information, with the Python code in this section. Pretend that you
 don't have this information, and see if you can retrieve the winning numbers in a reasonable amount of time. The
 size for each seed is about 3 megabytes. In an industry-grade version, this size could be several gigabytes or even terabytes, and the secrete $\beta$ (see code) much larger than the one used in this test, and hard to find. 


Increasing the number of candidate seeds (with only one leading to winning numbers) may not significantly increase the security of the system, as hackers may pre-compute tables with billions of digits for as many integers ($\beta$ in the code)  
as they can~\cite{rkan92}. A much better solution is to
 interlace winning numbers from multiple seeds, as discussed in the last bullet item in section~\ref{tms20}. In this case, the hacker
 does not know which seed is used at any given iteration, as many combinations are possible. Also, working with trillions rather than billions of digits makes it considerably harder for hackers. To this day, only the first $10^{13}$ digits of $\sqrt{2}$ are known, see \href{https://en.wikipedia.org/wiki/Square_root_of_2#Records_in_computation}{here}.
Typically, such computations require
 months of computing time. 

The Python code below is private and not shared with the player. The operator uses it to generate the winning numbers. It is also on GitHub \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/lottery_fast.py}{here},
 and named \texttt{lottery\_fast.py}. The variables \texttt{y0\_1}, \texttt{z0\_1}, \texttt{beta\_1}, and 
\texttt{alpha\_1} in the code correspond respectively to $y_0', z_0', \beta'$ and $\alpha'$  in section~\ref{puosaw}.
In this case, the quadratic irrational is $x_0 = -\lfloor\sqrt{\beta}\rfloor  + \sqrt{\beta}$. You can skip
 the computation of the first $10^8$ digits and directly jump at iteration $1+ 10^8$ by
 setting \texttt{offset=10**8} in the code. The number of winning numbers produced is 
about one eighth of \texttt{n}.

The code allows you to determine the seed when starting at an arbitrary iteration \texttt{t+offset} rather
 than $t=1$. I use this functionality to create the large public seeds leading to winning numbers. Of course, \texttt{offset} is kept private. It also allows you to compute the winning numbers starting at an even much larger iteration $t=T$, with $T$ also kept secret. 
One issue is checking whether the digits produced are correct. In practice I use two different mechanisms to compute the digits, to double-check. I have seen implementations that are correct for the first few million digits, then fail later on in the sequence. \vspace{1ex}

 \begin{lstlisting}
import gmpy2

def big_seed(offset, beta):

    beta_1  = beta * (2**(2*offset)) 
    alpha_1 = int(gmpy2.isqrt(beta_1)) 
    y0_1 = 2 * (beta_1 - alpha_1*alpha_1)
    z0_1 = 1 + 4*alpha_1
    return(y0_1, z0_1)


n = 20000          # number of digits to compute 
offset = 10**7     # digits start at location 1 + offset
beta = 2
y0, z0 = big_seed(offset, beta)
y = y0
z = z0

digits = {}
winning_numbers = {}

for t in range(1, n): 
    if z < 2*y:
        y = 4*y - 2*z
        z = 2*z + 3
        digits[t + offset] = 1        
    else:
        y = 4*y
        z = 2*z - 1
        digits[t + offset] = 0   
    if t > 8 and t % 8 == 5:
        w = 0
        for k in range(8):
            w += digits[t + offset - k] * (2**k)
        winning_numbers[t + offset - k] = w
        print(t + offset, w)

filename = "lottery_seed_y" + str(offset) + "_" + str(beta) + ".txt"
OUT=open(filename,"w")
OUT.write(str(y0))
OUT.close()

filename = "lottery_seed_z" + str(offset) + "_" + str(beta) + ".txt"
OUT=open(filename,"w")
OUT.write(str(z0))
OUT.close()

filename = "lottery_winning_numbers_" + str(offset) + "_" + str(beta) + ".txt"
OUT=open(filename,"w")
for time in winning_numbers:
    number = winning_numbers[time]
    OUT.write(str(time) + "\t" + str(number) + "\n")
OUT.close()
\end{lstlisting}

\subsection{Collision risks}\label{uitres}


This problem is related to the probability of finding (or not) a substring in a string~\cite{noonan99}. In this context, the player may use the wrong  seeds yet by some incredible luck, find the past winning numbers in his own sequence. This is called a collision. Given the constraints of the system, I show that the chance of such a collision is incredibly close to zero.

Here the substring consists of the past 2000 winning numbers, in other words $m=8\times 2000 \approx 2^{14}$ bits. 
The string consists of numbers originating from a random-looking sequence, other than that used by the operator. In terms of string terminology, the associated alphabet -- the letters -- consists of two symbols: 0 and 1. 
Since the winning numbers are guaranteed to show up within the first trillion iterations, the size of the string (the long random-looking sequence) is about $N=2^{40}$ bits. 

Finally, if the operator publishes a set of $K + 1$ seeds with one of them leading to the past winning numbers, what is the chance that at least one of the $K$ remaining sequences also leads to the same past winning numbers, albeit (in all likelihood) starting at a different position? 
Now I give an approximate answer to this question. The probability of the $m$-bit subsequence of winning numbers being in a given larger $N$-bit sequence is
\begin{equation}
1- \Big(1- \frac{1}{2^m}\Big)^{N-m+1} \approx \frac{N-m+1}{2^m} \approx 2^{-15960}.\label{tortelle}
\end{equation}
So, even if $K=2^{300}$, it is still considerably smaller than $2^m$, and the risk of at least one collision across the $K$ sequences is 
  not larger than (\ref{tortelle}) multiplied by $K$. In short, the operator can offer a selection of $2^{300}$ seeds, claiming
 that only one of them leads to the winning numbers within a trillion iterations. 
 

\subsection{Exercises}

The following original exercises complement the theory. Several are good candidates as job interview questions for
 software engineers, or exam questions in computer science programs. They range in complexity from relatively simple (requiring less than one hour of work) to difficult. To help
 you decide which exercises to work on depending on your interests, I added a title to each of them.

\begin{Exercise}\label{kn5z21990fr} {\em Partial sums of binary digits} --  Let $s_t(x_0) = d(1,x_0) +\cdots + d(t,x_0)$ be the sum of the first $t$ binary digits of a real number $x_0\in [0, 1[$. Thus $0\leq s_t(x_0)\leq t$. Typically, it is easier to study these sums rather than the individual digits, as they are less volatile.  Show that $s_{t+1}(x_0/2) = s_t(x)$, for $t=0,1,2$ and so on.  By convention, $s_0(x_0)=0$. Also show that 
\begin{equation}
 2x_0 = \sum_{t=1}^\infty \frac{s_t(x_0)}{2^t}, \label{presaton}
\end{equation}
where $d(t,x_0)$ is the $t$-th binary digit of $x_0$. Finally, using a \textcolor{index}{greedy algorithm}\index{greedy algorithm}, for any real number $x_0\in[0, 1]$, show how to compute $s_t(x_0)$ without computing the individual digits.\vspace{1ex} \\ 
{\bf Solution} \\
Formula~(\ref{presaton}) is obtained by summing the left and right hand sides of the following equalities:
\begin{align}
x_0 & = \frac{d(1,x_0)}{2} +  \frac{d(2,x_0)}{4}+ \frac{d(3,x_0)}{8} + \cdots, \nonumber \\
\frac{x_0}{2} & =   \hspace{47pt}  \frac{d(1,x_0)}{4} +  \frac{d(2,x_0)}{8} + \cdots, \nonumber \\
\frac{x_0}{4} & =   \hspace{94pt}   \frac{d(1,x_0)}{8} + \cdots, \nonumber
\end{align}
and so on. As for the greedy algorithm, it works as follows. Let $0\leq x_0<1$ and
$$
h_n(x_0) = \sum_{t=1}^n \frac{\varphi_t(x_0)}{2^t}, \quad n=1,2,\dots
$$
Iteratively compute $h_n(x_0)$ for $n=1,2$ and so on, by defining
$\varphi_{n}(x_0)$ as the largest integer with $0\leq \varphi_{n}(x_0)\leq n$, such that $h_{n}(x_0) \leq 2x_0$. Then
 $\varphi_{t}(x_0)= s_t(x_0)$ for all $t$. Also, as $n\rightarrow\infty$, $h_n(x_0)\rightarrow 2x_0$. 

Note that if you replace 
the constraint $0\leq \varphi_{n}(x_0)\leq n$ by $0\leq \varphi_{n}(x_0)\leq 1$, 
then you get the binary digit representation of 
 $2x_0$ instead, with $\varphi_{t}(x_0)= d(t, 2x_0)$ for all $t$ and $h_n(x_0)\rightarrow 2x_0$, assuming $x_0<\frac{1}{2}$.  
\end{Exercise}


\begin{Exercise}\label{kn5es} {\em Autocorrelation in the sequence of winning numbers} -- Given a seed
 $(y_0,z_0)$, show that the autocorrelation in the sequence $\{w_t\}$ with $t=1,2$ and so on, is $\frac{1}{2}$.
To the contrary, the autocorrelation in the sequence $w_t, w_{t+8}, w_{t+16}$ and so on, is zero regardless of where you start. \vspace{1ex} \\ 
{\bf Solution} \\
   The sequence $w_t, w_{t+8}, w_{t+16},\dots$ consists of successive blocks of non-overlapping bits in the binary representation of $x_0$, or in other words, digits in base 256. These blocks are independent and thus uncorrelated assuming $x_0$ is a \textcolor{index}{strongly normal} number. To the contrary, the sequence $w_t, w_{t+1}, w_{t+2},\dots$ consists of overlapping
 blocks. For instance, $w_t$ and $w_{t+1}$ both have 8 bits, but share 7 bits. Thus the autocorrelation.
\end{Exercise}

\begin{Exercise}\label{kn5es} {\em Some digit sequences are more random than others} -- Identify some 
 quadratic irrationals that exhibit
 a less random behavior than others, in their binary digit sequence. Can you explain why? \vspace{1ex} \\ 
{\bf Solution} \\
 If you pick up a million quadratic irrationals and look at the first million digits for each of them, you are bound to find some extremes. Actually, failure to find such extremes would be an indication of lack of randomness. You would expect these numbers, at least some of them, given enough digits, to {\em locally} exhibit some patterns. 

In the end, the number of zeros and ones in a random sequence of $n$ digits is governed by the \textcolor{index}{law of the iterated logarithm}\index{law of the iterated logarithm}, see section~\ref{iterlawsd}. It is typically different from $n/2$, with a discrepancy 
in the order $\sqrt{n}$. The maximum discrepancy is of the order $\sqrt{n\log\log n}$. A lower discrepancy, say of order $n^{1/4}$, is actually a sign of non-randomness, contrarily to intuition: see section~\ref{azxa} for examples and discussion.

Other metrics such as the length of the maximum run, are governed by similar laws. 
 I looked at those statistics to identify outliers, and shared my results in Table~\ref{tabuchi}. For instance
 the seed $y_0=90, z_0=91$ leading to $x_0 = (-45 +\sqrt{2203})/2$, fails the 
\textcolor{index}{prime test of randomness}\index{prime test (of randomness)} discussed in section~\ref{vcfprng}, at least for the first $\num{20000}$ digits.

In general, it is a good idea to skip the first million digits, as increased randomness typically kicks in later in sequences that are otherwise initially less random.  
\end{Exercise}

\begin{Exercise}\label{knoy54} {\em Square-free integers and seeds to avoid} -- Numbers of the form $(z_0-1)^2 +8$ where $z_0>1$ is an integer, rarely contain a square factor. These numbers  appear in Formula~(\ref{popot2i}) with $y_0=1$, and thus satisfy $y_0 < z_0$
 as required in Theorem~\ref{vgfd2}. To the contrary, among all positive integers, only a proportion $6/\pi^2$ consists of square-free
 integers. Identify integers of the form $(z_0-1)^2 +8$ that are not square-free, and show how rare they are. \vspace{1ex} \\ 
{\bf Solution} \\
 You can use the Python code in section~\ref{tr3te4} to answer this question. Among the first \texttt{Niter} integers of the form
 $(z_0-1)^2 +8$, the variable \texttt{accepted} counts those that are square-free. Seeds $(y_0,z_0)$ leading to non square-free integers $(z_0-1)^2 +8y_0$ must be removed because they introduce cross-correlations among the various sequences of winning numbers across multiple seeds. The fact that these seeds are rare means that very few must be rejected.
\end{Exercise}

\begin{Exercise}\label{pqcorr} {\em Cross-correlation between binary digit sequences} -- This is a difficult exercise. Let 
 $X=x_0$ be a positive real number with random digits, each digit with probability $\frac{1}{2}$ of being one. If $p, q$ are two strictly positive co-prime integers, free of factors of the form $2^k$ for $k=1,2$ and so on, then the correlation between the binary digit sequences of $pX$ and $qX$, is equal to $(pq)^{-1}$. By digit sequences, I mean the digits starting after the decimal point.  
How would you use this fact to prove that the binary digits of $x_0= -1+\sqrt{2}$ and $x'_0=-2 +\sqrt{7}$ are
 uncorrelated? Also show that the correlation between the binary digits of $-4 + \sqrt{18}$ and $-7+\sqrt{50}$ is $1/15$.
\vspace{1ex} \\ 
{\bf Solution} \\
Use the Python code in Exercise~\ref{knoy55wc} to do some preliminary investigations, starting with $p=1$ and $q=3$, to empirically confirm that the correlation is $(pq)^{-1}$. 

If $x_0= -1+\sqrt{2}$ and $x'_0=-2 +\sqrt{7}$, then there is no real number $X$ such that
 $x_0=pX, x'_0=qX$ and $p, q$ are positive integers. Otherwise $x_0/x'_0 = p/q$ would be a rational number, and we know
 that $x_0/x'_0$ is irrational. Indeed, the only way to make $p/q$ irrational is to have $p, q\rightarrow\infty$. Then the correlation
  $(pq)^{-1}$ in question (for the digit sequences of $x_0$ and $x'_0$) is zero, the desired result. 

To prove that the correlation is $(pq)^{-1}$, for the digit sequences of $pX$ and $qX$ assuming $p,q$ are co-prime positive integers free of powers of 2,
 see \href{https://stats.stackexchange.com/questions/450922/cross-correlations-in-digit-distributions}{here}. The proof is unfinished. It uses the same notations as in the Python code in Exercise~\ref{knoy55wc}. It relies on analyzing the carry-over mechanism when doing the multiplications $pX$ and $qX$ using the grade school algorithm (implemented in the Python code). I invite you to finish my proof. 

Finally, $\sqrt{18} = 3\sqrt{2}$ and $\sqrt{50} = 5\sqrt{2}$. Using $X=\sqrt{2}, p=3$, and $q=5$, the correlation between the two corresponding digit sequences is $(pq)^{-1}= 1/15$. It can be verified empirically. This assumes that $X=\sqrt{2}$ is \textcolor{index}{strongly normal}\index{normal number!strongly normal}, a conjecture widely believed to be true.
\end{Exercise}



%--------------------------------------------------------------
\end{comment}
%-------------------------------------------------------------


\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{refstats} % Entries are in the refs.bib file in same directory as the tex file
\printindex



\hypersetup{linkcolor=red} % red %
\hypersetup{linkcolor=red}



\end{document}