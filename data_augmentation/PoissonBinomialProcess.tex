\documentclass[10pt]{article}
\usepackage{amsmath}    % need for subequations
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bigints}
\usepackage{graphicx}   % need for figures
\usepackage{subfig}
\usepackage{verbatim}   % useful for program listings
\usepackage{color}      % use if color is used in text
%\usepackage{subfigure}  % use for side-by-side figures
\usepackage{parskip}
\usepackage{float}
\usepackage{courier}
\usepackage{exercise}
\usepackage{sistyle}
\SIthousandsep{,}
%\usepackage{numprint}
\setlength\parindent{0pt}

\newtheorem{prop}{Proposition}


\renewcommand{\DifficultyMarker}{}
\newcommand{\AtBeginExerciseHeader}{\hspace{-21pt}}  %-0.2pt
\renewcommand{\ExerciseHeader}{\AtBeginExerciseHeader\textbf{\ExerciseName~\ExerciseHeaderNB} \ExerciseTitle}
\renewcommand{\AnswerHeader}{\large\textbf{\AnswerName~\ExerciseHeaderNB}\smallskip\newline}
\setlength\AnswerSkipBefore{1em}

\usepackage{makeidx}
%\usepackage[noautomatic,nonewpage]{imakeidx}
%\usepackage[columns=3,indentunit=15pt,justific=raggedright, columnsep=10pt, font=footnotesize]{idxlayout} %%%%%%%%%%%%%
%%\usepackage[indentunit=15pt,justific=raggedright, columnsep=10pt, font=normalsize]{idxlayout} %%%%%%%%%%%%%
%\makeindex[columns=1]
\makeindex

%\usepackage[columns=3]{idxlayout}
%https://mirrors.rit.edu/CTAN/macros/latex/contrib/imakeidx/imakeidx.pdf  doc about imakeidx
\usepackage[nottoc]{tocbibind}


\usepackage[colorlinks = true,
          linktocpage=true,
            pagebackref=true, % add back references to bibliography
            linkcolor = red,
            urlcolor  = blue,
            citecolor = red,
%            refcolor  =red,
            anchorcolor = blue]{hyperref}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{index}{rgb}{0.88,0.32,0}

%------- source code settings
\usepackage{listings}
\lstset{frame=tb,
  language=Perl,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

%-----------------------------------------------------------------

\usepackage{blindtext}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }


\setlength{\baselineskip}{0.0pt} 
\setlength{\parskip}{3pt plus 2pt}
\setlength{\parindent}{20pt}
\setlength{\marginparsep}{0.0cm}
\setlength{\marginparwidth}{0.0cm}
\setlength{\marginparpush}{0.0cm}
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.4}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

\usepackage[symbols,nogroupskip,acronym]{glossaries-extra}
%\usepackage[xindy,symbols,nogroupskip,sort=def,acronym]{glossaries}
\makenoidxglossaries %%%%%%%%%%%%%%%
%\setlength{\glsdescwidth}{1.3\hsize}

\newglossary*{gloss}{Glossary}

\newglossaryentry{gls:mip}{type=gloss,name={$m$-interlacing},description={Superimposition of $m$ Poisson-binomial processes, each
with its own (shifted, stretched) {\em lattice space}, and its own intensity and scaling factor. See pages },text={$m$-interlacing}}

\newglossaryentry{gls:anisotropy1}{type=gloss,name={Anisotropy},description={A property of a point process: the points are evenly scattered in all directions. The point distribution is stochastically invariant under rotations. See pages },text={anisotropy}}

\newglossaryentry{gls:attract}{type=gloss,name={Attraction / Repulsion},description={The larger the scaling factor $s$, the less repulsion (also called {\em inhibition}) among the points of the process. Cluster processes discussed here exhibit both attraction (points tend to cluster together) and repulsion among cluster centers (due to the underlying lattice structure). See pages },text={attraction}}

\newglossaryentry{gls:be}{type=gloss,name={Boundary Effect},description={Also called edge effect. Bias in estimated point counts or nearest neighbor distances, due to unobserved points located outside but close to the finite window of observations. Also the result of missing points in the window of observations, in simulated point processes especially when the scaling factor $s$ is large. Special techniques are used to handle this problem. See pages },text={boundary effect}}

\newglossaryentry{gls:cr}{type=gloss,name={Confidence Region},description={A confidence region of  level $\gamma$ is a 2-D set of minimum area covering a proportion $\gamma$ of the mass of a bivariate probability distribution. See pages },text={confidence region}}

\newglossaryentry{gls:cc}{type=gloss,name={Connected Component},description={A set of vertices in a graph that are connected to each other by paths. 
See also \gls{gls:nng}. See pages },text={connected component}}

\newglossaryentry{gls:empdis}{type=gloss,name={Empirical Distribution},description={Cumulative frequency histogram attached to a statistic (for instance, nearest neighbor distances), and based on observations. When the number of observations tends to infinity and the bin sizes tends to zero, this step function tends to the theoretical cumulative distribution function of the statistic in question. See pages },text={empirical distribution}}

\newglossaryentry{gls:ergo}{type=gloss,name={Ergodicity},description={A statistic such as the interarrival times is ergodic if it has the same asymptotic distribution, whether it is computed on many observations from a single realization of the process, or averaged across many realizations, each with few observations. See pages },text={ergodicity}}

\newglossaryentry{gls:homo1}{type=gloss,name={Homogeneity},description={A property of a point process, characterized by an homogeneous intensity function, that is, constant or independent of the location. See pages },text={homogeneous}}

\newglossaryentry{gls:mi}{type=gloss,name={Identifiability},description={A models is identifiable if it is uniquely defined by its parameters. Then it is possible to estimate each parameter separately. A trivial example of non-identifiability is when we have two parameters, say $\alpha,\beta$, but they only occur in a product $\alpha\beta$. In that case, if $\alpha\beta=6$, it is impossible to tell whether $\alpha=2,\beta=3$ or $\alpha=1,\beta=6$. See pages },text={model identifiability}}

\newglossaryentry{gls:index1}{type=gloss,name={Index Space},description={Consists of the indices $h,k\in \mathbb{Z}$, attached to the points $X_k$ in one dimension, or $(X_h,Y_k)$ in two dimensions. See pages },text={index space}}

\newglossaryentry{gls:intensity1}{type=gloss,name={Intensity},description={Core parameter of the Poisson-binomial process. Denoted as $\lambda$. It represents the granularity of the underlying lattice, that is, the point density. In $d$ dimensions,  $\mbox{E}[N(B)]=1$ for any hypercube $B$ of length $1/\lambda$. Here $N$ is the point count. When $\lambda$ is constant (not depending on the location), the process is homogeneous. See pages },text={intensity}}

\newglossaryentry{gls:ia}{type=gloss,name={Interarrival Time},description={In one dimension, random variable measuring the distance between a point of the process and its closest neighbor to the right, on the real axis. Interarrival times are also called {\em increments}. See pages },text={interarrival times}}

\newglossaryentry{gls:lattice1}{type=gloss,name={Lattice Space},description={In two dimensions, it consists of the locations $(h/\lambda,k/\lambda)$ with $h,k\in\mathbb{Z}$. The distribution of a point $(X_h,Y_k)$ is centered at $(h/\lambda,k/\lambda)$. The concept can be extended to any dimension. See pages },text={lattice space}}

\newglossaryentry{gls:lsc}{type=gloss,name={Location-scale},description={A random variable $X$ has a location-scale distribution with two parameters, the scale $s$ and location $\mu$, if any linear transformation $a+bX$ has a distribution of the same family, with parameters respectively $b^2s$ and $\mu+a$. Here $\mu$ is the expectation and $s$ is proportional to the variance of the distribution. See pages },text={location-scale distributions}}

\newglossaryentry{gls:modulo}{type=gloss,name={Modulo Operator},description={Sometimes, it is useful to work with point ``residues" modulo $\frac{1}{\lambda}$, instead of the original points, due to the nature of the underlying lattice. It magnifies the patterns of the point process. By definition, 
$X_k \bmod{\frac{1}{\lambda}}=X_k-\frac{1}{\lambda}\lfloor \lambda X_k \rfloor$ where the brackets represent the integer part function. See pages },text={modulo}}

\newglossaryentry{gls:nng}{type=gloss,name={NN Graph},description={Nearest neighbor graph. The vertices are the points of the process. Two vertices (the points they represent) are connected if at least one of the two points is nearest neighbor to the other one. This graph is undirected. See pages },text={nearest neighbor graph}}

\newglossaryentry{gls:pc}{type=gloss,name={Point Count},description={Random variable, denoted as $N(B)$, counting the number of points of the process in a particular set $B$, typically an interval $[a, b]$ in one dimension, and a square or circle in two dimensions. See pages },text={point count}}

\newglossaryentry{gls:pb55}{type=gloss,name={Point Distribution},description={Random variable representing how a point of the process is distributed in a domain $B$; for instance, for a stationary Poisson process, points are uniformly distributed on any compact domain $B$ (say, an interval in one dimension, or a square in two dimensions). See pages },text={point distribution}}

\newglossaryentry{gls:quant}{type=gloss,name={Quantile function},description={Inverse of the cumulative distribution function (CDF) $F$, denoted as $Q$. 
Thus if $P(X<x)=F(x)$, then $P(X<Q(x))=x$. See pages },text={quantile function}}


\newglossaryentry{gls:sf}{type=gloss,name={Scaling Factor},description={Core parameter of the Poisson-binomial process. Denoted as $s$, proportional to the variance of the distribution $F$ attached to the points of the process. It measures the level of repulsion among the points (maximum if $s=0$, minimum if $s=\infty$). In $d$ dimensions, the process is stationary Poisson of intensity $\lambda^d$ if $s=\infty$, and coincides with the fixed {\em lattice space} if $s=0$. See pages },text={scaling factor}}

\newglossaryentry{gls:sv}{type=gloss,name={Shift vector},description={The lattice attached to a 2-D Poisson-binomial process consists of the 
vertices $(\frac{h}{\lambda},\frac{k}{\lambda})$ with $h,k\in \mathbb{Z}$. A shifted process has its lattice translated by a shift vector $(u,v)$. The 
new vertices are $(u+\frac{h}{\lambda},v+\frac{k}{\lambda})$. See page },text={shift vector}}

\newglossaryentry{gls:stdp}{type=gloss,name={Standardized Process},description={Poisson-binomial process with intensity $\lambda=1$, scaling factor $s=1$, and shifted (if necessary) so that the lattice space coincides with $\mathbb{Z}$ or $\mathbb{Z}^2$. See page },text={standardized}}

\newglossaryentry{gls:state1}{type=gloss,name={State Space},description={Space where the points of the process are located. Here, $\mathbb{R}$ or $\mathbb{R}^2$. See also {\em index space} and {\em lattice space}. See pages },text={state space}}

\newglossaryentry{gls:statio}{type=gloss,name={Stationarity},description={Property of a point process: the point distributions in two sets of same shape and area, are identical. The process is stochastically invariant under translations. See pages },text={stationary}}


%\setlength{\glsdescwidth}{1.2\hsize}
%\glsxtrnewsymbol[description={Cumulative distribution function}]{CDF}{{CDF}}
%\glsxtrnewsymbol[description={Standardized CDF of the $X_k$'s, see Formula~(\ref{eq:intro00}) page}]{F(x)}{\ensuremath{F(x)}}  
%\glsxtrnewsymbol[description={Same as $F(x/s)$}]{F_s(x)}{\ensuremath{F_s(x)}}  
%\glsxtrnewsymbol[description={Density attached to $F(x)$}]{f(x)}{\ensuremath{f(x)}}
%\glsxtrnewsymbol[description={Density attached to $F_s(x)$}]{f_s(x)}{\ensuremath{f_s(x)}}
%\glsxtrnewsymbol[description={Intensity or granularity of the process}]{slambda}{\ensuremath{\lambda}}  % a large $\lambda$ corresponds to high a granularity lattice space
%\glsxtrnewsymbol[description={Scaling factor, proportional to the variance of $F_s$}]{s}{\ensuremath{s}}% control the distance between two neighbor points; most repulsive when s = 0

%https://tex.stackexchange.com/questions/161443/glossary-style-long-but-without-the-indentation-before-the-notations
% use $\gls{F}$ for referencing

\begin{document}

\hypersetup{linkcolor=blue}
%inserting a glossary entry in gloss: \gls{gls:keyword1} \\


\begin{center}
{\Large \bf{Stochastic Processes and Simulations\\  
 \addvspace{1ex}
A Machine Learning Perspective}} \\ 
\addvspace{5ex}
\end{center}
 
\begin{center}
Vincent Granville, Ph.D.\\
vincentg@MLtechniques.com\\
 \href{https://mltechniques.com/}{www.MLtechniques.com}\\
\quad 
Version 6.0, June 2022
\end{center}
\quad\\ \\
{\bf Note}: External links (in blue) and internal references (in red) are clickable throughout this document. Keywords highlighted in orange are indexed; those in red are both indexed and in the glossary section. 


\hypersetup{linkcolor=red} % red %
\tableofcontents  %% \thispagestyle{empty} after \tableofcontents to remove page numbering


%\pagebreak
%\listoffigures
%\listoftables
\hypersetup{linkcolor=red}

% list of symbols -- https://tex.stackexchange.com/questions/348640/how-to-effectively-use-list-of-symbols-for-a-thesis

%\pagebreak

%=======================
\section*{About this Textbook}
\addcontentsline{toc}{section}{About this Textbook}
%=======================

This scratch course on stochastic processes covers significantly more material than usually found in traditional books or classes. The approach is original:  I introduce a new yet intuitive type of random structure called perturbed lattice or Poisson-binomial process, 
as the gateway to all the stochastic processes. Such models have started to gain considerable momentum recently, especially in sensor data, cellular networks, chemistry, physics and engineering applications. I present state-of-the-art material in simple words, in a compact style, including new research developments and open problems. I focus on the methodology and principles, providing the reader with solid foundations and numerous resources: theory, applications, illustrations, statistical inference, references, glossary, educational spreadsheet, source code, stochastic simulations, original exercises, videos and more.  \\
\quad \\
Below is a short selection highlighting some of the topics featured in the textbook. Some are research results published here for the first time. \\
\quad \\

\begin{center}
%\begin{table}[H]
%\begin{tabular}{ll}
%\begin{tabular}{p{\q}p{\q}}
\begin{tabular}{p{\dimexpr3.5cm-2\tabcolsep}p{\dimexpr12cm-2\tabcolsep}}
\hline
GPU clustering & Fractal supervised clustering in GPU (graphics processing unit) using image filtering techniques akin to neural networks, automated black-box detection of the number of clusters, unsupervised clustering in GPU using density (gray levels) equalizer \\
\hline
Inference &  New test of independence,  spatial processes, model fitting, dual confidence regions, minimum contrast estimation, oscillating estimators, mixture and surperimposed models, radial cluster processes, 
exponential-binomial distribution with infinitely many parameters, generalized logistic distribution\\
\hline
Nearest neighbors & Statistical distribution of distances and Rayleigh test, Weibull distribution, properties of nearest neighbor graphs, size distribution of connected components, geometric features, hexagonal lattices, coverage problems, simulations, model-free inference \\
\hline
Cool stuff & Random functions, random graphs, random permutations, chaotic convergence, perturbed Riemann Hypothesis (experimental number theory), attractor distributions in extreme value theory,
central limit theorem for stochastic processes, numerical stability, optimum color palettes, cluster processes on the sphere\\
\hline
Resources & 28 exercises with solution expanding the theory and methods presented in the textbook, well documented source code and formulas to generate various deviates and simulations, simple recipes (with source code) to design your own data animations as MP4 videos -- see ours on YouTube\\
\hline
\end{tabular}
%\end{table}
\end{center}


\quad \\

This first volume deals with point processes in one and two dimensions, including spatial processes and clustering. The next volume in this series will cover other types of stochastic processes, such as Brownian-related and random, chaotic dynamical systems. The point process which is at the core of this textbook is called the Poisson-binomial process (not to be confused with a binomial nor a Poisson process) for reasons that will soon become apparent to the reader. Two extreme cases are the standard Poisson process, and fixed (non-random) points on a lattice. Everything in between is the most exciting part. 


%------------------------------------------
\subsection*{Target Audience}
\addcontentsline{toc}{subsection}{Target Audience}

College-educated professionals with an analytical background (physics, economics, finance, machine learning, statistics, computer science, quant, mathematics, operations research, engineering, business intelligence), students enrolled in a quantitative curriculum, decision makers or managers working with data scientists, graduate students, researchers and college professors, will benefit the most from this textbook. The textbook is also intended to professionals interested in automated machine learning and artificial intelligence.

It includes many original exercises requiring out-of-the-box thinking, and offered with solution. Both students and college professors will find them very valuable. Most of these exercises are an extension  of the core material.  Also, a large number of internal and external references are immediately accessible with one click, throughout the textbook: they are highlighted respectively in red and blue in the text. The material is organized to facilitate the reading in random order as much as possible and to make navigation easy. It is written for busy readers.

The textbook includes full source code, in particular for simulations, image processing, and video generation. You don't need to be a programmer to understand the code. It is well documented and easy to read, even for people with little or no programming experience. Emphasis is on good coding practices. The goal is to help you quickly  develop and implement your own machine learning applications from scratch, or use the ones offered in the textbook.
The material also features professional-looking spreadsheets allowing you to perform interactive statistical tests and simulations in Excel alone, without statistical tables or any coding. 
The code, data sets, videos and spreadsheets are available on my GitHub repository.  

The content in this textbook is frequently of graduate or post-graduate level and thus of interest to researchers. Yet the unusual style of the presentation makes it accessible to a large audience, including students and professionals with a modest analytic background (a standard course in statistics). It is my hope that it will entice beginners and practitioners faced with data challenges, to explore and discover the beautiful and useful aspects of the theory, traditionally inaccessible to them due to jargon. 
 

%-----------------------------------------------------
\subsection*{About the Author}\label{sec:vg}
\addcontentsline{toc}{subsection}{About the Author}

Vincent Granville, PhD is a pioneering data scientist and machine learning expert, co-founder of Data Science Central (acquired by a publicly traded company in 2020), former VC-funded executive, author and patent owner. Vincent's past corporate experience includes Visa, Wells Fargo, eBay, NBC, Microsoft, CNET, InfoSpace and other Internet startup companies (one acquired by Google). Vincent is also a former post-doct from Cambridge University, and the National Institute of Statistical Sciences (NISS). He is currently publisher at \href{http://www.datashaping.com}{DataShaping.com}. He makes a living as an independent researcher working on stochastic processes, dynamical systems, experimental math and probabilistic number theory. 

Vincent published in Journal of Number Theory, Journal of the Royal Statistical Society (Series B), and IEEE Transactions on Pattern Analysis and Machine Intelligence, among others. He is also the author of multiple  books, including ``Statistics: New Foundations, Toolbox, and Machine Learning Recipes", ``Applied Stochastic Processes, Chaos Modeling, and Probabilistic Properties of Numeration Systems" with a combined reach of over 250,000, as well as ``Becoming a Data Scientist'' published by Wiley.
For details, see my Google Scholar profile, \href{https://scholar.google.com/citations?user=pWFKWPMAAAAJ&hl=en}{here}.

\pagebreak

%==========================
\section{Poisson-binomial or Perturbed Lattice Process}
%==========================

I introduce here one of the simplest point process models. The purpose is to illustrate, in simple English, the theory of point processes using one of the most elementary and intuitive examples, keeping applications in mind. Many other point processes will be covered in the next sections, both in one and two dimensions. Key concepts, soon to be defined, include: 
\vspace{1ex} \\

%----------------------------
\begin{center}
%\begin{table}[H]
\begin{tabular}{llr}
\hline
\multicolumn{1}{c}{Category} & \multicolumn{1}{c}{Description} & \multicolumn{1}{c}{Book sections} \\
\hline
\hline
Top parameters & Intensity $\lambda$ -- granularity of the process  &  \ref{liisc}, \ref{estim1}\\
 & Scaling factor $s$ -- quantifies point repulsion or mixing  &  \ref{cils1}, \ref{estim1}\\
 & Distribution $F$ -- location-scale family, with $F_s(x)=F(x/s)$ & \ref{s:def}, \ref{bore}\\
\hline
Properties &  Stationarity and ergodicity & \ref{stationarity}, \ref{fpbp}\\
 & Homogeneity and anisotropy & \ref{homgprop} \\
 & Independent increments &  \ref{indinc}, \ref{indep1}\\
\hline
Core distributions &  Interarrival times $T$ & \ref{intro2}, \ref{link20} \\
& Nearest neighbor distances  & \ref{ssnn}, \ref{lattnn}\\
 & Point count $N(B)$ in a set $B$  &  \ref{pca23}, \ref{fpbp}\\
 & Point distribution (scattering, on a set $B$) & \ref{intro2} \\
\hline
Type of process &  Marked point process & \ref{mpp} \\
& Cluster point process & \ref{s:clp}, \ref{illus}\\
 & Mixtures and interlacings (superimposed processes) & \ref{sm1}, \ref{spa2}\\
\hline
Topology & Lattice space (index space divided by $\lambda$) & \ref{s:clp}, \ref{hm} \\
  &State space  (where the points are located) & \ref{s:clp}\\
 & Index space (hidden space of point indices: $\mathbb{Z}$ or $\mathbb{Z}^2$) & \ref{hm}, \ref{permut}\\
\hline
Other concepts &  Convergence to stationary Poisson point process & \ref{convpoisson}, \ref{convpp}\\
   & Boundary effects  & \ref{boundary}\\
  & Dimension (of the state space) & \ref{intro2}\\
  & Model identifiability & \ref{hardid}\\
\hline
\end{tabular}
%\end{table}
\end{center}
%----------------------------



\quad \\

I also present several probability distributions that are easy to sample from, including logistic, uniform, Laplace and Cauchy. I use them in the simulations. I also introduce new ones  such as the \textcolor{index}{exponential-binomial distribution}\index{distribution!exponential-binomial} (the distribution of interarrival times), and a new type of \textcolor{index}{generalized logistic distribution}\index{distribution!generalized logistic}. One of the core distributions is the \textcolor{index}{Poisson-binomial}\index{distribution!Poisson-binomial} with an infinite number of parameters. The Poisson-binomial process is named after that distribution, attached to the  
\gls{gls:pc}\index{point count distribution}\index{point process!point count distribution} (a random variable) counting the number of points found in any given set. By analogy, the Poisson point process is named after the Poisson distribution for its point count. Poisson-binomial processes are also known as \textcolor{index}{perturbed lattice point processes}\index{perturbed lattice process}.
Lattices\index{lattice}, also called \textcolor{index}{grids}\index{grid}, are a core topic in this textbook, as well as \textcolor{index}{nearest neighbors}\index{nearest neighbors}.

Poisson-binomial processes are different from both Poisson and \textcolor{index}{binomial processes}\index{point process!binomial}. However, as we shall see and prove, they converge to a \textcolor{index}{Poisson process}\index{point process!Poisson} when a parameter called the \gls{gls:sf}\index{scaling factor} (closely related to the variance), tends to infinity. In recent years, there has been a considerable interest in perturbed lattice point processes, see \cite{ghosh2020,poi103}. The \textcolor{index}{Poisson-binomial process}\index{point process!Poisson-binomial} is lattice-based, and indeed, \textcolor{index}{perturbed lattice point processes}\index{lattice!perturbed lattice}\index{point process!perturbed lattice process}\index{perturbed lattice process} and Poisson-binomial processes are one and the same. The name ``Poisson-binomial" has historical connotations and puts emphasis on its combinatorial nature, while ``perturbed lattice" is more modern, putting emphasis on topological features and modern applications such as cellular networks.  

Poisson-binomial point processes with small \gls{gls:sf}\index{scaling factor} $s$ are good at modeling lattice-based structures such as crystals, exhibiting \textcolor{index}{repulsion}\index{repulsion (point process)} (also called \textcolor{index}{inhibition}\index{inhibition (point process)}) among the points, see Figure~\ref{fig:pbr4b}.  They are also widely used in cellular networks, see references in Section~\ref{s:clp}. 

%------------------------------
\subsection{Definitions}\label{s:def}

A \textcolor{index}{point process}\index{point process} is a (usually infinite) collection of points, sometimes called events in one dimension, randomly scattered over the real line (in one dimension), or over the entire space in higher dimensions. The points are denoted as $X_k$ with $k\in\mathbb{Z}$ in one dimension, or $(X_h,X_k)$ with $(h,k)\in \mathbb{Z}^2$ in two dimensions. The random variable $X_k$ takes values in $\mathbb{R}$, known as the 
\gls{gls:state1}
\index{state space}. 
In two dimensions, the state space is $\mathbb{R}^2$. The points are assumed to be independently distributed, though not identically distributed. Later in this textbook, it will be evident from the context when we are dealing with the one or two dimensional case. 

In one dimension, the \textcolor{index}{Poisson-binomial process}\index{point process!Poisson-binomial} is characterized by infinitely many points $X_k$, $k\in\mathbb{Z}$, each centered around $k/\lambda$, independently distributed with  
\begin{equation}
P(X_k<x)=F\Big(\frac{x-k/\lambda}{s}\Big),\label{eq:intro00}
\end{equation}
where 
\begin{itemize}
\item The parameter $\lambda>0$  is called the 
\gls{gls:intensity1}%\textcolor{index}{intensity}
\index{intensity function}\index{intensity function}; 
it represents the granularity of the process. The expected number of points in an interval of length $1/\lambda$ (in one dimension) or in a square of area $1/\lambda^2$ (in two dimensions), 
is equal to one. This generalizes to higher dimensions. 
The set $\mathbb{Z}/\lambda$ (or $\mathbb{Z}/\lambda \times \mathbb{Z}/\lambda$ in two dimensions)
 is the underlying 
\gls{gls:lattice1} %\textcolor{index}{lattice} 
\index{lattice}\index{grid} of the process (also called the \textcolor{index}{grid}), while $\mathbb{Z}$ (or $\mathbb{Z}^2$ in two dimensions) is called the \gls{gls:index1}\index{index!index space}. The difference between state and lattice space is illustrated in Figure~\ref{fig:index}.
\item The parameter $s >0$ is the 
\gls{gls:sf}\index{scaling factor}, 
closely related to the variance. It determines the degree of mixing among the $X_k$'s. When $s=0$, $X_k=k/\lambda$ and the points are just the lattice points; there is no randomness. When $s$ is infinite, the process becomes a classic \gls{gls:statio}\index{stationarity}\index{point process!stationary} Poisson point process of intensity $\lambda^d$, where $d$ is the dimension.
\item The cumulative distribution function (CDF) $F(x)$ is continuous and belongs to a family of 
\gls{gls:lsc} %\textcolor{index}{location-scale distributions}
\index{location-scale distribution}\index{distribution!location-scale}[\href{https://en.wikipedia.org/wiki/Location-scale_family}{Wiki}]. It is centered at the origin ($F(0)=\frac{1}{2}$), and symmetric ($F(x)=1-F(-x)$). Thus it has zero expectation, assuming the expectation exists. Its derivative, denoted as $f(x)$, is the density function; it is assumed to be unimodal (it has only one maximum), with the maximum value attained at $x=0$.
\end{itemize}
\noindent In two dimensions, Formula~(\ref{eq:intro00}) becomes
\begin{equation}
P[(X_h,Y_k)<(x,y)]=F\Big(\frac{x-h/\lambda}{s}\Big)F\Big(\frac{y-k/\lambda}{s}\Big).             \label{eq:intro00B}
\end{equation}
Typical choices for $F$ are
\begin{align}
 & \mbox{Uniform: } F(x) =  \frac{1}{2}+\frac{x}{2} \mbox{ if } -1\leq x \leq 1, \mbox{ with } F(x)=1 \mbox{ if } x>1 \mbox{ and } F(x)=0 \mbox{ if } x < -1 \nonumber\\
 & \mbox{Laplace: } F(x) = \frac{1}{2}+\frac{1}{2} \mbox{sgn}(x)(1+\exp(-|x|))\nonumber \\
& \mbox{Logistic: } F(x) = \frac{1}{1+\exp(-x)}\nonumber\\
& \mbox{Cauchy: } F(x) = \frac{1}{2}+\frac{1}{\pi}\arctan(x) \nonumber
\end{align}
where $\mbox{sgn}(x)$ is the sign function [\href{https://en.wikipedia.org/wiki/Sign_function}{Wiki}], with $\mbox{sgn}(0)=0$. Despite the appearance, I use the standard form of these well-known distributions, when the location parameter is zero, and the scaling factor is $s=1$. It looks unusual because I define them via their cumulative distribution function 
(CDF),  %
rather than via the more familiar density function. Throughout this textbook, I use the CDF and its inverse (the \gls{gls:quant}\index{quantile!quantile function}) for simulation purposes.


\begin{table}[H]
\[
\begin{array}{lccccc}
\hline
 F &  \mbox{Uniform} & \mbox{Logistic} & \mbox{Laplace} & \mbox{Cauchy} & \mbox{Gaussian} \\
\hline
 \mbox{Var}[F_s] & s^2/3 & \pi^2 s^2/3 & 2s^2 & \infty &  s^2 \\
\hline
\end{array}
\]
\caption{\label{tab123}Variance attached to $F_s$, as a function of $s$}
\end{table}
Table \ref{tab123} shows the relationship between $s$ and the actual variance, for the distributions in question. I use the notation $F_s(x)=F(x/s)$ and $f_s(x)$ for its density,  
interchangeably throughout this textbook. Thus, $F(x)=F_1(x)$ and $f(x)=f_1(x)$. In orther words, $F$ is the standardized version of $F_s$. In two dimensions, I use $F(x,y)=F(x)F(y)$, assuming independence between the two coordinates: see Formula~(\ref{eq:intro00B}). \vspace{1ex} \\
{\bf Remark}: The parameter $s$ is called the scaling factor because it is proportional to the variance of $F_s$, but visually speaking, it represents the amount of repulsion among the points of the process. See visual impact of a small $s$ in Figure~\ref{fig:pbr4b}, and of a larger one in Figure~\ref{fig:pbr}.  


%-----------------------------------------
\subsection{Point Count and Interarrival Times}\label{intro2}

An immediate result is that $F_s(x-k/\lambda)$ is centered at $k/\lambda$. Also, if $s=0$, then $X_k=k/\lambda$. If $s$ is very small, $X_k$ is very close to $k/\lambda$ most of the time. But when $s$ is large, the points $X_k$'s are no longer ordered, and the larger $s$, the more randomly they are permutated (or shuffled, or mixed) on the real line. 

Let $B=[a,b]$ be an interval on the real line, with $a<b$, and $p_k=P(X_k\in B)$. We have:
\begin{align}
p_k   & = F_s(b-t_k) - F_s(a-t_k) \nonumber \\
  & = F\Big(\frac{b-k/\lambda}{s}\Big)-F\Big(\frac{a-k/\lambda}{s}\Big)\label{eq:f0}
\end{align}
This easily generalizes to two dimensions based on Formula~(\ref{eq:intro00B}). As a consequence, the integer-valued random variable $N(B)$ counting the number of points of the process in  a set $B$, known as the \textcolor{index}{counting measure}\index{counting measure} [\href{https://en.wikipedia.org/wiki/Counting_measure}{Wiki}] or 
\gls{gls:pc} %\textcolor{index}{point count}
\index{point count distribution}, has a
\textcolor{index}{Poisson-binomial distribution}\index{distribution!Poisson-binomial} of parameters $p_k,k\in\mathbb{Z}$ 
[\href{https://en.wikipedia.org/wiki/Poisson_binomial_distribution}{Wiki}]. The only difference with a standard Poisson-binomial distribution is that here, we have infinitely many parameters (the $p_k$'s). Basic properties of that distribution yield:
\begin{align}
\mbox{E}[N(B)] & = \sum_{k=-\infty}^\infty p_k \label{eq:f1}\\
\mbox{Var}[N(B)] & = \sum_{k=-\infty}^\infty p_k(1-p_k)\label{eq:f2} \\ 
P[N(B)=0] & = \prod_{k=-\infty}^\infty (1-p_k) \label{eq:f3} \\
P[N(B)=1] & = \Big(\sum_{k=-\infty}^{\infty}\frac{p_k}{1-p_k}\Big) \cdot P[N(B)=0]\label{eq:f4}
\end{align}
It is more difficult, though possible, to obtain the higher moments $\mbox{E}[N^r(B)]$ or $P[N(B)=r]$ in closed form if $r>2$. This is due to the combinatorial nature of the 
Poisson-binomial distribution. But you can easily obtain approximated values using simulations.


Another fundamental, real-valued random variable, denoted as $T$ or $T(\lambda,s)$,  is the 
\gls{gls:ia} %\textcolor{index}{interarrival time} 
\index{interarrival times} 
between two successive points of the process, once the points are ordered on the real line. In two dimensions, it is replaced by the distance between a point of the process, and its nearest neighbor. Thus it satisfies (see Section~\ref{link20}) the following identity:
$$P(T>y)=P[N(B)=0], $$
with $B = ]X_0, X_0+y]$, assuming it is measured at $X_0$ (the point of the process corresponding to $k=0$). See Formula~(\ref{eq:1}) for the distribution of $T$. In practice, this intractable exact formula is not used; instead it is approximated via simulations. Also, the point $X_0$ is not known, since the $X_k$'s are in random order, and retrieving $k$ knowing $X_k$ is usually not possible.  The indices (the $k$'s) are hidden. However, see Section \ref{hm}. The fundamental question is whether using $X_0$ or any $X_k$ (say $X_5$), matters for the definition of $T$. This is discussed in Section~\ref{stationarity}
and illustrated in Table~\ref{tab234}.




Finally, the 
\gls{gls:pb55} \index{point distribution} %% 
is also of particular interest. In one dimension, this distribution can be derived from the distribution of interarrival times: the distance between two successive points. For instance, for a stationary  Poisson process on the real line (that is, the intensity $\lambda$ does not depend on the location), the points in any given set $B$ are uniformly and independently distributed in $B$, and the interarrival times have an exponential distribution of expectation $1/\lambda$. However, for Poisson-binomial processes, there is no such simple result. If $s$ is small, the points are more evenly spaced than the laws of pure randomness would dictate, see Figure~\ref{fig:pbr4b}. Indeed, the process is called \textcolor{index}{repulsive}\index{repulsion (point process)}\index{point process!repulsive}: it looks as if the points behave like electrical charges, all of the same sign, exercising repulsive forces against each other. Despite this fact, the points are still independently distributed. To the contrary, cluster processes later investigated in this textbook, exhibit point \gls{gls:attract}\index{attraction (point process)}\index{point process!attractive}: it looks as if the points are attracted to each other. \vspace{1ex} \\
{\bf Remark}: A \textcolor{index}{binomial process}\index{point process!binomial} is defined as a finite set of points uniformly distributed over a domain $B$ of finite area. Usually, the number of points is itself random, typically with a \textcolor{index}{binomial distribution}\index{binomial distribution}\index{distribution!binomial}.

%-----------------------------------------
\subsection{Limiting Distributions, Speed of Convergence}\label{convpoisson}

I prove in Theorem \ref{sums2} that Poisson-binomial processes converge to ordinary Poisson processes. In this section, I illustrate the rate of convergence, both for the interarrival times and the point count in one dimension. 

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{pb-converge.PNG}
%  \includegraphics[width=\linewidth]{pbx2.PNG}
\caption{Convergence to stationary Poisson point process of intensity $\lambda$}
\label{fig:pbconverge}
\end{figure}
In Figure~\ref{fig:pbconverge}, we used $\lambda=1$ and $B=[-0.75, 0.75]$; $\mu(B)=1.5$ is the length of $B$. The limiting values (combined with those of Table \ref{tab124}), as $s\rightarrow\infty$, are in agreement with $N(B)$'s moments converging to those of a Poisson distribution of expectation $\lambda\mu(B)$, and $T$'s moments to those of an exponential distribution of expectation $1/\lambda$. 
In particular, it shows that $P[N(B)=0]\rightarrow\exp[-\lambda\mu(B)]$ and $E[T^2]\rightarrow 2/\lambda$  as $s\rightarrow\infty$.  
These limiting distributions are features unique to stationary Poisson processes of intensity $\lambda$.

Figure~\ref{fig:pbconverge} illustrates the speed of convergence of the Poisson-binomial process to the stationarity Poisson process of intensity $\lambda$, as $s\rightarrow\infty$. Further confirmation is provided by Table~\ref{tab124}, and formally established by Theorem \ref{sums2}. Of course, when testing data, more than a few statistics are needed to determine whether you are dealing with a Poisson process or not. For a full test, compare the empirical \textcolor{index}{moment generating function}\index{moment generating function} (the estimated $\mbox{E}[T^r]$'s say for all $r\in[0,3]$) or the \gls{gls:empdis}\index{empirical distribution} of the interarrival times, with its theoretical limit (possibly obtained via simulations) corresponding to a Poisson process of intensity $\lambda$. The parameter $\lambda$ can be estimated based on the data. See details in Section \ref{inference}.


In Figure \ref{tab123}, the values of $\mbox{E}[T^2]$ are more volatile than those of $P[N(B)=0]$ because they were estimated via simulations; to the contrary, $P[N(B)=0]$ was computed using the exact Formula (\ref{eq:f3}), though truncated to 20,000 terms. The choice of a Cauchy or logistic distribution for $F$ makes almost no  difference. But a uniform $F$ provides noticeably slower, more bumpy convergence. The Poisson approximation is already quite good with $s=10$, and only improves as $s$ increases. Note that in our example, $N(B)>0$ if $s=0$. This is because $X_k=k$ if $s=0$; in particular, $X_0=0\in B=[-0.75, 0.75]$. Indeed $N(B)>0$ for all small  enough $s$, and this effect is more pronounced (visible to the naked eye on the left plot, blue curve in Figure \ref{fig:pbconverge}) if $F$ is uniform. Likewise, $E[T^2]=1$ if $s=0$, as $T(\lambda,s)=\lambda$ if $s=0$, and here $\lambda=1$. 

The results discussed here in one dimension easily generalize to higher dimensions. In that case $B$ is a domain such as a circle or square, and $T$ is the distance between a point of the process, and its nearest neighbor. The limit Poisson process is stationary with intensity $\lambda^d$, where $d$ is the dimension.

%-------------------------------------------
\subsection{Properties of Stochastic Point Processes}\label{stationarity}

In this section, we review key features of point processes in general, applied to the Poisson-binomial process introduced in Section~\ref{s:def}. A more comprehensive yet elementary presentation of some of these concepts (except those in Section~\ref{trtr}), for the one-dimensional case and for traditional stochastic models (Markov chains, renewal, birth and death, queuing and Poisson processes), is found in any textbook on the subject, for instance in ``Introduction to Stochastic Models" by R. Goodman \cite{goodman}.

\subsubsection{Stationarity}

There are various definitions of \textcolor{index}{stationarity}\index{stationarity}\index{point process!stationary} [\href{https://mathworld.wolfram.com/StationaryPointProcess.html}{Wiki}] for point processes.  The most common one is that the distribution of the point count $N(B)$ depends only on $\mu(B)$ (the length or area of $B$), but not on its location. The Poisson-binomial process is not \gls{gls:statio}. Assuming $\lambda=1$, if $s$ is small enough, the point count distribution attached to (say) $B_1=[0.3, 0.8]$ is different from that attached to $B_2=[5.8, 6.3]$, despite both intervals having the same length. This is obvious if $s=0$: in that case $N(B_1)=0$, and $N(B_2)=1$.  However, if $B_1=[a, b]$ and 
$B_2=[a+k/\lambda,b+k/\lambda]$, then $N(B_1)$ and $N(B_2)$ have the same distribution, regardless of $k\in\mathbb{Z}$; see Theorem~\ref{combiexp} for a related result. So, knowing the theoretical distribution of $N([x,x+1/\lambda])$ for each $0\leq x<1/\lambda$ 
is enough to know the distribution of $N(B)$ on any interval $B$. Since $\lambda$ is unknown when dealing with actual data, it must be estimated using techniques described in Section~\ref{inference}. This generalizes to two dimensions, with the interval $N([x,x+1/\lambda])$ replaced by the square $N([x,x+1/\lambda]) \times N([y,y+1/\lambda])$, with $0\leq x,y<1/\lambda$. Statistical testing is discussed in \cite{sss}, also available online, \href{http://eio.usc.es/pub/metma/descargas/comas-mateu-calduch-metmav.pdf}{here}.

The interarrival times $T$ face fewer non-stationarity issues, as evidenced by Theorem~\ref{et}, Table~\ref{tab234}, and Exercise~\ref{exercise8}. It should be favored over the point count $N(B)$, when assessing whether your data fit with a Poisson-binomial, or a Poisson point process model. In particular, it does not depend, for practical purposes, on the choice of $X_0$ in the definition of $T$ in Section~\ref{intro2}. The definition could be changed using (say) $X_5$, or any other $X_k$ instead of $X_0$, with no impact on the theoretical distribution.

\subsubsection{Ergodicity}
This brings us to the concept of \gls{gls:ergo}\index{ergodicity}\index{point process!ergodic}.  It is heavily used 
in the active field of \textcolor{index}{dynamical systems}\index{dynamical systems}: see \cite{dyn2,dyn3,dyn1} and my book \cite{vgdyn}
available \href{https://github.com/VincentGranville/Stochastic-Processes}{here}.
I will cover dynamical systems in details, in my upcoming book on this topic. For Poisson-binomial point processes, ergodicity means that you can estimate a quantity in two different ways:
\begin{itemize}
\item using one very long simulation of the process (a large $n$ in our case), 
\item or using many small realizations of the process (small $n$), and averaging the statistics obtained in each simulation
\end{itemize}
Ergodicity means that both strategies, at the limit, lead to to same value. This is best illustrated with the estimation of $\mbox{E}[T]$, or its higher moments. The expectation of the interarrival times $T$ is estimated, in most of my simulations, as the average distance between a point $X_k$, and its nearest neighbor to the right, denoted as $X'_k$. It is computed as an average of $X'_k-X_k$  over $k=-n,\dots,n$ with $n = 3 \times 10^4$, on a single realization of the process. The same methodology is used in the source code provided in Section~\ref{s:code}.  Likewise, $\mbox{E}[T^2]$ is estimated as the average $(X'_k-X_k)^2$ in the same way.

Table~\ref{tab234} is an exception. There I used $10^4$ realizations of a same Poisson-binomial process. In each realization I computed, among others, $T_0=X'_0 - X_0$. This corresponds to the actual definition of $T$ provided in Section~\ref{intro2}. Then I averaged these $T_0$'s over the $10^4$ realizations to get an approximated value for $T$. It turns out that both methods lead to the same result. This is thanks to ergodicity, as far as $T$ is concerned. I may as well have averaged $T_5=X'_5 -X_5$ over the $10^4$ realizations, and end up with the same result for $\mbox{E}[T]$. Note that not all processes are ergodic. The difference between stationarity and ergodicity is further explained \href{https://dsp.stackexchange.com/questions/1167/what-is-the-distinction-between-ergodic-and-stationary}{here}.

%-----
\subsubsection{Independent Increments}\label{indinc}

A one dimensional point process is said to have \textcolor{index}{independent increments}\index{independent increments} 
or independent \gls{gls:ia} \index{interarrival times}
if the point counts $N(B_1), N(B_2)$ for any two non-overlapping time intervals
 $B_1,B_2$ are independent.  It is shown in some textbooks, for instance \cite{karl} (available online \href{http://www.columbia.edu/~ks20/stochastic-I/stochastic-I-PP.pdf}{here}), that the only stationary 
\textcolor{index}{renewal process}\index{renewal process}\index{point process!renewal process} with independent increments is the stationary Poisson process. The proof is simple, and based on the fact that the only distribution having the memoryless property, is the exponential one. Another definition of independent increments [\href{https://en.wikipedia.org/wiki/Independent_increments}{Wiki}] is based on the independence of the successive interarrival times. If combined with ``identically distributed", it allows you, for Poisson-binomial process, to choose any arbitrary $k$ to define the interarrival times as the random variable $T=X'_k-X_k$, where $X'_k$ is the closest neighbor point of $X_k$, to the right on the real line. These two definitions of ``independent increments" are not equivalent, since the first one based on point count, is measured at arbitrary locations, while the second one, based on interarrival times, is (in one dimension) the interdistance between actual points of the process. The point count and interarrival times are related by the identity $P(T>y)=P[N(B_0)=0]$, where $B_0=]X_0,X_0+y]$, see Section~\ref{link20}.

An off-the-beaten-path test of independence is discussed in Section~\ref{indep1}, precisely to assess the assumption of independent increments, on simulated data. A related concept is the \textcolor{index}{memoryless property} [\href{https://en.wikipedia.org/wiki/Memorylessness}{Wiki}]. 


\subsubsection{Homogeneity}\label{homgprop}

An ordinary Poisson point process (the limit, as $s\rightarrow\infty$, of a Poisson-binomial process) is said to be 
\gls{gls:homo1} %\textcolor{index}{homogeneous}
\index{homogeneity}\index{point process!Poisson!non homogeneous} 
if the intensity $\lambda$ does not depend on the location. In the case of the Poisson process, homogeneity is equivalent to stationarity. Even for non-homogenous Poisson processes,
the point count $N(B_1)$ and $N(B_2)$, attached to two disjoint sets $B1,B_2$, are independently (though not identically) distributed. This is not the case for Poisson-binomial processes, not even for those that are homogeneous.  

Poisson-binomial processes investigated so far are homogeneous. I discuss non-homogeneous cases in Sections~\ref{sm1}, \ref{sm2} and \ref{s:clp}. A non-homogeneous Poisson-binomial process is one where the intensity $\lambda$ depends on the index $k$ attached to a point $X_k$.

%------------------------------------------
\subsection{Transforming and Combining Multiple Point Processes}\label{trtr}

I discuss here a few types of \textcolor{index}{point process operations}\index{point process operations}, including translation, rotation, superimposition, mixtures of point processes and marked point processes. Cluster point processes, a particular type of superimposed processes, are not introduced in this section: they are treated in detail in Section~\ref{s:clp} 
and~\ref{ssnn}. Another type of operation called \textcolor{index}{thinning}\index{point process!thinned}\index{thinning (point process)} (see \cite{thinned}, available online \href{http://www.stat.ucla.edu/~frederic/papers/thinning8b.pdf}{here}), is not described in this textbook.

\subsubsection{Marked Point Process}\label{mpp}

In one dimension, a \textcolor{index}{marked point process}\index{marked point process}\index{point process!marked process}  is similar to a couple of paired time series. It has two components: the base process, modeled here as a Poisson-binomial process, and a ``mark" (a random variable) attached to each point. In one dimension, the base process typically represents time occurrences of events, and marks represent some feature attached to each event. The definition easily generalizes to any dimension.

An example is the highest yearly flood occurrences for a particular river, over a long time period, say 200 years. Due to yearly recurrence,  a Poisson-binomial process where the intensity is $\lambda=1$ and the time unit is a year, is better suited than a standard Poisson process. The marks measure the intensity of each maximum yearly flood. Another, 3-D example, is the position of atoms in a crystal. The marks may represent the type of atom. 

Formally, in one dimension, a marked point process is a (usually infinite) set of points $(X_k,Y_k)$ with $k\in\mathbb{Z}$. The definition easily generalizes to higher dimensions. Typically, 
 the $Y_k$'s (the marks) are independently distributed, and independently distributed from the underlying process $(X_k)$. The underlying process can be a
Poisson-binomial process.

%---------------------------
\subsubsection{Rotation, Stretching, Translation and Standardization}\label{stret1}

In two dimensions, rotating a Poisson-binomial process is equivalent to rotating its underlying \textcolor{index}{lattice}\index{lattice} attached to the \gls{gls:index1}. Rotating the points has the same effect as rotating the lattice locations, because $F$ (the distribution attached to the points) belongs to a family of 
\gls{gls:lsc} %% location-scale distributions
\index{location-scale distribution}\index{distribution!location-scale}[\href{https://en.wikipedia.org/wiki/Location-scale_family}{Wiki}]. 
For instance, a $\pi/4$ rotation will turn the square lattice into a centered-square lattice [\href{https://en.wikipedia.org/wiki/Square_lattice}{Wiki}], but it won't change the main properties of the point process. Both processes, the original one and the rotated one, may be indistinguishable for all practical purposes unless the \gls{gls:sf} $s$ is small, creating
\gls{gls:mi} %% model identifiability
\index{identifiability}[\href{https://en.wikipedia.org/wiki/Identifiability}{Wiki}] issues. For instance, the theoretical correlation between the point coordinates $(X_h, Y_k)$ or the underlying lattice point coordinates $(h/\lambda,k/\lambda)$, measured on all points, remains equal to zero after rotation, because the number of points is infinite (this may not be the case if you observe points through a small window, because of 
\glspl{gls:be}\index{boundary effect}).
Thus, a Poisson-binomial process has a point distribution invariant under rotations, on a macro-scale. This property is called 
\gls{gls:anisotropy1}\index{anisotropy}\index{point process!anisotropic} [\href{https://en.wikipedia.org/wiki/Anisotropy}{Wiki}]. On a micro-scale, a few changes occur though: for instance the two-dimensional version of Theorem~\ref{combiexp} no longer applies, and the distance between the projection of two neighbor points on the X or Y axis, shrinks after the rotation. 

Applying a translation to the points of the process, or to the underlying lattice points, results in a \textcolor{index}{shifted point process}\index{shifted process}\index{point process!shifted}. It becomes interesting when multiple shifted processes, with different translation vectors, are combined together as in Section~\ref{sm1}. Theorem~\ref{combiexp} may not apply to the shifted process, though it can easily be adapted to handle this situation. One of the problems is to retrieve the underlying lattice space of the shifted process. This is useful for model fitting purposes, as it is easier to compare two processes once they have been standardized (after removing translations and rescaling). Estimation techniques to identify the shift are discussed in Section~\ref{ssnn}. 

By a 
\gls{gls:stdp}  Poisson-binomial point process, 
\index{standardized point process}\index{Poisson-binomial point process!standardized} I mean one in its canonical form, with intensity $\lambda=1$, scaling factor $s=1$, and free of shifts or rotations.  Once two processes are standardized, it is easier to compare them, assess if they are Poisson-binomial, or perform various machine learning procedures on observed data, such as testing, computing confidence intervals, cross-validation, or model fitting. In some way, this is similar to transforming and detrending time series to make them more amenable to statistical inference. There is also some analogy between the period or quasi-period of a time series, and the inverse of the intensity $\lambda$ of a Poisson-binomial process: in fact, $1/\lambda$ is the fixed increment between the underlying lattice points in the 
\gls{gls:lattice1}, %lattice space  
\index{lattice!lattice space} and can be viewed as the period of the process.

Finally, a two dimensional process is said to be \textcolor{index}{stretched}\index{stretching (point process)}\index{point process!stretched} if a different intensity is used for each coordinate for all the points of the process. It turns the underlying square lattice space into a rectangular lattice, and the \gls{gls:homo1} process into a non-homogeneous one, because the intensity varies  locally. Observed data points can be standardized using the
\textcolor{index}{Mahalanobis transformation}\index{Mahalanobis transformation} [\href{https://en.wikipedia.org/wiki/Mahalanobis_distance}{Wiki}], to remove stretching (so that variances are identical for both coordinates) and to decorrelate the two coordinates, when correlation is present. 


%----------------------------------------
\subsubsection{Superimposition and Mixing}\label{sm1}

Here we are working with two-dimensional processes. When the points of $m$ independent point processes with same distribution $F$ 
and same \gls{gls:index1}\index{index!index space} $\mathbb{Z}^2$ are bundled together, we say that the processes are \textcolor{index}{superimposed}\index{superimposition (point processes)}\index{point process!superimposed}. These processes are no longer Poisson-binomial, see 
Exercise~\ref{exercisec1}. Indeed, if the scaling factor $s$ is small and $m>1$ is not too small, they exhibit clustering around each lattice location in the \gls{gls:lattice1}\index{lattice!lattice space}. Also, the intensities or scaling factors of each individual point process may be different, and the resulting combined process may not be \gls{gls:homo1}\index{homogeneity}.
Superimposed point processes also called \textcolor{index}{interlaced}\index{interlaced processes}\index{point process!interlaced} processes. \vspace{1ex} \\
A \textcolor{index}{mixture}\index{mixture model}\index{point process!mixture} of $m$ point processes, denoted as $M$, is defined as follows: 
\begin{itemize}
\item We have $m$ independent point processes $M_1,\dots,M_m$ with same distribution $F$ and same index space $\mathbb{Z}^2$,
\item The intensity and scaling factor attached to $M_i$ are denoted respectively as $\lambda_i$ and $s_i$ ($i=1,\dots,m)$, 
\item The points of $M_i$ ($i=1,\dots,m)$ are denoted as $(X_{ih},Y_{ik})$; the index space consists of the $(h,k)$'s,
\item The point $(X_h,Y_k)$ of the mixture process $M$ is equal to $(X_{ih},Y_{ik})$ with probability $\pi_i > 0$, $i=1,\dots,m$.
\end{itemize} 

\noindent While mixing or superimposing Poisson-binomial processes seem like the same operation, which is true for \gls{gls:statio}\index{stationarity} Poisson processes, in the case of 
Poisson-binomial processes, these are distinct operations resulting in significant differences  when the scaling factors are very small (see Exercise \ref{exercise14e}). The difference
is most striking when $s=0$. In particular,
superimposed processes are less random than mixtures. This is due to the discrete nature of the underlying 
\gls{gls:lattice1}\index{lattice!lattice space}. However, with larger \glspl{gls:sf}, the behavior of mixed and superimposed processes tend to be similar. 


Several of the concepts discussed in Section~\ref{trtr} are illustrated in Figure~\ref{fig:hexa}, representing a realization of $m$ superimposed shifted stretched Poisson-binomial processes,
 called \gls{gls:mip}\index{$m$-interlacing}. For each individual process $M_i$, $i=1,\dots,m$, the distribution attached to the point $(X_{ih},X_{ik})$ (with $h,k\in \mathbb{Z}$) is
$$
P(X_{ih}<x, Y_{ik}<y) = F\Big(\frac{x-\mu_i -h/\lambda}{s}\Big)F\Big(\frac{y-\mu'_i - k/\lambda'}{s}\Big),  \quad i=1,\dots,m
$$
This generalizes Formula~(\ref{eq:intro00B}). The parameters used for the model pictured in Figure~\ref{fig:hexa} are:
\quad \\
\begin{itemize}
\item Number of superimposed processes: $m=4$; each one displayed with a different color, 
\item Color: red for $M_1$, blue for $M_2$, orange for $M_3$, black for $M_4$,
\item \gls{gls:sf}\index{scaling factor}: $s=0$ (left plot) and $s=5$ (right plot),
\item Intensity: $\lambda=1/3$ (X-axis) and $\lambda'=\sqrt{3}/3$ (Y-axis),
\item \Gls{gls:sv}\index{shift vector}, X-coordinate: $\mu_1=0, \mu_2=1/2, \mu_3=2, \mu_4=3/2 $,
\item \Gls{gls:sv}\index{shift vector}, Y-coordinate: $\mu'_1=0, \mu'_2=\sqrt{3}/2, \mu'_3=0, \mu'_4=\sqrt{3}/2 $,
\item $F$ distribution: standard centered \textcolor{index}{logistic}\index{distribution!logistic} with zero mean and variance $\pi^2/3$.
\end{itemize}
For simulation purposes, the points $(X_{ih},Y_{ik})$ of the $i$-th process $M_i$ ($i=1,\dots,m$), are generated as follows:  
\begin{align}
X_{ih} & =\mu_i + \frac{h}{\lambda} +s \cdot \log \Big(\frac{U_{ih}}{1-U_{ih}}\Big) \label{simm1}\\
Y_{ik} & =\mu'_i+ \frac{k}{\lambda'} +s \cdot \log\Big(\frac{U_{ik}}{1-U_{ik}}\Big) \label{simm2}
\end{align}
where $U_{ij}$ are uniformly and independently distributed on $[0,1]$ and $-n\leq h,k\leq n$. 
I chose $n=25$ in the simulation -- a window much larger than that of Figure~\ref{fig:hexa} -- to avoid
\glspl{gls:be}\index{boundary effect} in the picture. The boundary effect is sometimes called 
\textcolor{index}{edge effect}\index{edge effect (statistics)}. The unobserved data points outside the 
window of observations, are referred to as \textcolor{index}{censored data}\index{censored data} [\href{https://en.wikipedia.org/wiki/Censoring_(statistics)}{Wiki}]. 
Of course, in my simulations their locations and features (such as which process they belong to) are known by design. But in a real data set, they are truly 
missing or unobservable, and statistical inference must be adjusted accordingly 
\cite{censored}. 
See also Section~\ref{boundary}.


I discuss Figure~\ref{fig:hexa} in Section~\ref{sm2}. A simple introduction to mixtures of ordinary Poisson processes is found on the Memming blog, 
\href{https://memming.wordpress.com/2012/08/28/mixture-of-point-processes/}{here}. 
In Section~\ref{ssnn}, I discuss statistical inference: detecting whether a realization of a point 
process is Poisson or not, and detecting the number of superimposed processes (similar to estimating the number of clusters in a 
\textcolor{index}{cluster process}\index{cluster process}, or the number of components in a \textcolor{index}{mixture model}\index{mixture model}).  
 In Section~\ref{bbcl}, I introduce  a black-box version of the 
\textcolor{index}{elbow rule}\index{elbow rule} to detect the number of clusters or mixture components, or 
the number of superimposed processes. 

\subsubsection{Hexagonal Lattice, Nearest Neighbors}\label{sm2}

Here I dive into the details of the processes discussed in Section~\ref{sm1}. I also discuss Figure~\ref{fig:hexa}. The source code to produce Figure~\ref{fig:hexa} is discussed in Sections~\ref{nnsc} (nearest neighbor graph) and \ref{visusc} (visualizations). 
Some elements of \textcolor{index}{graph theory}\index{graph theory} are discussed here, as well as \textcolor{index}{visualization}\index{visualization} techniques. 

 Surprisingly, it is possible to produce a point process with a regular hexagonal \gls{gls:lattice1}\index{lattice!lattice space}  using simple operations on a small number ($m=4$) of square lattices: superimposition, stretching, and shifting. A 
\textcolor{index}{stretched lattice}\index{lattice!stretched}\index{stretching (point process)}\index{point process!stretched} 
is a square lattice turned into a rectangular lattice, by applying a multiplication factor to the X and/or Y coordinates.  A
\textcolor{index}{shifted lattice}\index{lattice!shifted} is a lattice where the grid points have been shifted via a translation. 

Each point of the process almost surely (with probability one) has exactly one nearest neighbor. However, when the 
\gls{gls:sf}\index{scaling factor} $s$ is zero, this is no longer true. On the left plot in Figure~\ref{fig:hexa}, each point (also called  
\textcolor{index}{vertex}\index{vertex (graph theory)} when $s=0$) has exactly 3 nearest neighbors. This causes some challenges when plotting the case $s=0$. The case $s>0$ is easier to plot, using arrows pointing from any point to its unique nearest neighbor. 
I produced the arrows in question with the \texttt{arrow} function in R, see source code in Section~\ref{visusc}, and online 
documentation \href{https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/arrows.html}{here}.
A bidirectional arrow between points A and B means that B is a nearest neighbor of A, and A is a nearest neighbor of B. All arrows on the left plot in 
Figure~\ref{fig:hexa} are bidirectional.  \Glspl{gls:be}\index{boundary effect} are easily noticeable, as some arrows point to nearest neighbors outside the window. 
Four colors are used for the points, corresponding to the  4 shifted stretched Poisson-binomial processes used to generate the hexagon-based process. The color indicates which of these 4 process, a point is attached to.

The source code in Section~\ref{nnsc} handles points with multiple nearest neighbors. It produces a list of all points with their nearest neighbors, using a 
\textcolor{index}{hash table}. A point with 3 nearest neighbors has 3 entries in that list: one for each nearest neighbor. A group of points that are all connected by arrows, is called a \gls{gls:cc}\index{connected components}\index{graph!connected components} [\href{https://en.wikipedia.org/wiki/Component_(graph_theory)}{Wiki}]. A path from a point of a connected component to another point of the same connected component, following arrows while ignoring their direction, is called a 
\textcolor{index}{path}\index{graph!path} in \textcolor{index}{graph theory}\index{graph theory}.


In my definition of connected component, the direction of the arrow does not matter: the underlying 
\textcolor{index}{graph}\index{graph} is considered \textcolor{index}{undirected}
\index{graph!undirected} 
[\href{https://en.wikipedia.org/wiki/Directed_graph}{Wiki}]. An interesting problem is to study the size distribution, that is, the number of points 
per connected component, especially for standard Poisson processes. See Exercise~\ref{exercise14g}. In graph theory, a point is called
a \textcolor{index}{vertex}\index{graph!vertex} or 
\textcolor{index}{node}\index{graph!node}, and an arrow is called an \textcolor{index}{edge}\index{graph!edge}. More about nearest neighbors is
discussed in Exercises~\ref{exercise14e} and \ref{exercise14f}.

Finally, if you look at Figure~\ref{fig:hexa}, the left plot seems to have more points than the right plot. But they actually have roughly the same number of points. The plot on the right seems to be more sparse, because there are large areas with no points. But to compensate, there are areas where several points are in close proximity.


\begin{figure}%[H]
\centering
\includegraphics[width=0.8\textwidth]{PB-hexa.png}
%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Four superimposed Poisson-binomial processes: $s=0$ (left), $s=5$ (right)}
\label{fig:hexa}
\end{figure}

%\pagebreak

%=======================
\section{Applications}
%=======================

Applications of Poisson-binomial point processes (also called \textcolor{index}{perturbed lattices point processes}\index{point process!perturbed lattice process}) are numerous. In particular, they are widely used in cellular and sensor network modeling and optimization. It also has applications in physics and crystal structures: see Figure~\ref{fig:quartz} featuring 
 man-made marble
 countertops. I provide many references in Section~\ref{s:clp}.

Here I focus on two-dimensional processes, to model lattice-based clustering. It is different from traditional clustering in two ways: clustering 
takes place around the vertices of the \gls{gls:lattice1}\index{lattice!lattice space}, and the number of clusters is infinite (one per \textcolor{index}{vertex}\index{lattice!vertex}). 
This concept is visualized in Figures~\ref{fig:pbr4} and \ref{fig:pbr4c}, showing representations of these 
\textcolor{index}{cluster processes}\index{cluster process}. 

The processes in Section~\ref{s:clp}  are different from the mixtures or superimposed  Poisson-binomial processes on \textcolor{index}{shifted rectangular lattices}\index{lattice!shifted}, discussed in Sections~\ref{sm2} and~\ref{ssnn}. The latter can produce clustering on 
\textcolor{index}{hexagonal}\index{lattice!hexagonal}\index{hexagonal lattice} lattice spaces, as
 pictured in Figure~\ref{fig:hexa}, with
applications to cellular networks [\href{https://en.wikipedia.org/wiki/Cellular_network}{Wiki}]. See also
an application to number theory (sums of squares)
 in my article ``Bernoulli Lattice Models -- Connection to Poisson Processes", available \href{https://www.vgranville.com/2022/02/bernoulli-lattice-models-connection-to.html}{here}. Instead, the cluster processes discussed here are based on square lattices and radial densities.

In  Section~\ref{s:clp}, I introduce \textcolor{index}{radial processes}\index{point process!radial} (called 
\textcolor{index}{child processes}\index{child process}) to model the cluster structure. The underlying distribution $F$
attached to the points $(X_h,Y_k)$ of the base process (called \textcolor{index}{parent process}\index{parent process})
is the logistic one. In Section~\ref{sec:gld}, I discuss a new type of 
\textcolor{index}{generalized logistic distribution}\index{distribution!generalized logistic}, which is easy to handle for simulation purposes, or to find 
its CDF (cumulative distribution function) and \gls{gls:quant}\index{quantile!quantile function}. 

In Section~\ref{permut}, I focus on the hidden or \textcolor{index}{inverse model}\index{inverse model}\index{hidden model} (in short, the unobserved lattice). It leads to infinite,
slightly \textcolor{index}{random permutations}\index{permutation!random permutation}. The final section deals with what the Poisson-binomial process 
was first designed for: randomizing mathematical series to transform them into 
\textcolor{index}{random functions}\index{random function}. The purpose is to study the effect of small random perturbations. Here it is applied to the famous \textcolor{index}{Riemann zeta function}\index{Riemann zeta function}. It leads to a new type of clusters called {\em sinks}, and awkward 2D Brownian motions with a very strong, unusual cluster structure, and beautiful data animations (see Section~\ref{videocluster}).   

Along the lines, I prove Theorem~\ref{prop1}, related to 
\textcolor{index}{Le Cam's inequality}\index{Le Cam's theorem}. It is a fundamental result about the convergence of the
\textcolor{index}{Poisson-binomial distribution}\index{distribution!Poisson-binomial}, to the Poisson distribution. 

 

%-------------------------------------------------
\subsection{Modeling Cluster Systems in Two Dimensions}\label{s:clp}


There are various ways to create points scattered around a center. When multiple centers are involved, we get a cluster structure. The point process consisting of the centers is called the \textcolor{index}{parent process}\index{point process!cluster process!parent process}\index{parent process}, while the point distribution around each center, is called the \textcolor{index}{child process}\index{point process!cluster process!child process}\index{child process}. So we are dealing with a two-layer, or hierarchical structure, referred to as a \textcolor{index}{cluster point process}\index{point process!cluster process}\index{cluster process}. Besides clustering, many other types of \textcolor{index}{point process operations} [\href{https://en.wikipedia.org/wiki/Point_process_operation}{Wiki}]\index{point process operations} are possible when combining two processes, such as thinning or superimposition. Typical examples of cluster point processes include 
\textcolor{index}{Neyma-Scott}\index{point process!cluster process!Neyman-Scott} (see \href{https://hpaulkeeler.com/tag/neyman-scott-point-process/}{here}) and 
\textcolor{index}{Matérn}\index{point process!cluster process!Matérn} (see \href{https://hpaulkeeler.com/simulating-a-thomas-cluster-point-process/}{here}).

Useful references include Baddeley's textbook ``Spatial Point Processes and their Applications" \cite{baddeley} available 
online \href{https://www.apps.stat.vt.edu/leman/VTCourses/BaddeleyPointProcesses.pdf}{here}, 
Sigman's course material (Columbia University) on one-dimensional \textcolor{index}{renewal processes}\index{point process!renewal process}\index{renewal process} for beginners, entitled ``Notes on the Poisson Process" \cite{karl}, available online \href{http://www.columbia.edu/~ks20/stochastic-I/stochastic-I-PP.pdf}{here}, 
Last and Kenrose's book ``Lectures on the Poisson Process" \cite{campoi}, and Cressie's comprehensive 900-page book ``Statistics for Spatial Data" \cite{cressie}. Cluster point processes are part of a larger field known as 
\textcolor{index}{spatial statistics}\index{spatial statistics}, 
encompassing other techniques such as geostatistics, kriging and tessellations. For lattice-based processes known as 
\textcolor{index}{perturbed lattice point processes}\index{point process!perturbed lattice process}\index{lattice}, more closely related to the theme of this textbook (lattice processes), and also more recent with applications to cellular networks, see the following references:
\begin{itemize}
\item ``On Comparison of Clustering Properties of Point Processes" \cite{bbvc}. Online PDF \href{https://arxiv.org/pdf/1111.6017.pdf}{here}.
\item ``Clustering and percolation of point processes" \cite{euclid}. Online version \href{https://projecteuclid.org/journals/electronic-journal-of-probability/volume-18/issue-none/Clustering-and-percolation-of-point-processes/10.1214/EJP.v18-2468.full}{here}.
\item ``Clustering comparison of point processes, applications to random geometric models" \cite{black}. Online version \href{https://arxiv.org/abs/1212.5285}{here}.
\item ``Stochastic Geometry-Based Tools for Spatial Modeling and Planning of Future Cellular Networks" \cite{poi102}. Online version \href{https://curve.carleton.ca/87495a55-a94a-4da3-a824-c075c80f6814}{here}.
\item ``Hyperuniform and rigid stable matchings" \cite{poi101}. Online PDF \href{https://arxiv.org/pdf/1810.00265.pdf}{here}. 
Short presentation available \href{https://www.wias-berlin.de/workshops/SGC20/SGC20_Last.pdf}{here}.
\item ``Rigidity and tolerance for perturbed lattices" \cite{poi103}. Online version \href{https://arxiv.org/abs/1409.4490}{here}.
\item ``Cluster analysis of spatial point patterns: posterior distribution of parents inferred from offspring" \cite{scott}.
\item ``Recovering the lattice from its random perturbations" \cite{oren}. Online version \href{https://arxiv.org/abs/2002.01508v2}{here}.
\item ``Geometry and Topology of the Boolean Model on a Stationary Point Processes" \cite{yogd}. Online version \href{https://www.researchgate.net/publication/325215252_Geometry_and_Topology_of_the_Boolean_Model_on_a_Stationary_Point_Processes_A_Brief_Survey}{here}.
\item ``On distances between point patterns and their applications" \cite{diez2010}. Online version \href{https://www.researchgate.net/publication/228566956_On_distances_between_point_patterns_and_their_applications}{here}.
\end{itemize}
More general references include two comprehensive volumes on point process theory by Daley and Vere-Jones \cite{dddj1,dddj2}, a chapter by Johnson \cite{nisox} (available 
 online \href{http://www.nisox.org/files/PDF/JohnsonSpatialPointProc.pdf}{here} or
\href{https://drive.google.com/file/d/1l5VI7gvS2TKUC11VrkOpph4ITZ8GIO81/view?usp=sharing}{here}),  books by Møller and Waagepetersen, focusing on statistical inference for spatial processes \cite{momo66,momo67}, and ``Point Pattern Analysis: Nearest Neighbor Statistics" by Anselin \cite{anselin} focusing on point inhibition/aggregation metrics, available 
\href{https://spatial.uchicago.edu/sites/spatial.uchicago.edu/files/9_points_2_r.pdf}{here}. See
also \cite{momo55} by Møller, available online \href{https://cimpatogo2018.sciencesconf.org/data/pages/Handout_Moller_CIMPA_Togo_2018.pdf}{here}, and ``Limit Theorems for Network Dependent Random Variables" \cite{econo6}, available online \href{https://arxiv.org/abs/1903.01059}{here}.
 

Here, I use a two-dimensional Poisson-binomial process as the parent process to generate the centers of the cluster process (the child process). The child process, around each center, has a 
\textcolor{index}{radial distribution}\index{radial distribution}. There are different ways to simulate \textcolor{index}{radial processes}\index{point process!radial}; the most popular method uses a bivariate Gaussian distribution for the child process. Poisson point processes with \textcolor{index}{non-homogeneous} \index{point process!non-homogeneous}\index{homogeneity}radial intensities are discussed in my article ``Estimation of the Intensity of a Poisson Point Process by Means of Nearest Neighbor Distances" \cite{vgstat}, freely available online 
\href{https://www.researchgate.net/publication/230268902_Estimation_of_the_intensity_of_a_Poisson_point_process_by_means_of_nearest_neighbor_distances}{here}. \vspace{1ex} \\
{\bf Remark}: By non-homogeneous intensity, I mean that the intensity $\lambda$ depends on the location, as opposed to a stationary Poisson process where $\lambda$ is constant. Estimating the \textcolor{index}{intensity function}\index{intensity function} of such a process is equivalent to a \textcolor{index}{density estimation} problem\index{density estimation},
using kernel density estimators [\href{https://en.wikipedia.org/wiki/Kernel_density_estimation}{Wiki}].

To simulate radial distributions (also called radial intensities in this case), I use a generalized logistic distribution instead of the Gaussian one, for the child process. The generalized logistic distribution has nice features: easy to simulate, easy to compute the CDF, and it has many parameters, offering a lot of flexibility for the shape of the density. The peculiarity of the Poisson-binomial process offers two options:

\begin{itemize}
\item Classic option: Child processes are centered around the points of the parent process, with exactly one child process per point.
\item Ad-hoc option: Child processes are centered around the bivariate lattice locations $(h/\lambda,k/\lambda)$, with exactly one child process per location, and $h,k\in \mathbb{Z}$.
\end{itemize}
In the latter case, if $s$ is small, the child process attached to the index $(h,k)$ has its points distributed around $(X_h, X_k)$ -- a point of the parent process -- thus it won't be much different from the classic option. This is because if $s$ is small, then $(h/\lambda,k/\lambda)$ is close to $(X_h, X_k)$ on average. It becomes more interesting when $s$ is neither too small nor too large.

In my simulations, I used a random number of points (up to 15) for the child process, and the parameter $\lambda$ is set to one. I used a generalized logistic distribution for the radial distribution.

\subsubsection{Generalized Logistic Distribution}\label{sec:gld}

In two dimensions, each point $(X', Y')$ of a child process attached to a center $(X, Y)$ of the parent process, is generated as follows, using the code in Section \ref{s:code2}.
\begin{align}
X' & = X + \log\Big(\frac{U}{1-U}\Big) \cos(2\pi V)\label{hlog1}\\
Y' & = Y + \log\Big(\frac{U}{1-U}\Big) \sin(2\pi V)\label{hlog2}
\end{align}
Here $U$ and $V$ are independent uniform deviates on $[0, 1]$. Let $Q = Q(U)=\log\frac{U}{1-U}$. It has a logistic distribution, centered at $\mu=0$ and with scaling parameter $\rho=1$. 

I now introduce a \textcolor{index}{generalized logistic distribution}\index{distribution!generalized logistic} sharing the same features: very easy to sample, with a simple CDF, but this time with 5 parameters rather than 2: $\mu\in\mathbb{R}$ and $\alpha,\beta,\rho,\tau>0$. The location parameter is $\mu$, and the scaling parameter is $\rho$. The general form of its 
\gls{gls:quant}\index{quantile} is given by
\begin{equation}
Q(u)=\mu+\rho\Big[\log \Big(\frac{\tau u^{1/\beta}}{1-u^{1/\beta}}\Big)\Big]^{1/\alpha}, \quad 0<u<1. \label{eq:qq}
\end{equation}
The standard \textcolor{index}{logistic distribution}\index{distribution!logistic} corresponds to $\alpha=\beta=\tau=1$. In the general form, $\alpha=p/q$ where $p,q$ are strictly positive co-prime odd integers, to avoid problems with negative logarithms. The function $Q$ is known 
as the \gls{gls:quant} [\href{https://en.wikipedia.org/wiki/Quantile_function}{Wiki}]\index{quantile!quantile function}. It is the inverse of the CDF function $P(Z<z)$. The CDF is given by:
\begin{equation}
P(Z<z) = \Big[1+\tau \exp\Big(-\Big(\frac{z-\mu}{\rho}\Big)^\alpha\Big) \Big]^{-\beta}. \label{eq:gld1}
\end{equation}
Here $\rho$ is the dispersion or scaling parameter. It was denoted as $s$ when attached to the parent process, but one could choose a different value ($\rho\neq s$) for the child process, thus the introduction of the symbol $\rho$.  Despite the 5 parameters, the CDF is rather simple. Also it is straightforward to generate deviates for this distribution, using (\ref{eq:qq}) with uniform deviates on $[0, 1]$, for $u$. This technique is known as 
\textcolor{index}{inverse transform sampling} [\href{https://en.wikipedia.org/wiki/Inverse_transform_sampling}{Wiki}]\index{inverse transform sampling}.

In addition, the moments (and thus the variance) can be directly computed using the CDF. Whenever the density is symmetric around the origin (if $\mu=0,\alpha=\beta=1$ in our case)  we have $\mbox{E}[Z^r]=0$ if $r$ is an odd integer, and if $r$ is even, we have
\begin{equation}
\mbox{E}[Z^r]=2r\int_0^\infty x^{r-1} P(Z>z)dz. \label{eq:moment}
\end{equation}
See Exercise \ref{exercise5} in Section \ref{ex:1} for a proof of this identity.  An alternative formula to compute the moments is provided by the quantile function, see theorem \ref{sums7}:
\begin{equation}
E[Z^r]=\int_0^1  Q^r(u)du \label{eq:moment2}
\end{equation}
Formulas (\ref{eq:moment}) and (\ref{eq:moment2}) may be used to solve some integration problems. For instance, if a closed form can be found for (\ref{eq:moment2}), then the integral in (\ref{eq:moment}) has the same value. I just mention three results here; more details are found in Exercise \ref{exercise6} in Section \ref{ex:1}.
\begin{itemize}
\item If $\alpha=1,\beta=1/6$, then $\mbox{E}[Z]=\mu+\rho\log\tau -\frac{\rho}{2}(\sqrt{3}\pi +4\log 2 +3\log 3)$ and the distribution is typically not symmetric. 
\item If $\alpha=1$ and $\tau=e^{1/\beta}$, then $\beta^{-1}\mbox{E}[(Z-\mu)/\rho]\rightarrow \pi^2/6$ as  $\beta\rightarrow 0$. This is a consequence of (\ref{eq:limtau}). However, the limiting distribution has zero expectation. See Exercise \ref{exercise7} in Section \ref{ex:1} for details.
\item If $\alpha=\beta=1$, then $\mbox{E}[(Z-\mu)/\rho]=\log\tau$ and $\mbox{Var}[Z/\rho]=\frac{\pi^2}{3}$. The standard logistic distribution corresponds to $\tau=1$.
\end{itemize}
Finally, the \textcolor{index}{moment generating function} [\href{https://en.wikipedia.org/wiki/Moment-generating_function}{Wiki}] (MGF) \index{moment generating function} can easily be computed using the quantile function, as a direct application of the \textcolor{index}{quantile theorem} \ref{sums7}\index{quantile!fundamental theorem}. If $\mu=0$ and $\alpha=\rho=1$, we have:
\begin{align}
\mbox{E}[\exp(tZ)] & =\int_0^1 \exp\Big[t\log\Big(\frac{\tau u^{1/\beta}}{1-u^{1/\beta}}\Big)\Big]  du\nonumber \\
    & = \tau^t \int_0^1 u^{t/\beta}(1-u^{1/\beta})^{-t} du \nonumber \\
   & = \beta\tau^t \int_0^1 v^{t+\beta-1}(1-v)^{-t}dv\nonumber \\
  & = \beta\tau^{t}B(\beta+t,1-t) \label{eq:qt},
\end{align}
where $B$ is the \textcolor{index}{Beta function}\index{Beta function} [\href{https://en.wikipedia.org/wiki/Beta_function}{Wiki}]. Note that I made the change of variable $v=u^{1/\beta}$ when computing the integral. Unless $\alpha=\tau=1$, it is clear from the moment generating function that this 5-parameter \textcolor{index}{generalized logistic distribution}\index{distribution!generalized logistic}  is different from the
4-parameter one described in [\href{https://en.wikipedia.org/wiki/Generalized_logistic_distribution}{Wiki}]. Another generalization of the logistic distribution is the 
\textcolor{index}{metalog distribution} [\href{https://en.wikipedia.org/wiki/Metalog_distribution}{Wiki}]\index{metalog distribution}\index{distribution!metalog}.\vspace{1ex} \\
{\bf Remark}: If $\alpha=1$, we face a 
\gls{gls:mi} %\textcolor{index}{model identifiability} 
 issue [\href{https://en.wikipedia.org/wiki/Identifiability}{Wiki}]\index{identifiability}. This is because if $\tau_1\exp(\mu_1/\rho_1)=\tau_2\exp(\mu_2/\rho_2)$, the two CDF's are identical even if these two subsets of parameters are different. That is, it is impossible to separately estimate $\tau,\mu$ and $\rho$. However, in practice, we use $\mu=0$.


%===================
\subsubsection{Illustrations}\label{illus}
%===================

Figures~\ref{fig:pbr4b} and \ref{fig:pbr} show two extreme cases of the cluster processes discussed at the beginning of Section~\ref{s:clp}. The parent process modeling the cluster centers, is Poisson-binomial. It is simulated with  \gls{gls:intensity1}\index{intensity function} $\lambda=1$, using a uniform distribution for $F$. The \gls{gls:sf}\index{scaling factor} is $s=0.2$ for Figure~\ref{fig:pbr4b}, and
$s=2$ for Figure~\ref{fig:pbr}. The left plot is a zoom-in. Around each center (marked with a blue cross in the picture), up to 15 points are radially distributed, creating the overall cluster structure. These points are the actual, observed points of the process, referred to as the child process. 
The distance between a point $(X', Y')$ and its cluster center $(X,Y)$ 
has a \textcolor{index}{half-logistic distribution}\index{distribution!half-logistic} 
[\href{https://en.wikipedia.org/wiki/Half-logistic_distribution}{Wiki}]. The simulations are performed
using Formulas~(\ref{hlog1}) and (\ref{hlog2}).

 
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{pbx2-index-zoom-s02.PNG}
\caption{Radial cluster process ($s=0.2, \lambda=1$) with centers in blue; zoom in on the left}
\label{fig:pbr4b}
\end{figure}

The contrast between Figures~\ref{fig:pbr4b} and \ref{fig:pbr} is due to the choice of the scaling factor $s$. The value $s=0.2$, close to zero,  strongly reveals the underlying lattice structure. Here this effect is strong because of  the choice of $F$ (it has a very thin tail), and the relatively small variance of the distance between a point and its associated cluster center.
It produces repulsion among neighbor points: we are dealing with a 
\textcolor{index}{repulsive process}\index{repulsion (point process)}\index{point process!repulsive}, also called \textcolor{index}{perturbed lattice point processes}\index{point process!perturbed lattice process}. When $s=0$,  all the randomness
is gone: the \gls{gls:state1}\index{state space} is the \gls{gls:lattice1}\index{lattice!lattice space}. See left plot in Figure~\ref{fig:hexa}.  Modeling applications include 
optimum distribution of sensors (for instance cell towers), crystal structures and bonding patterns of molecules in chemistry.


\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{pbx2-zoom-s10.PNG} 
\caption{Radial cluster process ($s=2, \lambda=1$) with centers in blue; zoom in on the left}
\label{fig:pbr}
\end{figure}

By contrast, $s=2$ makes the cluster structure much more apparent. This time, there is \gls{gls:attract} among neighbor points: we are dealing with an
\textcolor{index}{attractive process}\index{attraction (point process)}. It can model many types of structures, associated to human activities or natural phenomena, such 
as the distribution of galaxies in the universe. Figure~\ref{fig:quartz} provides an example, related to the manufacture of kitchen countertops. I discuss other types of
cluster patterns generated by Poisson-binomial processes, in Sections~\ref{videocluster} and \ref{ssnn}. 


\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{marble4b.png} 
\caption{Manufactured marble lacking true lattice randomness (left)}
\label{fig:quartz}
\end{figure}

\noindent Figure~\ref{fig:quartz} shows luxury kitchen countertops called ``Inverness bronze Cambria quartz", on the left. While the quality (and price) is far superior to all 
other products from the same company, the rendering of marble veins is not done properly. It looks man-made: not the kind of patterns you would
find in real stones. The pattern is too regular, as if produced using a very small value of the scaling factor $s$. An easy fix is to use patterns generated by the cluster processes described here, incidentally called perturbed lattices. To increase randomness, increase $s$. It will improve the design. I am currently talking to the company, as I plan to buy these countertops. The picture on the right shows a more realistic rendering of randomness. 
 

%---------------------------------------------
\subsection{Infinite Random Permutations with Local Perturbations}\label{permut}

The unobserved \textcolor{index}{index}\index{index} $k$ attached to any point $X_k$ of the Poisson-binomial point process, gives rise to an interesting random process called the 
\textcolor{index}{hidden process}\index{hidden model}
 or \textcolor{index}{index process}\index{index!index process}, see Section~\ref{hm}. It can be used to generate infinite, locally random permutations (here in one dimension), using the following algorithm: \vspace{1ex}\\
{\bf Algorithm}: Generate a locally random permutation of order $m$
\begin{itemize}
\item {\bf Step 1}: Generate a 1-D realization of a Poisson-binomial process with $2n+1$ points $X_{-n},\dots,X_n$.

\begin{itemize}
\item Let $L(X_k)=k$, for $-n\leq k \leq n$. The function $L$ is stored as an 
\textcolor{index}{hash table}\index{hash table} [\href{https://en.wikipedia.org/wiki/Hash_table}{Wiki}] in your source code; the keys of your hash table are the $X_k$'s. In practice, no two $X_h, X_k$ with $h\neq k$ have the same value $X_h=X_k$, so this collision problem won't arise. 
\end{itemize}

\item {\bf Step 2}: Sort the $2n+1$ points $X_k$, with $-n\leq k\leq n$. 
\begin{itemize} 
\item Denote as $X_{(k)}$ the $k$-th point after ordering. 
\end{itemize}

\item {\bf Step 3}: Select $m$ consecutive ordered points, say $X_{(1)},\dots,X_{(m)}$ with $m$ much smaller than $n$
\begin{itemize}
\item Retrieve their original indices: $\sigma(k)=L(X_{(k)})$, $k=1,\dots,m$
\item Set $\tau(k)=L(X_{(k+1)})$, $k=1,\dots,m$ (so $X_{\tau(k)}$ is the closest point to $X_{\sigma(k)}$, to the right)
\end{itemize}
\end{itemize}
Now $\sigma$ is a  \textcolor{index}{random permutation}\index{random permutation}\index{permutation!random permutation} on $\{1,\dots,m\}$ 
 [\href{https://en.wikipedia.org/wiki/Random_permutation}{Wiki}]. To produce the plots in Figure~\ref{fig:pbpermut}, I used $m=10^3, n=3\times 10^4$ and a Poisson-binomial process with $\lambda=1,s=3$ and a logistic distribution for $F$. Since the theory is designed to produce infinite rather than finite permutations, 
\textcolor{index}{boundary effects}\index{boundary effect} can take place. To minimize them, take both $m$ and $n$ large. The boundary effects, if present (for instance when using a thick tail distribution for $F$ such as Cauchy, or when using a large $s$)  will be most noticeable  for $\sigma(k)$  when $k$ is close to $1$ or close to $m$. 

These permutations can be used to model local reshuffling in a long series of events. Effects are mostly local, but tend to
spread to longer distances on average, when $s$ is large or $F$ has a thick tail. For instance, in Figure~\ref{fig:pbpermut}, the biggest shift in absolute value is $\sigma(k)-k =35$, occurring at $k=108$ (see the peak on the left plot). However, peaks (or abysses) of arbitrary height will occur if $m$ is large enough, unless you use a uniform distribution for $F$, or any distribution with a finite support domain.

The right plot in Figure~\ref{fig:pbpermut} shows the  joint \gls{gls:empdis}\index{empirical distribution} (data-based as opposed to theoretical) of the discrepancies  $\sigma(k) - k$ and $\tau(k)-k$ in the index space $\mathbb{Z}\times\mathbb{Z}$. Of course, since the index $\tau(k)$ points to the closest neighbor of  $X_{\sigma(k)}$ to the right, that is, to $X_{\tau(k)}$, we have $\tau(k)\geq 1+\sigma(k)$, which explains why the main diagonal is blank. Other than that, the plot shows independence, symmetry, and 
\textcolor{index}{anisotropy}\index{anisotropy} (absence of directional trend in the scattering). It means that 
\begin{itemize}
\item Given a point $X_k$, the index $\tau(k)$ of its nearest neighbor to the right is randomly distributed around $k$, according to some radial distribution,
\item Given a point $X_k$, its order $\sigma(k)$ once the points are ordered,  is randomly distributed around $k$, according to the same radial distribution,
\item There is independence between the two.  
\end{itemize}

%\usepackage{lipsum} % just for some dummy text
%\usepackage{enumitem}
%\setenumerate[2]{label=\alph*.}
%\begin{document}
%\begin{enumerate}
%\item
%\begin{enumerate}
%\item \lipsum[1]
%\item \lipsum[2]
%\end{enumerate}
%\item \lipsum[3]
%\end{enumerate}
%\end{document}

\begin{figure}%[H] 
\centering
\includegraphics[width=0.85\textwidth]{PB-RandomPermut.PNG}
\caption{Locally random permutation $\sigma$; $\tau(k)$ is the index of $X_k$'s closest neighbor to the right}
\label{fig:pbpermut}
\end{figure}

Two metrics used to compare or describe these permutations are the average and maximum \textcolor{index}{index discrepancy}\index{index!index discrepancy}, measured as the average and maximum value of $|\sigma(k)-k|$ for $1\leq k \leq m$. It gets larger as $s$ increases. Another metric of interest, related to the 
\textcolor{index}{entropy}\index{entropy}\index{permutation!entropy} of the permutation 
 [\href{https://www.aptech.com/blog/permutation-entropy/}{Wiki}] \cite {pentropy}, 
is the 
correlation between the integer numbers $k$ and $\sigma(k)-k$, computed over $k=1,\dots,m$.
While the example featured in Figure~\ref{fig:pbpermut} exhibits essentially a zero correlation, some other cases not reported here, exhibit a strong correlation. See also \cite{pentrop2}, available online on arXiv, \href{https://arxiv.org/abs/2003.13728}{here}. For an elementary introduction to permutations, see \cite{introp}.


%----------------------------------------------------------------------------------
\subsection{Probabilistic Number Theory and Experimental Maths}

Experimental mathematics is an active field where machine learning techniques are used to discover conjectures and patterns in number theory -- for instance about the distribution of twin primes or the digits of $\pi$. References on this topic include \cite{nt1,nt2,nt3} and my book \cite{vgdyn}, 
available \href{https://github.com/VincentGranville/Stochastic-Processes}{here}. Here I discuss two problems. The first one 
(Section~\ref{slogs}) deals with the hypothetical count distribution of large factors  in some intervals, in very large composite integers used in cryptography. It is obtained by simulations and heuristic arguments, and illustrates what probabilistic number theory is about. In the process, I prove a version of Le Cam's theorem: the fact that under certain circumstances, the Poisson-binomial distribution tends to a Poisson distribution. 


The second problem (Section~\ref{rh}) deals with the \textcolor{index}{Riemann zeta function}\index{Riemann zeta function} $\zeta$ and the famous Riemann hypothesis (RH), featuring unusual, not well-known patterns  It leads to heuristic arguments supporting RH. I then apply small perturbations to $\zeta$ (more specifically, to its sister function, the \textcolor{index}{Dirichlet eta function}\index{Dirichlet eta function} $\eta$) using a Poisson-binomial process, to see when and if the patterns remain. The purpose is to check whether RH can be extended to a larger class of chaotic, 
\textcolor{index}{random functions}\index{random function}, 
unrelated to Dirichlet $L$-functions [\href{https://en.wikipedia.org/wiki/L-function}{Wiki}]. Would such an extension be possible, it could offer new potential paths to proving RH. Unfortunately, while I exhibit such extensions, they only occur when the perturbations are incredibly small. RH has a \$1 million award attached to it and offered by the Clay Institute, see \href{https://www.claymath.org/millennium-problems/}{here}.

%-------------
\subsubsection{Poisson Limit of the Poisson-binomial Distribution, with Applications}\label{slogs}


Historically, the problem discussed here has its origins in a new optimization technique to detect rare, hard-to-find global minima or roots of peculiar, highly irregular or discrete functions. An example is $g(b) = 2 - \cos(2\pi b) - \cos(2\pi a/b)$ with $a = 7919 \times 3083$,  pictured in Figure~\ref{fig:pbfactor}. The technique uses a {\em divergent} 
fixed point algorithm [\href{https://en.wikipedia.org/wiki/Fixed-point_iteration}{Wiki}] 
that somehow leads to a solution. This material will be included in one of my upcoming textbooks.
An overview of the (yet unpublished) technique can be found in  ``A New Machine Learning Optimization Technique - Part 1" available online, 
\href{https://www.vgranville.com/2022/02/a-new-machine-learning-optimization.html}{here}.  Initially, the application in mind was factoring numbers that are a product of two very large primes (the roots), typically used as encryption keys in cryptographic systems. 

The bottom plot in Figure~\ref{fig:pbfactor} represents the function $g(b)$ with
$b\in [2900, 3100]$. It has a global minimum in that interval: $g(b)=0$, occurring at $b=3083$ (one of the only two factors of $a$). Note that $g$ is differentiable and smooth everywhere. But its almost aperiodic oscillations have such a high frequency, that $g$ looks chaotic to the naked eye, making the minimum invisible. Also, it is not possible to efficiently find the minimum using standard optimization algorithms. The top part of Figure~\ref{fig:pbfactor} shows the same function, after discretization and magnification of the dip (caused by the minimum). Now the root-searching fixed-point algorithm can detect the minimum. It does that by emitting a strong signal when entering the deep valley, after a relatively small number of iterations. The blue curve is a smooth version of the discretized,  step function in red [\href{https://en.wikipedia.org/wiki/Step_function}{Wiki}].


\begin{figure}%[H]
\centering
\includegraphics[width=0.69\textwidth]{PB-factoring2.PNG}
%  \includegraphics[width=\linewidth]{pbx2.PNG}
\caption{Chaotic function (bottom), and its transform (top) showing the global minimum}
\label{fig:pbfactor}
\end{figure}

The remaining of this discussion focuses on a particular aspect of the problem in question. It features another case of convergence of the \textcolor{index}{Poisson-binomial distribution}\index{distribution!Poisson-binomial}\index{Poisson-binomial distribution}  to the 
\textcolor{index}{Poisson distribution}\index{distribution!Poisson}\index{Poisson distribution}. The first case was a byproduct of
the convergence of \textcolor{index}{Poisson-binomial processes}\index{point process!Poisson-binomial}\index{Poisson-binomial point process} to standard 
\textcolor{index}{Poisson processes}\index{Poisson point process}\index{point process!Poisson} (see Theorem~\ref{sums2}). It implied that  their point count distribution -- a Poisson-binomial -- must also converge to a Poisson distribution.  \\

\noindent {\bf Description of the Problem} 
 
\noindent I now describe the number theoretic problem in probabilistic terms. The simulation consists of generating $m$ independently distributed random positive integers $Z_1,\dots, Z_m$ with $Z_k$ uniformly distributed on $\{0,1,...,n+k-1\}$. Here $n$ is fixed but arbitrary, and we are interested in $n\rightarrow\infty$. Also, $m$ is larger than $n$, with $m/n \rightarrow \alpha > 1$ as $n\rightarrow\infty$, typically with $\alpha=2$. The probability that $Z_k$ is zero is denoted as $p_k=P(Z_k=0)$. The number of $Z_k$'s equal to zero, with $1\leq k \leq m$, is denoted as $N(n,m)$ or simply $N$. In mathematical notations,
\begin{equation}
  N = \sum_{k=1}^m \chi(Z_k =0), \label{plp}
\end{equation}
where $\chi$ is the indicator function [\href{https://en.wikipedia.org/wiki/Indicator_function}{Wiki}], equal to one if its argument is true, and to zero otherwise. 
Thus $N$, the counting random variable, has a 
Poisson-binomial distribution [\href{https://en.wikipedia.org/wiki/Poisson_binomial_distribution}{Wiki}] of parameters $p_1,\cdots,p_m$, similar to that discussed
in Formula~(\ref{eq:f1}). The goal is to prove that when $n\rightarrow\infty$, the limiting distribution of $N$ is Poisson with expectation $\log \alpha$. I then discuss the implications of this result, regarding the distribution of large factors in very large integers. The main result, Theorem~\ref{prop1}, is a particular case of \textcolor{index}{Le Cam's inequality}\index{Le Cam's theorem} [\href{https://en.wikipedia.org/wiki/Le_Cam's_theorem}{Wiki}]; see also \cite{lecam}, available online 
\href{http://www-stat.wharton.upenn.edu/~steele/Papers/PDF/LIaPA.pdf}{here}.  \\

\begin{theorem}\label{prop1}
As $n\rightarrow\infty$ and $m/n\rightarrow\alpha>1$, the discrete Poisson-binomial distribution of the counting random variable $N$ defined by Formula~(\ref{plp}), tends to a Poisson distribution of expectation $\log\alpha$.
\end{theorem}
\begin{proof}
$ $ \newline 
Clearly, $p_k=P(Z_k=0)=1/(n+k)$. Let $$q_0 = \prod_{k=1}^m (1-p_k) \mbox{ and } \mu=\sum_{k=1}^m \frac{p_k}{1-p_k}.$$
We have, as in Formula~(\ref{eq:f3}) and (\ref{eq:f4}), $P(N=0)=q_0$ and $P(N=1)=q_0 \mu$. In Exercise~\ref{exercise14}, I prove that as $n\rightarrow\infty$, 
$P(N=k)=q_0 \mu^k/k!$ for all positive integers $k$.  Thus
$$\sum_{k=0}^\infty P(N=k) = q_0\exp(\mu)=1.$$
This corresponds to a Poisson distribution. It follows that $\mu=-\log q_0$. To complete the proof, I now show that $q_0\rightarrow 1/\alpha$, as $n\rightarrow\infty$
 and $m/n\rightarrow \alpha > 1$. 
We have 
\begin{align}
\log q_0 & = \log \prod_{k=1}^m (1-p_k)  =\sum_{k=0}^\infty \log(1-p_k)  =-\sum_{k=1}^m  p_k +O\Big(\frac{1}{n}\Big)\nonumber\\
& = -\sum_{k=1}^m  \frac{1}{n+k} +O\Big(\frac{1}{n}\Big)  =  -\sum_{k=1}^m  \frac{1}{k} +\sum_{k=1}^{n}  \frac{1}{k}+O\Big(\frac{1}{n}\Big)\nonumber\\
& = -\log m + \log n +O\Big(\frac{1}{n}\Big) = -\log(m/n) + O\Big(\frac{1}{n}\Big)  = -\log\alpha + O\Big(\frac{1}{n}\Big)\nonumber
\end{align}
and thus $q_0 \rightarrow 1/\alpha$ as $n\rightarrow\infty$. \qed
\end{proof}
\quad \\
\noindent {\bf Chance of Detecting Large Factors in Very Large Integers} \nopagebreak \vspace{1ex} \\
In encryption systems, one usually works with very large integers $a$ that only have two factors $b$ and $a/b$, both large prime numbers, in order to make encryption keys secure. The reason is because factoring a very large integer that only has two large factors, is very difficult. The encryption keys are a function of the numbers $b$ and $a/b$. The question is as follows: for a very large, ``random" integer $a$ (random in the sense that it could have any number of factors, including none), what are the chances that it has at least one factor $b$ in the range $b\in[n, m]$? I assume here that $n,m$ are very large too, with $m/n=\alpha>1$ and $m<\sqrt{a}$. The answer, according to Theorem~\ref{prop1}, is $P(N)>0=1-\exp[-\log \alpha]=1-1/\alpha$. The expected number of factors, in that range, is $\mbox{E}[N] = \log\alpha$. These are rough approximations, as it assumes randomness in the distribution of residues $a \bmod{b}$ when $a$ is fixed and $b\in[n,m]$. This is explained in the next paragraph. By definition, $a \bmod{b}$ is the remainder in the integer division $a/b$ [\href{https://en.wikipedia.org/wiki/Euclidean_division}{Wiki}], also called {\em residue} in elementary number theory;
$\bmod$ is the modulo operator [\href{https://en.wikipedia.org/wiki/Modulo_operation}{Wiki}] and $b$ is called the {\em modulus}. In our example at the begining of Section~\ref{slogs}, $a=7919 \times 3083$.
 I used the interval $[n,m]=[2900,3200]$ to find a factor of $a$. The chance to find one (namely, $b=3083$) is approximately $1-2900/3200\approx 9.4\%$ assuming you don't know what the factors are, $a$ is random, and you picked up the interval $[n,m]$ at random. 

To summarize, for a very large, arbitrary integer $a$, the number of factors in an interval $[n, m]$ with $n$ large, $m<\sqrt{a}$, and $m/n\approx\alpha>1$, approximately has a Poisson distribution of expectation $\log\alpha$. The explanation is as follows:
\begin{itemize}
\item $a \bmod{n}\in \{0,\dots, n-1\}$ is random, 
\item $a \bmod{(n+1)}\in \{0,\dots, n\}$ is random, 
\item $a \bmod{(n+2)}\in \{0, \dots,n+1\}$ is random,\\
  \vdots 
\item $a \bmod{(m-1)}\in \{0,\dots,m-2\}$ is random, 
\item $a \bmod{m}\in \{0,\dots,m-1\}$ is random, 
\end{itemize}
and the above $m-n+1$ residues are mutually independent to some extent. The integer $a$ has a factor in $[n,m]$ if and only if at least one of the above residues is zero. Thus Theorem~\ref{prop1} applies, at least approximately. \\

\noindent {\bf Integer Sequences with High Density of Primes} 

\noindent The Poisson-binomial distribution, including its Poisson approximation, can also be used in this context. The problem was originally posted \href{https://mathoverflow.net/questions/374305/sequences-with-high-densities-of-primes-how-to-boost-them-to-get-even-more-and}{here}, and the purpose was to find fast-growing integer sequences with a very high density of primes.

The probability that a large integer $x$ is prime is about $1/\log x$, a consequence of the prime number theorem [\href{https://en.wikipedia.org/wiki/Prime_number_theorem}{Wiki}]. Let
 $x_1,x_2,\dots$ be a strictly increasing sequence of positive integers, and $N$ denote the number of primes among $x_n,x_{n+1},\dots,x_{n+m}$ for some large $n$. Assuming the sequence is independently and congruentially equidistributed, then $N$ has a Poisson-binomial distribution of parameters $p_n,\dots,p_{n+m}$, with $p_k=1/\log x_k$. It is unimportant to know the exact definition of congruential equidistribution. Roughly speaking, it means that the joint empirical distribution of residues across the $x_k$'s, is asymptotically undistiguishable from that of a sequence of random integers. Thus a sequence where 60\% of the terms are odd integers, do not qualify (that proportion should be 50\%). 

This result is used to assess whether a given sequence of integers is unusually rich, or poor, in primes. If it contains far more large primes than the
expected value $p_n+\dots+p_{n+m}$, then we are dealing with a very interesting, hard-to-find sequence, useful both in cryptographic applications and for its own sake. One can build confidence intervals for the number of such primes, based on the Poisson-binomial distribution under the assumption of independence and congruential equidistribution. A famous example of such a sequence (rich in prime numbers) is  $x_k=k^2-k+41$ [\href{https://en.wikipedia.org/wiki/Ulam_spiral}{Wiki}].

If $n,m\rightarrow\infty$ and $p_n^2+\dots+p_{n+m}^2\rightarrow 0$, then the distribution of $N$ is well approximated by a Poisson distribution, thanks to Le Cam's theorem
 [\href{https://en.wikipedia.org/wiki/Le_Cam's_theorem}{Wiki}]. 


%----------------------------------------------------
\subsubsection{Perturbed Version of the Riemann Hypothesis}\label{rh}

When I first investigated Poisson-binomial processes, it was to study the behavior of some mathematical functions represented by a series. The idea was to add little random perturbations to the index $k$ in the summation, in short, replacing $k$ by $X_k$, turning the mathematical series into a 
\textcolor{index}{random function}\index{random function}, and see what happens. Here this idea is applied to the \textcolor{index}{Riemann zeta function}\index{Riemann zeta function} [\href{https://en.wikipedia.org/wiki/Riemann_zeta_function}{Wiki}]. The purpose is to empirically check whether the Riemann Hypothesis 
[\href{https://en.wikipedia.org/wiki/Riemann_hypothesis}{Wiki}] still holds under small perturbations, as non-trivial zeros of the Riemann zeta function $\zeta$ are very sensitive to little perturbations. Instead of working with $\zeta(z)$, I worked with its sister, the \textcolor{index}{Dirichlet eta function}\index{Dirichlet eta function} $\eta(z)$ with $z=\sigma + it \in \mathbb{C}$ 
[\href{https://en.wikipedia.org/wiki/Dirichlet_eta_function}{Wiki}]: it has the same non-trivial zeros in the critical strip $\frac{1}{2} < \sigma < 1$, and its series converges in the critical strip, unlike that of $\zeta$. Its real and imaginary parts are respectively equal to

\begin{align}
 \Re[{\eta(z)}] & = \Re[{\eta(\sigma+it)}]  = - \sum_{k=1}^\infty (-1)^k \frac{\cos(t\log k)}{k^\sigma} \label{eta1} \\
 \Im[{\eta(z)}] & = \Re[{\eta(\sigma+it)}]  = - \sum_{k=1}^\infty (-1)^k \frac{\sin(t\log k)}{k^\sigma} \label{eta2}
\end{align}

Note that $i$ represents the imaginary unit, that is $i^2=-1$. I investigated two cases: $\sigma=\frac{1}{2}$ and $\sigma=\frac{3}{4}$. I used a Poisson-binomial process with intensity $\lambda=1$, scaling factor $s=10^{-3}$ and a uniform $F$ to generate the $(X_k)$'s and replace the index $k$ by $X_k$ in the two sums. I also replaced $(-1)^k$ by $\cos\pi k$. The randomized (perturbed) sums are 

\begin{align}
\Re[{\eta_s(z)}]  & = \Re[{\eta_s(\sigma+it)}]  = - \sum_{k=1}^\infty \cos(\pi X_k)\cdot \frac{\cos(t\log X_k)}{X_k^\sigma}\label{eta1b} \\
 \Im[{\eta_s(z)}] & = \Re[{\eta_s(\sigma+it)}]  =- \sum_{k=1}^\infty cos(\pi X_k)\cdot \frac{\cos(t\log X_k)}{X_k^\sigma} \label{eta2b}
\end{align}
Proving the convergence of the above (random) sums is not obvious. The notation $\eta_s$ emphasizes the fact that the $(X_k)$'s have been created using the scaling factor $s$; 
if $s=0$, then $X_k=k$ and $\eta_s=\eta$. 

Figure~\ref{fig:riemann} shows the orbits of $\eta_s(\sigma+it)$ in the complex plane, for fixed values of $\sigma$ and $s$. The orbit consists of the points
$P(t)=(\Re[{\eta_s(\sigma+it)}],\Im[{\eta_s(\sigma+it)}])$  with $0<t<200$, and $t$ increasing by increments of $0.05$. The plots are based on a single realization of the Poisson-binomial process. The sums converge very slowly, though there are ways to dramatically increase the convergence: for instance, Euler's transform [\href{https://en.wikipedia.org/wiki/Series_acceleration}{Wiki}] or 
Borwein's method [\href{https://bit.ly/3EthJXY}{Wiki}]. 
I used $10^4$ terms to approximate the infinite sums. 

\begin{figure}[H]
\centering
\includegraphics[width=0.52\textwidth]{PB-Riemann.PNG}
%\includegraphics{PB-Riemann2.PNG}
%  \includegraphics[width=\linewidth]{pbx2.PNG}
\caption{Orbit of $\eta$ in the complex plane (left), perturbed by a Poisson-binomial process (right)}
\label{fig:riemann}
\end{figure}

Let's look at the two plots on the left in Figure \ref{fig:riemann}. A hole around the origin is noticeable when $\sigma=0.75$. This suggests that $\eta$ has no root with real part $\sigma=0.75$, at least if $0<t<200$, as the orbit never crosses the origin, and indeed stays far away from it at all times. For larger $t$'s the size of the hole may decrease, but with appropriate zooming, it may never shrink to an empty set. This is conjectured to be true for any $\sigma\in ]\frac{1}{2}, 1[$ and any $t$; indeed, this constitutes the famous \textcolor{index}{Riemann Hypothesis}\index{Riemann hypothesis}. To the contrary, if $\sigma=0.5$, the orbit crosses the origin time and again, confirming the well-know fact that $\eta$ has all its non-trivial zeros (infinitely many), on the critical line $\sigma=\frac{1}{2}$. This is the other part of the Riemann Hypothesis. 

I noticed that the hole observed when $\sigma=0.75$ shifts more and more to the left as $\sigma$ decreases. Its size also decreases, to the point that when $\sigma=\frac{1}{2}$ (but not before), the hole has completely vanished, and its location has shifted to the origin. For $\sigma=0.75$, it seems that there is a point on the X-axis, to the left-hand side of the hole but close to it, where the orbit goes through time and again, playing the same role as the origin does to $\sigma=\frac{1}{2}$. That special point, let's name it $h(\sigma)$, exists for any $\sigma\in[\frac{1}{2},1[$, and depends on $\sigma$. It moves to the right as $\sigma$ increases. At least that's my conjecture, which is a generalization of the Riemann Hypothesis.

Let's now turn to the two plots on the right in Figure~\ref{fig:riemann}. I wanted to check if the above features were unique to the Riemann zeta function. If that is the case, it further explains why the Riemann Hypothesis is so hard to prove (or possibly unprovable), and why it constitutes to this day one of the most famous unsolved mathematical problems of all times. Indeed, there is very little leeway: only extremely small perturbations keep these features alive. For instance, using $s=10^{-3}$ and a uniform $F$, that is, a microscopic perturbation, the orbits shown on the right are dramatically changed compared to their left counterparts. Key features seem to barely be preserved, and I suspect the hole, when $\sigma=0.75$ no longer exists if you look at larger values of $t$: all that remains is a lower density of crossings where the hole used to be, compared to no crossing at all in the absence of perturbations ($s=0$). 

I will publish an eBook with considerably more details about the Riemann Hypothesis (and the twin prime conjecture) in the near future. The reason, I think, why such little perturbations have a dramatic effect, is because of the awkward \textcolor{index}{chaotic convergence}\index{chaotic convergence} of the above series: see details with illustrations in Exercises~\ref{exercise11} and \ref{exercise12}, as well as \href{https://mathoverflow.net/questions/379650/more-mysteries-about-the-zeros-of-the-riemann-zeta-function}{here}. The Riemann function gives rise to a number of interesting probability distributions, some related to dynamical systems, some defined on the real line, and some on the complex plane. This will be discussed in another upcoming book.  \vspace{1ex} \\
{\bf Remark}: The conjecture that if $\sigma\in]1/2,1[$, the hole never shrinks to a single point no matter how large $t$ is (a conjecture weaker than the Riemann Hypothesis) must be interpreted as follows: it never shrinks to a point in any finite interval $[t,t+\tau]$. 
If you consider an infinite interval, this may not be true due to the universality of the Riemann 
zeta function [\href{https://en.wikipedia.org/wiki/Zeta_function_universality}{Wiki}]. An approach to the Riemann hypothesis, featuring new developments, and not involving complex analysis, can be found in my article ``Fascinating Facts About Complex Random Variables and the Riemann Hypothesis", \href{https://www.vgranville.com/2022/02/fascinating-facts-about-complex-random.html}{here}. For an introduction to the Riemann zeta function and Dirichlet series, see \cite{ivic}. See also Section~\ref{videocluster1} in this textbook.

%---------------------------------
\subsection{Videos: Fractal Supervised Classification and Riemann Hypothesis}\label{videocluster}

This section combines many of the topics discussed in the textbook. The purpose is twofold: to teach you how to produce data videos, and to illustrate several of the topics covered throughout this textbook. Section~\ref{visusc} contains the source code, data sets and instructions to produce the videos. Here I focus on the statistical and mathematical methodology. To view one of the videos on YouTube, click on its thumbnail picture in Figure~\ref{fig:example67}.  

\begin{figure}[H]%
    \centering
    \subfloat[\centering Dirichlet 1]{\href{https://www.youtube.com/watch?v=H77ULp6HVsE}{\includegraphics[width=2.5cm]{thumb1b.png}} }%
    \qquad
    \subfloat[\centering Dirichlet 2]{\href{https://www.youtube.com/watch?v=FUxAeW4JEXA}{\includegraphics[width=2.5cm]{thumb2b.png}} }%
   \qquad
    \subfloat[\centering clustering]{\href{https://www.youtube.com/watch?v=dNPSEh-X6uw}{\includegraphics[width=2.5cm]{thumb3b.png}} }%
    \caption{Data animations -- click on a picture to start a video}%
    \label{fig:example67}%
\end{figure}

%\begin{figure}[H]
%   \centering
%   \begin{minipage}{0.45\textwidth} 
%        \centering
%        \includegraphics[width=0.9\textwidth]{pb-cluster3.png} % first figure itself
%        \caption{first figure}
%    \end{minipage}\hfill 
%    \begin{minipage}{0.45\textwidth} 
%        \centering
%        \includegraphics[width=0.9\textwidth]{pb-cluster3.png} % first figure itself
%        \caption{first figure}
%    \end{minipage}\hfill
%\end{figure}

The two leftmost videos illustrate the beautiful, semi-chaotic convergence of the series attached to the \textcolor{index}{Dirichlet eta function}\index{Dirichlet eta function} 
$\eta(z)$ [\href{https://en.wikipedia.org/wiki/Dirichlet_eta_function}{Wiki}] in the complex plane. 
Details are in Section~\ref{videocluster1}, including the connection to the famous 
\textcolor{index}{Riemann Hypothesis}\index{Riemann hypothesis} [\href{https://en.wikipedia.org/wiki/Riemann_hypothesis}{Wiki}]. The rightmost video shows 
\textcolor{index}{fractal supervised clustering}\index{fractal clustering} performed in GPU (graphics processing unit),  using 
\textcolor{index}{image filtering techniques}\index{filtering (image processing)} that act as a \textcolor{index}{neural network}\index{neural network}. It is discussed in Section~\ref{videocluster2}. For a short beginner introduction on how to produce these videos, read my
article ``Data Animation: Much Easier than you Think!", \href{https://www.datasciencecentral.com/data-animation-much-easier-than-you-think/}{here}.

\subsubsection{Dirichlet Eta Function}\label{videocluster1}

Let $z=\sigma+it$ be a complex number, with $\sigma$ the real part, and $t$ the imaginary part. The Dirichlet eta function $\eta(z)$ provides an analytic continuation [\href{https://en.wikipedia.org/wiki/Analytic_continuation}{Wiki}] of the Riemann  series $\zeta(z)$ in the complex plane ($z\in\mathbb{C}$). The two functions are defined as:
\begin{align}
\zeta(z) & = \sum_{k=1}^\infty \frac{1}{k^s}, \quad \sigma =\Re(z) > 1, \label{zeta213} \\
\eta(z) & = \sum_{k=1}^\infty \frac{(-1)^{k+1}}{k^s}, \quad \sigma =\Re(z) > 0. \label{zeta214}
\end{align}
Thus, the function $\zeta$ can be uniquely extended to $\sigma>0$, using $\zeta(z)=(1-2^{1-z})^{-1}\eta(z)$, while preserving
Formula~(\ref{zeta213}) if $\sigma>1$: the first series converges if and only if $\sigma>1$, and the second one if and only if $\sigma>0$.
Both functions, after the analytic continuation of $\zeta$, have the same zeroes in the critical strip $0<\sigma<1$. The famous Riemann Hypothesis [\href{https://en.wikipedia.org/wiki/Riemann_hypothesis}{Wiki}]
claims that all the infinitely many zeroes in the critical strip occur at $\sigma=\frac{1}{2}$. This is one of the seven Millenium Problems, with a \$1 million
prize, see \href{https://www.claymath.org/millennium-problems/riemann-hypothesis}{here}. For another one, ``P versus NP", see 
Exercise~\ref{cliquebc}, about finding the maximum cliques of a \gls{gls:nng}\index{graph!nearest neighbor graph}\index{nearest neighbors!nearest neighbor graph}.


More than $10^{13}$ zeroes of $\zeta$ have been computed. The first two million are in Andrew Odlyzko's table,
\href{http://www.dtc.umn.edu/~odlyzko/zeta_tables/index.html}{here}. See the OEIS sequences
 \href{https://oeis.org/A002410}{A002410} and \href{https://oeis.org/A058303}{A058303}. You can find zeroes with the free online version of Mathematica using the \texttt{FindRoot[]} and \texttt{Zeta[]} functions, \href{https://bit.ly/3AXzAGl}{here}. 
 For fast computation, several methods are available, for example
the Odlyzko–Schönhage algorithm [\href{https://bit.ly/330YLeD}{Wiki}]. The statistical properties are studied in 
Guilherme França and André LeClair  \cite{rie34} 
(available online \href{https://arxiv.org/abs/1307.8395}{here}),
in André LeClair   in the context of random walks \cite{rie35} 
(available online \href{https://www.mdpi.com/2073-8994/13/11/2014}{here})
and in Peter J. Forrester and Anthony Mays in the context of random matrix theory \cite{may99} (available online \href{https://arxiv.org/abs/1506.06531}{here}). 
I discuss recent developments about the Riemann Hypothesis in my article ``Fascinating Facts About Complex Random Variables and the Riemann Hypothesis", \href{https://www.vgranville.com/2022/02/fascinating-facts-about-complex-random.html}{here}. See also my contributions on MathOverflow: ``More mysteries about the zeros of the Riemann zeta function"
(\href{https://mathoverflow.net/questions/379650/more-mysteries-about-the-zeros-of-the-riemann-zeta-function}{here}) and
 ``Normal numbers, Liouville function, and the Riemann Hypothesis"
 (\href{https://mathoverflow.net/questions/391736/normal-numbers-liouville-function-and-the-riemann-hypothesis}{here}). \\
\quad \\
\noindent {\bf Connection to Poisson-binomial Processes}

\noindent  Rather than directly studying $\eta(z)$, I am interested in applying small perturbations to the summation index $k$ (playing the role of a one-dimensional lattice) in Formula~(\ref{zeta214}). In short, replacing $k$ by $X_k$, $k=1,2$ and so on, where the $(X_k)$'s constitute a Poisson-binomial process. This turns $\eta(z)$ into a \textcolor{index}{random function}\index{random function} 
$\eta'(z)$ with real and imaginary parts defined respectively by Formulas~(\ref{eta1b}) and (\ref{eta2b}). 
The question is this:  does the Riemann Hypothesis also apply to the randomized version? 

Unfortunately, the answer is negative, unless the \gls{gls:sf}\index{scaling factor} $s$ in the underlying Poisson-binomial process is very close to zero: see Section~\ref{rh}. 
In short, if $s>0$, $\eta'(z)$ -- unlike $\eta(z)$ -- 
may have zeroes in the critical strip
$0<\sigma<1$, with $\sigma\neq \frac{1}{2}$. A positive answer would have provided some hope and a new path of attack. Another similar attempt, somewhat more promising, is discussed in my
article ``Deep visualizations to Help Solve Riemann's Conjecture", \href{https://www.vgranville.com/2022/02/deep-visualizations-to-help-solve.html}{here}. Again, $\sigma$ is the real part of $z$. Note that if $s=0$, then
$\eta(z)=\eta'(z)$.

The videos in Figure~\ref{fig:example67} (on the left) show the successive partial sums of $\eta'(z)$ in the complex plane. The orbits in the video, depending on $s$ and $z=\sigma + it$, show the chaotic convergence using $\num{10000}$ terms in the summation Formulas~(\ref{eta1b}) and (\ref{eta2b}). If $t$ is large (say $t=10^5$), you usually need much more than $\num{10000}$ terms to reach the convergence zone. Also, I use a \textcolor{index}{Weibull}\index{Weibull distribution}\index{distribution!Weibull} 
or \textcolor{index}{Fréchet}\index{Fréchet distribution}\index{distribution!Fréchet}
distribution of parameter $\gamma$, for the underlying Poisson-binomial process: see
 Formula~(\ref{gam11}).  For standardization purposes discussed in the same section, the \gls{gls:intensity1}\index{intensity function} is set to $\lambda=\Gamma(1+\gamma)$. 

The middle video in Figure~\ref{fig:example67} shows the convergence path of  two orbits (corresponding to two different parameter sets) at the same time, to make comparisons easier. It would be interesting to use a zero of the Riemann zeta function for $z=\sigma+it$: for instance,
$\sigma=\frac{1}{2}$ and $t\approx 14.134725$. The algorithm to produce the partial sums is in the 
\href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference.xlsx}} spreadsheet, in the the \texttt{Video\_Riemann} tab. The parameters $\sigma,t,s,\gamma$ are in cells 
\texttt{B2:B5} for the
first $z$, and \texttt{C2:C5} for the second one. For more details and source code, see Section~\ref{etavis}.\\
 \quad \\
\noindent{\bf The Story Told by the Videos}

\noindent The video starts with a chaotic orbit that looks like a \textcolor{index}{Brownian motion}\index{Brownian motion}. The orbit then gets smoother, jumping from sink to sink until eventually entering a final sink and converging. When $s=0$, the behavior is similar to that pictured in Figure~\ref{fig:pbelbow1}. When $s>0$ and $\gamma\neq 0$,
the whole orbit looks like a Brownian motion. As $s$ and $\gamma$ get larger, the Brownian motion starts exhibiting a strong clustering structure, with well separated clusters called ``sinks".  This is visible in Figure~\ref{fig:pbelbow2}. See the discussion accompanying these figures, for additional details about the sinks, and the Brownian versus clustered-Brownian behavior. My question ``Is this a Brownian motion?", posted 
\href{https://mathoverflow.net/questions/414938/is-this-a-brownian-motion}{here} on MathOverflow, brings more insights.


The cause of the sinks is well-known, and explained in Exercise~\ref{exercise12}, for the one-dimensional case. The orbits are very sensitive to small changes in the parameters, especially to
tiny moves from the base model $s=0$. Large values of $t$ produce a large number of sinks; the behavior is radically different when $t$ is close to zero. Values of $\sigma$ between $0.1$ and $0.6$ produce similar patterns. Outside this range, the patterns are noticeably different. 

The video featuring two orbits has this peculiarity: the orbit on the left, with $s=0$, is non-Brownian; the one on the right with $s=0.05$ and $\gamma=0.005$ is slightly Brownian (barely, because $s$ is still very close to zero, yet the curve is a bit less ``curvy"). Despite the tiny difference in $s$, which makes you think that both orbits should converge to close locations, in reality the two orbits move in radically different directions from the very beginning: this is a typical feature of chaotic \textcolor{index}{dynamical systems}\index{dynamical systems}. In this case, it is caused by choosing a large value for $t$ ($t\approx 5.56\times 10^6$).

The observations (2D points) that generate the orbits, are realizations of a new, very rich class of point processes.   
Such point processes could have applications in specific contexts (possibly astronomy), as potential modeling tools. Identifying the sinks and counting their number
can be done using \textcolor{index}{unsupervised clustering}\index{clustering!unsupervised} techniques. One might even use the technique
described in Section~\ref{spa2}. Finally, the color harmony results from using harmonics, that is, cosine waves with specific periods: see 
Section~\ref{etavis} for explanations. The next step is to design a black-box algorithm for palette creation, and to automatically generate and add a soundtrack to the video, using related mathematical formulas that produce harmonic sounds. In short, AI-generated art!

%------------------------------
\subsubsection{Fractal Supervised Classification}\label{videocluster2}

The rightmost video in Figure~\ref{fig:example67} shows \textcolor{index}{supervised clustering}\index{clustering!supervised} in action, from the first frame representing the 
\textcolor{index}{training set}\index{training set} with 4 groups, to the last one showing the cluster assignment of any future observation (an arbitrary point location 
in the \gls{gls:state1}\index{state space}). Based on \textcolor{index}{image filtering}\index{filtering (image processing)} techniques acting as a \textcolor{index}{neural network}\index{neural network}, the video illustrates how machine learning algorithms are performed in GPU (graphics processing unit). 
\textcolor{index}{GPU-based clustering}\index{GPU-based clustering} [\href{https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units}{Wiki}] is very fast, not only because it uses graphics processors and memory, but the algorithm itself has a computational complexity that beats (by a long shot) any traditional classifier. It does not require the computation of \textcolor{index}{nearest neighbor distances}\index{nearest neighbors!nearest neighbor distances}.

The video medium also explains how the clustering is done, in better ways than any text description could do. You can view the video (also called 
\textcolor{index}{data animation}\index{data animation})
on YouTube, \href{https://www.youtube.com/watch?v=dNPSEh-X6uw}{here}. The source code and instructions to help you create your own
videos or replicate this one, is in Section~\ref{codefr}. See Section~\ref{spa2} for a description of the underlying supervised clustering methodology.

I use the word ``fractal" because the shape of the clusters, and their boundaries in particular, is arbitrary. The boundary may be as fractal-like as a shoreline. It also illustrates the concept of
\textcolor{index}{fuzzy clustering}\index{clustering!fuzzy} [\href{https://en.wikipedia.org/wiki/Fuzzy_clustering}{Wiki}]: towards the middle of the video, when the entire state space is eventually classified, constant 
cluster re-assignments are taking place along the cluster boundaries. A point, close to the fuzzy border between clusters A and B, is sometimes assigned to A in a given video frame, and may be assigned to B in the next one. By averaging cluster assignments over many frames, it is possible to compute the probability that the point belongs to A or B. Another question is whether 
the algorithm (the successive frames) converge or not. It depends on the parameters, and in this case, 
\textcolor{index}{stochastic convergence}\index{stochastic convergence} is observed. In other words, despite boundaries changing all the time, their average location is almost constant,
and the changes are small. Small portions of a cluster, embedded in another cluster, don't disappear over time. \\
\quad \\
\noindent {\bf Color Palette Optimization} 

\noindent Finally, the choice of the colors is not arbitrary. You want to use high contrast colors, so that the eye can easily distinguish between two clusters. To achieve this goal, I designed a 
\textcolor{index}{palette optimization}\index{palette optimization} algorithm, especially useful when the number of clusters is large.  Let $m$ be the number of clusters. It works as follows.

\begin{itemize}
\item Step 1: Generate $m$ random colors $(R_i,G_i,B_i)$, $i=1,\dots,m$, each one assigned to a cluster. The RGB vector represents the red, green and blue components; each coordinate is an integer between $0$ and $255$. Compute the minimum distance $\delta$ between any pair of colors. 
\item Step 2: Pick up one of the colors $c$ and generate a random color $c'$. Compute the new minimum distance $\delta'$, assuming $c$ is replaced by $c'$. If $\delta'>\delta$, replace 
$c$ by $c'$ otherwise don't make the change. Repeat Step 2 until the coloring of the clusters is visually good enough.
\end{itemize}
In step 1, in combination with using random colors, one can include prespecified colors such as red, blue, dark green and orange, as they constitute a good starting point. Interestingly, what the algorithm accomplishes is this: finding points (colors) in 3D, randomly distributed around lattice vertices in the RGB cube; the optimum lattice is the one maximizing distances between 
vertices. In short, I created a realization of a 3D Poisson-binomial point process, where the points are the colors! Two solutions achieving the optimum when $m=4$ are the color sets
 \{black, yellow, purple, cyan\} and \{white, red, green, blue\}. For $m>4$, see \href{https://mathoverflow.net/questions/415618/lattice-like-structure-with-maximum-spacing-between-vertices}{here} 

%\pagebreak

%=======================
\section{Statistical Inference, Machine Learning, and Simulations}\label{inference}
%=======================

This section covers a lot of material, extending far beyond Poisson-binomial processes. The main type of processes investigated here is the 
\gls{gls:mip}\index{$m$-interlacing} defined in Section~\ref{sm1},
 as opposed to the radial cluster processes studied in Section~\ref{s:clp}. An $m$-process is a superimposition of $m$ shifted Poisson-binomial processes, well suited to model cluster structures. In Section~\ref{spa2}, I discuss supervised and unsupervised clustering algorithms applied to
simulated data generated by $m$-processes. The technique, similar to \textcolor{index}{neural networks}\index{neural network}, relies on 
\textcolor{index}{image filtering}\index{filtering (image processing)} performed in the 
\textcolor{index}{GPU}\index{GPU-based clustering}\index{clustering!GPU-based} (graphics processing unit). It leads to 
\textcolor{index}{fractal supervised clustering}\index{clustering!fractal clustering}\index{fractal clustering}, illustrated with
data animations. I discuss how to automatically detect the number of clusters in Section~\ref{bbcl}.

Before getting there, I describe different methods to estimate the core parameters of these processes. First in one dimension in Section~\ref{estpar}, then in two dimensions in Section~\ref{spa1}. 
The methodology features a new test of independence (Section~\ref{indep1}), model fitting via the 
 \gls{gls:empdis}\index{empirical distribution}, and 
\textcolor{index}{dual} \gls{gls:cr}\index{dual confidence region}\index{confidence region!dual region} in the context of 
\textcolor{index}{minimum contrast estimation}\index{minimum contrast estimation}
 (Section~\ref{cils1}). I show that the \gls{gls:pc}\index{point count distribution} expectations are almost 
\gls{gls:statio}\index{stationarity} but exhibit small periodic oscillations (Section~\ref{bbei}) and that 
the increments (point counts across non-overlapping, adjacent intervals) are almost independent.   

In many instances, Poisson-binomial processes exhibit patterns that are invisible to the naked eye. In Section~\ref{hardid}, I show examples of such patterns. Then, I discuss \gls{gls:mi}\index{identifiability}, and the need for statistical or machine learning techniques to unearth the invisible patterns. 
\Glspl{gls:be}\index{boundary effect}, their impact, and how to fix this problem, is discussed mainly in Section~\ref{boundary}. 

%---------------------------------
\subsection{Model-free Tests and Confidence Regions}\label{ci0}

In 1979, Bradley Efron published his seminal article ``Bootstrap Methods: Another Look at the Jackknife" 
\cite{be1979}, 
available
online \href{https://projecteuclid.org/journals/annals-of-statistics/volume-7/issue-1/Bootstrap-Methods-Another-Look-at-the-Jackknife/10.1214/aos/1176344552.full}{here}. 
It marked the beginning of a new era in statistical science: the development of model-free, data driven techniques. Several chapters in my book
  ``Statistics: New Foundations, Toolbox, and Machine Learning Recipes" \cite{vgstats} published in 2019
 (available online \href{https://github.com/VincentGranville/Machine-Learning}{here})
  deal with extensions and modern versions of this methodology. I follow the same footsteps here, first discussing the general principles, and then showing how it applies
  to estimating the 
\gls{gls:intensity1}\index{intensity function} $\lambda$ and 
\gls{gls:sf}\index{scaling factor}\index{scaling factor} $s$ 
of a Poisson-binomial process. As in 
Jesper Møller \cite{momo55}, my methodology is based on 
\textcolor{index}{minimum contrast estimation}\index{minimum contrast estimation}: see slides 114-116 \href{https://cimpatogo2018.sciencesconf.org/data/pages/Handout_Moller_CIMPA_Togo_2018.pdf}{here} or
\href{https://drive.google.com/file/d/1y5TZXvAL8fP9G5UkmV3npKgoVB0YWtXk/view?usp=sharing}{here}. See also \cite{hghf} for other examples of this method in the
context of point process inference.

There are easier methods to estimate $\lambda$ and $s$: I describe some of them in Section~\ref{estpar}. However, the goal here 
is to provide a general framework that applies to any multivariate parameter. I chose the parameters $\lambda,s$ as they are central to Poisson-binomial processes. By now,
you should be familiar with them. They serve as a test to benchmark the methodology. Yet, the standard estimator of $\lambda$ is slightly biased, and the method in this section provides
an alternative to obtain unbiased estimates. It assumes that \gls{gls:be}\index{boundary effect} are properly handled. I describe how to deal with them 
in Section~\ref{bbei}.


\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{PB-ci2.PNG}
\caption{Minimum contrast estimation for $(\lambda,s)$}
\label{fig:pbci}
\end{figure}

The idea behind minimum contrast estimation is to use proxy statistics as substitutes for the parameter estimators. It makes sense here as it is not clear what
combination of variables represents $s$. 


\subsubsection{Methodology and Example}\label{cils1}

The observations consist of $2n+1$ points $X_k$ ($k=-n,\dots,n$) realization of a one-dimensional Poisson-binomial process of intensity $\lambda$ and scaling factor $s$, obtained by simulation.  I chose a logistic $F$ in the simulation. Unless $F$ has an unusually thick or thin tail, it has little impact on the \gls{gls:pb55}\index{point distribution}. Let
\begin{align} 
R & =\frac{1}{2n+1}\Big[\max_{|k|\leq n} X_k-\min_{|k|\leq n} X_k\Big], \label{bk00} \\
B_k  & = \Big[\frac{k}{R}, \frac{k+1}{R}\Big[, \quad k=-n,\dots,n-1 \label{bk01} 
\end{align}
and
\begin{equation}
p  =\frac{1}{2n} \sum_{k=-n}^{n-1} \chi[N(B_k)=0], \quad
q  =\frac{1}{2n} \sum_{k=-n}^{n-1} \chi[N(B_k)=1],
 \label{kappa}
\end{equation}
where $\chi$ is the indicator function 
[\href{https://en.wikipedia.org/wiki/Indicator_function}{Wiki}] and $N(B_k)$ is the number of points in $B_k$. If there is a one-to-one mapping between $(\lambda,s)$ and $(p,q)$, then one can easily compute $(p,q)$ using Formula~(\ref{kappa}) applied to the observed data, and then retrieve $(\lambda,s)$ via the inverse mapping. It is even possible to build 2D confidence regions for the bivariate parameter $(\lambda, s)$. That's it! 

I now explain how to implement this generic method to our example. I also address some of the challenges. First, the problem is to find good proxy statistics for 
the model parameters $\lambda,s$. I picked up $p$ and $q$ because it leads to an easy implementation in Excel. However, 
\gls{gls:ia}\index{interarrival times} (their mean and variance) 
are better, requiring smaller samples to achieve the same level of accuracy. Next, we are not sure if the mapping in question is one-to-one. 

\noindent The scatterplot in Figure~\ref{fig:pbci} illustrates the method. The X axis represents $p$, and the Y axis represents $q$. There are two main features:
%\quad \\
\begin{itemize}
\item {\bf Observed data}. The purple dots correspond to values of $(p,q)$ derived from the observations, and 
 computed with Formula~(\ref{kappa}). I tested three sets of observations (thus the three purple dots), each with $\num{20001}$ points (that is, $n=\num{10000}$). 

\item {\bf Theoretical model}. The four overlapping clusters show the distribution of $(p,q)$ for four different values
of $(\lambda,s)$. Each cluster -- identified by its color -- has $100$ points corresponding to $100$ simulations. Each simulation within a same cluster uses 
the same hand-picked $(\lambda,s)$. Also, each  simulation consists of $2n+1$ data points, to match the number of observations. The purpose of these simulations is to find the inverse mapping via numerical approximations. Four colors is just a small beginning. In  Table~\ref{tab81232}, each cluster is summarized by two statistics: its computed center in the $(p,q)$--space, associated to the hand-picked parameter vector $(\lambda,s)$.
\end{itemize}
%\quad \\
\noindent {\bf Point Estimates}

\noindent Let us focus on the rightmost purple dot in Figure~\ref{fig:pbci}, corresponding to one of the three observations sets. Its coordinates vector 
 is denoted as $(p_0,q_0)$.
The $(p,q)$--space is called the \textcolor{index}{proxy space}\index{proxy space}. In this case, it is equal to $[0,1]\times [0,1]$. If
the proxy spaced contained only the four points $(p,q)$ listed in Table~\ref{tab81232}, the estimated value $(\lambda_0,s_0)$ of $(\lambda,s)$ would be the center of the orange cluster.  That is, $(\lambda_0,s_0)=(1.4, 0.6)$ because $(0.3275,0.4113)$  is the closest cluster center to the purple dot $(p_0,q_0)$ in the proxy space. 

But let's imagine that I hand-picked $10^5$ vectors $(\lambda,s)$ instead of four, thus generating $10^5$ cluster centers and a very large Table~\ref{tab81232} with $10^5$ entries. Then again, the best estimator of $(\lambda,s)$ would still  
be the one obtained by minimizing the distance between the purple dot $(p_0,q_0)$ computed on the observations, and the $10^5$ cluster centers. In practice, the hand-picking is automated 
(computerized) and leads to a 
black-box implementation of the estimation procedure. 

\begin{table}[H]
\[
\begin{array}{ccc}
\hline
 \mbox{Cluster} &  (\lambda,  s) & (p, q) \\
\hline
 \mbox{Orange} & (1.4, 0.6) & (0.3275,  0.4113)\\
\mbox{Gray} & (1.4, 0.5) & (0.3186, 0.4216) \\
\mbox{Yellow} & (1.6,  0.7) & (0.3321, 0.3995)\\
\mbox{Blue} & (1.8, 0.6)&  (0.3371,  0.4007)\\
\hline
\end{array}
\]
\caption{\label{tab81232}Extract of the mapping table used to recover $(\lambda,s)$ from $(p,q)$}
\end{table}


\noindent Thanks to the
\textcolor{index}{law of large numbers}\index{law of large numbers} 
\href{https://en.wikipedia.org/wiki/Law_of_large_numbers}{[Wiki]}, the cluster centers quickly converge to their theoretical value as $n$ increases. The cluster
 centers $(p, q)$ in Table~\ref{tab81232} can be computed as a function
of $(\lambda, s)$ using a mathematical formula. It facilitates the construction of the inverse
mapping, avoiding tedious simulations: see Section~\ref{bbei}. 
\quad \\

\noindent {\bf Confidence Regions}\nopagebreak

\noindent Again, for the sake of illustration, let us focus on the rightmost purple dot $(p_0,q_0)$. Imagine that contour lines are drawn around each cluster center. A \textcolor{index}{contour line}\index{contour line} of level
$\gamma$ ($0\leq\gamma\leq 1$) is a closed curve (say an ellipse) oriented in the same direction as the cluster in question, and centered at the cluster center. Its interior covers a proportion $\gamma$ of the points of the cluster, in the proxy space. In this case, the contour line of level $\gamma$, around the 
cluster center $(p,q)$ is obtained as follows. 

\noindent First define
\begin{equation}
H_n(x,y,p,q)=\frac{2n}{1-\rho_{p,q}^2}\cdot 
\Big[\Big( \frac{x-p}{\sigma_p}\Big)^2 
-2\rho_{p,q}\Big(\frac{x-p}{\sigma_p}\Big)\Big(\frac{y-q}{\sigma_q}\Big) 
+ \Big(\frac{y-q}{\sigma_q}\Big)^2\Big],\label{gauss2d}
\end{equation}
with
\begin{equation}
\sigma_p
=\sqrt{p(1-p)},
\quad \sigma_q=\sqrt{q(1-q)},
\quad \rho_{p,q}=-\frac{pq}{\sqrt{pq(1-p)(1-q)}}.
\end{equation}
Then the contour line is the set of points $(x,y)\in[0,1]\times[0,1]$ satisfying $H_n(x,y,p,q)=G_\gamma$. Here 
$G_\gamma$ is a quantile of some \textcolor{index}{Hotelling distribution}\index{Hotelling distribution}\index{distribution!Hotelling}
 [\href{https://bit.ly/3uQ4a3t}{Wiki}]. I included a table of the $G_\gamma$ function, obtained by simulations, in my spreadsheet (see next section); it is also 
pictured on the right plot in Figure~\ref{fig:pbcr}.

This classic asymptotic result is a consequence of the \textcolor{index}{central limit theorem}\index{central limit theorem}, see \href{https://stats.stackexchange.com/questions/372336/confidence-regions-on-bivariate-normal-distributions-using-hat-sigma-mle}{here}. For detailed explanations, see Exercise~\ref{exhotelling}. Note that $G_\gamma$ does not depend on $n$, $p$ or $q$. At least not asymptotically.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{PB-CR.PNG}
\caption{Confidence region for $(p,q)$ -- Hotelling's quantile function on the right}
\label{fig:pbcr}
\end{figure}

We now have a mechanism to find any 
\gls{gls:cr}\index{confidence region} \href{https://en.wikipedia.org/wiki/Confidence_region}{[Wiki]} 
of 
\textcolor{index}{level}\index{confidence level} $\gamma$. It works -- when $n$ is not too small -- as follows: 
\quad \\
\begin{itemize}
\item {\bf Step 1}. Let $(p_0,q_0)$ be the estimator of ($p,q$), computed on your observations set with Formula~(\ref{kappa}).
\item {\bf Step 2}. Find all $(x,y)$'s satisfying $H_n(x,y,p,q)=G_\gamma$, where $(p,q)$ is replaced by $(p_0,q_0)$. These $(x,y)$'s form the boundary of your confidence 
region in the proxy space.
\item {\bf Step 3}. Apply the inverse mapping described earlier (see Table~\ref{tab81232} and spreadsheet section below) to map $(x,y)$ to $(\lambda,s)$. Do it for all $(x,y)$ on the boundary obtained in step 2. 

\end{itemize}
\quad \\
The resulting $(\lambda,s)$'s obtained in step 3 form the boundary of your confidence region in the original parameter space. The methodology described here is generic and applicable to any estimation problem involving multidimensional parameters, regardless of the complexity. In Exercise~\ref{exhotelling}, I introduce a new type of confidence region called 
\textcolor{index}{dual} \gls{gls:cr}\index{confidence region!dual region}\index{dual confidence region},
obtained by swapping the roles of $(p,q)$ and $(x,y)$ in Formula~(\ref{gauss2d}). This new concept is also discussed
 \href{https://stats.stackexchange.com/questions/564702/two-possible-definitions-of-confidence-regions-which-one-to-choose}{here} 
and \href{https://www.datasciencecentral.com/model-free-inference-for-machine-learning-professionals/}{here}.

Again, the choice of $p,q$ as proxy statistics is not ideal, but it leads to an easy implementation in Excel, offering educational value. A different choice may lead to more narrow confidence regions, that is, a higher confidence level $\gamma$. Or to put in another way, it may require a smaller 
\textcolor{index}{sample size}\index{sample size} [\href{https://en.wikipedia.org/wiki/Sample_size_determination}{Wiki}] (that is, smaller observations sets) to produce the same level of confidence. This is true if you choose proxy statistics that, unlike $p$ and $q$, are independent.

\noindent {\bf Remark}: Most authors use $1-\alpha$ for the \textcolor{index}{confidence level}\index{confidence level}, based on a long tradition. 
A different but related concept called ``significance level" is denoted as  $\alpha$: it is technically defined as
``one minus the confidence level" [\href{https://statisticsbyjim.com/hypothesis-testing/critical-value/}{Wiki}]. 
Then the ``critical value" is denoted as $Z_\alpha$. Here, I use $\gamma$ instead of $\alpha$ or $1-\alpha$, and a single term ``confidence level" to avoid confusions.

\quad \\
\noindent {\bf Observed Data, Simulations}

\noindent I produced the observation sets using simulated Poisson-binomial processes of intensity $\lambda=1.4$ and scaling factor $s=0.6$. The simulations and all the computations are in the Excel spreadsheet discussed at the end of Section~\ref{ci0}. The \textcolor{index}{simulations}\index{simulation} are useful in different ways: 
\begin{itemize}
\item It helps you assess if the methodology works. It does not work if the estimates are occasionally or frequently very different from the actual values used in the simulation, or if increasing the sample size does not lead to convergence to the actual values (unless you use a flawy pseudo-random number generator). My methodology passes these tests.
\item It helps you decide which proxy statistics to choose from when faced with multiple options. Choose the one consistently requiring the smallest sample size, if there is
such a clear winner. Simulations also have a benchmarking potential, by allowing you to compare your method with other solutions.
\item It helps you assess when \glspl{gls:be}\index{boundary effect} become an issue, see Section~\ref{bbei}. 
\end{itemize}
In my example, boundary effects were never an issue because
$n$ was large enough, given the small value of $s$: the approximation errors intrinsic to the method were much larger than the minuscule bias caused by ignoring  boundary effects.  Still, it is always good practice to quantify all potential sources of bias. In some occasions, the
\index{index}{pseudo-random number generator}\index{pseudo-random number generator} itself was one of the major sources of inaccuracies 
(see Section~\ref{prng0988}), until it got identified and replaced by a better one. In other
occasions, roundoff errors caused by \textcolor{index}{numerical instability}\index{numerical stability} were to blame. It got fixed by using more 
stable computations or \textcolor{index}{high precision computing}\index{high precision computing} [\href{https://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic}{Wiki}]. 
\quad \\ 

\noindent {\bf Hierarchy of Estimation Methods}

\noindent The estimation method previously discussed is at level 3 on a scale from 1 to 4. The levels are as follows:
\begin{itemize}
\item {\bf Level 1}. An obvious, natural statistic makes sense to estimate the parameter of interest: for instance, the average computed on the observations, to estimate the theoretical mean value.  In addition, an exact or asymptotic formula exists, to determine the bounds of a confidence interval of level $\alpha$, given a sample size of $n$. This requires that a stochastic model underlined by a  parametric family of distributions, is a potential fit for the data. Then the estimation procedure consists of finding the parameter value achieving the best fit, and computing the confidence interval.
\item {\bf Level 2}. The situation is identical to level 1, except that no simple formula exists for the distribution. Then one uses simulations and numerical approximations instead.
\item {\bf Level 3}. No natural statistic exists to estimate the parameter. At this level, generally (but not always) no simple exact formula exists to compute the theoretical distribution. An asymptotic formula valid as $n\rightarrow\infty$, may still be available. Typically, simulations are required. A proxy statistic can be used to estimate the quantity of interest. In my example, this is true for the parameter $s$: see the methodology discussed to build confidence regions. This level corresponds to
model-free estimation, in the sense that no model-based formula, whether exact, approximated or asymptotic, is used to perform the computations and derive the confidence regions. 
\item {\bf Level 4}. This level is known as true \textcolor{index}{model-free}\index{model-free inference}, or data-driven inference [\href{https://www.nature.com/articles/s41467-017-02288-4}{Wiki}].  There may be no natural or simple stochastic model explaining the patterns in the the observations, and the parameter of interest can be rather obscure. In this case, one can use \textcolor{index}{resampling}\index{resampling} techniques [\href{https://en.wikipedia.org/wiki/Resampling_(statistics)}{Wiki}] to produce the simulated
data needed to compute confidence regions or to perform statistical tests. This is explained in my book ``Statistics: New Foundations, Toolbox, and Machine Learning Recipes" \cite{vgstats}. 
\end{itemize}
\noindent With my estimation procedure, if you only use resampled observations to produce the clusters in Figure~\ref{fig:pbci}, you can obtain a confidence region for 
$(p,q)$. However, it is impossible to retrieve $(\lambda,s)$ since this parameter (the scaling factor $s$ in particular) is associated to the model, while $(p,q)$ is not. The confidence region of level $\gamma$ would be (say) an ellipse centered at $(p_0,q_0)$ in the proxy space, containing a proportion $\gamma$ of the cluster centers. The value $(p_0,q_0)$ would still be computed the same way, using Formula~(\ref{kappa}). Somehow though, you would still be able to test whether two observations sets have the same $(\lambda,s)$. They would have the same $(\lambda, s)$
 -- something that you can not test without the model -- if and only if they have the same $(p,q)$ -- something that you can easily test without using any model. 

Confidence regions and statistical tests based on resampled observations may be slightly biased or asymptotically unbiased. Sometimes the bias can be quantified and corrected. Popular methods include 
bootstrapping [\href{https://en.wikipedia.org/wiki/Bootstrapping_(statistics)}{Wiki}] and $k$-fold \textcolor{index}{cross-validation}\index{cross-validation} [\href{https://en.wikipedia.org/wiki/Cross-validation_(statistics)}{Wiki}]. Also see ``A New Distribution-Free Approach to Constructing the Confidence Region for Multiple Parameters" \cite{crmodelfree}, 
available online \href{https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0081179}{here}. 
\quad \\

\noindent {\bf Spreadsheet}

\noindent The spreadsheet is available on my GitHub repository, here: \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_independence.xlsx}} (click on the link to access it).  
Look for the
 \texttt{Confidence\_Region} tab. I simulated $N=\num{10000}$ observations sets, each with $n$ observations. I used the values 
 $p_0, q_0$ in cells \texttt{B1}, \texttt{B2}, and a bivariate Bernoulli model with these values as parameters, to generate the observations. The source code related to the
Bernoulli model is in column \texttt{Y}. 
The Bernoulli model is described 
in Exercise~\ref{exhotelling}, as well as \href{https://stats.stackexchange.com/questions/564702/two-possible-definitions-of-confidence-regions-which-one-to-choose}{here} 
and \href{https://www.datasciencecentral.com/model-free-inference-for-machine-learning-professionals/}{here}.

Each row in the spreadsheet table represents  one of the $N$ sets, with the estimated proportions $p,q$ in columns \texttt{D} and \texttt{E}, then 
$\sigma_p,\sigma_q,\rho_{p,q}$ in columns \texttt{H}, \texttt{I}, \texttt{J}, and $G_\gamma$ in column \texttt{G}. These quantities were computed using the source code in column \texttt{Y}, based on Formulas~(\ref{kappa}) and (\ref{gauss2d}).
The rows are sorted by the values in column \texttt{G}. 
 The confidence region featured in Figure~\ref{fig:pbcr} corresponds to the $(p,q)$'s in the first $\num{9000}$ rows, after the sorting in question. Thus  the confidence level is  a $\gamma=90\%$. The corresponding $G_\gamma=4.595$ is in cell \texttt{G:9001}.


%--------------------
\subsubsection{Periodicity and Amplitude of Point Counts}\label{bbei}

Let  $(X_k)$, with $k\in\mathbb{Z}$, represents the points of a one-dimensional Poisson-binomial process of intensity $\lambda$ and scaling factor $s$. We are interested in point counts 
$N_\tau(t)=N[B_\tau(t)]$ in  the interval $B_\tau(t)=[t, t+\tau[$. Let
 $$\phi_\tau(t) = \mbox{E}[N_\tau(t)].$$ 
By virtue of 
Theorem~\ref{combiexp}, $\phi_\tau(t)=1$ if $\tau=1/\lambda$. More generally, regardless of $\tau$, the function $\phi_\tau(t)$ is periodic of
period $1/\lambda$. That is, $\phi_\tau(t)=\phi_\tau(t+1/\lambda)$. This latter statement is also true 
for $\mbox{Var}[N_\tau(t)]$, $P[N_\tau(t)=0]$, and $P[N_\tau(t)=1]$. This fact is trivial if you look at Formulas~(\ref{eq:f1}), (\ref{eq:f2}), (\ref{eq:f3}) and (\ref{eq:f4}),
 used to compute the four quantities in question.

The amplitude of the oscillations is extremely small even with a scaling factor as low as $s=0.3$ (assuming $F$ is logistic). It quickly tends to zero as $s\rightarrow\infty$. So, 
the process is almost \gls{gls:statio}\index{stationarity} unless $s$ is very close to zero. Thus, in most inference problems, the choice of the (non-overlapping) intervals has very little impact. In particular, $\phi_\tau(t)\approx \lambda\tau$. The small amplitude of $\phi_\tau(t)$ is pictured in Figure~\ref{fig:pbperiod}.

\begin{figure}[H]
\centering
\includegraphics[width=0.51\textwidth]{PB-period.PNG}
\caption{Period and amplitude of $\phi_\tau(t)$; here $\tau=1,\lambda=1.4, s=0.3$}
\label{fig:pbperiod}
\end{figure}

Assuming that $\phi_\tau(t)$ is constant and equal to $\lambda\tau$ results in a tiny error, unless $s$ is very close to zero. To the contrary, 
\glspl{gls:be}\index{boundary effect} are a bigger source of bias, this time when $s$ is large. Simulations can quantify the amount of bias, see Section~\ref{boundary}. See also
the spreadsheet section below.\\
\quad \\
\noindent {\bf Spreadsheet}

\noindent The functions $\phi_\tau(t)=\mbox{E}[N_\tau(t)]$, $\mbox{Var}[N_\tau(t)]$, $P[N_\tau(t)=0]$ and $P[N_\tau(t)=1]$ are tabulated in the spreadsheet 
 \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_independence.xlsx}}. See columns \texttt{D} to \texttt{I} in the \texttt{Periodicity} tab.  
The parameters are $\lambda=1.4$ (cell \texttt{B1}) and $s=0.3$ (cell \texttt{B2}). The source code to produce this table is in column \texttt{AI}. Here $\tau=1$.

Also, columns \texttt{U} to \texttt{Z} contains the computations to estimate $\lambda$ based on a realization of a Poisson-binomial process in column \texttt{S}. I generated $2n+1$ points, with $n=\num{5000}$.
The estimator, 
 denoted as $\lambda_0$, is in column \texttt{Z}. I computed different versions of $\lambda_0=N_\tau(t)/\tau$, based on different values of $t$ and $\tau$. The point counts 
 $N_\tau(t)$ are computed on the simulated realization. The true value $\lambda=1.4$ (used for the simulation in column \texttt{S}) is stored in cell \texttt{B4}, while $s=12$ is stored in cell \texttt{B5}. The purpose is to find optimum $t,\tau$ that minimize the boundary effects, to get an unbiased estimator $\lambda_0$ of the intensity $\lambda$. 

\begin{figure}[H]
\centering
\includegraphics[width=0.535\textwidth]{PB-bias.PNG}
\caption{Bias reduction technique to minimize boundary effects}
\label{fig:pbbias}
\end{figure}

Figure~\ref{fig:pbbias} shows how $\lambda_0$ (on the Y axis) varies depending on the choice of a parameter $\alpha$ (on the X axis, and also in column \texttt{U} in the spreadsheet). The parameter 
$\alpha$, with $0.96<\alpha\leq 1$ in the picture, 
determines the interpercentile range $[L_\alpha,U_\alpha] = [t, t+\tau[$ used to compute $\lambda_0$. When $\alpha=1$ (the leftmost position on the X axis), 
 $L_\alpha$ is the minimum, and $U_\alpha$ is the maximum value among the points of the process. The bias is also maximum. The smaller $\alpha$, the fewer points used to compute $\lambda_0$, and the further away we are from the boundaries, thus reducing the bias to almost zero if $\alpha$ is small enough. Yet the smaller $\alpha$, the more unstable $\lambda_0$ is. Thus one needs to find the right balance between a too large and a too small value of $\alpha$. 

In my example, if you look at Figure~\ref{fig:pbbias}, $\alpha=0.992$ achieves this goal, yielding an estimate $\lambda_0=1.400$ correct to three digits. Note that $\alpha=1$ yields a biased value of $\lambda_0$ between $1.380$ and $1.390$ depending on the simulation 
 (close to $1.380$ in Figure~\ref{fig:pbbias}). A technique such as the automated \textcolor{index}{elbow rule}\index{elbow rule}, described in Section~\ref{bbcl}, can be used to detect the optimum $\alpha$, and thus the optimum $\lambda_0$.


%------------------------
\subsubsection{A New Test of Independence}\label{indep1}



You use this kind of tests for instance to assess whether the \glspl{gls:pc}\index{point count distribution} $N(B)$ in various non-overlapping domains $B$ 
are independent or not. Generally, one works with domains of same area. The most popular test of independence is the  
$\chi^2$ (chi-squared) test [\href{https://en.wikipedia.org/wiki/Chi-squared_test}{Wiki}]. One drawback of $\chi^2$ is that it requires binning the data. The bin size
can not be too small, and the bins may be arbitrary. My approach is different, and avoids this problem. It is also well suited to detect small deviations from
independence.  

It works as follows. I compare 
the empirical distribution of count frequencies with what it should be 
if the counts were independent. 
I offer two solutions: one based on the R-squared [\href{https://en.wikipedia.org/wiki/Coefficient_of_determination}{Wiki}], and one based on the 
\textcolor{index}{Kolmogorov-Smirnov statistic}\index{Kolmogorov-Smirnov test} [\href{https://bit.ly/3uJMMNK}{Wiki}]. The latter is similar to the approach discussed by Zhang 
 in his article ``A Kolmogorov-Smirnov type test for independence between marks and points of marked point processes" 
\cite{js2014}, 
available online \href{https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-8/issue-2/A-Kolmogorov-Smirnov-type-test-for-independence-between-marks-and/10.1214/14-EJS961.full}{here}. 


\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{PB_ind.PNG}
\caption{A new test of independence (R-squared version)}
\label{fig:pbindp}
\end{figure}

Let $(X_k)$ be the points of a Poisson-binomial process $M_A$ of intensity $\lambda=1$ and scale factor $s=0.7$, with a logistic $F$.  Exercise~\ref{exercise43} shows -- using theoretical arguments -- that the point counts are not independent. Here I establish the same conclusion via statistical testing. The purpose is to
illustrate how the test works, so that you can use it in other contexts. I chose three intervals $B_1=[-1.5,-0.5[$, $B_2=[-0.5,0.5[$, and $B_3=[0.5, 1.5[$. The data consists of $m=\num{1000}$ realizations of the process in question, each one consisting of $41$ points 
$X_k$, $k=-20,\dots,20$. The number $41$ is large enough in this case, to eliminate \glspl{gls:be}\index{boundary effect}. 
The data, computations and results are 
in the spreadsheet \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_independence.xlsx}}, described later in this section.

The point counts attached to a realization $\omega$ of the point process, is denoted as $N_\omega$. The aggregated point count over the $m$ realizations is denoted as $N$, and the set of $m$ realizations is denoted as $\Omega$. Now, for $i=1,2,3$ and $j_1,j_2,j_3\in\mathbb{N}$, I can define the following quantities:
\begin{align}
p_i(j) & =  \frac{1}{m}\sum_{\omega\in\Omega} \chi[N_\omega(B_i)=j], \nonumber \\
p(j_1,j_2,j_3) & =\frac{1}{m}\sum_{\omega\in\Omega} \mbox{ } \prod_{i=1}^3 \chi[N_\omega(B_i)=j_i],\label{id5678} \\
%\chi[N_\omega(B_1)=j_1]\chi[N_\omega(B_2)=j_2]\chi[N_\omega(B_3)=j_3], \nonumber \\
p'(j_1,j_2,j_3) &=\frac{1}{m^3}\prod_{i=1}^3 \mbox{  } \sum_{\omega\in\Omega} \chi[N_\omega(B_i)=j_i], \label{id5679} 
\end{align}
where $\chi$ is the indicator function [\href{https://en.wikipedia.org/wiki/Indicator_function}{Wiki}]. For instance, $p_1(3)=0.043$ means that in $43$ realizations out of $m=\num{1000}$, the domain $B_1$ contained exactly $3$ points. Also, $p'(j_1,j_2,j_3) =p_1(j_1)p_2(j_2)p_3(j_3)$. The three point counts $N(B_1),N(B_2)$, $N(B_3)$ are independently distributed if and only if Formulas~(\ref{id5678}) and (\ref{id5679}) represent 
the same quantity when $m=\infty$. In other words, the three point counts are independently distributed if $p\rightarrow p'$ pointwise [\href{https://en.wikipedia.org/wiki/Pointwise_convergence}{Wiki}], as $m\rightarrow\infty$. 

To avoid future confusion, $p$ and $p'$ are denoted as $p_A$ and $p'_A$ to emphasize the fact that they are attached to the process $M_A$. 
To test for independence, I simulated $m$ realizations of a sister point process $M_B$: one with the same marginal distributions for the three point counts, using the
estimates $p_i(j)$ obtained from $M_A$, but this time with guaranteed independence of the point counts, by design. Likewise, I
define the functions $p_B$ and $p'_B$. Let $\rho_A$ be the correlation between $p_A$ and $p'_A$, computed across all triplets satisfying
$$\min \{p_A(j_1,j_2,j_3), p'_A(j_1,j_2,j_3)\}>\epsilon.$$ 
I chose $\epsilon=0$. In my example, there were fewer than $7\times 7  \times 7= 343$ such triplets. 
Finally, the statistic of the test is $\rho_A^2$. \\

\noindent {\bf Results and Interpretation} 

\noindent In the spreadsheet \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_independence.xlsx}}, 
the tab \texttt{Dataset\_A} corresponds to $M_A$, and \texttt{Dataset\_B} corresponds to $M_B$. The same computations are done in
tab \texttt{Dataset\_C} for another point process $M_C$, identical to $M_A$ except that this time $s=4$. With such a ``large" $s$, $M_C$ is not that different
from a stationary Poisson point process: in particular, the point counts are almost independent (no statistical test could detect that they are not, unless using extremely large samples). 

The main findings are displayed in Figure~\ref{fig:pbindp}.  Blue represents the $M_A$ process, gray represents $M_B$, and red represents $M_C$. Each blue dot
corresponds to a vector $(p_A, p'_A)$ attached to a particular $(j_1,j_2,j_3)$. In case of perfect independence, all the dots should be on the main diagonal. Blue dots
are two far away from the main diagonal, and thus the point counts in $M_A$ are not independent. To the contrary, $M_B$ (supposed to exhibit independence by construction) and $M_C$ 
(known from theory to exhibit near-independence) are close enough to each other and to the main diagonal. If you repeat the experiment with $M_B$ a hundred times, you will get a hundred
gray regression lines, generating a confidence curve for the test. Note that the $R^2$ displayed for the three regression lines in Figure~\ref{fig:pbindp}, are identical to
$\rho_A^2, \rho_B^2,\rho_C^2$, confirming the somewhat poor performance of $M_A$. The slope of the regression line is also an indicator of lack of independence, if it is not close
enough to one. Again, $M_B$ is the loser here, when measured against the slope. The intercept of the regression line (when different enough from zero) further confirms this.

A version of this test, available in the spreadsheet, relies on the Kolmogorov-Smirnov statistics instead of the R-squared. It works with aggregated rather than raw frequencies. In short, 
you replace the empirical probabilities $p_A,p'_A$ (the frequencies) by empirical aggregated probabilities $P_A,P'_A$, that is,
 the  \glspl{gls:empdis}\index{empirical distribution}. The statistic of the test is the uniform norm 
[\href{https://en.wikipedia.org/wiki/Uniform_norm}{Wiki}] $\delta_A=||P_A-P'_A||_\infty$. It leads to the same conclusion. Since the argument of the functions $p_A,p'_A$ are the triplets $(j_1,j_2,j_3)$
and are unordered, there are many different ways to build the empirical distribution. However, the differences among these constructions are minuscule. See also the section
``Interactions in Point Pattern Analysis" in \cite{ppindep}, available online \href{https://www.geo.fu-berlin.de/en/v/soga/Geodata-analysis/Spatial-Point-patterns/Analysis-of-spatial-point-patterns/Interactions/index.html}{here}.\\

\noindent {\bf About the Spreadsheet} 

\noindent The interactive spreadsheet is on my GitHub repository: 
see  \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_independence.xlsx}}. The \texttt{Summary} tab controls the parameters $s$, $\lambda$, and the upper/lower bounds of the intervals $B_1,B_2,B_3$. It also contains
the results: the R-squared's $\rho_A^2,\rho_B^2,\rho_C^2$ respectively in cells \texttt{B11}, \texttt{C11}, \texttt{D11}, and 
the Kolmogorov-Smirnov statistics $\delta_A,\delta_B,\delta_C$ respectively in cells \texttt{B12}, \texttt{C12}, \texttt{D12}.  Columns \texttt{J}, \texttt{K}, \texttt{L} represent the triplets
$(j_1,j_2,j_3)$, also available in concatenated format in column \texttt{I}. For the $M_A$ process, the 
empirical probabilities $p_A,p'_A$ are
in columns \texttt{Q}, \texttt{R}, and the empirical distributions $P_A,P'_A$ are in columns \texttt{S}, \texttt{T}. For $M_B$ and $M_C$, the corresponding values are in
columns \texttt{Z} to \texttt{AD} and \texttt{AI} to \texttt{AM} respectively.

In the \texttt{Dataset\_A} and \texttt{Dataset\_C} tabs, each row (except the first one) represents a realization of the underlying point process, respectively
$M_A$ and $M_C$. The $41$ points of each realization are in columns \texttt{F} to \texttt{AT}.  The first row (same columns) stores the indices of the points in question, in the \gls{gls:index1}\index{index!index space}\index{index}. I used the logistic distribution for $F$. 

The \texttt{Dataset\_B} tab corresponds to $M_B$. It is organized
differently. The actual points of each realization are not computed as they are not needed this time. Thus they are not in the spreadsheet. Instead, point counts summarizing each ``unobserved" realization are in columns \texttt{I}, \texttt{J}, \texttt{K}, corresponding respectively to $B_1, B_2, B_3$. 
Columns \texttt{Q} and \texttt{R}, representing the values of $p_B$ and $p'_B$ (with the argument in column \texttt{F}), are derived from these counts. Remember that $M_B$ was designed so that (1) $p'_B=p'_A$ and (2) the point counts $N(B_1), N(B_2), N(B_3)$ are independent.


%-------------------------------------
\subsection{Estimation of Core Parameters}\label{estpar}
%-----------------------------------
It is assumed that the point process covers the entire \gls{gls:state1}\index{state space} $\mathbb{R}$ or $\mathbb{R}^2$ with infinitely many points,  and that only a finite number of points are observed through a finite (typically rectangular) window or interval.  Here I focus on the one-dimensional case. For processes in two dimensions, see Section~\ref{spa1}.

\subsubsection{Intensity and Scaling Factor}\label{estim1}

In one dimension, the two most fundamental parameters are the \gls{gls:intensity1}\index{intensity function} $\lambda$ and the \gls{gls:sf}\index{scaling factor} $s$. 
The standard estimator of $\lambda$ proposed here is asymptotically unbiased [\href{https://en.wikipedia.org/wiki/Consistent_estimator}{Wiki}],
 see Section~\ref{bbei}. For a more generic, model-free method yielding an unbiased estimator simultaneously for $s$ and $\lambda$, along with \glspl{gls:cr}\index{confidence region},
see Section~\ref{cils1}. The goal of this section is to offer efficient estimators, easy to compute, and taking advantage of the properties of the underlying model. \\
\quad \\
\noindent {\bf Estimation of $\lambda$}\label{lambda1865}

\noindent There are various ways to estimate the intensity $\lambda$ (more specifically, $\lambda^d$ in $d$ dimension) 
using \gls{gls:ia}\index{interarrival times} $T$, \textcolor{index}{nearest neighbors}\index{nearest neighbors} (in two dimensions) or the \gls{gls:pc}\index{point count distribution} $N(B)$ computed on some interval $B$. A good estimator with small variance, assuming \glspl{gls:be}\index{boundary effect} are mitigated (see Section~\ref{boundary}), is the total number of observed points divided by the area (or length, in one dimension) of the window of observations. 

Another estimator is based on Theorem~\ref{et}: the expected value of the interarrival time is $1/\lambda$. Thus, if you average all the interarrival times across all the 
observed points (called events in one dimension), you get an unbiased estimator of $1/\lambda$. Its multiplicative inverse will be a slightly biased estimator of $\lambda$; if
the number of points is large enough (say $> 50$), the bias is negligible. \\
\quad \\
\noindent {\bf Estimation of $s$}

\noindent Once $\lambda$ has been estimated, the scaling factor $s$ can be estimated by leveraging Theorem~\ref{sums}. The strategy is as follows. Let $\lambda_0$ be your estimate of $\lambda$. By virtue of Theorem~\ref{sums}, the interarrival times satisfy $\mbox{E}[T^r(\lambda, s)] = \mbox{E}[T^r(1, \lambda s)] / \lambda^r$ for any $r>0$.  This result does not depend on the distribution $F$.

\noindent With $r=2$, let
\begin{itemize}
\item $\tau_0$ be your estimate of the squared interarrival times (the average squared value), computed on your data set,
\item $\tau'=(\lambda_0)^r \cdot \tau_0$, where $\lambda_0$ is your estimate of $\lambda$ (see above subsection),
\item $s'$ be the solution to $\mbox{E}[T^r(1, s')]=\tau'$.
\end{itemize}
Then $s_0=s'/\lambda_0$ is an estimate of $s$.   

\noindent {\bf Example}: Here $F$ is the logistic distribution, and I chose $r=2$. Any $r>0$ except $r=1$ would work. If $\lambda_0=1.45$ and $\tau_0=0.77$, then 
$\tau'=(\lambda_0)^2 \tau_0=1.61$. Looking at the $\mbox{E}[T^2(1, s')]$ table, to satisfy $\mbox{E}[T^2(1, s')]\approx 1.61$, you need $s'=0.65$. Thus $s_0=s'/\lambda_0 = 0.45$. These numbers match those obtained by simulation. 
To view or download the table, look at the $\mbox{E}[T^2]$ tab in \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{ \texttt{PB\_inference.xlsx}}. 

The equation  $\mbox{E}[T^2(1, s')]=\tau'$, where $s'$ is the unknown, can be solved using numerical methods. The easiest way is to build 
a granular table of $\mbox{E}[T^2(1, s)]$ for various values of $s$, by simulating Poisson-binomial processes of
intensity $\lambda=1$ and scaling factor $s$. Then finding $s'$ consists in browsing and interpolating the table in question the old fashioned way, to identify the value of $s$ closest to satisfying $\mbox{E}[T^2(1, s)]=\tau'$. This can of course be automated. There are two ways to perform the simulations in question: 
\begin{itemize}
\item generating one realization of each process with a large number of points (that is, one realization for each $0<s<20$ with $\lambda=1$ and $s$ increments equal to $0.01$),  
\item or generating many realizations of each process, each one with a rather small number of points. 
\end{itemize}
Either way, the results should be almost identical due to \gls{gls:ergo}\index{ergodicity} if the same $F$ is used in both cases. The simulations also allow you to compute the theoretical variance of the estimators in question (at least a very good approximation). This is useful when multiple estimators (based on different statistics) are available, to choose the best one: the one with minimum  variance. Simulations also allow you to compute \textcolor{index}{confidence intervals}\index{confidence interval} for your estimators, as discussed in Section~\ref{ci0}. The source code for the simulations can be found in Section~\ref{s:codeperl}. \\

\noindent {\bf Alternative Estimation Method for $s$}

\noindent It is also possible to use the point count $N(B)$ to estimate $s$. The idea is to partition the \gls{gls:state1}\index{state space} 
(the real line in one dimension, where the points reside)  into short intervals $B_k=\Big[\frac{k}{\lambda},\frac{k+1}{\lambda}\Big[$, $k=0,\pm 1, \pm 2$ and so on, covering the observed points; beware of the \gls{gls:be}\index{boundary effect}. This assumes that $\lambda$ is known or estimated. Let $N_k=N(B_k)$ be the random variable counting the number of observed points in $B_k$. We have $\mbox{E}[N_k]=1$. Also $\mbox{Var}[N_k]\leq 1$ does not depend on $k$ thanks to the
choice of $B_k$ (see Section~\ref{bbei}). The variance is maximum and equal to one when $s=\infty$. 

It is possible, for any value of $s$ and $\lambda$, to compute the theoretical variance $v(\lambda,s)=\mbox{Var}[N_k]$ using either simulations 
or Formula~(\ref{eq:f2}) with $a=0$ and $b=1/\lambda$. It slightly depends on $F$, but 
barely. Now compute the empirical variance of $N_k$ as the average $(N_k-1)^2$ across all the $B_k$'s, based on your observations, assuming $\lambda$ is known or estimated.  This empirical variance is denoted as $v_0(\lambda)$. The estimated value of $s$ is the the one that makes the empirical and theoretical variances identical, that is, the {\em unique} value of $s$ that solves the equation $v(\lambda,s)=v_0(\lambda)$. This method easily generalizes to higher dimensions, see Section~\ref{spa1}. The fact that $\mbox{E}[N_k]=1$
 is a direct consequence of Theorem~\ref{combiexp}.

See the $N_k$ tab in \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference.xlsx}}, for a Poisson-binomial process simulation with a \textcolor{index}{generalized logistic}\index{distribution!generalized logistic} $F$, and computation of $\mbox{E}[N_k]$ and $\mbox{Var}[N_k]$ in Excel. You can download the spreadsheet from the same location.



\subsubsection{Model Selection to Identify $F$}\label{bore}

It is more difficult if not impossible to retrieve the distribution $F$ attached to each point $X_k$. However, see Exercise~\ref{exercise13}. In many cases,
two different $F$'s result in essentially the same model, causing \gls{gls:mi}\index{identifiability} issues.
The situation if much easier if $s$ is very small, small enough that $|X_k - \frac{k}{\lambda}| < \frac{1}{2\lambda}$ for most $k\in\mathbb{Z}$.
Then the index attached to a point $X$, usually unknown, is now equal to 
$$L(X)=\underset{k\in\mathbb{Z}}{\arg\min} \Big|X - \frac{k}{\lambda}\Big|.$$ 
That is, $X=X_k$ with $k=L(X)$. See definition of $\arg\min$ \href{https://en.wikipedia.org/wiki/Arg_max}{here}. This assumes that $\lambda$ is known or estimated.
In this particular situation, assuming $s$ is also known or estimated, the  \gls{gls:empdis}\index{empirical distribution} of $s\cdot (X-L(X))$
computed over many points $X$, converges to $F$ as the number of observed points tends to infinity. See also Section~\ref{hm} about the 
\textcolor{index}{hidden process}\index{hidden model}, and Exercise~\ref{exerciseb1}.

A more practical situation is when one has to decide which $F$ provides the best fit to the data, given a few potential candidates for $F$.  In that case, one may compute (using simulations) the theoretical expectation $\eta(r,\lambda,s, F)=\mbox{E}[T^r(\lambda,s)]$ as a function of $r>0$ for various $F$'s, and find which $F$ provides the best fit to
the estimated $\mbox{E}[T^r(\lambda,s)]$, denoted as  $\eta_0(r,\lambda,s, F)$ and computed on the data (the expectation being replaced by an average when computed on the data). By best fit, I mean finding $F$ that minimizes (say)
\begin{equation}
\gamma(F)=\int_{0}^2 |\eta(r,\lambda,s, F) - \eta_0(r,\lambda,s, F)| dr. \label{mce5}
\end{equation}
\noindent Again, $s$ and $\lambda$ should be estimated first. However, a simultaneous estimation of $\lambda,s,F$ is feasible and consists of finding the parameters
$\lambda,s,F$ minimizing $\gamma(F)$, now denoted as $\gamma(\lambda,s,F)$. See Section~\ref{estim1} to estimate $\lambda$ and $s$ separately: this
stepwise procedure is simpler and less prone to \textcolor{index}{overfitting}\index{overfitting} [\href{https://en.wikipedia.org/wiki/Overfitting}{Wiki}].

The estimation technique introduced here, especially Formula~(\ref{mce5}),  is sometimes referred to as \textcolor{index}{minimum contrast estimation}\index{minimum contrast estimation}.
See slides 114--116 in the presentation entitled ``Introduction to Spatial Point Processes and Simulation-Based Inference",
by Jesper Møller \cite{momo55}, available online \href{https://cimpatogo2018.sciencesconf.org/data/pages/Handout_Moller_CIMPA_Togo_2018.pdf}{here}
 or \href{https://drive.google.com/file/d/1y5TZXvAL8fP9G5UkmV3npKgoVB0YWtXk/view?usp=sharing}{here}. 


%---------
\subsubsection{Theoretical Values Obtained by Simulations}

This section highlights some simulation results obtained with the source code in Section~\ref{s:codeperl} to compute moments $\mbox{E}[T^r]$
of the \gls{gls:ia}\index{interarrival times} $T=T(\lambda,s)$ for various $\lambda,s$ as well as statistics related to the 
\gls{gls:pc}\index{point count distribution} random variable $N(B)$, where 
$B=[a,b]$ is an interval.  More such results are displayed in Figure~\ref{fig:pbconverge}, where a Cauchy, uniform, and logistic $F$ are compared.   The goal is to: 
\begin{itemize}
\item show that except if $F$ has a finite support or $s$ is very small, the choice of $F$ has very little impact (see Figure~\ref{fig:pbconverge}),
\item show how fast the Poisson-binomial process converges to a stationary Poisson process as $s$ increases (see Figure~\ref{fig:pbconverge}),
\item show that any point of the process can be used to compute the {\em theoretical distribution} of $T$, thus choosing $X_0$ or any $X_k$, or averaging over many points, yields the same theoretical distribution (see Table~\ref{tab234}),
\item show that you can use one realization of the process with many points, or many realizations of the process, each with few points,
to compute the theoretical distribution of $T$.
\end{itemize}
\noindent The last fact in the above list illustrates the \gls{gls:ergo}\index{ergodicity} of $T$.

\begin{table}[H]
\[
\begin{array}{lccccc}
\hline
   & \mbox{Formula} & \mbox{Value} & \mbox{Uniform} & \mbox{Logistic} & \mbox{Cauchy}  \\
  &  s=\infty & s=\infty & s=39.85 & s=39.85 & s=39.85  \\
\hline
\hline
\mbox{E}[N(B)]    &  \lambda \mu(B)&  3/2 &  1.5019  & 1.5000 & 1.4962  \\
\mbox{Var}[N(B)]   &  \lambda \mu(B) & 3/2 & 1.4738 & 1.4906 & 1.4872   \\
\mbox{P}[N(B)=0]   & e^{-\lambda \mu(B)} & 0.2231  &   0.2196 & 0.2221 & 0.2230 \\
\mbox{E}[T]   &  1/\lambda & 1 & 1.0003 & 0.9999 & 1.0010  \\
 \mbox{Var}[T]  & 1/\lambda^2 & 1 &  0.9680 & 0.9888  & 1.0029  \\
\mbox{E}[\sqrt{T}]   & \frac{1}{2} \sqrt{\pi/\lambda} & 0.8862 &   0.8865 & 0.8862 & 0.8873\\
\hline
\end{array}
\]
\caption{\label{tab124}Poisson process ($s=\infty$) versus $F_s$ (with $s=39.85$)}
\end{table}

\noindent Table~\ref{tab124} summarizes some statistics produced with the source code in Section \ref{s:codeperl}, with $\lambda=1, r=1/2$ and $B=[a, b]$. Here, $a=-0.75$ and $b=0.75$. The notation $\mu(B)$ stands for $b-a$. In two dimensions, it represents the area of the set $B$ (typically, a square or a circle). In one dimension, when $s=\infty$, $N(B)$ has a Poisson distribution of expectation $\lambda\mu(B)$, and $T$ has an exponential distribution of expectation $1/\lambda$. The limiting process is a stationary Poisson process of intensity $\lambda$. The exact formula for $\mbox{E}[\sqrt{T}]$, when $s=\infty$, was obtained with the online version of Mathematica: you can check the computation, \href{https://bit.ly/30t354T}{here}. In general, convergence to the Poisson process, when $s\rightarrow\infty$, is slower and more bumpy if $F$ is uniform, compared to using a logistic or Cauchy distribution for $F$.

%\npdecimalsign{.}
%\nprounddigits{2}
\begin{table}[H]
\[
\begin{array}{lccccccccccc}
\hline
k \mbox{ } (\mbox{in } X_k) & -5 &-4&-3 & -2&-1&0&1&2&3&4&5\\
\hline
\hline
E[T_k] & 0.99 &0.98&1.01 & 0.99&1.01&1.00&1.00&1.02&1.00&1.00&1.01\\
E[T_k^{1/2}] & 0.90 &0.90&0.90 & 0.91&0.90&0.91&0.91&0.91&0.90&0.90&0.91\\
E[T_k^{3/2}] & 1.24 &1.24&1.24 & 1.27&1.22&1.29&1.27&1.26&1.26&1.24&1.27\\
E[T_k^2] & 1.70 &1.68&1.70 & 1.75&1.67&1.79&1.76&1.70&1.74&1.71&1.75\\
\hline
\end{array}
\]
\caption{\label{tab234}Moments $\mbox{E}[T_k^r]$ of interarrival times, for $r=0.5,\dots,2$ and $k=-5,\dots,5$}
\end{table}

\noindent Table~\ref{tab234} displays various moments obtained by simulation, from averaging $T_k^r$ across $10^4$ realizations of a Poisson-binomial process with a logistic $F$ and $s=\lambda=1$, for small values of $k$, yielding about 2 digits of accuracy.
Each realization consisted of $2n+1$ points $X_{-n},X_{-n+1},\dots,X_0,\dots,X_{n-1},X_{n}$, with $n=30$ large enough to 
avoid significant \textcolor{index}{boundary effects}\index{boundary effect} (see Section~\ref{boundary}). The interarrival time $T_k$ was defined as the distance between $X_k$, and its closest neighbor $X'_k$ to the right.
The purpose was to check whether the choice of $k$ matters. The conclusion from looking at the table, is that it does not matter.
This empirically justifies the choice $k=0$  in our definition of $T$ in Section~\ref{intro2}. 

Another way to measure $T$ is by averaging the various $T_k=X'_k-X_k$, say for $-10^4 < k < 10^4$,  measured on a single realization of the same Poisson-binomial process, with a very large $n$, say $n=3\times 10^4$. Here $X'_k$ is the closest neighbor to $X_k$, to the right on the real axis. It yields the same result. The theoretical value for $r=1$ is $\mbox{E}[T]=1/\lambda$, according to Theorem~\ref{et}. Also for $r=2$, the theoretical value if $s=\infty$ is $\mbox{E}[T^2]=2/\lambda^2$ due to the Poisson process approximation. The value reported in Table~\ref{tab234} is around $1.72$, and this is for $s=1$. We are not that far from the Poisson limit!



%=======================
\subsection{Hard-to-Detect Patterns and Model Identifiability}\label{hardid}
%=====================

Poisson-binomial and related point processes such as \glspl{gls:mip}\index{$m$-interlacing}, exhibit many hard-to-detect patterns. Some can not even be detected with statistical tests. Depending on model parameters, many are not visible to the naked eye. In some cases, this is due to \gls{gls:mi}\index{identifiability}: two apparently different models, with different sets of parameters, are
statistically identical and indistinguishable from each other.  Most of the times though, the differences are real but subtle or imperceptible. To the contrary, on occasions, the naked eye perceives differences when there are none, akin to visual illusions.   

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{pbx2-index-zoom-s05.PNG} % s=0.5 close to s = 0.2 to naked eye
%\includegraphics[width=0.8\textwidth]{pbx2-zoom-s10.PNG} % s=10 
\caption{Radial cluster process ($s=0.5, \lambda=1$) with centers in blue; zoom in on the left}
\label{fig:pbr4}
\end{figure}

In this section, I explore some these peculiarities. As a starter, let's look at Figures~\ref{fig:pbr4b} and \ref{fig:pbr}. They clearly represent two distinct models: lattice structure, versus random point distribution.  But what about
Figures~\ref{fig:pbr4} versus \ref{fig:pbr4c}? Actually, all four feature the same model. The only difference is the choice of the scaling factor $s$. The first two represent 
 two extremes: $s=0.2$ versus $s=2$. But the last two correspond to in-between cases ($s=0.5$ versus $s=1$), and look similar. Also, unless you have experience dealing with these processes, it is not easy to tell whether or not the point pattern in Figure~\ref{fig:pbr4c}, despite looking a bit more ``random" than in Figure~\ref{fig:pbr4},  corresponds to pure randomness (a stationary Poisson process). The answer is negative despite the appearances: the points are too evenly spread to represent pure
 randomness. 

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{pbx2-index-zoom-one.PNG}  % s=1 close to s=10 to naked eye
%\includegraphics[width=0.8\textwidth]{pbx2-index-zoom-s10.PNG}    % s= ???
%\includegraphics[width=0.8\textwidth]{pbx2-zoom-s10.PNG} % s=10
\caption{Radial cluster process ($s=1, \lambda=1$) with centers in blue; zoom in on the left}
\label{fig:pbr4c}
\end{figure}

\noindent Other examples of hard-to-detect differences include:
\begin{itemize}
\item Discriminating between two different $F$'s (the distribution attached to the points), for instance logistic versus Gaussian or Cauchy, unless $s$ is very small.
\item If $s$ is large, the process is hard to distinguish from a stationary Poisson process: see Figure~\ref{fig:pbr}.
\item Point count statistics (expectation, variance and so on) are periodic, but amplitudes are extremely small.
\item The cluster structure in \glspl{gls:mip}\index{$m$-interlacing} may be invisible unless some transformation is applied: see left plot in Figure~\ref{fig:residues}. 
  \textcolor{index}{Nearest neighbor distances}\index{nearest neighbors!nearest neighbor distances} are generally better at detecting differences, compared to point counts.
\item Unless $s$ is very small, it may be impossible to detect if the underlying \gls{gls:lattice1}\index{lattice!lattice space} is square or hexagonal, or if we are dealing with an
  $m$-interlacing or a \textcolor{index}{$m$-mixture}\index{$m$-mixture}. 
\end{itemize}

\noindent To the contrary, in some cases, the naked eye perceives non-existent differences. For instance, the fact that the right plot in Figure~\ref{fig:hexa} has fewer points than the left plot,
 when in fact they both have the same number. In fact, the Poisson-binomial model is a good framework to test and benchmark statistical techniques in contexts that require a very high 
level of  precision. For instance, those aimed at detecting exoplanets, early signs of cancer, or subtle patterns in the stock market.

% yyy
% write white papers $5000 for 10 pages
% making music with R
%   https://towardsdatascience.com/compose-and-play-music-in-r-with-the-rmusic-package-b2afa90761ea
% create palettes

%xxxxxx Add to index / glossary:  ergodicity, deviates; F, mixture (processes), stationarity, independent increments, 
 %  numerical stability, nearest neighbors, empirical distribution, inhibition repulsion attraction;  adjust textcolor accordingly

% anacortes fiber optic

% check where F_s is listed [make list of symbols as manual section? Or add to glossasy along with F.  include F, Fs etc   maybe in introductory section]

%xxxxxxxx  Future research section or thm 'in progress' or double-starred exercise [measure theory]: characterization of PB processes is a PB process uniquely characterized by its poisson-binomial distrib? [the pk]
 %  1) can you retrieve the pk from F yes by construction
 %  2) can you retrieve F from the pk??? can pk be arbitrary??
 %  3) do non PB processes with same PB distrib exist?
 %  4) can PB be characterized by distrib of T [in 1 dim, in 2 dim]

%Add stuff below in inference section
%Biblio: yyy
%  (2nd order stats characterizing clustering or inhibition) https://spatial.uchicago.edu/sites/spatial.uchicago.edu/files/9_points_2_r.pdf
%   distance metrics compared, point patterns: http://www.stat.ucla.edu/~frederic/papers/ppMeasures104.pdf
%graph with many well contrasted colors: [palette creation] choose 10,000 random RGBs, pick up the set that has max NN distance in the RGB cube


% DSC/NewDSC: Bernoulli Lattice Models – Connection to Poisson Processes
  %  https://www.datasciencecentral.com/profiles/blogs/bernouilli-lattice-models-connection-to-poisson-processes
  % https://datasciencecentral.ning.com/profiles/blogs/bernouilli-lattice-models-connection-to-poisson-processes/
  % https://www.vgranville.com/2022/02/bernoulli-lattice-models-connection-to.html
  % redo in latex, post on github as PDF


%------------------------------------
\subsection{Spatial Statistics, Nearest Neighbors, Clustering}\label{ssnn}

I already discussed \textcolor{index}{spatial processes}\index{spatial process} based on the Poisson-binomial point process model, using \textcolor{index}{radial distributions}\index{radial distribution} and an infinite number of clusters, in
Section~\ref{s:clp}. The \textcolor{index}{cluster processes}\index{cluster process} investigated in this section are different (see Exercise~\ref{exercisec1}):    
it is a \textcolor{index}{superimposition}\index{superimposition (point processes)}\index{point process!superimposed} of $m$ 
\textcolor{index}{shifted}\index{point process!shifted} Poisson-binomial processes 
called an \gls{gls:mip}\index{$m$-interlacing}\index{interlaced processes}, or a \textcolor{index}{mixture}\index{point process!mixture}\index{mixture model} of $m$ such processes, 
called an \textcolor{index}{$m$-mixture}\index{$m$-mixture}. It represents a structure with $m$ clusters. They were introduced in Section~\ref{sm1} and \ref{sm2}, and further investigated in Exercises~\ref{exercise14e} and \ref{exercise14f}. Simulation of \textcolor{index}{$m$-interlacings}\index{$m$-interlacing} is straightforward, using Formulas~(\ref{simm1}) and (\ref{simm2}). The concept is also very intuitive. A realization with $m=5$ is shown in Figure~\ref{fig:residues}, with a different color assigned to each individual process of the $m$-interlacing. Full source code is available in Part 2 of Section~\ref{nnsc}.




The main purpose is to discuss a new type of \textcolor{index}{GPU-based clustering algorithm}\index{GPU-based clustering}\index{clustering!GPU-based} (Section~\ref{spa2}), using image filtering techniques taking place in the graphics processing unit (GPU)
[\href{https://en.wikipedia.org/wiki/Graphics_processing_unit}{Wiki}] to accelerate the computations [\href{https://en.wikipedia.org/wiki/AI_accelerator}{Wiki}].  In addition, we are interested in estimating the parameters of the model (Section~\ref{spa1}), including automated detection of the number of clusters (Section~\ref{bbcl})
using a modern black-box version of the \textcolor{index}{elbow rule}\index{elbow rule} [\href{https://en.wikipedia.org/wiki/Elbow_method_(clustering)}{Wiki}].

There are three kinds of clustering: \textcolor{index}{Supervised clustering}\index{clustering!supervised} 
[\href{https://en.wikipedia.org/wiki/Supervised_learning}{Wiki}],
 \textcolor{index}{unsupervised clustering}\index{clustering!unsupervised} [\href{https://en.wikipedia.org/wiki/Cluster_analysis}{Wiki}], and semi-supervised clustering. 
Shervine Amidi's cheatsheets related to his machine learning class CS 229 at Stanford university, provide easy-to-read, very useful summarized information about the various techniques. You can access them \href{https://stanford.edu/~shervine/teaching/cs-229/}{on his webpage} or on \href{https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/cheatsheet-unsupervised-learning.pdf}{Github}. 
Clustering is one of the main techniques in machine learning
 [\href{https://en.wikipedia.org/wiki/Machine_learning}{Wiki}]. It is a good candidate for machine learning automation (abbreviated as AutoML), a field of AI, especially using the methodology described in this section.

\subsubsection{Stochastic Residues} \label{sr40}

Each individual process of the  combined point process (the $m$-mixture or $m$-interlacing) has its own 
\gls{gls:sv}\index{shift vector}, which determines the center of a cluster. By translation, the cluster is replicated around each lattice location
in the \gls{gls:lattice1}\index{lattice!lattice space}, and thus in the \gls{gls:state1}\index{state space} as well. As a result, for statistical inference, it is customary to study the
process (the observed data) \gls{gls:modulo}\index{modulo operator (point processes)} $2/\lambda$ or $1/\lambda$, where statistical patterns are magnified and easier to detect. By modulo $2/\lambda$, I mean the following:  instead of
studying the original points $(X,Y)$, we focus on $(X\bmod 2/\lambda,Y\bmod 2/\lambda)$. The transformed data, after the modulo operation, is called the residual data, or 
\textcolor{index}{stochastic residues}\index{stochastic residues}.
After the modulo operation, we are left with $m$ clusters: one per individual process.  The fact that there are $m=5$ clusters (albeit with huge overlap) in Figure~\ref{fig:residues} is apparent on the right plot featuring the residues, but not on the left plot. In Section~\ref{spa2}, we shall see how to identify these clusters. Typically, in the context of unsupervised clustering, we don't known which individual process a point of the combined process belongs to. 

\begin{figure}%[H]
\centering
\includegraphics[width=0.8\textwidth]{PB-residues.PNG}
%  \includegraphics[width=\linewidth]{pbx2.PNG}
\caption{Realization of a 5-interlacing with $s=0.15$ and $\lambda=1$: original (left), modulo $2/\lambda$ (right)}
\label{fig:residues}
\end{figure}

\noindent {\bf Remark}: The \gls{gls:modulo} operator\index{modulo operator (point processes)} is defined as  $\alpha \bmod{\beta} =\alpha-\beta \cdot \lfloor \alpha/\beta\rfloor$, 
where the brackets represent the floor function (also called integer function [\href{https://en.wikipedia.org/wiki/Floor_and_ceiling_functions}{Wiki}]). It is identical to the one used in modular arithmetic [\href{https://en.wikipedia.org/wiki/Modular_arithmetic}{Wiki}], except that here, $\alpha,\beta$ are usually real numbers rather than integers. 


%---------
\subsubsection{Inference for Two-dimensional Processes}\label{spa1}

Let us assume for now that we are dealing with a single two-dimensional Poisson-binomial point process. Some of the methodology  discussed in Section~\ref{estpar} for the one-dimensional case can be generalized to higher dimensions. The 
\gls{gls:pc}\index{point count distribution} in a square of side
$1/\lambda$ has expectation equal to one, according to a multidimensional version of Theorem~\ref{combiexp}. So, one way to estimate $\lambda$ is to partition the window of observations $W$ into small squares $B_{h,k}(\lambda)=\Big[\frac{h}{\lambda}, \frac{h+1}{\lambda}\Big[ \times 
\Big[\frac{k}{\lambda}, \frac{k+1}{\lambda}\Big[$ for various values of the (unknown) $\lambda$,
compute the number of points $N_{h,k}(\lambda)$ (called \gls{gls:pc}\index{point count distribution}) in each of these squares, and find $\lambda$ that minimizes the empirical variance
$$v(\lambda)=\sum_{h,k}\Big(N_{h,k}(\lambda)-1\Big)^2$$
computed on the observations. The sum is over $h,k\in \mathbb{Z}\cap W'$, where $W$ is the window of observation, 
and $W'$ is slightly smaller than $W$ to mitigate
\glspl{gls:be}\index{boundary effect}. In short, your estimate of the \gls{gls:intensity1}\index{intensity function} $\lambda$ is defined as $\lambda_0=\underset{\lambda}{\arg\min} \mbox{ } v(\lambda)$.

The benefit of this approach is that it also allows you to easily estimate the \gls{gls:sf}\index{scaling factor} $s$. Since $v(\lambda)$ also depends on the unknown $s$, let's denote it as 
$v(\lambda,s)$. Also, let $V(\lambda,s)$ be the theoretical variance of the point count $N(B)$ in $B=\Big[0,\frac{1}{\lambda}\Big[ \times \Big[0,\frac{1}{\lambda}\Big[$, computed using simulations or via the Formula~(\ref{eq:f2}). The 
estimated value of $s$, assuming $\lambda_0$ is the estimate of $\lambda$, is the solution to the equation $v(\lambda_0,s)=V(\lambda_0,s)$. 

Another simple estimator, this time for $\lambda^d$, is the total number of observed points in the observation window $W$, divided by the area of $W$. Here $d=2$ is the dimension of the 
\gls{gls:state1}\index{state space}. Estimators of $\lambda$ and $s$ may also be obtained using \textcolor{index}{nearest neighbor}\index{nearest neighbors} distances, just like I did with
\gls{gls:ia}\index{interarrival times} in one dimension in Section~\ref{estim1}. I haven't checked if the random variable $S$, defined as the size of the 
\glspl{gls:cc}\index{connected components}\index{graph!connected components} associated to the 
 \textcolor{index}{undirected} \gls{gls:nng}\index{graph!undirected}\index{graph!nearest neighbor graph}\index{nearest neighbors!nearest neighbor graph} 
(see Exercise~\ref{exercise14g}), is of any use to estimate $s$. 
\textcolor{index}{Confidence intervals}\index{confidence interval} can be built as in Section~\ref{ci0}. \\

\noindent {\bf Other Possible Tests}

\noindent Besides estimating the core parameters, many other properties or features can be tested. They are too numerous to be treated in details here, so I only provide a quick
summary. See Exercise~\ref{exercise4} for more details.
\begin{itemize}
\item \Gls{gls:anisotropy1}\index{anisotropy}: it means that the \gls{gls:pb55}\index{point distribution} is statistically identical in any direction; 
an example is a \textcolor{index}{cluster process}\index{cluster process} with a radial
point distribution around each cluster center, see section~\ref{s:clp}.  Testing for anisotropy can be done using $\rho(z,r)=N[B(z,r)]/(\pi r^2)$ where $B(z,r)$ is a circle of radius $r$ centered at $z$, and $N$ is the \gls{gls:pc}\index{point count distribution}. In case of anisotropy, and assuming $r$ is not too small so that each circle has at least 20 points, there should be only little variations among the $\rho(z,r)$'s computed at different $(z,r)$.  Simulate a truly anisotropic process (stationary Poisson) with the same number of points in the window of observations, to find exactly what ``only little variations" means.
\item \textcolor{index}{Stationarity}: This consists of testing whether $N[B(z,r+t)]-N[B(z,r)]$ depends only on $t$, and not on $r$. Here, using squares
centered at $z$ and of side $r$ for $B(z,r)$, would show lack of stationarity if $s$ is small and you try different values of $t$, say $t=1/(2\lambda)$ and
$t=1/\lambda$.
\item Correlation: Are the $X$ and $Y$ coordinates correlated? The standard Poisson-binomial process assumes independence between $X$ and $Y$, see
Formula~(\ref{eq:intro00B}). So a non-zero correlation indicates that the data does not fit to a standard Poisson-binomial model. Using different stretching factors for the $X$ and $Y$ coordinates (see Section~\ref{stret1}) can have an impact on the correlation when $s$ is small. 
\item Independence: This test, discussed in Section~\ref{indep1}, is used for instance to assess whether the point counts $N(B)$ in various non-overlapping domains $B$ 
are independent or not. Generally, one uses domains of same area $\mu(B)$ for the test. In one dimension, it is also used to test whether \textcolor{index}{increments}\index{independent increments} 
(that is, successive \gls{gls:ia}\index{interarrival times}) are independent.
\item \Gls{gls:ergo}\index{ergodicity}: For some statistics based on simulations (as opposed to a real-life dataset), one can use a single realization
of the process with many points or a large window of observations, to make inference. Or one can use many realizations, each one with few points or small window, to compute the same statistic and average the observed values across all the realizations.  If the results are statistically the same in both cases, the statistic in question is ergodic, for the point process model in question. A good example is the \textcolor{index}{nearest neighbor distance}\index{nearest neighbors!nearest neighbor distances}, 
between two neighbor points of the process.
\item Repulsion (or \gls{gls:attract}):  An \textcolor{index}{attractive point process} is one where points tend to cluster together, leaving large areas empty, and some areas filled with many nearby points. An example is a \textcolor{index}{cluster process}\index{cluster process}. The opposite is a repulsive process: points tend to stay as far away as possible from each other. The most extreme case is when the \gls{gls:sf}\index{scaling factor} $s$ is zero, as in the left plot in Figure~\ref{fig:hexa}. Typically, the degree of attraction is determined by $s$. However, a cluster process can be both: for instance, if the unobserved cluster centers come from a
\textcolor{index}{parent point process}\index{parent process} with a very small $s$.
\item Number of clusters: Determining the number $m$ of clusters in an \gls{gls:mip}\index{$m$-interlacing} (superimposition of $m$ point processes), or the number of components in an \textcolor{index}{$m$-mixture}\index{$m$-mixture} (mixture of $m$ point processes), is not easy and usually
not feasible if cluster overlap is substantial, at least not exactly. This is discussed in Section~\ref{spa2}. A black-box version of the \textcolor{index}{elbow rule}\index{elbow rule} (the traditional tool to estimate the number of clusters) 
is discussed in Section~\ref{bbcl}.  
\item \Glspl{gls:sv}\index{shift vector}: They are discussed in Section~\ref{stret1} and \ref{sm1} in the context of $m$-interlacings (a superimposition of $m$ processes). Each of the $m$ 
individual processes has a shift vector attached to it: it determines the position of a cluster center 
\gls{gls:modulo}\index{modulo operator (point processes)} $1/\lambda$. If these vectors are well separated and $s$ is small, they can be retrieved. See discussion in Section~\ref{spa2}, and Figure~\ref{fig:map}, featuring 5 different shift vectors ($m=5$) and thus 5 clusters.
\item \textcolor{index}{Homogeneity}\index{homogeneity} and \textcolor{index}{stretching}\index{stretching (point process)}: In Section~\ref{sm1}, I mention the fact that stretched processes are not homogeneous because different intensities apply to the X and Y coordinates: observations are stretched using different stretching factors for each coordinate. More generally, the process is non-homogeneous if the \gls{gls:intensity1}\index{intensity function} depends on the location
in the \gls{gls:state1}\index{state space}. Whether the process is homogeneous or not is thus easy to test, using the \gls{gls:pc}\index{point count distribution} statistic
$N(B)$ computed at various locations.
\item \textcolor{index}{$m$-mixture}\index{$m$-mixture} versus 
\gls{gls:mip}\index{$m$-interlacing}: To decide whether you are dealing with a mixture rather than a superimposition of $m$ point processes, one has to look at the point count distribution on a square $B_\lambda$ of area $1/\lambda^2$. If there is no stretching involved, the theoretical expectation of the point count is $E[N(B_\lambda)]=m$ if the process is an $m$-interlacing; in that case, the number of points in each $B_\lambda$ is also very stable. The first thing to do is to estimate $\lambda$ (see the beginning of Section~\ref{spa1}), then look at the empirical variance of $N(B_\lambda)$ computed on the observations. When $s$ is small enough, $N(B_\lambda)$ is almost constant (equal to $m$) for a $m$-interlacing; it almost has a 
\textcolor{index}{binomial distribution}\index{binomial distribution}\index{distribution!binomial} for an $m$-mixture; see also Exercise~\ref{exerciseb1}. Again, simulations are useful to decide which model provides the best fit.
\item Size of \glspl{gls:cc}\index{connected components}: An interesting problem is to identify the connected components in the 
\textcolor{index}{undirected graph}\index{graph!undirected} of nearest neighbors associated to a point process, see Exercise~\ref{exercise14g}. These 
connected components are featured in Figure~\ref{fig:hexa}. Their size distribution is of particular interest: for instance, on the left plot in  Figure~\ref{fig:hexa},
corresponding to $s=0$, there is only one connected component of infinite size; on the right plot, there are infinitely many small connected components (about 50\% only have two points). It is still an open question as to whether or not this statistic can be used to discriminate between different types of point processes, or whether its theoretical distribution is exactly the same for a large class of point processes (that is, it is an \textcolor{index}{attractor distribution}\index{attractor (distribution)}) and thus of little practical value.  
\end{itemize}

\noindent Below I discuss a statistical test that I used many times, to check how different a set of observed points is, compared to one arising from a simple two-dimensional Poisson-binomial point process, or from a stationary Poisson point process, or more generally from any kind of stochastic point process. \\

\noindent {\bf  Rayleigh Test}

\noindent The \textcolor{index}{Rayleigh test}\index{Rayleigh test} is a generic statistical test to assess whether two data sets consisting of points in two dimensions, arise from the same type of stochastic point process. It assumes that the underlying point process model is uniquely characterized by the distribution of nearest neighbor distances. The most popular use is when the assumed model is a stationary Poisson process: in that case, the statistic of the test has a 
\textcolor{index}{Rayleigh distribution}\index{Rayleigh distribution}\index{distribution!Rayleigh}.
It generalizes to higher dimensions; in that case the Rayleigh distribution becomes a 
\textcolor{index}{Weibull distribution}\index{Weibull distribution}\index{distribution!Weibull}. In short, what the test actually does, is comparing the 
 \glspl{gls:empdis}\index{empirical distribution} of nearest neighbor distances computed on the two datasets, 
possibly after \textcolor{index}{standardization}\index{standardized point process}, to assess if from a statistical
point of view, they are indistinguishable. 

The test is performed as follows. Let's say you have two data sets consisting of points in two dimensions, observed through a window. You compute the
 \gls{gls:empdis}\index{empirical distribution} of the nearest neighbor distances for both datasets, based on the observations, after taking care of \glspl{gls:be}\index{boundary effect}. Let $\eta_1(u)$ and
$\eta_2(u)$ be the two distributions in question. The statistic of the test is
\begin{equation}
V=\int_{-\infty}^\infty |\eta_1(u) - \eta_2(u)| du = \int_{0}^1 |\nu_1(u) - \nu_2(u)| du, \label{ks1}
\end{equation}
where $\nu$ is the empirical \gls{gls:quant}\index{quantile!quantile function}, that is, the inverse of the empirical distribution. An alternative test is based on
$W=\sup_u |\eta_1(u) - \eta_2(u)|$, or on $W'=\sup_u |\nu_1(u) - \nu_2(u)|$. The test based on $W$ is the traditional Kolomogorov-Smirnov test 
[\href{https://en.wikipedia.org/wiki/Kolmogorov-Smirnov_test}{Wiki}] with known tabulated values. In Excel, it is easier to use the empirical quantile function,
readily available as the \texttt{PERCENTILE} Excel function. In practice, the integral in Formula~(\ref{ks1}) is replaced by a sum computed over 100 equally spaced value of $u\in [0,1]$. The advantage of $W$ is that it is known (asymptotically) not to depend on the underlying (possibly unknown) point process model that the data originates from.

I provide an illustration in \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference.xlsx}}:  see the ``Rayleigh test" tab in the spreadsheet. I compare two data sets, one from a simulation of a two-dimensional Poisson-binomial process
 with $s=20$, and one with $s=0.4$. In both cases, $\lambda$ is set to $1.5$ in the simulator; its estimated value on the generated data set is close to $1.5$. I then compare the \textcolor{index}{nearest neighbor distances}\index{nearest neighbors!nearest neighbor distances} (their empirical quantile function) with the theoretical distribution of a two-dimensional stationary Poisson process of
intensity $\lambda^2$. The theoretical distribution is Rayleigh of expectation $1/(2\lambda)$. The dataset with $s=20$ is indistinguishable, at least using the Rayleigh test, from a realization of a stationary Poisson process. This was expected: as $s\rightarrow\infty$, the Poisson-binomial process converges to a Poisson process by virtue of Theorem~\ref{sums2}, and the convergence is very fast. But the data set with $s=0.4$ is markedly different from a Poisson point process realization, as seen by looking at the statistic $V$ or $W'$. 


Tabulated values for the statistics $V$ and $W'$ can be obtained by simulations. For $W$, they have been known since at least 1948, since $W$ is the
Kolomogorov-Smirnov statistic \cite{kst}. Here I simply used tabulated values of the Rayleigh distribution since I was comparing the simulated data with a realization of stationary Poisson process. 
\textcolor{index}{Confidence bands}\index{confidence band}  [\href{https://en.wikipedia.org/wiki/Confidence_and_prediction_bands}{Wiki}] for the
empirical quantile function can be obtained using \textcolor{index}{resampling}\index{resampling} methods [\href{https://en.wikipedia.org/wiki/Resampling_(statistics)}{Wiki}].  Modern resampling methods are discussed in details in my book
``Statistics: New Foundations, Toolbox, and Machine Learning Recipes" \cite{vgstats} available 
\href{https://github.com/VincentGranville/Machine-Learning}{here}; see the chapters ``Model-free, Assumption-free Confidence Intervals" and
``Modern Resampling Techniques for Machine Learning". See also Section~\ref{ci0} in this textbook.


\begin{figure}%[H]
\centering
\includegraphics[width=0.8\textwidth]{PB_Rayleigh.PNG} 
\caption{Rayleigh test to assess if a point distribution matches that of a Poisson process}
\label{fig:rayleigh7}
\end{figure}

Figure~\ref{fig:rayleigh7} illustrates the result of my test, using the empirical quantile function of the nearest neighbor distances, and the statistic $V$ for the test. No re-sampling or confidence bands were needed, the conclusion is obvious: $s=0.4$ provides a simulated data set markedly different from a Poisson point process realization (the gray curve is way off) while $s=20$ is indistinguishable from a Poisson point process (the red and blue curves, representing the empirical quantile function of the \textcolor{index}{nearest neighbor distances}\index{nearest neighbors!nearest neighbor distances}, are almost identical). Interestingly, the scatterplot corresponding to $s=0.4$ (rightmost in Figure~\ref{fig:rayleigh7}) seems more random than with $s=20$ (middle plot), but actually, the opposite is true. The plot with $s=0.4$ corresponds to a \textcolor{index}{repulsive process}\index{repulsion (point process)}, where points are more away from each other than pure chance would dictate; thus it exhibits fewer big empty spaces and less clustering, falsely giving the impression of increased randomness.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{PB-map.PNG}
%  \includegraphics[width=\linewidth]{pbx2.PNG}
\caption{Unsupervised (left) versus supervised clustering (right) of Figure~\ref{fig:residues}}
\label{fig:map}
\end{figure}

%----------
\subsubsection{Clustering Using GPU-based Image Filtering}\label{spa2}

In this section, I describe a methodology for very fast supervised and unsupervised 
\textcolor{index}{clustering}\index{clustering}. The data is first transformed into a 
$400 \times 400$ two-dimensional array called {\em bitmap}. The points are referred to as pixels, and the array represents an image stored in 
\textcolor{index}{GPU}\index{GPU-based clustering} 
(the graphics processing unit) [\href{https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units}{Wiki}]. The functions applied to the bitmap are standard image processing techniques such as high pass \textcolor{index}{filtering}\index{filtering (image processing)} or 
\textcolor{index}{histogram equalization}\index{histogram equalization} [\href{https://en.wikipedia.org/wiki/Histogram_equalization}{Wiki}]. 
The easy-to-read source code is in Section~\ref{plgd}; it is accompanied by detailed comments about the methodology. I encourage you to read it. 

The input data consists of a realization (obtained by simulation) of an 
\textcolor{index}{$m$-interlacing}\index{$m$-interlacing} (that is, a superimposition of $m$ shifted Poisson-binomial processes) with each individual process represented by a different color: see Figure~\ref{fig:residues}. The left plot in Figure~\ref{fig:residues} shows the data points observed through a small window $B=[-10,10]\times[-10,10]$. The right plot corresponds to a much bigger window, with all points taken \gls{gls:modulo}\index{modulo operator (point processes)} $2/\lambda$. So, despite the bigger window, the point locations, after the modulo operation, are in $[0, 2/\lambda]\times[0, 2/\lambda]$. I chose $\lambda=1$ for the 
\gls{gls:intensity1}\index{intensity function}, in the simulations. The modulo operation (see Section~\ref{sr40}) magnifies the cluster structure, invisible on the left plot, and visible on the right plot.  

The end result is displayed in Figure~\ref{fig:map}. The left plot corresponds to unsupervised clustering, including locating the 
\glspl{gls:sv}\index{shifted process}\index{shift vector} attached to each 
individual process of the $m$-mixture. The right plot corresponds to supervised clustering of the entire \gls{gls:state1}\index{state space}:  the color of a point represents the individual point process it belongs to; in this case the data set is the training set. 

\noindent {\bf Remark}: For the simulations, see source code \texttt{PB\_NN.py} in Section~\ref{nnsc} (Part 2), or Formulas (\ref{simm1}) and (\ref{simm2}); $m$-mixtures are described in Exercise~\ref{exercise14e} and Sections~\ref{sm1}, \ref{sm2} and \ref{ssnn}. See \cite{vgar} (available online, \href{https://arxiv.org/abs/0804.1448}{here}) for a similar use of GPU in the context of nearest neighbor clustering. \\

\noindent {\bf Supervised Clustering with High Pass Filter}

\noindent Here the data set represents the training set. The algorithm consists of filtering the whole $400\times 400$ bitmap 3 times. Each time, a local filter is applied 
to each pixel $(x,y)$. Initially, the color $c(x,y)$ attached to the pixel represents the cluster it belongs to, in the training set (or in other words, the individual point process it originates from in the $m$-mixture): its value
is an integer between $0$ and $m-1$ if it is in the training set, and $255$ otherwise.  The new color assigned to $(x,y)$ is
\begin{equation}
c'(x,y)=\underset{j}{\arg \max} \sum_{u=-20}^{20}\sum_{v=-20}^{20}\frac{\chi[c(x-u,y-v)=j]}{\sqrt{1+u^2+v^2}}, \label{filt786}
\end{equation}
where $\arg \max \mbox{ } g(j)$ [\href{https://en.wikipedia.org/wiki/Arg_max}{Wiki}] is the value of $j$ that maximizes $g(j)$, 
and $\chi[A]$ is the indicator function [\href{https://en.wikipedia.org/wiki/Indicator_function}{Wiki}]: $\chi[A]=1$ if $A$ is true, and $0$ otherwise.
The boundary problem (when $x-u$ or $y-v$ is outside the bitmap) is handled in the source code. Have a look at my solution (Part 2 of source code
in Section~\ref{plgd}), though there are many other ways
to handle it. 

After filtering the whole bitmap 3 times, thanks to the large size of the filtering window ($21\times 21$ pixels), all pixels are assigned to a cluster (a color different from $255$). This means that any future point (not in the training set) can easily and efficiently be classified: first, find its location on the bitmap; then its cluster is the color assigned to that location. It is worth asking whether convergence occurs (and to what solution) if you were to filter the bitmap many times.  I have not investigated this problem, however, I studied convergence for a similar type of filter, in my paper 
``Simulated Annealing: A Proof of Convergence" \cite{vgieee}. 

While the algorithm is very fast, the bottleneck is the large size of the local filter window. The amount of time required to color the bitmap is proportional to the size of that window: in our case, $21\times 21$ pixels. There is a way to accelerate this by a factor about $20$, using a caching mechanism. See Exercise~\ref{fastfilter}. \\

\noindent {\bf Connection to Neural Networks}

\noindent The filtering system is essentially a \textcolor{index}{neural network}\index{neural network} [\href{https://en.wikipedia.org/wiki/Neural_network}{Wiki}]. The image before the first iteration (Figure~\ref{fig:residues}, right plot), consisting of the training set,  is the input layer. The final image obtained after 3 iterations 
(Figure~\ref{fig:map}, right plot) is the output layer. The intermediate iterations correspond to the hidden layers. In each layer, a pixel color is a function of quantities computed on neighbor pixels, in the previous layer. This is a classic example of neural network! See Luuk Spreeuwers' PhD thesis ``Image Filtering with Neural Networks" defended in 1992 \cite{luuk} (available online, \href{https://ris.utwente.nl/ws/portalfiles/portal/255169420/Thesis_L_Spreeuwers.pdf}{here}), for more about image filters used as neural networks. 

\noindent The pre-processing step consists of transforming the data set into a bitmap. In the next section about unsupervised clustering, the post-processing step called ``equalizer" plays the role of the sigmoid function in neural networks.\\
 
\noindent {\bf Unsupervised Clustering with Density Equalization}

\noindent A similar filter is used for unsupervised clustering. Much of what I wrote for unsupervised clustering also applies here. I recommend that you first read the above section about supervised clustering. Indeed, both supervised and unsupervised clustering are implemented in parallel in the source code, within the same loop. The main difference is that the color (or cluster) $c(x,y)$ attached to a pixel $(x,y)$ is not known. Instead of colors, I use gray levels representing the density of points at any location on the bitmap: the darkest, the higher the density. I start with a bitmap where $c(x,y)=1$ if $(x,y)$ corresponds to the location of an observed point on the bitmap, and $c(x,y)=0$ otherwise. Again, I filter the whole $400\times 400$ bitmap 3 times with the same 
$20\times 20$ filter size. The new gray level assigned to pixel $(x,y)$ at iteration $t$ is now
\begin{equation}
c'(x,y)=\underset{j}{\arg \max} \sum_{u=-20}^{20}\sum_{v=-20}^{20}\frac{c(x-u,y-v)\cdot 10^{-t}}{\sqrt{1+u^2+v^2}}. \label{filt787}
\end{equation}
The first time this filter is applied to the whole bitmap, I use $t=0$ in Formula~(\ref{filt787}); the second time I use $t=1$, and the third time I use $t=2$. The purpose is to dampen the effect of successive filtering, otherwise the image (left plot in Figure~\ref{fig:map}) would turn almost black everywhere after a few iterations, making it impossible to visualize the cluster structure. The second and third iterations, with the dampening factor, provide an improvement over using a single iteration only. 

After filtering the image, I applied a final post-processing step to enhance the gray levels: see Part 4 of the source code in Section~\ref{plgd}. It is a purely cosmetic step consisting in binning and rescaling the histogram of gray levels to make the image nicer and easier to interpret. This step, called equalization,  can be automated; I will discuss it in details in an upcoming textbook. I chose
a data set with significant overlap among the clusters to show the power of the methodology. Indeed, if you look at the raw data (Figure~\ref{fig:residues}, left plot), the 
cluster structure is invisible to the naked eye. This algorithm was able to only partially recover the cluster structure. The centers of the clusters visible in 
Figure~\ref{fig:map} (left plot) roughly correspond to some of the 
shift vectors attached to the $m$-mixture. Retrieving the 
\glspl{gls:sv}\index{shifted process}\index{shift vector} was one of the goals. 

The dataset used here is produced by the program \texttt{PB\_NN.py} in Section~\ref{nnsc}. You can download the dataset from my
GitHub repository, \href{https://github.com/VincentGranville/Point-Processes/blob/main/Data/PB_NN.txt}{here}. The first column is
the cluster number: an integer $i\in\{0,\dots,m-1\}$ with $m=5$; the fourth column is the X coordinate, and column $5+i$ is the Y coordinate. To produce
the images and manipulate the palettes, I used the Pillow graphics library. See
Section~\ref{plgd}.

%------------
\subsubsection{Black-box Elbow Rule to Detect Outliers and Number of Clusters}\label{bbcl}

In the context of unsupervised clustering, one of the most popular recipes to identify the number of clusters, is the 
\textcolor{index}{elbow rule}\index{elbow rule} [\href{https://en.wikipedia.org/wiki/Elbow_method_(clustering)}{Wiki}]. It is usually performed manually. Here, I show how it can be automated and applied to other problems, such as outlier detection. The idea is a follows: a clustering algorithm (say $k$-means 
[\href{https://en.wikipedia.org/wiki/K-means_clustering}{Wiki}]) can identify a cluster structure with any number of clusters on a given data set; typically, a function $v(m)$ provides a statistical summary of the best cluster structure consisting of $m$ clusters, for $m=1,2,3$ and so on. For instance, $v(m)$ is the sum of the squares of the distances from any observed point to its assigned cluster center.
The function $v(m)$ is decreasing, sharply initially for small values of $m$, then much more slowly for larger values of $m$, creating an elbow in its graph.  The value of $m$ corresponding to the elbow is deemed to be the optimal number of clusters. See Figure~\ref{fig:pbelbow1}. Instead of $v(m)$, I use the standardized version $v'(m)=v(m)/v(1)$. \\

\noindent{\bf Brownian Motions and Clustered Lévy Flights}  

\noindent I illustrate how to use the elbow rule to detect outliers in the next subsection. The same methodology applies to detect the number of clusters. First, let me introduce a new type of
point process: the 
\textcolor{index}{Brownian motion}\index{Brownian motion} [\href{https://en.wikipedia.org/wiki/Brownian_motion}{Wiki}], also called 
\textcolor{index}{Wiener process}\index{Wiener process}. This type of process will be studied in more details in Volume 2 of this textbook.  In one dimension, we start with $X_0=0$ and $X_k=X_{k-1}+R_k\theta_k$, for $k=1,2$ and so on. If the $R_k$'s are independently and identically distributed (iid) with an exponential distribution of expectation $1/\lambda$ and $\theta_k=1$, then the resulting process is a stationary Poisson point process of 
\gls{gls:intensity1}\index{intensity function} $\lambda$ on $\mathbb{R}^{+}$; the $R_k$'s are the successive 
\gls{gls:ia}\index{interarrival times}. If the $\theta_k$'s are iid with $P(\theta_k=1)=P(\theta_k=-1)=\frac{1}{2}$, and independent from the $R_k$'s, then we get a totally different type of process, which, after proper re-scaling, represents a time-continuous Brownian motion in one dimension. I generalize it to two dimensions, as follows. Start with $(X_0,Y_0)=(0,0)$. Then generate the points $(X_k, Y_k)$, with $k=1,2$ and so on, using the recursion
\begin{align}
X_k &  =  X_{k-1}+R_k \cos(2\pi\theta_k) \label{brown10} \\
Y_k & = Y_{k-1}+ R_k\sin(2\pi\theta_k) \label{brown11}
\end{align}
where $\theta_k$ is uniform on $[0, 1]$, and the radius $R_k$ is generated using the formula
\begin{equation}
R_k=\frac{1}{\lambda}\Big(-\log(1-U_k)\Big)^\gamma, \label{gam11}
\end{equation}
where $U_k$ is uniform on $[0,1]$. Also, $\lambda>0$, and the random variables $U_k,\theta_k$ are all independently distributed. If $\gamma>-1$, then
$\mbox{E}[R_k]=\frac{1}{\lambda}\Gamma(1+\gamma)$ where $\Gamma$ is the \textcolor{index}{gamma function}\index{Gamma function} 
[\href{https://en.wikipedia.org/wiki/Gamma_function}{Wiki}]. In order to standardize the process, I use
$\lambda=\Gamma(1+\gamma)$. Thus, $\mbox{E}[R_k]=1$ and if $\gamma>-\frac{1}{2}$,
$$\mbox{Var}[R_k]=\frac{\Gamma(1+2\gamma)}{\Gamma^2(1+\gamma)}-1.$$
We have the following cases:
\begin{itemize}
\item If $\gamma=1$, then $R_k$ has an exponential distribution.
\item If $-1<\gamma<0$, then $R_k$ has a \textcolor{index}{Fréchet distribution}\index{Fréchet distribution}\index{distribution!Fréchet}. If in addition, $\gamma>-\frac{1}{2}$, then its variance is finite. 
\item If $\gamma>0$, then $R_k$ has a \textcolor{index}{Weibull distribution}\index{Weibull distribution}\index{distribution!Weibull}, with finite variance. 
\end{itemize}
Interestingly, the Fréchet and Weibull distributions are two of the three 
\textcolor{index}{attractor distributions}\index{attractor (distribution)} in \textcolor{index}{extreme value theory}\index{extreme values}. In my opinion, Fréchet and Weibull should not be considered as two different families of distributions. See Section~\ref{sev45} for more details.

The two-dimensional process consisting of the points $(X_k,Y_k)$ is a particular type of \textcolor{index}{random walk}\index{random walk} 
[\href{https://en.wikipedia.org/wiki/Random_walk}{Wiki}]. The random variables $R_k$ represent the (variable) lengths of the successive increments. Under proper re-scaling, assuming the variance of $R_k$ is finite, it tends to a time-continuous
two-dimensional Brownian motion. However, if $\mbox{Var}[R_k]=\infty$, it may not converge to a Brownian motion. Instead, it is very similar to a 
\textcolor{index}{Lévy flight}\index{Lévy flight}, and produces a strong cluster structure, with well separated clusters when the number of points is finite, see Figure~\ref{fig:pbelbow1}. The 
Lévy flight uses a \textcolor{index}{Lévy distribution}\index{Lévy distribution}\index{distribution!Lévy} 
[\href{https://bit.ly/3rV7mrq}{Wiki}] for $R_k$, which also has infinite expectation and variance. Along with 
\textcolor{index}{Cauchy}\index{Cauchy distribution}\index{distribution!Cauchy} (also with infinite expectation and variance), it is one of the \textcolor{index}{stable distributions}\index{stable distribution} [\href{https://en.wikipedia.org/wiki/Stable_distribution}{Wiki}]. Such distributions are attractors 
for an adapted version of the \textcolor{index}{Central Limit Theorem}\index{central limit theorem} (CLT), just like the Gaussian distribution is the attractor
for the CLT. A well written, seminal book on the topic, is ``Limit Distributions for Sums of Independent Random Variables", by Gnedenko and Kolmogorov \cite{gk1954}.

For a simple introduction to Brownian and related processes, see the website RandomServices.org by Kyle Siegrist, especially the chapter on
standard Brownian motions, \href{https://www.randomservices.org/random/brown/Standard.html}{here}. My
book ``Applied Stochastic Processes, Chaos Modeling, and Probabilistic Properties of Numeration Systems" \cite{vgdyn} (available online 
\href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/README.md}{here}), offers a fresh perspective and discusses original topics related to dynamical systems, all without any reference to measure theory, and thus accessible to beginners.  \\

\noindent {\bf Elbow Rule to Detect Outliers}

\noindent Figure~\ref{fig:pbelbow1} shows a realization of a Brownian motion with $10^4$ points, using $\gamma=2$ and
$\lambda=\Gamma(1+\gamma)$ in Formula~(\ref{gam11}).  The goal is to detect the number of values, among the top $R_k$'s, that significantly outshine all the others. Here, they are not technically outliers in the sense that they are still deviates of the same distribution; rather, they are called extremes. 
The first step is to rank these values. The ordered values (in reverse order) are denoted as $R_{(1)},R_{(2)}$ and so on, with $R_{(1)}$ being the largest one. I
used $v(m)=R_{(m)}$ as the criterion for the elbow rule, that is, after standardization, $v'(m)=v(m)/v(1)$. 

On the right plot in Figure~\ref{fig:pbelbow1}, the Y axis on the left represents $v'(m)$, the X axis represents $m$, and the $Y$ axis on the right represents the strength of the elbow signal (the height of the red bar; I discuss later how it is computed). The top 10 values of $v'(m)$ ($m=1,\dots, 10)$ are
$$1.00, \quad
0.92, \quad
0.77,\quad
0.76,\quad
0.71,\quad
0.69,\quad
0.63,\quad
0.61,\quad
0.60,\quad
0.56, \quad
0.55,\quad
0.55.$$
Clearly, the third value $0.77$ is pivotal, as the next ones stop dropping sharply, after an initial big drop at the beginning of the sequence. So the ``elbow signal" is strongest at $m=3$, and the conclusion is that the first two values ($2=m-1$) outshine all the other ones. The purpose of the black-box elbow rule algorithm, is to automate the decision process: in this case deciding that the optimum is $m=3$. 

Note that in some instances, it is not obvious to detect an elbow, and there may be none. In my example, the elbow signal is very strong, because I chose a rather large value $\gamma=2$ in Formula~\ref{gam11}, causing the Brownian process to exhibit an unusually strong cluster structure, and large disparities among the top $v(m)$'s.
A larger $\gamma$ would generate even stronger disparities. A negative value of $\gamma$, say $\gamma=-0.75$, also causes strong disparities, well separated clusters, and an easy-to-detect elbow. The resulting process is not even Brownian anymore if $\gamma=-0.75$, since in that case, $\mbox{Var}[R_k]=\infty$. The
standard  Brownian motion corresponds to $\gamma=0$ and can still exhibit clusters depending on the realization. Finally, in our case, $m=3$ also corresponds to the number of clusters on the left plot in Figure~\ref{fig:pbelbow1}. This is a coincidence, one that happens very frequently, because the top $v(m)$'s (left to the elbow) correspond to unusually large values of $R_k$. Each of these very large values typically gives
 rise to the building of a new cluster, in the simulations. 

The elbow rule can be used recursively, first to detect the number of ``main" clusters in the data set, then to detect the number of sub-clusters within each cluster. The strength of the signal (the height of the red bar) is typically very low if the $v'(m)$'s have a low variance. In that case, there is no set of values outshining all the other ones, that is, no true elbow. For an application of this methodology to detect the number of clusters, see a recent article of Chikumbo \cite{vg5}, available online
\href{https://www.mdpi.com/2504-4990/1/2/42}{here}. An alternative to the elbow rule, to detect the number of clusters, 
is the silhouette method [\href{https://en.wikipedia.org/wiki/Silhouette_(clustering)}{Wiki}].

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{PB_elbow1.PNG} 
\caption{Elbow rule (right) finds $m = 3$ clusters in Brownian motion (left)}
\label{fig:pbelbow1}
\end{figure}

I now explain how the strength of the elbow signal (the height of the red bars in Figure~\ref{fig:pbelbow1}) is computed. First, compute the first and second order differences of the function $v'(m)$: 
$\delta_1(m)=v'(m-1)-v'(m)$ for $m>1$, and $\delta_2(m)=\delta_1(m-1)-\delta_1(m)$ for $m>2$. The strength of the elbow signal, at position $m>1$, 
is  $\rho_1(m)=\max[0,\delta_2(m+1)-\delta_1(m+1)]$. I used a dampened version of $\rho_1(m)$, namely $\rho_2(m)=\rho_1(m)/m$, to favor cluster
structures with few large clusters, over many smaller clusters. Larger clusters can always be broken down into multiple clusters, using the same clustering algorithm. 
The data, including formulas, charts, and simulation of the Brownian motion (done in Excel!), is on the \texttt{Elbow\_Brownian} tab,
in the \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference.xls}} spreadsheet. You can modify the parameters highlighted in orange in the spreadsheet: in this case, $\gamma$ in cell 
\texttt{B16}. Note that
$\lambda$ is set to $\Gamma(1+\gamma)$ in cell \texttt{B17}. \\

%---------
\noindent {\bf Back to the Riemann Zeta Function}

\noindent Here I revisit the \textcolor{index}{Riemann zeta function}\index{Riemann zeta function} [\href{https://en.wikipedia.org/wiki/Riemann_zeta_function}{Wiki}] first explored in Section~\ref{rh}. I investigate a related deterministic sequence $(X_k,Y_k)$ also starting at $(X_0,Y_0)=(0,0)$, exhibiting some amount of chaos, as many sequences do in \textcolor{index}{dynamical systems}\index{dynamical systems} [\href{https://en.wikipedia.org/wiki/Dynamical_system}{Wiki}]. This sequence, unlike that produced by Formulas~(\ref{brown10}),(\ref{brown11}) and (\ref{gam11}), converges, albeit chaotically. To the contrary, the Brownian motion sequence, at least when $\mbox{Var}[R_k]$ is finite, eventually covers the entire
plane when the number of points is infinite, producing a dense plot that has a \textcolor{index}{fractal dimension}\index{fractal dimension} [\href{https://en.wikipedia.org/wiki/Fractal_dimension}{Wiki}]. The purpose here is to determine the number of ``jumps"
in the deterministic sequence from starting point to convergence, using the elbow rule. The deterministic sequence represents the partial sums of the 
\textcolor{index}{Dirichlet eta function}\index{Dirichlet eta function} $\eta(z)$ [\href{https://en.wikipedia.org/wiki/Dirichlet_eta_function}{Wiki}] 
 in the complex plane, defined by Formulas~(\ref{eta1}) and (\ref{eta2}). It is redefined here using Formulas~(\ref{brown10}) and (\ref{brown11}), but this time with 
$\theta_k = t \log(k)$ and $R_k=(-1)^{k+1}k^{-\sigma}$, where $t>0$ and $0<\sigma<1$ are two parameters: the imaginary and real part of the argument $z$ of $\eta(z)$; in other words $z=\sigma+it$.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{PB_elbow2.PNG} 
\caption{Elbow rule (right) finds $m = 8$ or $m=11$ ``jumps" in left plot}
\label{fig:pbelbow2}
\end{figure}

The left plot in Figure~\ref{fig:pbelbow2} represents the partial sums $(X_k, Y_k)$ of $\eta(z)$ for the complex number $z=\sigma + it$, using the aforementioned formulas with $k$ terms ($k=1,\dots,10^4)$. The X axis represents the real part, the Y axis the imaginary part. In complex number notation, $(X_k,Y_k)$ is denoted as
$X_k + i Y_k$. Here $\sigma=\frac{1}{2}$ and $t=\num{24556.59}$. Not only this value of $\sigma + it$ is on the critical line 
[\href{https://mathworld.wolfram.com/CriticalLine.html}{Wiki}]  since $\sigma=\frac{1}{2}$, but it is actually an excellent approximation to a non-trivial root 
[\href{https://mathworld.wolfram.com/RiemannZetaFunctionZeros.html}{Wiki}] of the Riemann zeta function. Thus, starting at $(0,0)$,
after an infinite number of steps ($k=\infty$), we end up back at $(0,0)$ as shown on the left plot in Figure~\ref{fig:pbelbow2}. In between, the path is pretty wild!
No wonder why a proof of the famous Riemann Hypothesis [\href{https://en.wikipedia.org/wiki/Riemann_hypothesis}{Wiki}] still remains elusive.

I used the elbow rule to detect the number of sinks, denoted as $m$. A sink is when -- on its path to convergence -- the iterations get stuck for a while around a center, circling many times before resuming the normal path, creating the appearance of circular clusters. The final sink is centered at $(0,0)$ since $\sigma+it$ is a root of $\eta$. 
If $\sigma>0$ is close to zero, and $t$ is large, the number of sinks can be much larger, and you may need far more than $10^4$ iterations to reach the final sink, called ``black hole".
For the elbow rule, I first computed the empirical percentiles of the distance between
$(X_k, Y_k)$ and $(X_{k+\tau}, Y_{k+\tau})$ with $\tau=100$, ignoring the first $\num{1000}$ points where the path is most erratic. Then, I chose the $v(m)$'s as follows: $v(1)$ corresponds to the maximum distance, $v(2)$ to the 99-{\em th} percentile of the distances, $v(3)$ to the 98-{\em th} percentile, and so on. The remaining computations, once the $v(m)$ are computed, are identical to those in the previous section. The method found $m=8$ sinks.

The data, including formulas, charts, and iterative computations of $(X_k,Y_k)$ for $k=1,\dots,10^4$ (done in Excel), is on the 
\texttt{Elbow\_Riemann} tab,
in the \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference.xls}} spreadsheet. You can modify the parameters highlighted in orange in the spreadsheet: in this case, $\sigma$ in cell 
\texttt{B16}, and $t$ in cell \texttt{B17}. The reason why ``jumps" appear in the sequence $(X_k,Y_k)$ is explained and further illustrated for the one-dimensional
case -- the imaginary part of the Dirichlet eta function $\eta(z)$ -- in Exercise~\ref{exercise12}.
Tables of zeros of the Riemann zeta function (up to the first two million), published by Andrew Odlyzko, are available \href{http://www.dtc.umn.edu/~odlyzko/zeta_tables/index.html}{here}.


%-----------------------------------------------------------
\subsection{Boundary Effect}\label{boundary}

A realization of a point process (or data), say in two dimensions, is typically observed through a rectangular window. Yet there are nearest neighbors located outside the observation window. Thus, there is a boundary effect, which may create a \textcolor{index}{bias}\index{bias} in statistics such as 
\gls{gls:pc}\index{point count distribution} or \textcolor{index}{nearest neighbor distance}\index{nearest neighbors}\index{nearest neighbors!nearest neighbor distances}. The problem is obvious if you look at Figure~\ref{fig:hexa}. It is magnified if 
\begin{itemize}
\item the underlying distribution $F$ attached to each point of the Poisson-binomial process, has a thick tail (for instance, $F$ is Cauchy),
\item the number of observed points is small,
\item or the \textcolor{index}{scaling factor}\index{scaling factor} $s$ is large.
\end{itemize}
The problem is mentioned throughout this textbook: see \gls{gls:be}\index{boundary effect} in the glossary. 
In particular, see a solution in Section~\ref{bbei}, in the context of parameter estimation.

In the literature, it is sometimes referred to as 
\textcolor{index}{edge effect}\index{edge effect (statistics)}, see \cite{badxxx} available \href{https://projecteuclid.org/journals/annals-of-statistics/volume-25/issue-1/Kaplan-Meier-estimators-of-distance-distributions-for-spatial-point-processes/10.1214/aos/1034276629.full}{here}. The unobserved data, outside the window of observations, is called 
\textcolor{index}{censored data}\index{censored data}. Some statistics are more sensitive than others to boundary effects.  The standard fix is to compute statistics of interest in a sub-window (best if you don't know the underlying model), or to correct for the bias (if you know the model).

For instance, say you simulate $(2N+1)^2$ points $(X_h,Y_k)$ with $h,k\in\{-N,\dots,N\}$. To minimize boundary effects when computing the average distance between nearest neighbors, you only look at points with index $(h, k)$ satisfying $\max(|h|,|k|)\leq n$. Here, $n$ is smaller than $N$. By how much? The topic of this section is to answer that question. Some nearest neighbors may satisfy $\max(|h|,|k|)\leq n$ and are thus in the smaller window (determined by $n$), and some may not but are still in the bigger window (determined by $N$). Some nearest neighbors could even be outside the bigger window if the difference between $n$ and $N$ is not big enough; in that case, a wrong nearest neighbor will be picked up, and a wrong nearest neighbor distance (too large for the point in question) will be computed. The end result is a bias in the average nearest neighbor distance computed on the observations, making it appear slightly larger than it actually is. By choosing $N$ and $n$ carefully, this problem can be minimized.

The issue is well illustrated in Figure~\ref{fig:hexa}: what you see is the smaller window; yet some arrows are pointing outside the window. These arrows point to nearest neighbors outside the small window; thus these neighbors have to be located in a bigger window, and indeed a bigger window (not shown in the picture) was used to produce the image.

\subsubsection{Quantifying some Biases}

Now, let's quantify the bias in question. If $F$ has a thick tail or $s$ is large, a point $(X_h,Y_k)$ in the window of observations may have its index location $(h,k)$ far away (that is, $\max(|h|,|k|)>N$), and thus won't be generated (in other words, missing). Also, some points, despite having an index location 
$(h,k)$ with  $\max(|h|,|k|)\leq n$ in the \gls{gls:index1} \index{index!index space}, might have their actual location $(X_h,Y_k)$ in the \gls{gls:state1}\index{state space}, far outside the window in question. This point will be generated by the simulator, but may not be included in statistical estimations, and its creation is a waste of time. These are the two problems we face.

It turns out that some of the biases can be exactly computed, assuming you know the underlying model: in our case, a Poisson-binomial point process. Let $N=n$, 
$B(a_n)=[-a_n,a_n]\times [-a_n,a_n]$ be a square with $a_n>0$ to be determined later, and $p_{h,k}(a_n)=P[(X_h,Y_k)\in B(a_n)]$. In two dimensions, we have:

$$p_{h,k}(a_n)=\Big[F\Big(\frac{a_n-h/\lambda}{s}\Big)-F\Big(\frac{-a_n-h/\lambda}{s}\Big) \Big]\times 
\Big[F\Big(\frac{a_n-k/\lambda}{s}\Big)-F\Big(\frac{-a_n-k/\lambda}{s}\Big)\Big].$$

\noindent Also, let $I_n=  \{(h,k)\in \mathbb{Z}^2, \mbox{ with } \max(|h|,|k|)\leq n \}$, and 
\begin{align}
 \nonumber \\
N_1(n)= &\sum_{(h,k)\in I_n} p_{h,k}(a_n),\nonumber \\
N_2(n)= &\sum_{(h,k)\notin I_n} p_{h,k}(a_n),\nonumber \\
N_3(n)=&\sum_{(h,k) \in I_n} (1-p_{h,k}(a_n)) = (2n+1)^2-N_1(n). \nonumber 
\end{align}
The quantities $N_1(n),N_2(n),N_3(n)$ represent respectively the expected number of observed points in the small window $B(a_n)$, the expected number of missing (unobserved) points in the same window, and the expected number of points outside $B(a_n)$ that were generated by the simulator if $(h,k)\in I_n$. The bias, when counting the points in $B(a_n)$ generated by the simulator, is thus  $N_2(n)$. 


\begin{table}%[H]
\[
\begin{array}{cccccccc}
\hline
 F &  n & \lambda & s  & N_1(n) & N_2(n) & N_3(n) & \rho(n)\\
\hline
\hline
\mbox{Logistic} & 100 & 1 & 5 & \num{38712} & \num{1287} &  \num{1689}  &  3.2\% \\
\mbox{Logistic} & 100 & 1 & 1 & \num{39814} & \num{186} &  \num{589}  &  0.5\% \\
\mbox{Logistic} & 50 & 1 & 5 & \num{9356} & \num{644} &  \num{845}  &  6.4\% \\
\mbox{Logistic} & 50 & 1 & 1 & \num{9907} & \num{93} &  \num{294}  &  0.9\% \\

\mbox{Uniform} & 100 & 1 & 5 & \num{39600} & \num{400} &  \num{801}  &  1.0\% \\
\mbox{Uniform} & 100 & 1 & 1 & \num{40000} & \num{0} &  \num{401}  &  0.0\% \\
\mbox{Uniform} & 50 & 1 & 5 & \num{9800} & \num{200} &  \num{401}  &  2.0\% \\
\mbox{Uniform} & 50 & 1 & 1 & \num{10000} & \num{0} &  \num{201}  &  0.0\% \\
\hline
\end{array}
\]
\caption{\label{tabboundary}Bias due to boundary effect, for point count in 2-D}
\end{table}

\noindent For a fixed $n$, it is possible to find $a_n$ that minimizes
$N_2(n)/N_1(n)$, but in practice, $a_n=n/\lambda$ is good enough. Table~\ref{tabboundary} shows the bias $N_2(n)$ obtained with $\lambda=1$ and $a_n=n$. 
The ratio $\rho(n)=N_2(n)/(N_1(n)+N_2(n))$ is the proportion of bias. Assuming $\lambda=1$, the unbiased point count (expected value) is $4n^2$; the biased count is $N_1(n)$.

I used the \texttt{CDF} function in Section~\ref{scdf} to compute the statistics $N_1, N_2$ and $N_3$ in Table~\ref{tabboundary}. The source code, illustrating the use of 
a bivariate cumulative distribution function $F$,  is as follows: \vspace{1ex}
\quad \\
\begin{lstlisting}
N1=0
N2=0
N=2000  # should be infinite, but 2000 is good enough
n=100
llambda=1
s=5
aa=n  # aa corresponds to a_n in the text
type="Logistic"

for h in range(-N,N+1): 
  print(h)
  for k in range(-N,N+1): 
    ff=(CDF(type,llambda,s,h,aa)-CDF(type,llambda,s,h,-aa)) \
              * (CDF(type,llambda,s,k,aa)-CDF(type,llambda,s,k,-aa))
    if abs(k)<=n and abs(k)<=n:
      N1+=ff 
    else: 
      N2+=ff 
N3=(2*n+1)*(2*n+1)-N1
print("N1=",int(N1),"N3=",int(N3)) 
\end{lstlisting}
\quad \\
\noindent  For a fixed, large $n$, as $s$ increases, both $N_2(n)$ and $N_3(n)$ increase, but their ratio tends to $1$ as $s\rightarrow\infty$. This is because
as $s\rightarrow\infty$, the Poisson-binomial process tends to a stationary Poisson process.

\noindent {\bf Remark}: If $a_n=n/\lambda$, by virtue of Theorem~\ref{combiexp} generalized to two dimensions, $N_1(n)+N_2(n)= \lambda^2 \mu(B(a_n))= (2n)^2$.


\subsubsection{Extreme Values}\label{sev45}

Figure~\ref{fig:index} shows the points $(X_h,Y_k)$ of a Poisson-binomial process with a logistic $F$, in the \gls{gls:state1}\index{state space} (blue dots) and their 
index location $(h/\lambda,k/\lambda)$ in the lattice space (red crosses), connected by arrows. Here, $\lambda=1$, thus the \gls{gls:index1}\index{index!index space} and the \gls{gls:lattice1}\index{lattice!lattice space} are identical.  The source code to produce Figure~\ref{fig:index} is provided in Section~\ref{rrr4}.

This picture shows how far away a point can be from the lattice location it is attached to. If $s=0$, both locations coincide, but when $s$ is large, that is, when points are distributed as in
a \textcolor{index}{stationary Poisson process}\index{point process!Poisson},  the distance between the point and its lattice location can be very large, for most points. A large $s$ magnifies the 
\glspl{gls:be}\index{boundary effect} when performing simulations.
Note that both plots (left and right in Figure~\ref{fig:index}) have the same number of points. But points are clustered in some areas, and sparse in other areas on the right plot, giving the impression that there are fewer of them. Clearly, the  \gls{gls:empdis}\index{empirical distribution} of  the distance between nearest neighbors (especially extreme distances), or the average area of the largest empty zone, can be used to estimate the \textcolor{index}{scaling factor}\index{scaling factor} $s$ once $\lambda$ is known or estimated.

This brings me to my next discussion: \textcolor{index}{extreme values}\index{extreme values}, or \textcolor{index}{records}\index{records}. This is part of a field know as 
\textcolor{index}{order statistics}\index{order statistics} [\href{https://en.wikipedia.org/wiki/Order_statistic}{Wiki}] or extreme value theory [\href{https://en.wikipedia.org/wiki/Extreme_value_theory}{Wiki}]. Extreme values are different from \textcolor{index}{outliers}\index{outliers} [\href{https://en.wikipedia.org/wiki/Outlier}{Wiki}]: 
they can be predictable, with known distribution. To the contrary, outliers are usually considered as errors, glitches, or data points obeying a different model. In any case, both have an impact
on the window of observations, delimited by the ``boundary'', and have the potential to introduce biases. 

 
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{PB-index.PNG}
%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Each arrow links a point (blue) to its lattice index (red): $s=0.2$ (left), $s=1$ (right)}
\label{fig:index}
\end{figure}

One question is how far a point can be from its lattice location, and how frequently such ``extremes" occur. Even more interesting is the reverse question, associated to the 
inverse or \textcolor{index}{hidden model}\index{hidden model}: can a point $(X_h,Y_k)$ close to the origin, well within the small window of observations, have its lattice location 
$(h,k)$ very far away? Such a point  will not be generated by the point process simulator. It will be unaccounted for, introducing a bias; indeed, it is counted in $N_2(n)$. This happens with increased frequency as $s$ increases, requiring a larger and larger observation window (that is, larger $n$ and $N$), as seen in Table~\ref{tabboundary}. 

Unless $F$ has a finite support domain (for instance, if $F$ is uniform), unobserved points in the small window of observations -- even though their expected number is finite and rather small -- can be attached to any arbitrary lattice location, not matter how far away. In two dimensions, the probability $P[R>r]$ that the distance $R$ between a point and its lattice location is greater than $r$, is
$$P(R>r)=\int_{-\infty}^\infty \int_{-\infty}^\infty  \chi(x^2+y^2 > r) F\Big(\frac{x}{s}\Big)F\Big(\frac{y}{s}\Big)dx dy$$
where $\chi(A)$ is the indicator function, equal to one if $A$ is true, and to zero otherwise. 

\begin{figure}%[H]
\centering
\includegraphics[width=0.5\textwidth]{PB_Rdist.PNG}
%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Distance between a point and its lattice location ($s=1$)}
\label{fig:index2}
\end{figure}

The distance $R$ corresponds to the length of the arrow, in Figure~\ref{fig:index}. If $F$ is Gaussian, then $R$ has a 
\textcolor{index}{Rayleigh distribution}\index{distribution!Rayleigh}\index{Rayleigh distribution}  [\href{https://en.wikipedia.org/wiki/Rayleigh_distribution}{Wiki}]. In two dimensions, the distance between
two nearest neighbor points, for a stationary Poisson point process, also has a Rayleigh distribution, see Section~\ref{ssnn} and Exercise~\ref{exercise14b}. \\

\noindent {\bf Distribution of Records}

\noindent Now let $M_n$ be 
the maximum distance between a point and its lattice location, measured over $n$ points of the process, randomly selected. In other words
$M_n=\max(R_1,\dots,R_n)$ where $R_i$ ($i=1,\dots,n)$ is the distance between the $i$-th point, and its lattice location.   Depending on $F$, the standardized distribution of $M_n$ is asymptotically \textcolor{index}{Weibull}\index{Weibull distribution}\index{distribution!Weibull}, Gumbel or Fréchet: these are the tree potential \textcolor{index}{attractor distributions}\index{attractor (distribution)} in the context of extreme value 
theory [\href{https://en.wikipedia.org/wiki/Extreme_value_theory}{Wiki}].  The Rayleigh distribution is a particular case of the Weibull distribution. Surprisingly, in $d$ dimensions, the
distribution of the \textcolor{index}{nearest neighbor distances}\index{nearest neighbors!nearest neighbor distances}, for a stationary Poisson point process, is also Weibull, see Section~\ref{ssnn}. 

Figure~\ref{fig:index2} shows (on the Y-axis) the distance $R$ between a 
point $(X_h,Y_k)$ and its  location $(h/\lambda,k/\lambda)$ on the \gls{gls:lattice1}\index{lattice!lattice space}. These are the same points as on the
right plot in Figure~\ref{fig:index}; $R$ represents the length of the arrows. The points are ordered by how close they are to the origin $(0,0)$, and the X-axis represents their distance to the origin, that is, their norm. By looking at Figure~\ref{fig:index2}, it is easy to visualize the extreme values of $R$, and when they occur on the X-axis. \\

\noindent {\bf Distribution of Arrival Times for Records}

\noindent Now let us assume that $n$ is infinite, and let's look at the arrival times of the successive records in the sequence $R_1,R_2,R_3$ and so on. The $i$-th arrival time is denoted as $L_i$
with $L_1=1$, 
and defined   as follows: $L_{i+1}=\min\{j : R_j > R_{L_i}\}$. In other words, the $i$-th record is $R_{L_i}$. The random variable $L_i$ has the following properties:
\begin{itemize}
\item The distribution of $L_i$ does not depend on $F$.
\item Let $\eta_i$ be the probability that $R_i$ is a record. The $\eta_i$'s are independent Bernoulli random variables, and  $P(\eta_i=1)=1/i$.
\item $P(L_i\geq m) = P(\eta_1+\eta_2+\cdots+\eta_m\leq i)$. We are again dealing with a \textcolor{index}{Poisson-binomial distribution}\index{Poisson-binomial distribution}
\index{distribution!Poisson-binomial} [\href{https://en.wikipedia.org/wiki/Poisson_binomial_distribution}{Wiki}].
\item $\mbox{E}[L_i]=\infty$ if $i>1$. However, $\mbox{E}[\log L_i]\sim i-\gamma$ as $i\rightarrow \infty$, where $\gamma=0.5772\dots$ is
the Euler–Mascheroni constant [\href{https://bit.ly/35eVQQl}{Wiki}].
\item $\mbox{Var}[\log L_i]\sim i - \pi^2/6$ as $i\rightarrow\infty$.
\end{itemize}
These results, and many others, are found in chapter 19 ({\em A Record of Records}) in Balakrishnan handbook entitled ``Order Statistics: Theory \& Methods" \cite{order2}. 
See pages 517--525.



%------------------------------------
\subsection{Poor Random Numbers and Other Glitches}\label{flaws}


All machine learning and modeling techniques are subject to a number of issues. I discussed the boundary effect in Section~\ref{boundary}, creating biases in some statistical
measurements, and how to address it. Perturbed lattice point processes, referred to as Poisson-binomial processes in this textbook, are unusually stable structures. However on occasions, one may face \textcolor{index}{numerical stability}\index{numerical stability} or precision issues. For instance, the detection of 
\glspl{gls:cc}\index{connected components} (those generated by the \textcolor{index}{nearest neighbors}\index{nearest neighbors}) can fail if the 
\gls{gls:sf}\index{scaling factor} $s$ is zero. In that case, a point can have multiple nearest neighbors, causing problems. This is addressed in Part 3 of the source code in Section~\ref{nnsc}. Another example is  caused by the chaotic convergence of some mathematical series: see Exercise~\ref{exercise12}, with a solution. Limiting distributions near a singularity are another typical source of problems, see Exercise~\ref{exercise7}, entitled {\em small
 paradox}. Iterative algorithms such as the filter-based classifier in Section~\ref{ssnn}, used to produce
Figure~\ref{fig:map}, may not converge or converge to a wrong solution depending on the parameters. 

But generally speaking, iterative systems going awry are rare when dealing with lattice-based point processes. This is in contrast to discrete \textcolor{index}{dynamical systems}
\index{dynamical systems}, where a
simple recursion such as $x_{n+1}=4x_n(1-x_n)$ with $0<x_0<1$ (called the chaotic \textcolor{index}{logistic map}\index{logistic map}) yields erroneous values with not a single correct digit after as little as $n=50$ iterations, when using single-precision arithmetic [\href{https://en.wikipedia.org/wiki/Single-precision_floating-point_format}{Wiki}]. This is not an issue to compute average-based statistics due to the \gls{gls:ergo}\index{ergodicity} of the dynamical system, mimicking a stochastic process. It becomes an issue when looking at a single path, or when computing statistics such as long-range auto-correlations to assess the randomness of the sequence. 

Surprisingly, in some instances, using a faulty algorithm can be a blessing. For instance, to find the global minimum of the chaotic curve pictured in Figure~\ref{fig:pbfactor}, standard optimization 
techniques such as the \textcolor{index}{fixed point algorithm}\index{fixed point algorithm} [\href{https://en.wikipedia.org/wiki/Fixed-point_iteration}{Wiki}], fail. Instead, I used a fixed point algorithm that by design, never converges. Yet as the iterations approach the (magnified) global minimum of the transformed function, it emits a signal before moving away to nowhere. 
It is possible to retrieve the global minimum via the signal. This will be discussed in an upcoming textbook. 

In our context, since I heavily rely on massive \textcolor{index}{simulations}\index{simulation}, in particular to estimate a number of theoretical distributions with good enough accuracy or to compare two very similar 
 \glspl{gls:empdis}\index{empirical distribution}, an excellent \textcolor{index}{pseudo-random number}\index{random numbers}\index{pseudo-random number generator} (PRNG) generator is paramount. Nowadays, most programming languages and even Excel offer decent PRNGs. See also \href{https://stackoverflow.com/questions/4720822/what-is-the-best-pseudo-random-number-generator-as-of-today}{here} 
for a discussion on this topic. I have used billions of binary digits of peculiar \textcolor{index}{transcendental numbers}\index{transcendental number} [\href{https://en.wikipedia.org/wiki/Transcendental_number}{Wiki}] on many occasions: they provide some of the best non periodic PRNGs. 
You can get one million binary digits of (say) $\sqrt{2}$, online in less than one second, on the Sage 
\textcolor{index}{symbolic math}\index{symbolic math} calculator, \href{https://sagecell.sagemath.org/?z=eJzz0yguLCrRMNLUKShKTbY1NAACTb3ikiKNpMTiVFsjTQCp3gnT&lang=sage}{here}. 
 I now discuss a situation where my PRNG dramatically failed, and a new type of PRNG that I am currently developing. 

\subsubsection{A New Type of Pseudo-random Number Generator}\label{prng0988}

The problem arose when I was performing simulations related to ``six degrees of separation" [\href{https://en.wikipedia.org/wiki/Six_degrees_of_separation}{Wiki}]. 
I needed to generate a few million IDs (each one representing an individual), and for each individual, randomly assign (say) 20 friends, then for each friend, another 20 friends and so on. 
The purpose was to find out whether there was a path between any two people, involving no more than six degrees of separation, and to estimate the average number of degrees of separation between two random people. The pseudo-random generator (PRNG) that I used was able to generate only 32,767 distinct numbers, and thus it miserably failed. So you might want to check if your PRGN has a similar issue.

Over the last 10 years, I have designed and tested many types of PRGN for cryptographic applications. The most interesting ones will be discussed in an upcoming textbook. Here I discuss the most recent one (still a work in progress) as it is simple and related to the material discussed in the subsection ``chance of detecting large factors in very large integers", in Section~\ref{slogs}. Its sequence is defined as follows: $y_n=\bmod(x_n,c)$ with $c=2$ and
$$x_{n+1} = x_n +\sum_{k=1}^r a_k\bmod(x_n,p_k)+\sum_{k=1}^r b_k\bmod(n+1,p_k).$$
The initial value is $x_1$, a positive integer; $p_1,p_2$ and so on are the prime numbers, with $p_1=2$. Also, $a_k,b_k\in \{-1, 0, 1\}$. The sequence is periodic, though the period may start after a large number of iterations. In general, the larger $r$, the larger the period. This PRNG is further discussed \href{https://math.stackexchange.com/questions/4308185/period-or-lack-of-for-a-probably-new-pseudo-random-number-generator}{here}.

\begin{table}%[H]
\[
\begin{array}{crrrrrrrrr}
\hline
k & 1& 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\ \hline
\hline
p_k & 2& 3 & 5 & 7 & 11 & 13 & 17 & 19 & 23\\ 
a_k& -1 & 1 & 1 & 1 & -1 & -1 & 0 & 1 & -1 \\ 
b_k&  -1 & -1 & -1 & -1& 1&  1 & 0 & -1 & 1\\ 
\hline
\end{array}
\]
\caption{\label{tabprng}Parameter set used in pseudo-random number generator}
\end{table}

\noindent The parameter set in Table~\ref{tabprng} yields a 
period equal to $\num{643032390}=2\times3\times5\times7^3\times11\times13\times19\times23$. Detecting the period of these PRNG's, either via an algorithm or through theoretical considerations, is an interesting problem in and of itself. The period grows exponentially fast with the number of prime numbers involved. The number of iterations before the period starts to kick in can be very large. This makes it difficult to detect the period. But to make things easier, the period typically has a simple form, involving the product of consecutive primes. So one can try an integer $q$  (a simple product of primes) and check if for some $n$ large enough, $x_n = x_{n+q}, x_{n+1}=x_{n+q+1},\dots,
x_{n+200}=x_{n+q+200}$. If this is the case, $q$ is a potential candidate for the period.

%\pagebreak

%=======================
\section{Theorems}\label{sthm}
%=======================

The theorems presented here are selected for their practical and educational value. The proofs are usually short, constructive, and sometimes subtle. These results are used in one way or another throughout this textbook, including in the simulations. The reader is invited to try proving some of them on her own, before reading my solutions. I have added many comments, which are just as important as the theorems or the proofs. Emphasis is on making this material accessible to many practitioners as well as beginners, and hopefully, fun to read. 

\noindent {\bf Remark}: Unless otherwise specified, the theorems are valid for the one-dimensional case. Generalizations to higher dimensions are provided for several theorems,
following the proof.


\subsection{Notations}

I use the notation $t_k=k/\lambda$ and $F_s(x) = F(x/s)$. The density attached to $F$, if it exists, is denoted as $f$. Also $B=[a, b]$ with $a<b$ is an interval on the  real line. In two dimensions, $B$ may be a rectangle or a circle, and I use the notation $\mu(B)$ for the area of $B$. The notation ``$\mbox{Left} \equiv \mbox{Right}$'' means that ``$\mbox{Left}$" is a shorter notation for ``$\mbox{Right}$'': by definition, they both represent the same thing.

The random variable  $T(\lambda,s)$ measuring \gls{gls:ia}\index{interarrival times} is sometimes denoted as $T$. It represents the distance between two successive points of the process, once the points are ordered by value on the real line. In higher dimensions, $T$ is the distance between a point of the process, and its closest neighbor. The random variable counting the number of points of the process in $B$ is denoted as $N(B)$ and called \gls{gls:pc}\index{point count distribution}. 
\subsection{Link between Interarrival Times and Point Count}\label{link20}

The fundamental property linking $N$ and $T$ is as follows: $T>y$ (with $y\geq 0$) if and only if no points of the process are in the interval $B_0=]X_0,X_0+y]$, that is, if and only if $N(B_0)=0$. Since $X_0$ is a random variable, this translates into the following.

\begin{align}
P(T>y) & =\int_{-\infty}^\infty f(x)P(N(B_0)=0 | X_0 = x) dx  \nonumber \\
 & =  \int_{-\infty}^\infty f(x)\prod_{k\neq 0}\Big(1-P(X_k\in B_0) \Big)dx \nonumber \\
 & =  \int_{-\infty}^\infty \frac{f(x)}{1-p_0(x,y)}\prod_{k\in\mathbb{Z}}\Big(1-p_k(x,y) \Big)dx \label{eq:1}
\end{align}
where 
\begin{equation}
p_k(x,y) =P(X_k \in ]x, x+y])=F_s(x+y-t_k)-F_s(x-t_k). \label{eq:2}
\end{equation}
A different way to compute the distribution of the interarrival times is offered by Theorem \ref{sums5}. Note that $T$ depends on $\lambda$ and $s$, since $t_k=k/\lambda$. By analogy with the Poisson-binomial distribution attached to the counting random variable $N(B)$, the distribution of $T$ is said to be \textcolor{index}{exponential-binomial} \index{distribution!exponential-binomial} of parameters $p_k(x,y), k\in \mathbb{Z}$. When $s\rightarrow\infty$, the limit is a standard exponential distribution, as seen in Theorem \ref{sums2}. 


\noindent I am now in a position to state and prove some important results. Unless otherwise specified, the theorems apply to the one-dimensional case. Following each proof, when possible, I discuss how the result generalizes to higher dimensions.

%--------------------------------------------------------
\subsection{Point Count Arithmetic}\label{pca23}

Here is a pretty curious arithmetic-related result, easy to prove.

\begin{theorem}
\label{combiexp}
Regardless of the distribution $F_s$, if $\lambda\cdot(b-a)$ is an integer, then $\mbox{E}[N(B)]=\lambda\cdot\mu(B)=\lambda\cdot(b-a)$.
\end{theorem}
\begin{proof}
$ $ \newline 
For any function $F_s$, we have the following trivial equality. Assuming $\lambda\cdot(b-a)=1$, 
$$\mbox{E}_n[B]\equiv \sum_{k=-n}^n \Big(F_s(b-t_k)-F_s(a-t_k)\Big)=F_s(b+t_n)-F(a-t_n).$$
Since $F_s$ is a probability distribution, we have $F_s(b+t_n)\rightarrow 1$ and $F_s(a-t_n)\rightarrow 0$, thus $F(b+t_n)-F(a-t_n)\rightarrow 1$ as $n\rightarrow\infty$. Note that $1=\lambda\cdot(b-a)$. A similar argument involving $2r$ terms on the right hand side, $r$ of them that tend to $1$, minus $r$ of them that tend to $0$ as $n\rightarrow\infty$, leads to the limit $(r\times 1) - (r \times 0) = r$ if $r=\lambda\cdot(b-a)$ is a positive integer. To conclude, note that $\mbox{E}[N(B)]=\lim_{n\rightarrow\infty} \mbox{E}_n[N(B)]$. \qed
\end{proof}

By contrast, for a Poisson process, $\mbox{E}[N(B)]=\lambda\mu(B)$ is always true. For Poisson-binomial processes, this is not the case, as illustrated in Theorem \ref{sums6}. This fact can be used to test whether you are dealing with a Poisson, or a Poisson-binomial process. For related results, see Section~\ref{bbei}.

In two dimensions, the theorem generalizes as follows. Let $B=[a_1, b_1] \times [a_2, b_2]$ be a rectangle, and assume both $\lambda\cdot(b_1-a_1)$ and  $\lambda\cdot(b_2-a_2)$ are integers.
Then $\mbox{E}[N(B)]=\lambda^2\mu(B)=\lambda^2(b_1-a_1)(b_2-a_2)$. In $d$ dimensions $\lambda^2\mu(B)$ becomes
$\lambda^d\mu(B)$,  and $\mu(B)$ is the hypervolume of $B$.

%------------------------------------------------------------------------------
\subsection{Link between Intensity and Scaling Factor}\label{liisc}

The following theorem allows us to reduce the parameter space from two to one parameter.

\begin{theorem}
\label{sums}
Regardless of the distribution $F_s$, the interarrival times satisfy 
$$T(\lambda,s)=\frac{T(1,\lambda s)}{\lambda}.$$
In particular, this also holds when $s=\infty$, corresponding the standard Poisson process.
\end{theorem}
\begin{proof}
$ $ \newline 
After replacing $t_k$ by $k/\lambda$ in (\ref{eq:1}), and since $F_s(z)=F(z/s)$,  we have: 
$$p_k(x,y) = F\Big(\frac{x+y-k/\lambda}{ s}\Big)-F\Big(\frac{ x-k/\lambda}{s}\Big).$$
The expression $F((x+y-k/\lambda)/ s)$ can be rewritten as 
$F((\lambda\cdot(x+y)-k/\lambda')/ s')$ with $\lambda'=1$ and $s'=\lambda s$. This works too if $y=0$. With the change of variable $\lambda\cdot(x+y)=x'+y$, we have $dx = (dx')/\lambda$ and the expression becomes $F((x'+y-k/\lambda')/ s')$. The variables are $x,x'$, and $y$ is assumed to be fixed. 
Integral (\ref{eq:1}), after these changes,  must be updated as follows:
\begin{itemize}
 \item The dummy variable $x$ is replaced by the dummy variable $x'$
 \item The value of the integral is divided by $\lambda$ because $dx =  (dx')/\lambda$
 \item The bounds are still from $-\infty$ to $\infty$
 \item $\lambda$ is replaced by $\lambda'=1$ and $s$ by $s'=\lambda s$
\end{itemize}
That is: $P(T(\lambda,s)>y) = P(T(\lambda',s')/\lambda>y) =P(T(1,\lambda s)/\lambda >y)$, thus
$T(\lambda,s)=T(1,\lambda s)/\lambda$ \qed
\end{proof}

Theorem \ref{sums} has important practical implications. Instead of working with two parameters $\lambda, s$, when dealing with interarrival times, you can replace $T(\lambda,s)$ by $T^*(s')=\frac{1}{\lambda}T(1,s')$, with $s'=\lambda s$, thus reducing the number of effective parameters from two to one. I use this fact in Section~\ref{estim1} to facilitate estimation techniques, and to compute the  \gls{gls:empdis} [\href{https://en.wikipedia.org/wiki/Empirical_distribution_function}{Wiki}] \index{empirical distribution} of $T$ more efficiently.

It would be interesting to see how Theorem~\ref{sums} (and its proof) can be adapted to the two-dimensional case, where interarrival times are replaced by 
distances between a point of the process and its \textcolor{index}{nearest neighbor}\index{nearest neighbors}. Simulations show that the situation is different. 
In two dimensions, $x$ is replaced by $(x_1,x_2)$, and $dx$ becomes $dx_1 dx_2$. The product over $k$ becomes a double product over $h,k$. Also, $F_s(x-k/\lambda)$ is replaced by $F_s(x_1-h/\lambda)F_s(x_2-k/\lambda)$, and $dx_1 = (dx_1')/\lambda, dx_2 = (dx_2')/\lambda$. This suggests that the denominator $\lambda$ 
in Theorem~\ref{sums} should be replaced by $\lambda^2$ in two dimensions. See also Exercise~\ref{exercise14b}.


%---------------------------------------------------------------------------
\subsection{Expectation and Limit Distribution of Interarrival Times}

Here I discuss the one-dimensional case. For the two-dimensional case, see Exercise~\ref{exercise14b}. The proof of the next theorem justifies the choice of $X_0$ as the reference point to define interarrival times; $X_5$ (say) would have led to the same distribution. We already know that if $s=0$ then $T=1/\lambda$, and if $s=\infty$ then $T$ has an exponential distribution of expectation $1/\lambda$. If $s$ is small enough and $F$'s tail is not too thick, then $\mbox{E}[T]=1/\lambda$ and $T$'s distribution is also independent from $X_0$, see Exercise \ref{exercise8}. Now, the result below is valid for any $s\geq 0$.

\begin{theorem}
\label{et}
If  $F$ has a finite expectation, then $\mbox{E}[T(\lambda,s)]=1/\lambda$, regardless of $F$ and $s$.
\end{theorem}
\begin{proof}
$ $ \newline \
Let $(X_k)$ with $k=-n,\dots,n$ be a finite version of a Poisson-binomial point process, with $2n+1$ points. 
One of the points, say $X_{k_1}$, is the minimum, and another one, say $X_{k_2}$, is the maximum.  The range for the $X_k$'s is $X_{k_2}-X_{k_1}$,
with $\mbox{E}[X_{k_2}]=n/\lambda$ and $\mbox{E}[X_{k_1}]=-n/\lambda$. So the expectation of the range is $2n/\lambda$. Since there are $M=2n$ interarrival times between $X_{k_1}$ and $X_{k_2}$, the average interarrival time, that is the average distance between two successive points, is $\frac{1}{\lambda}\frac{2n}{M}=\frac{1}{\lambda}$. This is true whether $n$ is finite or infinite. To finalize the proof, due to the symmetry of the problem (there is nothing special about $X_0$ 
versus, say, $X_5)$, it does not matter, as far as the theoretical expectation is concerned, whether $T$ is defined as the distance between $X_0$ and the next point to the right, or between $X_5$ (or any other point) and the next point.
\qed
\end{proof}

If $F$ is Cauchy, $T$'s expectation may not exist. But in practice, we work with symmetric \textcolor{index}{truncated}\index{truncated distribution}\index{distribution!truncated} \textcolor{index}{Cauchy 
distributions}\index{Cauchy distribution}\index{distribution!Cauchy} [\href{https://en.wikipedia.org/wiki/Truncated_distribution}{Wiki}], that have zero expectation.  Since the choice of the point $X_0$ does not matter in the definition of $T$, one might replace $X_0$ by the closest point to the origin. At least that point is known (observable) while $X_0$ is not. The next theorem, 
though surprisingly easy to prove, is much deeper than Theorem~\ref{et}. I use it to solve Exercise~\ref{exercise13}.

\begin{theorem}
\label{et2}
If  $F$ has a density $f$, then 
\begin{equation}
   \lim_{s\rightarrow 0} P\Big[\frac{1}{s}\Big(T(\lambda,s)-\frac{1}{\lambda}\Big)<y\Big] = \int_{-\infty}^{\infty} F(y-x)f(x)dx.\label{eq:et2}
\end{equation}
\end{theorem}
\begin{proof}
$ $ \newline \
Note that $\mbox{E}[T(\lambda,s)]=\frac{1}{\lambda}$ by virtue of Theorem \ref{et}. When $s\rightarrow 0$, then $X_k\rightarrow \frac{k}{\lambda}$. It is then easy to establish (see Exercise~\ref{exercise8}) that  
$$P(T<y)=\int_{-\infty}^\infty F\Big(x+\frac{y-1/\lambda}{s}\Big)f(x)dx.$$
 This can be rewritten as 
$$ P\Big[\frac{1}{s}\Big(T(\lambda,s)-\frac{1}{\lambda}\Big)<y\Big] =\int_{-\infty}^{\infty} F(y+x)f(x)dx = \int_{-\infty}^{\infty} F(y-x)f(x)dx.$$
The last equality is justified by the fact that $f$ is symmetric, thus $f(x)=f(-x)$. The integral on the right hand side of Formula (\ref{eq:et2}) represents the \textcolor{index}{self-convolution}\index{convolution of distributions}
[\href{https://en.wikipedia.org/wiki/Convolution_of_probability_distributions}{Wiki}] of $F$. \qed
\end{proof}

%---------------------------------------------------------------------------
\subsection{Convergence to the Poisson Process}\label{convpp}

This theorem establishes, under mild conditions, the convergence to a \textcolor{index}{Poisson process}\index{point process!Poisson} when $s\rightarrow\infty$.

\begin{theorem}
\label{sums2}
If the distribution $F$ has a density $f$, continuous almost everywhere, then the Poisson-binomial process converges to a stationary Poisson point process of intensity $\lambda^d$, as $s\rightarrow\infty$. Here $d$ is the dimension of the \gls{gls:state1}\index{state space}.
\end{theorem}
\begin{proof}
$ $ \newline \
I proceed in two steps, to prove the result when $d=1$.\\ \\
{\em Step 1} \\ \\
From (\ref{eq:2}), we have
$p_k(x,y) =\int_a^b f(u)du,$
where $f$ (the density) is the derivative of $F$,  $b=\frac{1}{\lambda s}(\lambda (x+y)-k)$, and $a=\frac{1}{\lambda s}(\lambda x-k)$. This integral has interval length $b-a=\frac{y}{s}$ and midpoint $\frac{1}{2}(a+b)=\frac{1}{2s}(2x+y) - \frac{k}{\lambda s}$. In particular, 
$$p_k(x,y)\sim \frac{y}{s}f\Big(\frac{2x+y}{2s}-\frac{k}{\lambda s}\Big) \mbox{ as } s\rightarrow\infty,$$
$$J_n \equiv \sum_{k=-n}^{n} p_k(x,y)\sim \int_{-n}^{n}p_\nu(x,y) d\nu=
\frac{y}{s}\int_{-n}^n f\Big(\frac{2x+y}{2s}-\frac{\nu}{\lambda s}\Big)d\nu.$$
With the change of variable $\tau=-\nu/(\lambda s)$, we obtain
$$J_n \sim \frac{y}{s}\cdot \Big[\lambda s\int_{-n/(\lambda s)}^{n/(\lambda s)} f\Big(\frac{2x+y}{2s}+\tau\Big)d\tau\Big]=\lambda y\int_{-n/(\lambda s)}^{n/(\lambda s)} f\Big(\frac{2x+y}{2s}+\tau\Big)d\tau.$$
Here $\lambda$ is fixed. When $n\rightarrow\infty$, $s\rightarrow \infty$ and $n/s\rightarrow \infty$ (say $s\sim\sqrt{n}$ or $s\sim n/(\log n)$, we have
$$J_n\rightarrow \lambda y\int_{-\infty}^\infty f\Big(\frac{2x+y}{2s}+\tau\Big)d\tau =\lambda y,$$
because $f$ is a density and thus integrates to one. \\ \\
{\em Step 2} \\ \\
Regardless of $k$, we have $p_k(x,y)\rightarrow 0$ as $s\rightarrow\infty$. So the denominator $1-p_0(x,y)$ in (\ref{eq:1}) can be ignored when $s=\infty$. We also have:
\begin{align}
\log\Big[\prod_{k\in\mathbb{Z}} \Big(1-p_k(x,y)\Big)\Big] & =\sum_{k=-\infty}^\infty\log\Big(1-p_k(x,y)\Big) \nonumber \\ 
 & \sim -\sum_{k=-\infty}^\infty p_k(x,y) \nonumber \\
 & = -J_\infty = -\lambda y. \nonumber 
\end{align}
Thus,
$$\prod_{k\in\mathbb{Z}} \Big(1-p_k(x,y)\Big)\sim \exp(-\lambda y) \mbox{ as } s\rightarrow \infty.$$
This product does not (at the limit) depend on $x$. Finally, we get
$$P(T>y)\sim \exp(-\lambda y)\int_{-\infty}^\infty \frac{f(x)}{1-p_0(x,y)}dx\sim \int_{-\infty}^\infty f(x) dx = \exp(-\lambda y),$$
as $f$ is a density and thus integrates to one. So, $T$ has an exponential distribution of parameter $\lambda$ as $s\rightarrow\infty$. This implies that the limiting point process must be Poisson of intensity $\lambda$. \qed
\end{proof}

The takeaway from the proof of Theorem \ref{sums2} (see bottom of Step 1) is that to simulate a realistic Poisson process as a limit of a Poisson-binomial process (pretty much regardless of $F$), you generate your $2n+1$ points ($k$ between $-n$ and $n$), you choose a large $n$ and a large $s$, but $s$ must be an order of magnitude smaller than $n$, to make 
\gls{gls:be}\index{boundary effect} [\href{https://en.wikipedia.org/wiki/Boundary_problem_(spatial_analysis)}{Wiki}] \index{boundary effect} negligible. For instance, $s=\sqrt{n}$ or $s=\frac{n}{\log n}$ will do. 

Theorem \ref{sums2} generalizes to higher dimensions. It is somewhat similar to 
the \textcolor{index}{Central Limit Theorem} [\href{https://en.wikipedia.org/wiki/Central_limit_theorem}{Wiki}] (CLT)\index{central limit theorem}, 
in the sense that it works regardless of the continuous distribution $F$. Even if the 
 \gls{gls:index1}\index{index!index space} $\mathbb{Z}$ (the support domain for the \textcolor{index}{index}\index{index} $k$) was relatively random, it would still work. What is remarkable is that even if $F$ is a \textcolor{index}{Cauchy distribution}\index{distribution!Cauchy}, known to have no expectation nor variance, it still works, and convergence to the Poisson process is even faster than if $F$ was uniform. This is because the Cauchy distribution, with its thick tail, does a great job at mixing the points of the process. To the contrary, the standard CLT fails with a Cauchy distribution, as the sum of iid Cauchy random variables always has a Cauchy distribution, thus never converging to a Gaussian distribution. This is because the Cauchy distribution, like the Gaussian one, belongs to a family of 
\textcolor{index}{stable distributions}\index{stable distribution} [\href{https://en.wikipedia.org/wiki/Stable_distribution}{Wiki}]. 

In our case, convergence to a Poisson process is quite fast, with $s=40$, assuming $\lambda=1$, yielding an excellent approximation regardless of $F$, see Table~\ref{tab124}. Consequently, the interest here is in small values of $s$. There might be a different way to prove Theorem~\ref{sums2}, using \textcolor{index}{Le Cam's inequality}\index{Le Cam's theorem} \cite{lecam} applied to the point count distribution. It would amount to proving that as $n\rightarrow\infty$, regardless of $B$, the Poisson-binomial distribution of $N(B)$ tends to a Poisson distribution of expectation $\lambda^d\mu(B)$, where $d$ is the dimension, and $\mu(B)$ is the area of $B$ in two dimensions, or the length of the interval $B$ in one dimension. See Theorem~\ref{prop1} for such a proof, in a similar context. 

%--------------------------------------------------------
\subsection{The Inverse or Hidden Model}\label{hm} 

Now I turn to what is usually referred to as the \textcolor{index}{hidden model}, \index{hidden model} as in hidden Markov models [\href{https://en.wikipedia.org/wiki/Hidden_Markov_model}{Wiki}]. Given an observed point $x$ (a point of the process), what is the probabability that $x=X_k$, for a specific $k$? In other words, can you retrieve the unknown \textcolor{index}{index}\index{index} $k$ attached to an observed point $x$? The related discrete random variable, indicating the \textcolor{index}{index}\index{index} $k$ attached to $x$, is denoted as $L(x)$, and takes on integer values in $\mathbb{Z}$. It is assumed that $\lambda,s$ are known or estimated. Another random variable of interest, denoted as $K$ and also taking on integer values (positive or negative) is is the index of the point closest to the point $X_0$, on the right-hand side on the real axis. Related material in the 
literature includes ``Recovering the lattice from its random perturbations" by Yakir \cite{oren} (2020) available online
\href{https://arxiv.org/abs/2002.01508v2}{here} and 
``Cloaking the Underlying Long-Range Order of Randomly Perturbed Lattices" by Klatt \cite{glattip}, available
 \href{https://arxiv.org/abs/2001.08161}{here}. See also how I use the function $L$ in an application to generate locally random permutations, in Section~\ref{permut}. Now I can state two new theorems. 

\begin{theorem}
\label{sums4}
Let us assume that $F_s$ has a derivative $f_s$ (the density), continuous and strictly positive everywhere. For any $h\in\mathbb{Z}$, we have
$$
P(L(x)=k) =C\cdot f_s(x-t_k), \mbox{ with } C=\sum_{h\in\mathbb{Z}} f_s(x-t_h).$$
 \end{theorem}
\begin{proof}
$ $ \newline 
Let $B_\epsilon(x)=[x-\epsilon,x+\epsilon]$. We have
\begin{equation} 
\lim_{\epsilon\rightarrow 0}\frac{P(X_k\in B_\epsilon(x))}{P(X_h\in B_\epsilon(x))} = \frac{f_s(x-t_k)}{f_s(x-t_h)}.\nonumber
\end{equation}
Thus $P(L(x)=k)\propto  f_s(x-t_k)$, and the proportionality constant is such that the sum over all $k\in\mathbb{Z}$, must be one. 
\qed
\end{proof}

\begin{theorem}
\label{sums5}
The interarrival time $T$ and $K$ are connected by the following formula:
$$P(T(\lambda,s)<y)=\sum_{k\neq 0} P(K=k)\int_{-\infty}^\infty F_s\Big(x+y-\frac{k}{\lambda}\Big)f_s(x)dx.$$
 \end{theorem}
\begin{proof}
$ $ \newline 
We have: $K=k$ if and only if $k$ is the smallest index such that $X_k>X_0$. Thus,
\begin{align}
P(T(\lambda,s)<y) & = P(X_K-X_0<y)\nonumber\\
  &=\sum_{k\neq 0} P(K=k)P(X_k-X_0<y) \nonumber \\
  &= \sum_{k\neq 0} P(K=k)\int_{-\infty}^\infty F_s\Big(x+y-\frac{k}{\lambda}\Big)f_s(x)dx. \nonumber
\end{align}
The last integral is the result of the convolution between the random variables $X_k$ and $-X_0$.
\qed
\end{proof}

Theorem \ref{sums5} provides us with a way to compute the $P(K=k)$'s. You need to solve a linear system with an infinite number of variables and an infinite number of equations. The unknowns are the $P(K=k)$'s. In practice, especially if $\lambda=1$, you can just reduce it to $-n\leq k \leq n$, with $k\neq 0$ and $n=10$, as $P(K=k)$ quickly decays to zero when $k$ becomes large in absolute value. Pick up a different $y$ in the integral, for each of the $2n$ equations, to get an invertible system. 

The distribution of the interarrival times is combinatorial in nature, and in principle you could use the theory of \textcolor{index}{order statistics} [\href{https://en.wikipedia.org/wiki/Order_statistic}{Wiki}]\index{order statistics}
to get the exact distribution. References on this topic includes \cite{order2,order3}. When the random variables are independently but not identically distributed, 
one may use the Bapat-Beg theorem [\href{https://bit.ly/3oOqd7j}{Wiki}] to find the joint distribution of the order statistics of the sequence $(X_k)$, and from there, obtain the theoretical distribution of $T$. This approach is difficult, and not recommended. Simulations are the preferred option.

\subsection{Special Cases with Exact Formula}

Again, let's focus on the counting random variable $N(B)$, with $B=[a, b]$ an interval with $a<b$. An exact, closed-form formula for the expectation and variance, is available in a few cases, in particular if $F$ is a uniform or Laplace distribution. The formulas are complicated, and difficult to obain as they require laborious though trivial computations that could easily be automated with AI. Also, they are of not of much use since simulations are \textcolor{index}{numerically stable}\index{numerical stability} and do well in this context, with few iterations, to estimate these quantities. So, I will only mention one of these exact formulas in Theorem \ref{sums6}. It is probably the most interesting one. It is in agreement with other results obtained previously. 

\begin{theorem}
\label{sums6}
If $F_s$ is \textcolor{index}{uniform} \index{distribution!uniform} on $[-s, s]$, $\lambda=1$, and $B=[-s,s]$, then
$$\mbox{E}[N(B)]= -1+2\sum_{0\leq k\leq 2s}\Big(1-\frac{k}{2s}\Big)=-1+\frac{\lfloor 2s\rfloor(1+\lfloor 2s\rfloor)}{2s}. $$
The brackets represent the integer part function.
 \end{theorem}
\begin{proof}
$ $ \newline 
Let $\lambda=1, B=[a, b]$ and $p_k=F_s(b-k)-F_s(a-k)$. Here 
$$F_s(x-k)=\frac{1}{2}+\frac{x-k}{2s}, \mbox{ with } -s\leq x-k\leq s, \mbox{ and } s>0.$$
We have two cases, each with three sub-cases: \\ \\
If $b-a\leq 2s$ then 
\begin{itemize}
 \item If $a-s\leq k\leq b-s$ then $p_k=\frac{1}{2}-\frac{a-k}{2s}$.
 \item If $b-s\leq k \leq a+s$ then $p_k=\frac{b-a}{2s}$.
 \item If $a+s\leq k \leq b+s$ then $p_k=\frac{1}{2}+\frac{b-k}{2s}$.
\end{itemize}
If $b-a\geq 2s$ then 
\begin{itemize}
 \item If $a-s\leq k\leq a+s$ then $p_k=\frac{1}{2}-\frac{a-k}{2s}$.
 \item If $a+s\leq k \leq b-s$ then $p_k=1$.
 \item If $b-s\leq k \leq b+s$ then $p_k=\frac{1}{2}+\frac{b-k}{2s}$.
\end{itemize}
If $k\notin [a-s,b+s]$, then $p_k=0$. Let $B=[a,b]$. The above results can be used to compute (in closed form) the quantities
$$\mbox{E}[N(B)]=\sum_{k=-\infty}^\infty p_k, \quad
\mbox{Var}[N(B)]=\sum_{k=-\infty}^\infty p_k(1-p_k).$$
In particular, if $a=-s$ and $b=s$, there are some simplifications, and we obtain the result announced in the theorem.
\qed
\end{proof}

Note that if $s/2$ is an integer, the above result is compatible with Theorem \ref{eq:1} since $\mbox{E}[N(B)]=2s = b-a$. Also, as $s\rightarrow\infty$, $\mbox{E}[N(B)]\sim 2s=b-a$. In general though, $\mbox{E}[N(B)]$ is not an exact function of $\mu(B)=\lambda\cdot (b-a)$, confirming that the Poisson-binomial process is different from a Poisson process, and very much so in particular if $F$ is the uniform distribution.

If $F$ is the \textcolor{index}{Laplace distribution},\index{distribution!Laplace} an exact, closed-form formula can also be obtained for $\mbox{E}[N(B)]$ and $\mbox{Var}[N(B)]$, and for higher moments. 
See Exercise \ref{exercise1} in Section \ref{ex:1}.

\subsection{Fundamental Theorem of Statistics}

Many of the distributions used in this textbook for simulation purposes can easily be sampled using \textcolor{index}{inverse transform sampling} [\href{https://en.wikipedia.org/wiki/Inverse_transform_sampling}{Wiki}]\index{inverse transform sampling}. The fact that -- as in Table \ref{tab124} -- estimated quantities such as mean, variance or quantiles, converge to the desired theoretical values is due to the convergence of the  \gls{gls:empdis} [\href{https://en.wikipedia.org/wiki/Empirical_distribution_function}{Wiki}] \index{empirical distribution}
\index{distribution!empirical} (measured on the observations) to its theoretical limit (associated to the model). This can be re-stated as follows, and was used in particular to compute the moment generating function of the generalized logistic distribution in Section \ref{sec:gld}.

\begin{theorem}
\label{sums7}
Let $Z$ be a random variable with cumulative distribution function $F(z)=P(Z<z)$ with $z\in\mathbb{R}$, and quantile function $Q(u)=F^{-1}(u)$, with $0\leq u \leq 1$. Let $g$ be a measurable function. Then we have
$$E[g(Z)] = \int_0^1 g(Q(u))du=\int_{-\infty}^{\infty}g(z)dF(z)=\int_{-\infty}^{\infty}g(z)f(z)dz.$$
 \end{theorem}
\begin{proof}
$ $ \newline
Use the change of variable $u=F(z)$ in the leftmost integral. Then $Q(u)$ becomes $Q(F(z))=F^{-1}(F(z))=z$, $du$ becomes $dF(z)=f(z)dz$, and the interval of integration changes from $[0, 1]$ to the entire real line. \qed
\end{proof}
Here $f$ is the density attached to $F$, assuming it exists. The righmost equality is well known, but the leftmost is not. Surprisingly, this unnamed, little known theorem, rarely if ever mentioned, has a crucial role. It is routinely and unconsciously used by all machine learning practioners almost on a daily basis, at least in the version that applies to empirical, observation-based statistics. The above version applies to theoretical (mathematical) statistics. I suggest to call it the \textcolor{index}{quantile theorem}\index{quantile!fundamental theorem}. 

For instance, the moment generating function of $Z$ is defined as $\mbox{E}[\exp(tZ)]$. It can be computed via the \gls{gls:quant} $Q$, \index{quantile!quantile function} using $g(z)=\exp(tz)$, see Formula (\ref{eq:qt}) for the generalized logistic distribution. See also Exercises \ref{exercise6} 
and \ref{exercise7} in Section \ref{ex:1}, for a different application.

\pagebreak

%=====================================
\section{Exercises, with Solutions}\label{ex:1}
%=====================================

While the purpose of these exercises is to  strengthen the learning experience and to generate out-of-the-box thinking, perhaps even more importantly, they provide additional methodological and technical material, complementing and extending the main text. 

Starred exercises are more difficult. Several of the problems require only simulations, statistical analysis, and testing hypotheses on a computer. They are marked as [S] and should help you hone your machine learning and computing skills; they may not be easier or less challenging than the mathematical problems. Exercises involving mathematics or probability theory are marked as [M], while those combining both simulations and mathematics are marked as [MS]. Solutions or hints are provided for each problem.  

\subsection{Full List}\label{se12}

Table~\ref{tabex21} provides a listing all the exercises. To access any exercise, click on its red number, to the left. The NN abbreviation stands for ``nearest neighbors".

\begin{table}[H]
%\noindent\makebox[\linewidth]{\rule{1.0\linewidth}{0.4pt}}
    \begin{minipage}{.5\linewidth}
  %    \caption{}
      \centering
        \begin{tabular}{ll}
   \ref{exercise1} & Point count, Laplace distribution   \\
   \ref{exercise3} * & Convergence to Poisson process   \\
   \ref{exercise6} * & Limit of generalized logistic distribution\\ 
   \ref{exercise7} & Small paradox \\
   \ref{exercise8} & Exact distribution of interarrival times \\
   \ref{exercise13} * & Retrieving $F$ from interarrival times\\
   \ref{exercise14} * & Poisson limit of Poisson-binomial distribution\\
   \ref{exercise2} &  A few simple theorems  \\
   \ref{exercise4}  & Testing stationarity, independent increments  \\
   \ref{exercise43}  & Interdependencies in point counts \\
   \ref{exercise9} & Boundary effect \\
   \ref{exerciseb1} & A curious, Poisson-like point process \\
   \ref{sphere90} * & Poisson-binomial process on the sphere \\
   \ref{exercisec1} & Taxonomy of point processes 
        \end{tabular}
    \end{minipage}%
    \begin{minipage}{.5\linewidth}
      \centering
   %     \caption{}
        \begin{tabular}{ll}
   \ref{exercise14b} & Distribution of NN distances \\
   \ref{exercise14c} & Cell networks: coverage problem \\
   \ref{exercise14d} & Optimum circle covering of the plane \\
   \ref{exercise14e} & Interlaced lattices, lattice mixtures, NN\\
   \ref{exercise14f} * & Lattice topology and algebra\\
   \ref{exercise14g} ** & NN graph: size of connected components\\
   \ref{cliquebc} & NN graph: maximum clique problem\\
   \ref{exercise5} & Computing moments using the CDF\\
   \ref{exercise10} & Simulations: generalized logistic distribution \\
   \ref{exercise11} & Riemann Hypothesis \\
   \ref{exercise12} * & Convergence acceleration of math series\\
   \ref{fastfilter} & Fast image filtering algorithm  \\
   \ref{exhotelling} ** & Confidence regions: theory, computations \\
   \ref{exercise92} * &  Minimum set covering 90\% of a distribution
        \end{tabular}
    \end{minipage} 
%\noindent\makebox[\linewidth]{\rule{1.0\linewidth}{0.4pt}}
%\rule{8cm}{0.4pt}
    \caption{\label{tabex21}List of exercises}

\end{table}


%------------------------------------------


\subsection{Probability Distributions, Limits and Convergence}

The focus here is on the distribution $F$ including some of its  limiting cases, the distribution of arrival times, and convergence to the Poisson process. The Laplace, generalized logistic, Borel, and Poisson-binomial distributions are investigated.

\begin{Exercise}\label{exercise1}
{\bf [M]} {\bf Point count, Laplace distribution}. If $F$ is a \textcolor{index}{Laplace distribution} \index{distribution!Laplace} and $\lambda=1$, find $E[N(B)]$, where $B=[a,b]$ is an interval with $\lfloor a \rfloor \leq \lfloor b \rfloor < \lfloor a \rfloor +1$. Here the brackets represent the integer part function, and $F_s(x)=F(x/s)$. See Theorem~\ref{sums6}, solving the same problem with a uniform rather than Laplace distribution. \vspace{1ex}   \\
{\bf Solution} \vspace{1ex}   \\
Let  $p_k=F_s(b-k)-F_s(a-k)$ with $s>0$, and let  $\mbox{sgn}$ stands for the sign function, with $\mbox{sgn}(0)=0$. Here 
$$F_s(x-k)=\frac{1}{2}+\frac{1}{2}\mbox{sgn}(x-k)\Big[1-\exp\Big(-\frac{1}{s}\cdot|x-k|\Big)\Big] $$
We have three cases:
\begin{itemize}
 \item If $k\leq a < b$ then $p_k=\frac{1}{2}\Big[\exp(-(a-k)/s) - \exp(-(b-k)/s)\Big] $
 \item If $a\leq k \leq b$ then $p_k=1-\frac{1}{2}\Big[\exp(-(b-k)/s) +   \exp((a-k)/s)\Big] $
 \item If $a< b\leq k$ then $p_k=\frac{1}{2}\Big[\exp((b-k)/s) -   \exp((a-k)/s)\Big] $
\end{itemize}
If $\lfloor a \rfloor \leq \lfloor b \rfloor < \lfloor a \rfloor +1$, then the second case is empty and  
$\lfloor a\rfloor = \lfloor b\rfloor$. As a result, the computations simplify to
\begin{align}
2\mbox{E}[N(B)]& =\alpha\sum_{k\leq a}\phi^k -\beta\sum_{k\leq a}\phi^k 
+ \frac{1}{\beta}\sum_{k\geq b} \Big(\frac{1}{\phi}\Big)^k- \frac{1}{\alpha}\sum_{k\geq b}\Big(\frac{1}{\phi}\Big)^k \nonumber \\
 & = (\alpha-\beta)\Big[\sum_{k\leq a}\phi^k  +  \frac{1}{\alpha\beta}   \sum_{k\geq b}\Big(\frac{1}{\phi}\Big)^k  \Big] \nonumber \\
 & =(\alpha-\beta)\cdot\frac{\phi}{\phi-1}\cdot\Big[  \phi^{\lfloor a \rfloor} +\frac{1}{\alpha\beta}\Big(\frac{1}{\phi}\Big)^{\lfloor a\rfloor+1} \Big ] \nonumber
\end{align}
where $\alpha= \exp(-a/s)\geq \beta= \exp(-b/s)$ and $\phi=\exp(1/s)$. We are dealing with geometric series, which are easily summable. The last equality is due to the fact that  $\lfloor a\rfloor = \lfloor b \rfloor$. Note that $b$ can not be an integer in this case, so a sum with integer index $k\geq b$ actually starts at $k=\lfloor b \rfloor +1 = \lfloor a \rfloor + 1$.
Also, when combining the various sums, make sure that their indices don't overlap, otherwise double counting will occur. This is not happening here.
\end{Exercise}

\begin{Exercise}\label{exercise3}
{\bf [M*]} {\bf Convergence to Poisson process}. Prove Theorem~\ref{sums2} in Section~\ref{sthm}.\vspace{1ex} \\
{\bf Hint} \vspace{1ex} \\
It is about the same level of difficulty as proving the 
\textcolor{index}{Central Limit Theorem} [\href{https://en.wikipedia.org/wiki/Central_limit_theorem}{Wiki}]\index{central limit theorem} 
Central Limit Theorem (CLT). If understanding a proof of the CLT is beyond your mathematical level, you will find this exercise really difficult. However, you can focus on just proving that the interarrival time $T(\lambda,s)$ follows an exponential distribution, which in turn characterizes the Poisson process. This is not easy either. If you look at my proof, you will notice that a some point, I approximate a sum $\sum_{-n}^n$ by an integral $\int_{-n}^n$ as $n\rightarrow\infty$, implicitly using a version of the Euler-Maclaurin summation formula [\href{https://bit.ly/3m7FxKA}{Wiki}] in its simplest form. 
\end{Exercise}


\begin{Exercise} \label{exercise6} {\bf [M*]} 
{\bf Limit of generalized logistic distribution}. Compute the expectation of the \textcolor{index}{generalized logistic distribution}\index{distribution!generalized logistic}, when $\alpha=1$ and $1/\beta$ is a positive integer. If $\alpha=1$ and $\tau=e^{1/\beta}$, prove that $\frac{1}{\beta\rho}(\mbox{E}[Z]-\mu)\rightarrow-\pi^2/6$ as  $\beta\rightarrow 0$. See Formula (\ref{eq:gld1}) for the cumulative distribution function. \vspace{1ex} \\
{\bf Solution} Instead of using Formula (\ref{eq:moment}) to compute the expectation, I use Formula (\ref{eq:moment2}), with $r=1$. Using Formula (\ref{eq:qq}), the expectation can be rewritten as
$$\mbox{E}[Z]=\mu+\rho\int_{0}^1 \log \Big(\frac{\tau u^{m}}{1-u^{m}}\Big)du = \mu - \rho\Big[m\log \tau + \int_0^1 \log(1-u^m)du\Big],$$
where $m=1/\beta$ is an integer. Also, $1-u^m$ is a polynomial of degree $m$, and its roots are the $m$-th roots of $1$ in the complex plane (see \href{https://en.wikipedia.org/wiki/Root_of_unity}{here}), that is
$$(1-u^m)=-\prod_{k=0}^{m-1}\Big[u-\exp\Big(\frac{2 k\pi i}{m}\Big)\Big].$$
Thus,
$$\log(1-u^m)=\log(-1)+\sum_{k=0}^{m-1}\log \Big[u-\exp\Big(\frac{2 k\pi i}{m}\Big)\Big].$$
Since $\log(-1)=\pi i$ and $\int \log (u - c) du = (u - c)\log(u - c) - u$ if $c$ is a constant (whether complex or real), we finally have
$$\mbox{E}[Z]= \mu - \rho m\log \tau -\rho\pi i - \rho\sum_{k=0}^{m-1} \Big[(1-c_{k,m})\log(1-c_{k,m})-1+c_{k,m}\log(-c_{k,m})\Big] $$
where $c_{k,m} =\exp(2 k\pi i/m)$. This involves computing complex logarithms  [\href{https://en.wikipedia.org/wiki/Complex_logarithm}{Wiki}]. When combining the real and imaginary parts from all the terms, only real numbers are left. This tedious computation is best achieved using some automated tool. See the result for $\mu=0,\tau=1,\rho=1$ and $m=8$, 
using the online version of Mathematica, \href{https://bit.ly/3oZzA49}{here}. In this case, the final result is 
$$  -4\log 2 - (\pi/2) \cot(\pi/8) - \sqrt{2} \log(\cot(\pi/8)).$$
Assuming $\alpha=1$ and $\beta=1/m$, when $m\rightarrow\infty$, we have the asymptotic expansion
\begin{equation}
\mbox{E}[Z]= \mu+\rho\Big[\log\tau -m - \frac{\xi}{m}  + o\Big(\frac{1}{m}\Big)\Big], \mbox{ with } \xi = \lim_{m\rightarrow \infty} \Big(m\int_0^1 \log(1-u^m)du\Big)=-\frac{\pi^2}{6}. \label{eq:limtau}
\end{equation}
The value of $\xi$ was obtained by replacing $\log(1-u^m)$ by its Taylor series in the integral, then integrating term by term, and finally taking the limit as $m\rightarrow \infty$. It can also be obtained with the change of variable $v=u^m$ in the integral.  Let $\alpha=1$ and $\tau=e^{1/\beta}=e^m$. From (\ref{eq:limtau}), we get: $\mbox{E}[Z]\rightarrow \mu$ and $\frac{1}{\beta\rho}(\mbox{E}[Z]-\mu)\rightarrow \pi^2/6$ as  $\beta\rightarrow 0$. 
\end{Exercise}

\begin{Exercise}\label{exercise7}{\bf [MS]} 
 {\bf Small paradox}. Let $Z_\beta$ be a random variable with generalized logistic distribution, with $\mu=0$, $\rho=\alpha=1$ and $\tau=e^{1/\beta}$.
Using simulations based on Formula (\ref{eq:qq}) for the \gls{gls:quant}\index{quantile!quantile function} $Q(u)$, with $u$ a uniform deviate on $[0,1]$, try to guess the expectation and variance of the limit random variable $Z_*=\lim_{\beta\rightarrow 0}\beta^{-1}Z_{\beta}$. The exact values are respectively zero and one. Using numerical approximations, show that $\lim_{\beta\rightarrow 0} \beta^{-1}\mbox{E}[\mbox{Z}_{\beta}]\approx 1.645$. All of this can be done in Excel using the \texttt{rand} function. Then obtain the exact values for the three quantities in question. The last one, equal to $\pi^2/6$, was computed in Exercise \ref{exercise6}. Thus, in this case, the expectation and limit operators can not be swapped 
(one yields the answer $0$, the other one yields $\frac{\pi^2}{6}$). This is because the limiting distribution  $P(Z_*<z)$ 
is \textcolor{index}{truncated}\index{truncated distribution}\index{distribution!truncated}, as shown in the solution below. \vspace{1ex} \\
{\bf Solution} Despite the appearance, this is an easy exercise. As in Exercise \ref{exercise6}, let  $m=1/\beta$ and $m\rightarrow \infty$. The problem here, when approximating 
$$\beta^{-1}\mbox{E}[Z_{\beta}]=m\int_0^1 Q(u)du =m\int_0^1\Big[\log(\tau u^m) - \log(1-u^m)\Big]du
=-m\int_0^1\log(1-u^m)du,$$
is that the computation of $\log(1-u^m)$ is \textcolor{index}{numerically unstable}\index{numerical stability} \cite{num} 
%\href{https://amzn.to/30H6xsV}{numerically unstable}
when $0<u<1$ and $m$ is large, resulting in erroneous results, whether you do it in Excel or Python. The problem is said to be ill-conditioned. To avoid this problem, use the change of variable $v=u^m$, yielding
$$-m\int_0^1  \log(1-u^m) du= -\int_0^1\frac{\log(1-v)}{v^{1-1/m}}dv \rightarrow -\int_0^1 v^{-1}\log(1-v)dv = \frac{\pi^2}{6} \mbox{ as } m\rightarrow\infty.$$
Base your Excel computations on the last integral, using a sum to approximate it. Now it works! \vspace{1ex} \\
The fact that $\mbox{E}[Z_*]=0$ and $\mbox{Var}[Z_*]=1$ will be apparent from your simulations involving the quantile function $Q(u)$. However, to prove it rigorously, rather than using $Q$, it is easier to work with the CDF $P(Z_\beta<z)$, with $\mu=0,\rho=\alpha=1,\beta=1/m$ and $\tau=e^m$, using Formula (\ref{eq:gld1}):
$$P(Z_*<z)=\lim_{m\rightarrow\infty}P(Z_{1/m}<mz)=\lim_{m\rightarrow\infty}\frac{1}{(1+\tau^m\exp(-mz))^m}
=\lim_{m\rightarrow\infty}\frac{1}{(1+\exp[m(1-z)])^m}.$$
If $z>1$, the above limit is one, otherwise it is equal to $\exp[-(1-z)]$. The density $f_{Z_*}$ (the derivative of the CDF) is also equal to 
$f_{Z_*}(z)=\exp[-(1-z)]$ with support domain $z<1$. Thus we have $\mbox{E}[Z_*]=\int_{-\infty}^1 z f_{Z_*}(z) dz =0$ and 
$\mbox{Var}[Z_*]=\mbox{E}[Z_*^2]=\int_{-\infty}^1 z^2  f_{Z_*}(z)dz= 1$. 
\end{Exercise}

\begin{Exercise}\label{exercise8}
{\bf [M]}  {\bf Exact distribution of interarrival times}. Find the distribution of the \textcolor{index}{interarrival times}\index{interarrival times} $T(\lambda,s)$ if all the points are ordered, that is, if $X_k<X_{k+1}$ for all $k\in\mathbb{Z}$. This
happens when $s$ is small enough, and the tail of $F$ is not too thick.\vspace{1ex} \\
{\bf Solution} \vspace{1ex}   \\ 
We have $X_k = \frac{k}{\lambda} + s Z_k$ and $X_{k+1} = \frac{k+1}{\lambda} + s Z_{k+1}$ where $Z_k,Z_{k+1}$ are two independent random variables of distribrution $F$. Thus the interarrival time $X_{k+1}-X_{k}$ has the same distribution as $T=\frac{1}{\lambda}+ s(Z_{k+1}-Z_k)$ and does not depend on $k$. You may as well use $k=0$ for its computation. The result is
$$P(T<y)=P\Big(\frac{1}{\lambda}+ s(Z_{1}-Z_0)<y\Big)=P\Big(Z_{1}-Z_0<\frac{y-1/\lambda}{s}\Big)=\int_{-\infty}^\infty F\Big(x+\frac{y-1/\lambda}{s}\Big)f(x)dx$$
where $f$ is the density attached to $F$.  Since $f$ is symmetric and centered at the origin, the distribution $P(T<y)$ is the \textcolor{index}{self-convolution} of $F$ 
[\href{https://en.wikipedia.org/wiki/Convolution_of_probability_distributions}{Wiki}]\index{convolution of distributions}, also denoted as $F * F$.   \vspace{1ex} \\
Examples: 
\begin{itemize}
\item If $F$ is normal with zero mean and unit variance, then $T$ is almost normal, with mean $1/\lambda$ and variance $2s$, assuming $s$ is small compared to $1/\lambda$. But $T$'s distribution can not be exactly normal, because in this case, the $X_k$'s can not all be {\em perfectly} naturally ordered unless $s=0$ (then $X_k=k/\lambda$).  
\item If $F$ is uniform on $[-1,1]$ then $T$ has a symmetric \textcolor{index}{triangular distribution}\index{distribution!triangular} [\href{https://en.wikipedia.org/wiki/Triangular_distribution}{Wiki}] of mean $1/\lambda$ and support domain $[\frac{1}{\lambda}-2s, \frac{1}{\lambda}+2s]$. This is the exact solution if $0\leq s < \frac{1}{2\lambda}$. In this case, the $X_k$'s are all naturally ordered.
\end{itemize}
For a more formal result, see Theorem~\ref{et2}. See also Exercise \ref{exercise13}.
\end{Exercise} 

\begin{Exercise}\label{exercise13}{\bf [M*]} 
{\bf Retrieving $F$ from the interarrival times distribution}. I assume here that $F$ has a density $f$. Given the limit distribution of the standardized interarrival times, the purpose is to retrieve the distribution of $F$. If you are familiar with the concept of 
\textcolor{index}{characteristic function}\index{characteristic function} [\href{https://en.wikipedia.org/wiki/Characteristic_function_(probability_theory)}{Wiki}], 
this exercise is easy. If not, you should first get familiar with this concept. Thus this exercise is marked as difficult. 

The \textcolor{index}{standardized interarrival times}\index{interarrival times!standardized}\index{standardized arrival times}
is defined as $\frac{1}{s}[T(\lambda,s)-\frac{1}{\lambda}]$ and has zero expectation by virtue of Theorem~\ref{et}. By virtue of Theorem~\ref{sums}, it can be rewritten as 
$\frac{1}{\lambda s}[T(1,\lambda s)-1]$. Its limit, as $s\rightarrow 0$, is denoted as $T^*$. One of the simplest cases, besides Gaussian and Cauchy, is the following: If $T^*$ has a standard 
\textcolor{index}{Laplace distribution}\index{distribution!Laplace}\index{Laplace distribution} [\href{https://en.wikipedia.org/wiki/Laplace_distribution}{Wiki}]
(that is, symmetric centered at zero and with variance $\frac{\pi^2}{3}$), show that $F$ is a 
\textcolor{index}{modified Bessel distribution}\index{distribution!modified Bessel}\index{Bessel function} of the second kind (see reference \cite{bessel}, available online 
\href{https://www.researchgate.net/publication/46553454_A_Modified_Bessel_Distribution_of_the_Second_Kind}{here}). Note that as a consequence of L'Hôpital's rule 
[\href{https://bit.ly/3sQXEbL}{Wiki}], $T^*$ is the derivative of $T(\lambda,s)$ with respect to $s$, evaluated at $s=0$. \vspace{1ex}   \\
{\bf Solution} \vspace{1ex}   \\
By virtue of Theorem~\ref{et2}, we have 
$$P(T^*<y)=\int_{-\infty}^{\infty} F(y-x)f(x)dx,$$
which is a \textcolor{index}{convolution}\index{convolution of distributions} of $F$ with itself.
Thus $T^*$ has the distribution of the sum of two independent random variables, say $Z_1,Z_2$, of distribution $F$. Its characteristic function is therefore
$$\mbox{E}[\exp(-it T^*)]=\frac{1}{1 + t^2} = \mbox{E}[\exp(-it Z_1)]\times \mbox{E}[\exp(-it Z_2)]=\Big(\mbox{E}[\exp(-it Z_1)]\Big)^2.$$
Thus $\mbox{E}[\exp(-it Z_1)]=(1 + t^2)^{-1/2}$. Taking the inverse \textcolor{index}{Fourier transform}\index{Fourier transform} to retrieve the density of $Z_1$, which is the density attached to $F$, one finds
$$f(x)=\frac{1}{2\pi}\int_{-\infty}^\infty \frac{\cos(tx)}{\sqrt{1+t^2}} dt = \frac{1}{\pi}K_0(x),$$
where $K_0$ is the modified \textcolor{index}{Bessel function}\index{Bessel function} of the second kind 
[\href{https://mathworld.wolfram.com/ModifiedBesselFunctionoftheSecondKind.html}{Wiki}]. 
More about the Laplace distribution and its generalization can be found in \cite{laplace}. The cases when $T^*$ is Gaussian or Cauchy are easy because these distributions belong to \textcolor{index}{stable families of distributions}\index{stable distribution}\index{distribution!stable distribution} 
[\href{https://en.wikipedia.org/wiki/Stable_distribution}{Wiki}]: 
in that case, $F$ is respectively Gaussian or Cauchy.
\end{Exercise}

\begin{Exercise}\label{exercise14}{\bf [M*]} 
{\bf Poisson limit of Poisson-binomial distribution}.  Theorem~\ref{prop1} shows that a particular case of the Poisson-binomial distribution converges to the Poisson distribution. In the proof, I established the values of $P(N=0), P(N=1)$ for the counting random variable $N$. I also stated (without proving it), that as $n\rightarrow\infty$ and $m/n\rightarrow\alpha$, we have $P(N=k)\rightarrow q_0\mu^k/k!$ for all positive integers $k$.  The purpose of this exercise is to prove this latter statement. 
This in turn completes the proof of Theorem~\ref{prop1}. The notations refer to the theorem in question.  In particular, $q_0=P(N=0)$ and $\mu=P(N=1)/P(N=0)$.
This exercise reveals  the true combinatorial nature of the Poisson-binomial distribution, in all its complexity. This is also related to \textcolor{index}{Le Cam's inequality}\index{Le Cam's theorem}. \vspace{1ex}   \\
{\bf Solution} \vspace{1ex}   \\
For $P(N=0)$ and $P(N=1)$, see proof of Theorem~\ref{prop1}. Let $p_k=1/(n+k)$ as in the proof of Theorem~\ref{prop1}. We have, with the convention that a sum or product such as
$\sum_{k_2\neq k_1}$ is over $k_2=1,\dots,k_1-1,k_1+1,\dots,m$:
\begin{align}
P(N=2)& =\sum_{k_1=1}^m p_{k_1} \sum_{k_2\neq k_1} p_{k_2}\prod_{k_1 \neq k\neq k_2} (1-p_k) \nonumber \\
&= \sum_{k_1=1}^m \sum_{k_2\neq k_1} \frac{p_{k_1}}{1-p_{k_1}}\frac{p_{k_2}}{1-p_{k_2}}\prod_{k=1}^m (1-p_k)\nonumber \\
&= q_0 \sum_{k_1=1}^m \sum_{k_2\neq k_1} \frac{p_{k_1}}{1-p_{k_1}}\frac{p_{k_2}}{1-p_{k_2}}. \label{dds}
\end{align}
Let $S$ denotes the double sum in Formula~(\ref{dds}). We have
\begin{align}
S & =\frac{1}{2}\sum_{k_1=1}^m \Big[ \Big(\sum_{k_2=1}^m \frac{p_{k_1}}{1-p_{k_1}}\frac{p_{k_2}}{1-p_{k_2}}\Big)-  \Big( \frac{p_{k_1}}{1-p_{k_1}}\Big)^2\Big]\label{ddd2}\\
 & = \frac{1}{2}\Big[\sum_{k_1=1}^m \sum_{k_2=1}^m \frac{p_{k_1}}{1-p_{k_1}}\frac{p_{k_2}}{1-p_{k_2}}\Big] - \frac{1}{2}\sum_{k_1=1}^m \Big( \frac{p_{k_1}}{1-p_{k_1}}\Big)^2 \nonumber \\
&  =\frac{1}{2} \Big(\sum_{k=1}^m \frac{p_{k}}{1-p_{k}}\Big)^2 - \frac{1}{2}\sum_{k=1}^m \Big( \frac{p_{k}}{1-p_{k}}\Big)^2 = \frac{\mu^2}{2}- \frac{1}{2}\sum_{k=1}^m \Big( \frac{p_{k}}{1-p_{k}}\Big)^2 \nonumber
\end{align}
with 
$$\mu = \sum_{k=1}^m \frac{p_{k}}{1-p_{k}}=\sum_{k=1}^m\frac{1}{n+k-1}
=\sum_{k=n}^{m-1}\frac{1}{k} \rightarrow \log\alpha \mbox{ as  } n\rightarrow\infty,  \frac{m}{n} \rightarrow \alpha >1,$$
and
$$ \sum_{k=1}^m \Big( \frac{p_{k}}{1-p_{k}}\Big)^2 = \sum_{k=1}^m \frac{1}{(n+k-1)^2}\rightarrow 0 \mbox{ as } n\rightarrow\infty.$$
Note that the fraction $\frac{1}{2}$ in Formula~(\ref{ddd2}) is required to eliminate double counting the products in the double summation. For $P(N=3)$, we have a triple summation over indices $k_1, k_2, k_3$,
and because there are $3!=6$ ways to re-arrange distinct $k_1,k_2,k_3$, the fraction $\frac{1}{2}=\frac{1}{2!}$ becomes $\frac{1}{3!}$; likewise, because of the triple product, $\mu^2$ becomes $\mu^3$. 

Now I proved that $P(N=2)=q_0\frac{\mu^2}{2!}$, and provided hints as to why $P(N=3)=q_0\frac{\mu^3}{3!}$. The general case if left to the reader.
\end{Exercise}


\subsection{Features of Poisson-binomial Processes}\label{fpbp}

In this section, I discuss properties of point processes, such as ergodicity, homogeneity, stationarity and independent increments. One exercise deals with boundary effects, a result of data censoring. A curious Poisson-like point process is also investigated.

\begin{Exercise}\label{exercise2}
{\bf [M]}  {\bf A few simple theorems}. Prove all theorems in Section~\ref{sthm}, except Theorem~\ref{sums2}.\vspace{1ex} \\
{\bf Hint} \vspace{1ex}   \\
Of course you can just look at the proof that I provided for each theorem. However, it is a much better learning experience to try to prove them on your own without reading my solution, and possibly to generalize the theorems, for instance from one dimension to any dimension, if applicable. Your proofs might even be shorter, more rigorous, complete or elegant than mines. In fact, mines are mostly sketch proofs. For each of the theorems in question, some key trick is required to make progress towards a short, easy but subtle proof. 
\end{Exercise} 


\begin{Exercise}\label{exercise4}
{\bf [S]}  
{\bf Ergodicity, independent increments}. Test on simulated data (for various realizations of Poisson-binomial processes) if and when the following assumptions are violated, depending on $s$, $F$ and other parameters.
\begin{itemize}
\item Independent increments: point counts in non-overlapping intervals are independent.
\item In two dimensions, zero correlation between the X and Y coordinates of a point.
\item Ergodicity for interarrival times, in one dimension.
\item Homogeneity: the point density is {\em statistically} the same anywhere on the plane or on the real line.
\item Anisotropy: the point density, even if non homogeneous, does not show directional trends.
\item Stationarity: the point count in $[a,b]$ is {\em statistically} the same as in $[a+c,b+c]$, regardless of $a,b,c$.
\item Aperiodicity: the point density on the real line does not exhibit periodic behavior.
\end{itemize}
To perform the simulations to test the various assumptions, you can use the source code in Section~\ref{s:codeperl}. \vspace{1ex} \\
\noindent {\bf Solution} \vspace{1ex} \\
Some of these assumptions (aperiodicity, stationarity) are violated when the \gls{gls:sf}\index{scaling factor} $s$ is close enough to zero. If $s$ is large (say $s=40,\lambda=1$), the process is not statistically different from a stationary Poisson point process, so no statistical test will be able to detect violations, even if present. That said, 
\gls{gls:ia}\index{interarrival times} exhibit \gls{gls:ergo}\index{ergodicity} and independence. For instance, while $T$ is defined as the distance between $X_0$ and its closest neighbor to the right, you can replace $X_0$ by $X_1$, $X_2$, or any $X_k$, and $T$'s distribution remains unchanged. There is also stationarity in the following sense, regardless of $s$: 
\glspl{gls:pc}\index{point count distribution} in $[a, b]$ and $[a+c, b+c]$ have the same distribution if $c$ is a multiple of $1/\lambda$. 

The largest departure from  \textcolor{index}{stationarity}\index{stationarity} occurs with small $s$, and using a uniform distribution for $F$. If $F$ is Cauchy, things look prettier (more stationary) as $F$ has a thick tail and does a better job at mixing the points. To illustrate the non-stationarity,  use $\lambda=1.4, s=0.15$ with the logistic distribution for $F$. 
Let $B_1=[0,0.8]$ and $B_2=[6,6.8]$. Then $\mbox{Var}[N(B_1)]\approx0.506 \neq \mbox{Var}[N(B_2)]\approx 0.312$. Now if you increase the scaling factor from
 $s=0.15$ to $s=0.6$, the process is almost stationary. In particular the two variances in question range from $0.878181$ to $0.878193$ depending on the interval. The same is true for other statistics. For all practical purposes, the distribution of $N([a,b])$ depends only on $b-a$ if $s\geq 0.6$. Statistical tests would not be able to detect the minuscule lack of stationarity.

I used the exact Formula~(\ref{eq:f2})
to compute point count variances. This formula is implemented in the source code in Section~\ref{enpi}. 
In Exercise~\ref{exercise43}, I prove non-independence for point counts over non-overlapping domains. 
\end{Exercise}

\begin{Exercise}\label{exercise43} {\bf [M]}  {\bf Joint distribution of point counts}. Let $B_1, B_2$ be non-overlapping domains. For Poisson point processes, the 
\glspl{gls:pc}\index{point count distribution} $N(B_1)$ and and $N(B_2)$ are independent random variables. Is this also true for Poisson-binomial processes? Consider the one-dimensional case, with $B_1=[a, b[$, $B_2=[b, c[$. \vspace{1ex} \\
\noindent {\bf Solution} \vspace{1ex} \\
The answer is negative, although the dependence is weak [\href{https://en.wikipedia.org/wiki/Weakly_dependent_random_variables}{Wiki}]. Let $B_{12}=B_1\cup B_2$, 
$q_1=P[N(B_1)=0]$, $q_2=P[N(B_2)=0]$ and $q_{12}=P[N(B_{12})=0]=P[N(B_1)=0, N(B_2)=0]$. It suffices to prove that in general,
$q_{12}\neq q_1 q_2$. 

Let $X_k$, $k\in\mathbb{Z}$ be the points of the Poisson-binomial process. According to Formula~(\ref{eq:f3}), we have:
\begin{align}
q_{12} & =\prod_{k=-\infty}^\infty  \Big[1- F\Big(\frac{c-k/\lambda}{s}\Big) + F\Big(\frac{a-k/\lambda}{s}\Big)\Big] \nonumber \\
q_1 & = \prod_{k=-\infty}^\infty  \Big[1- F\Big(\frac{b-k/\lambda}{s}\Big) + F\Big(\frac{a-k/\lambda}{s}\Big)\Big]\nonumber \\
q_2 & = \prod_{k=-\infty}^\infty  \Big[1- F\Big(\frac{c-k/\lambda}{s}\Big) + F\Big(\frac{b-k/\lambda}{s}\Big)\Big]\nonumber
\end{align}
There is no reason why we would have $q_{12}=q_1q_2$. For instance, if $F$ is the logistic distribution, $\lambda=1.4, s=0.29$,
$a=0, b=0.8$ and $c=1.6$, we have (approximately) $q_{1}=0.2329$, $q_2=0.2306$, $q_{12}=0.0177$, and $q_1q_2 = 0.0537$. 

However the dependence is weak. Also, we have asymptotic independence, with full independence when $s=\infty$, thanks to
Theorem~\ref{sums2}. Formula~(\ref{eq:f3}) is implemented in the source code, in Section~\ref{enpi}. See also Section~\ref{indep1}, featuring
 a test of independence for the point counts.
\end{Exercise}

\begin{Exercise}\label{exercise9}
{\bf [S]}  {\bf Boundary effect}. The purpose is to assess the impact of the \gls{gls:be}\index{boundary effect}, in one dimension. Assuming $\lambda=1$, use a small value of $n$, say $n=300$, to generate $2n+1$ points $X_k$, $k=-n,\dots,n$ of a Poisson-binomial process. Estimate $\mbox{E}[T]$, the expectation of the 
\gls{gls:ia}, using all the $2n+1$ points.  
Do the same, this time using $N=10^4$,  to generate $2N+1$ points $X_k$, $k=-N,\dots,N$ of the same Poisson-binomial process.
But only use the $2n+1$ innermost points (closest to zero) still with $n=300$,  in your 
estimation of $E[T]$. These $2n+1$ points won't be the same as in the first simulation. Also, some closest neighbors won't be among the $2n+1$ innermost points but instead, in the larger set of $2N+1$ points. Now your estimate takes into account nearest neighbors that were unobserved in the first simulation (called
 \textcolor{index}{censored data}\index{censored data}) because they were outside the boundary. Compare your two estimates of $\mbox{E}[T]$. The first one is slightly biased due to boundary effects, the latter one almost has no bias. 
Compare the impact of using a Cauchy versus a uniform distribution for $F$, by looking at the loss of accuracy when estimating $\mbox{E}[T]$ based on a single realization of the process.  \vspace{1ex}\\
{\bf Hint} \vspace{1ex}\\
Try a simulation with $s=0.5$, and one with $s=10$. A large $s$, a thick tail (Cauchy versus uniform $F$), or a small value of $n$, all magnify the boundary effect, resulting in loss of accuracy in the estimates. Source code to compute $\mbox{E}[T]$ can be found in Section~\ref{evm}.
\end{Exercise} 

\begin{Exercise}\label{exerciseb1}
{\bf [M]}  {\bf A curious, Poisson-like point process}. If we use a uniform distribution for $F$, and $s=\frac{1}{2\lambda}$, is the resulting process a stationary Poisson process? What if we use a \textcolor{index}{mixture}\index{point process!mixture} of $m$ such processes in equal proportions, called a \textcolor{index}{$m$-mixture}\index{$m$-mixture}? (see Exercises~\ref{exercise14e} and \ref{exercise14f}). Assume here that we work with 2-dimensional Poisson-binomial point processes.
 \vspace{1ex} \\
{\bf Solution} \vspace{1ex} \\
In this case, each point  $(X_h,Y_k)$ of the process is uniformly distributed on a square with sides of length $1/\lambda$ and centered at $(h/\lambda,k/\lambda)$.
The support domains of these uniform distributions form a 
\textcolor{index}{partition}\index{partition} [\href{https://en.wikipedia.org/wiki/Partition_of_a_set}{Wiki}] of $\mathbb{R}^2$: they don't overlap, and there is also no empty space left. So the points of the process are uniformly and independently 
distributed on each square $B$ of area $1/\lambda^2$. But there is only one point in any such $B$. 
The process is a Poisson-binomial point process of intensity $\lambda$ (by construction), but it can not be a standard Poisson process.  If we mix $m$ such
processes, the resulting process has a \gls{gls:pc}\index{point count distribution}\index{point process!point count distribution} $N(B)$ with identical \textcolor{index}{binomial distributions}\index{binomial distribution}\index{distribution!binomial} 
[\href{https://en.wikipedia.org/wiki/Binomial_distribution}{Wiki}] on any square $B$ of area $1/\lambda^2$,
 with $N(B)\in\{0,\dots,m\}$ and $\mbox{E}[N(B)]=1$.  It is is not a Poisson process either since $N(B)$ does not have a Poisson distribution, though it is getting close.
\end{Exercise}

\begin{Exercise}\label{sphere90}
{\bf [S*]}  {\bf Poisson-binomial process on the sphere}.  Build a Poisson-binomial process on the sphere. You can start with a circle first, a cube or a torus. Study its properties, such as the distribution of \textcolor{index}{nearest neighbor distances}\index{nearest neighbors!nearest neighbor distances} or the size of \glspl{gls:cc}\index{connected components}, via simulation. Note that in this case, the point process has a finite number of points. 
See also ``Nearest Neighbor and Contact Distance Distribution for Binomial Point Process on Spherical Surfaces" \cite{sphere}, avaiable 
online \href{https://arxiv.org/abs/2005.07330}{here}. \vspace{1ex} \\
{\bf Solution} \vspace{1ex} \\
The first step is to define a lattice on the sphere. One way to do it is to build an inscribed polyhedron inside the sphere [\href{https://mathworld.wolfram.com/Circumsphere.html}{Wiki}], and use its \textcolor{index}{vertices}\index{vertex (graph theory)} as the lattice locations in the \gls{gls:lattice1}\index{lattice!lattice space}. See \cite{physicj}, available online \href{https://iopscience.iop.org/article/10.1088/1742-6596/1581/1/012054}{here}. 
An easier way is as follows:  
\begin{itemize}
\item plot longitudes and latitudes at equally spaced angles,
\item the points where they intersect are the lattice locations, 
\item the angle between two successive parallels (latitude or longitudes) is the \gls{gls:intensity1}\index{intensity function}. 
\end{itemize}
The disadvantage of this method is that  it creates two poles, and the lattice locations are not evenly distributed on the sphere. The resulting process is not 
 \gls{gls:homo1}\index{homogeneity}. For a solution with evenly distributed lattice locations, see \href{https://www.maths.unsw.edu.au/about/distributing-points-sphere}{here}. 

Now around each lattice location, generate a random point on the surface of the sphere. The point is specified by two independent random variables: an angle $\theta$ uniformly distributed 
on $[0,2\pi]$, and a radius $R$ measuring the distance to the lattice location on the surface of the sphere. It makes sense to require $R\leq \pi\rho$, where $\rho$ is
the radius of the sphere. The \gls{gls:sf}\index{scaling factor} can be defined as $s=\mbox{E}[R]$. Note that there are no \glspl{gls:be}\index{boundary effect} here. The next step  is the create \textcolor{index}{clusters on the sphere}\index{cluster process!on the sphere}. See \cite{sphere343}, available online \href{https://bit.ly/3utjJOm}{here}. Also, one can  study the conditions to obtain convergence to
a stationary Poisson point process on the sphere.

Another possible generalization is \textcolor{index}{random lines}\index{random lines}. In two dimensions, a line is characterized by two quantities: 
its distance $R$ to the origin, and its orientation $\theta$. A similar methodology can be used to produce a Poisson-binomial line process, with the angle $\theta$ uniformly distributed on
$[0, 2\pi]$.  In this case, the lattice space could be $(\mathbb{Z}/\lambda) \times (\mathbb{Z}/\lambda)$, where $\lambda$ is the intensity. Also see ``Generating stratified random lines in a square" \cite{line12}, available online \href{https://jcgt.org/published/0006/02/03/}{here}.
This is a typical \textcolor{index}{stochastic geometry}\index{stochastic geometry} problem.

\end{Exercise}

\begin{Exercise}\label{exercisec1}
{\bf [S]}  {\bf Taxonomy of point processes}. The purpose of this exercise is to prove that each type of point process studied in details in this textbook, is unique. In other words, the overlap between the different classes of point processes is small, despite \gls{gls:mi} issues\index{identifiability}. Here, I ask you to verify, via examples, that \glspl{gls:mip}\index{$m$-interlacing} defined in Section~\ref{sm1} are different from \textcolor{index}{$m$-mixtures}\index{$m$-mixture}, stationary Poisson processes, Poisson-binomial point processes, and the radial cluster processes discussed in Section~\ref{s:clp}.
\vspace{1ex} \\
{\bf Solution} \vspace{1ex} \\
As usual, the differences are most striking when the \gls{gls:sf}\index{scaling factor} $s$ is very small. In that case, for $m$-interlacings, each lattice
location in the \gls{gls:lattice1}\index{lattice!lattice space} has exactly $m$ points of the process clustered around it. For Poisson-binomial and $m$-mixtures,
that number is one. For radial cluster processes (with a Poisson-binomial parent process), the number in question is random and depends on the location. 
For Poisson point processes (the limit of some of these processes when $s\rightarrow\infty$) the underlying lattice space becomes meaningless.
\end{Exercise}

\subsection{Lattice Networks, Covering Problems, and Nearest Neighbors}\label{lattnn}

In this section dealing with two-dimensional problems, various lattices underlying Poisson-binomial processes are explored, including lattice properties and the construction of hexagonal lattices using superimposed rectangular lattices. Another problem with applications to cellular networks is covering of the 
plane with circles, when the centers are distributed as a Poisson process, or on a fixed lattice.  Finally, nearest neighbor distances and 
connected components are analyzed, giving this section a ``graph theory" flavor.

\begin{Exercise}\label{exercise14b}{\bf [MS]} 
{\bf Distribution of nearest neighbor distances}.  In two dimensions, $T(\lambda,s)$ represents the distance between a point of the process and its \textcolor{index}{nearest neighbor}\index{nearest neighbors}. 
\begin{itemize}
\item Prove that when $s\rightarrow\infty$, the limiting distribution of $T$ is \textcolor{index}{Rayleigh}\index{distribution!Rayleigh}\index{Rayleigh distribution} [\href{https://en.wikipedia.org/wiki/Rayleigh_distribution}{Wiki}] of mean $\frac{1}{2\lambda}$. 
\item Show by simulations or logical arguments, that unlike in the one dimensional case (see Theorem~\ref{et}), $T$'s expectation depends on $s$. 
\item Also, show that depending on $F$, the maximum \textcolor{index}{nearest neighbor distance}\index{nearest neighbors!nearest neighbor distances}, computed over the infinitely many points of the process, can have a finite expectation. Is this true too when $s\rightarrow\infty$, that is, for stationary Poisson point processes? 
\item Finally, what is $T$'s distribution if $T$ is replaced by the distance between an arbitrary location in $\mathbb{R}^2$, and its closest neighbor among the points 
of the process? 
\end{itemize}
{\bf Solution} \nopagebreak \vspace{1ex}   \\ 
In two dimensions, the fact that $\mbox{E}[T(\lambda,s)]$ depends on $s$, is obvious: if $s=0$, it is equal to $\frac{1}{\lambda}$, and if $s=\infty$, it is equal to $\frac{1}{2\lambda}$. Between these two extremes, there is a continuum of values, of course depending on $s$. The maximum nearest neighbor distance (over all the infinitely many points) always has a finite expectation if $F$ is uniform, regardless of $s<\infty$. To the contrary, for a Poisson point process, the maximum is infinite, see \href{https://mathoverflow.net/questions/412891/maximum-nearest-neighbor-distance-for-a-poisson-point-process/412895#412895}{here}. \vspace{1ex} \\
Now let's prove that $T$ has a Rayleigh distribution when $s=\infty$, corresponding to a Poisson process of intensity $\lambda^2$.  We have $P(T>y)=P[N(B)=0]$, where $B$ is a disc of radius $y$ centered at an arbitrary point of the process, and $N$ is the \gls{gls:pc}\index{point count distribution}, with an exponential distribution of mean $\lambda^2\mu(B)$ with $\mu(B)=\pi y^2$ being the area of $B$. Thus
$P(T>y)=\exp(-\lambda^2\pi y^2)$, that is, $P(T<y)= 1 - \exp(-\lambda^2\pi y^2)$. This is the CDF of a Rayleigh distribution of mean $\frac{1}{2\lambda}$.
\end{Exercise}

\begin{Exercise}\label{exercise14c}{\bf [M]} 
{\bf Cell networks: coverage problem}. Points are randomly distributed on the plane, with an average of $\lambda$ points per unit area. A circle of radius $R$ is drawn around each point. What is the proportion of the plane covered by these (possibly overlapping) circles? What if $R$ is a random variable, so that we are dealing with random circles?  Such \textcolor{index}{stochastic covering problems}\index{covering (stochastic)} are part of \textcolor{index}{stochastic geometry}\index{stochastic geometry} [\href{https://en.wikipedia.org/wiki/Stochastic_geometry}{Wiki}] \cite{davidc,stoyan}. See also Hall's book on coverings \cite{phall}. Applications include wireless networks [\href{https://en.wikipedia.org/wiki/Stochastic_geometry_models_of_wireless_networks}{Wiki}]. \vspace{1ex} \\
{\bf Solution} \vspace{1ex}   \\ 
The points are distributed according to a Poisson point process of intensity $\lambda$. The probability that an arbitrary location $x$ in the plane is not covered by any circle, is the probability that there is zero point from the process, in a circle of radius $R$ centered at $x$. This is equal to $\exp(-\lambda \pi R^2)$. Thus the proportion of the plane covered by the circles is $1-\exp(-\lambda \pi R^2)$. Now, let's say that we have two types of circles: one with radius $R_1$, and one with radius $R_2$, each type equally likely to be picked up. This is like having two independent, superimposed Poisson processes (see Section~\ref{sm1}), each with intensity $\lambda/2$, one for each type of circle. Now the probability $p$ that $x$ is not covered by any circle is thus a product of two probabilities:
$$ p = \exp\Big(-\frac{\lambda}{2}\pi R_1^2\Big)\times\exp\Big(-\frac{\lambda}{2}\pi R_1^2\Big)=\exp\Big(-\lambda\pi \frac{R_1^2 + R_2^2}{2}\Big).$$
You can generalize to $m$ types of circles, each type with a radius $R_k$ and probability $p_k$ to be picked up, with $1\leq k\leq m$. It leads to 
\begin{equation}
1-p=1-\exp\Big[-\lambda\pi \sum_{k=1}^m p_kR_k^2\Big], \label{radis}
\end{equation}
which is the proportion of the plane covered by at least one circle. If $R$, the radius of the circle, is a continuous random variable, the sum in Formula~(\ref{radis}) 
must be replaced by $\mbox{E}[R^2]$. A related topic is the smallest circle problem [\href{https://en.wikipedia.org/wiki/Smallest-circle_problem}{Wiki}].
\end{Exercise}

\begin{Exercise}\label{exercise14d}{\bf [M]} 
{\bf Optimum circle covering of the plane}. This is an old problem, mentioned by Kershner in 1939 \cite{circle34}, revisited in 1971 by Williams \cite{m6565}, and still active today, see \cite{coverx} (available online \href{https://theory.stanford.edu/~jvondrak/data/stoch_cover.pdf}{here}) and  
\cite{opf} (available online \href{https://arxiv.org/abs/cs/0311013}{here}). 
Unlike in Exercise~\ref{exercise14c}, the slightly overlapping circles of fixed radius, covering the entire plane, have centers located on a lattice rather than being the points of a Poisson process; in other words, the \gls{gls:sf} $s$ \index{scaling factor} of the underlying Poisson-binomial process is zero (the point process reduces to its \gls{gls:lattice1}). 

Applications include cellular network coverage, optimum location of sensor devices, and supply chain optimization such as optimum packing [\href{https://en.wikipedia.org/wiki/Circle_packing}{Wiki}]. The \textcolor{index}{circle covering problem}\index{covering (stochastic)} 
[\href{https://en.wikipedia.org/wiki/Overlapping_circles_grid}{Wiki}] consists of finding the lattice that achieves optimum coverage: each location
in the plane is covered by an average of $p>1$ circles; the optimum is reached when $p$ is minimum.  Compute $p$ both for the hexagonal lattice, and for the square lattice. Note that throughout this textbook, I worked with Poisson-binomial processes defined on a square lattice, except when considering lattice rotations, stretching, and superimposition in Section ~\ref{sm1}. \vspace{1ex} \\
{\bf Solution}  \vspace{1ex} \\
Let's start with circle centers located on a square lattice. For full coverage of the plane with as little overlapping as possible, the circles must be the smallest ones covering a square: the four vertices of the square must lie on the circle boundary, and the centers (both for the circle and square) coincide. For a unit square, such a circle must have a radius equal to $\sqrt{2}/2$ and an area equal to  $\pi/2$. 
It is easy to see that $p=\pi/2\approx 1.571$. 
This is illustrated  \href{https://en.wikipedia.org/wiki/Overlapping_circles_grid}{here}. For an 
\textcolor{index}{hexagonal lattice}\index{lattice!hexagonal} [\href{https://en.wikipedia.org/wiki/Hexagonal}{Wiki}], the circle must be the smallest one covering an hexagon and having the same center as the inscribed hexagon [\href{https://en.wikipedia.org/wiki/Hexagon}{Wiki}]. Computations (see \cite{circle34}) show that $p= 2\pi/\sqrt{27}$. This is indeed the minimum possible value for $p$. There are only five types of regular lattices, called \textcolor{index}{Bravais lattices}\index{lattice!Bravais lattice} [\href{https://en.wikipedia.org/wiki/Bravais_lattice}{Wiki}]. 
The hexagon is the regular polygon with the maximum number of sides, among those able to produce a {\em regular} \textcolor{index}{Voronoi tessellation}
\index{tessellation}\index{Voronoi tessellation} [\href{https://en.wikipedia.org/wiki/Voronoi_diagram}{Wiki}], and thus results in the optimum lattice and minimum $p$.
\end{Exercise}

\begin{Exercise}\label{exercise14e}{\bf [S]} {\bf Interlaced lattices, lattice mixtures and nearest neighbors}.  This is an additive number theory problem [\href{https://en.wikipedia.org/wiki/Additive_number_theory}{Wiki}], see also \cite{ant}. 
Let us consider a \textcolor{index}{mixture}\index{point process!mixture} (called \textcolor{index}{$m$-mixture}\index{$m$-mixture}) or 
\textcolor{index}{superimposition}\index{point process!superimposed} (called \gls{gls:mip}\index{$m$-interlacing})  
of $m$ \textcolor{index}{shifted} two-dimensional Poisson-binomial processes\index{shifted process} $M_1,\dots,M_m$ with \gls{gls:sf} \index{scaling factor}
$s=0$.  Thus, these are  non-random processes, where the \gls{gls:state1} of the $i$-th process $M_i$ corresponds to its shifted \gls{gls:lattice1}: $(X_{ih},X_{ik})=(\mu_i+h/\lambda,\mu'_i+k/\lambda)$ for each point $(X_{ih},X_{ik})$ 
of $M_i$, with $(h,k)\in\mathbb{Z}^2$ and $(\mu_i,\mu'_i)$ is the shift parameter vector of $M_i$, depending only on $i$. Assume that each $M_i$ has intensity $\lambda=1$. Perform simulations to 
compare the distribution of \textcolor{index}{nearest neighbor distances}\index{nearest neighbors!nearest neighbor distances} between $m$-interlacings and $m$-mixtures.  More specifically, we are interested in the number of unique values that it can take. Conclude from this experiment that $m$-interlacings with small $s$, are less ``random" than $m$-mixtures with the same $s$. Mixtures and superimposition of shifted processes are discussed in Section~\ref{sm1} and \ref{sm2}. By nearest neighbors, I mean among points of the $m$-mixture or $m$-interlacing, not between an arbitrary location and a point of the process, nor within each individual $M_i$ taken separately. \vspace{1ex} \\
{\bf Solution}\vspace{1ex} \\
For $m$-interlacings with $s=0$, we have exactly $m$ points $P_1,\dots,P_m$ in the square $[0, \frac{1}{\lambda}[ \times [0, \frac{1}{\lambda}[$ (or in any square of same area, for that matter), and thus $m$ pairs $\{P_i, P'_i\}$  ($i=1,\dots,m$) where $P'_i$ is the nearest neighbor (NN) to $P_i$.
Thus we have at most $m$ distinct NN distances $||P_i-P'_i||$. So for $m$-interlacings with $s=0$, the maximum number of unique values for the NN distance is $m$. \vspace{1ex} \\
For $m$-mixtures, the situation is different. Now we have between $1$ and $m$ points in the square $B=[0, \frac{1}{\lambda}[ \times [0, \frac{1}{\lambda}[$, assuming $s=0$. Each of these points has one NN, possibly in the same square or in an adjacent square. For instance, if $P_i\in B$, it has one NN: a point $P_j\in B$
or a shifted version of $P_j$ in an adjacent square. All combinations $i,j\in\{1,\dots,m\}$ are possible, and will necessarily show up (with probability one) in some squares of same area $1/\lambda^2$. Thus the number of unique NN distances is at least $m^2-1$, and at most $m\cdot(4m-1)$. The ``minus one" is because a point can not be its NN, that is, $P_i\neq P'_i$. \vspace{1ex} \\
Simulations confirm these findings, both for $m$-interlacings and $m$-mixtures. It is assumed here that the 
\glspl{gls:sv}\index{shift vector} $(\mu_i,\mu'_i)$ are arbitrary, as if they were randomly generated.
\end{Exercise}

\begin{Exercise}\label{exercise14f}{\bf [SM*]} {\bf Lattice topology and algebra} Using a superimposition of $m$ stretched shifted Poisson-binomial processes $M_1,\dots,M_m$, denoted as $M$ and called an 
\gls{gls:mip}\index{$m$-interlacing} 
in Exercise~\ref{exercise14e}, build a point process 
that has a regular \textcolor{index}{hexagonal lattice} as its 
\gls{gls:lattice1}, \index{lattice!lattice space}
with $m$ as small as possible. Note that each $M_i$ has a 
rectangular lattice space. Superimposed stretched shifted processes are defined in Section~\ref{sm1}. When $s=0$, $M$ is identical to its fixed (non-random) hexagonal lattice space, see left plot in Figure~\ref{fig:hexa}.  It is also clear from Figure~\ref{fig:hexa} that each point of $M$ has exactly 3 nearest neighbors. To the contrary, in a square lattice, each point (called 
\textcolor{index}{vertex}\index{vertex (graph theory)} in 
\textcolor{index}{graph theory}\index{graph theory}) has 4 nearest neighbors. In a rectangular (non-square) lattice, each vertex 
has 2 nearest neighbors. Is it possible to build a lattice where each vertex has 5 or 6 nearest neighbors? A line joining two nearest neighbor vertices is called an \textcolor{index}{edge}\index{edge (graph theory)}. In Figure~\ref{fig:hexa}, all edges have the same unit length. 
Use Formulas~(\ref{simm1}) and (\ref{simm2}) to generate a realization of $M$. The challenge is to find the minimum $m$ and then identify the parameters $\lambda,\lambda'$ and $\mu_i,\mu'_i$ ($i=1,\dots,m$) resulting in a regular hexagonal lattice when $s=0$. By regular, I mean that all edges have the same length, and only one regular polygon is used in the construction (in our case, an hexagon). \vspace{1ex} \\
{\bf Solution} \vspace{1ex}\\
The solution can be found in Section~\ref{sm1}. I used $m=4$, and I don't think you can use a smaller $m$. The parameters are 
$\lambda=1/3$, $\lambda'=\sqrt{3}/3$, $\mu_1=0, \mu_2=1/2, \mu_3=2, \mu_4=3/2 $ and $\mu'_1=0, \mu'_2=\sqrt{3}/2, \mu'_3=0, \mu'_4=\sqrt{3}/2$. You won't be able to build a regular lattice based on a 
single regular polygon [\href{https://en.wikipedia.org/wiki/Regular_polygon}{Wiki}] if each point has exactly 5 or exactly 6 (or more) nearest neighbors. But many \textcolor{index}{semi-regular lattices}\index{lattice!semi-regular} also called 
\textcolor{index}{tilings}\index{tiling (spatial processes)}
[\href{https://en.wikipedia.org/wiki/Euclidean_tilings_by_convex_regular_polygons}{Wiki}], 
such as square-hexagonal [\href{https://en.wikipedia.org/wiki/Truncated_square_tiling}{Wiki}], exist.  
This also illustrates the fact that lattices form 
a \textcolor{index}{group} [\href{https://en.wikipedia.org/wiki/Lattice_(group)}{Wiki}]\index{lattice!lattice group (group theory)}, where shifting (also called translation) corresponds to the addition operation, and stretching
is the scalar multiplication [\href{https://en.wikipedia.org/wiki/Scalar_multiplication}{Wiki}].  Each \gls{gls:sv}\index{shift vector} uniquely characterizes a lattice, and the other way around. Also, an infinite 2-D lattice shifted by the vector
$(\mu,\mu')=(h/\lambda,k/\lambda)$, regardless of $h,k\in\mathbb{Z}$, is topologically unchanged. The two lattices are 
\textcolor{index}{congruent}\index{lattice!congruent lattices} to each other \gls{gls:modulo}\index{modulo operator (point processes)} $1/\lambda$, in the same sense that (in one dimension) the numbers $30.628$ and $40.052$ are congruent 
 [\href{https://en.wikipedia.org/wiki/Modular_arithmetic}{Wiki}] to each other modulo $2.356$ (in the latter case, because
$30.628 - 40.052 = -4\times 2.356$ is a multiple of $2.356$).
\end{Exercise}

\begin{Exercise}\label{exercise14g}{\bf [MS**]} {\bf Nearest neighbors and size distribution of connected components}. Simulate
10 realizations of a \gls{gls:statio}\index{stationarity} Poisson process of intensity $\lambda=1$, each with $n=10^3$ points distributed over a square window. Identify the 
\glspl{gls:cc}\index{connected components}\index{graph!connected components} 
 [\href{https://en.wikipedia.org/wiki/Component_(graph_theory)}{Wiki}]
and their size (the number of points in each connected component). 
The purpose of the exercise is to study
the distribution of the size, denoted as $S$. In particular, what is the proportion of connected components with only 2 points ($P[S=2]$), 3 points
($P[S=3]$) and so on? For connected components, 
use the 
\textcolor{index}{undirected graph}\index{graph!undirected}, 
that is: points $V_i,V_j$ (also called vertices) are connected if $V_i$ is nearest neighbor to $V_j$, or the other way around.
The questions are:
\begin{itemize}
\item Estimate the probabilities in question via simulations. When computing the proportions using multiple realizations of the same process,
do we get a similar 
 \gls{gls:empdis}\index{empirical distribution} for $S$, across all realizations? Does the empirical distribution seem to convergence, when  increasing $n$, say from $n=10^3$ to $n=10^4$ or $n=10^5$?
\item Do the same experiment with a Poisson-binomial process, with $\lambda=1$ and $s=0.15$. Do we get the same distribution for $S$? What
about $P[S=2]$?
\item Generate a particular type of 
\textcolor{index}{random graph}\index{graph!random graph}\index{graph}\index{random graph}, called \textcolor{index}{random NN graph}\index{graph!random graph!random nearest neighbor graph}\index{nearest neighbors!nearest neighbor graph!random nearest neighbor graph}, as follows. Let $V_1,\dots,V_n$ be the $n$ 
\textcolor{index}{vertices}\index{vertex (graph theory)} of the graph (their locations do not matter). For the ``nearest neighbor" to vertex $V_k$ ($k=1,\dots, n$), randomly pick up one of the $n$ vertices except $V_k$ itself. Two points (vertices) can have the same nearest neighbor. 
Now study the distribution of $S$ via simulations. Is it the same as for the graph generated by the nearest neighbors in a stationary Poisson point process?
\item This is the most difficult part. Let $P(S=k), k=2,3,\dots$ be the size distribution for connected components of a stationary Poisson process; $S$ is a random variable. Of course, it does not depend on $\lambda$. Does it uniquely characterize the Poisson process, in the same way that the exponential distribution for 
\gls{gls:ia}\index{interarrival times}
uniquely characterizes the Poisson process in one dimension? Do we have $P(S=2)=\frac{1}{2}$, not only for Poisson processes, but also for a much larger class of point processes? 
\end{itemize}
 Useful references about random graphs [\href{https://en.wikipedia.org/wiki/Random_graph}{Wiki}] include ``The Probabilistic Method" by Alon and Spencer \cite{probme} 
(available online \href{http://math.bme.hu/~gabor/oktatas/SztoM/AlonSpencer.ProbMethod3ed.pdf}{here}), and 
``Random Graphs and Complex Networks" by Hofstad \cite{rvdh} (available online
 \href{https://www.win.tue.nl/~rhofstad/NotesRGCN.html}{here}). See also \href{https://math.stackexchange.com/questions/3883829/distribution-of-size-of-connected-components-in-erdos-renyi-random-graphs-in-the}{here}. \vspace{1ex}\\
{\bf Hints} \nopagebreak \vspace{1ex}\\
Beware of the \gls{gls:be}\index{boundary effect}; to minimize the impact, use a uniform distribution for $F$ (the distribution attached to the points of the
Poisson-binomial process) and $n>10^3$. When the \gls{gls:sf}\index{scaling factor} $s$ is zero, there is only one connected component of infinite size 
($P[S=\infty]=1$): this is a singularity, as illustrated on the left plot in Figure~\ref{fig:hexa}. But as soon as $s>0$, all the connected components are of finite size
and rather small. The smallest ones have two points as each point has a nearest neighbor, thus $P[S<2]=0$. When $s=\infty$, the process becomes a stationary Poisson process,
see Theorem~\ref{sums2}. 

I conjecture that stationary Poisson processes and some other (if not all) Poisson-binomial processes share the exact same discrete probability distribution for the size of connected components defined by nearest neighbors, and abbreviated as CCS distribution. Thus, 
unlike the \gls{gls:pc}\index{point count distribution} or 
\textcolor{index}{nearest neighbor}\index{nearest neighbors} distance distributions,
the CCS distribution can not be used to characterize a Poisson process. For random graphs, the CCS distribution is different from that of a Poisson process. I used a \textcolor{index}{Kolmogorov-Smirnov test}\index{Kolmogorov-Smirnov test}
[\href{https://en.wikipedia.org/wiki/Kolmogorov-Smirnov_test}{Wiki}] (see also \cite{kst} available online \href{https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-19/issue-2/On-the-Kolmogorov-Smirnov-Limit-Theorems-for-Empirical-Distributions/10.1214/aoms/1177730243.full}{here}) to compare the two empirical CCS distributions  -- the one attached to Poisson processes versus the one attached to random NN graphs --  and concluded, based on my sample size ($n=10^4$ points or vertices), that they were statistically different. 

To conclude, it appears that the CCS distribution can not be arbitrary. Many point processes seem to have the same CCS distribution, called \textcolor{index}{attractor distribution}\index{attractor (distribution)},  and these processes constitute the 
\textcolor{index}{domain of attraction}\index{domain of attraction} of the attractor. The concepts of domain of attraction and attractor is used in other contexts such as \textcolor{index}{dynamical systems}\index{dynamical systems} [\href{https://en.wikipedia.org/wiki/Attractor}{Wiki}] or extreme value theory [\href{https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution}{Wiki}] (also, see \cite{order2} page 317). The most well known analogy is the \textcolor{index}{Central Limit Theorem}, where the Gaussian distribution is the main attractor, and the Cauchy distribution is another one. In chapter 11 of ``The Probabilistic Method"  \cite{probme}, 
dealing with the size of connected components in random graphs, the author introduces a random variable $T_c$, also counting a number of vertices 
(called \textcolor{index}{nodes}\index{graph!node} in the book). Its distribution has all the hallmarks of an attractor. See  Theorem 11.4.2 (page 202) in the book in question.

To find the connected components, you can use the source code in Section~\ref{scc}. To simulate point processes, you can use the source code
 in Section~\ref{nnsc}: it produces an output file 
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Data/PB_NN_dist_full.txt}{\texttt{PB\_NN\_dist\_full.txt}}  
that can be used as input, without any change, to the connected components algorithm in Section~\ref{scc}.  Exercise~\ref{cliquebc} features a similar problem, dealing with cliques
 rather than connected components.
\end{Exercise}

\begin{Exercise}\label{cliquebc}{\bf [M]} {\bf Maximum clique problem}. In \textcolor{index}{undirected graphs}\index{graph!undirected} [\href{https://en.wikipedia.org/?title=Undirected_graph}{Wiki}], a 
\textcolor{index}{clique}\index{clique (graph theory)} is a set of vertices (also called nodes) all connected to each other.  In 
\textcolor{index}{nearest neighbor}\index{nearest neighbors} graphs, two points are connected if one of them is a closest neighbor to the other one.  How would you identify a clique of maximum size in such a graph? No need to design an algorithm from scratch; instead, search the literature. Finding the maximum clique 
[\href{https://en.wikipedia.org/wiki/Clique_problem}{Wiki}] is NP-hard [\href{https://en.wikipedia.org/wiki/NP-hardness}{Wiki}], and the problem is related to the 
``P versus NP" conjecture [\href{https://en.wikipedia.org/wiki/P_versus_NP_problem}{Wiki}].  The maximum clique problem has many applications,
in particular in social networks.  Probabilistic properties of cliques in \textcolor{index}{random graphs}\index{random graph} are discussed in
``Cliques in random graphs" \cite{erdos311} (available online \href{https://users.renyi.hu/~p_erdos/1976-05.pdf}{here}) and 
``On the evolution of random graphs" \cite{erdos310} (available online \href{http://snap.stanford.edu/class/cs224w-readings/erdos60random.pdf}{here}).
See also [\href{https://bit.ly/3uzQzNF}{Wiki}]. More recent articles include \cite{mec45,nmbv}, respectively available
 \href{https://arxiv.org/abs/1308.3297}{here} and \href{https://arxiv.org/abs/1508.01668}{here}. \vspace{1ex} \\
{\bf Solution}\nopagebreak \vspace{1ex}\\
In two dimensions, in an \textcolor{index}{undirected}\index{graph!undirected} \gls{gls:nng}\index{graph!nearest neighbor graph}\index{nearest neighbors!nearest neighbor graph}, 
the minimum size of a maximum clique is $2$ (as each point has a nearest neighbor), and the maximum size is $3$. A maximum clique must be a 
\gls{gls:cc}\index{connected components}. See definition of connected component in Exercise~\ref{exercise14g}. If each point has exactly one nearest neighbor, then a connected component of size $n>1$ has $n$ or $n-1$ edges
(the arrows on the right plot in Figure~\ref{fig:hexa}), while a clique of size $n$ has exactly $\frac{1}{2}n(n-1)$ edges. This is why maximum cliques of size larger than $3$ don't exist. But in $d$ dimensions, a maximum clique can be of size $d+1$. The maximum clique can be found using the MaxCliqueDyn 
algorithm [\href{https://en.wikipedia.org/wiki/MaxCliqueDyn_maximum_clique_algorithm}{Wiki}]. 
\end{Exercise}

\subsection{Miscellaneous}

This section features problems that don't fit well in any of the previous categories. 

\begin{Exercise} \label{exercise5} {\bf [M]} 
{\bf Computing moments using the CDF}. The purpose is to prove a formula to compute the moments of a random variable, using the cumulative distribution function (CDF), rather than the density. If $X$ is a univariate random variable with CDF $F(x)=P(X<x)$, and $r$ is a positive integer, prove the following:
\begin{itemize}
\item If $X$ is positive, then $\mbox{E}[X^r]=r\int_0^\infty x^{r-1}(1-F(x))dx$
\item If $X$ is symmetric around the origin and $r$ is even, then $\mbox{E}[X^r]=2r\int_0^\infty x^{r-1} (1-F(x))dx$. If $r$ is odd, $\mbox{E}[X^r]=0$.
\end{itemize}
{\bf Solution} A solution for the general case, or when $X$ is positive, can be found \href{https://math.stackexchange.com/questions/1260157/moments-of-a-random-variable-in-terms-of-its-cumulative-distribution-function}{here}. If $X$ is symmetric around the origin, then $F(x)=1-F(-x)$, and the result follows easily. 
\end{Exercise}

\begin{Exercise}\label{exercise10}
{\bf [S]} {\bf Simulations: generalized logistic distribution}. Implement a routine that generates deviates for the \textcolor{index}{generalized logistic distribution}\index{distribution!generalized logistic}, using the \gls{gls:quant}\index{quantile!quantile function} $Q(u)$ in Formula~(\ref{eq:qq}), with a uniform distribution on $[0,1]$ for $u$. Do the same for the 
\textcolor{index}{Laplace distribution}\index{distribution!Laplace} 
defined in Section~\ref{s:def}. Simulate 1-D and 2-D Poisson-binomial point processes, using a Laplace and generalized logistic distribution for $F$. For the generalized logistic distribution, try different values for the parameters $\alpha,\beta,\tau,\mu,\lambda$. \vspace{1ex}   \\ 
{\bf Hint} \vspace{1ex}   \\ 
Use \textcolor{index}{inverse transform sampling}\index{inverse transform sampling} to simulate Laplace deviates. That is, use the Laplace quantile function $Q(u)$ with uniform deviates on [0,1] for $u$; $Q(u)$ is the inverse of the Laplace cumulative distribution function $F$ listed in Section~\ref{s:def}.
\end{Exercise} 

\begin{Exercise}\label{exercise11}{\bf [S]} 
 {\bf Riemann Hypothesis}. Refer to Section~\ref{rh} for the material and notations discussed here. The hole in Figure~\ref{fig:riemann}, on the top left plot corresponding to $\sigma=0.75$ and $s=0$, is observed when $0\leq t \leq 200$. Try other intervals, say $[t, t+\tau]$, for much larger values of $t$ and (say) $\tau=200$. See if the hole gets any smaller. Try $s=10^{-2}$, instead of $s=10^{-3}$ in Formula~(\ref{eta1b}) and (\ref{eta2b}): now the hole is entirely gone. This shows how sensitive the $\eta$ function is to small perturbations. Finally, find the first 40 values $t=t_1,\dots,t_{40}$, with $t>0$, solutions of $\Im[\eta(\sigma+it)]=0$, when $\sigma=\frac{1}{2}$, using numerical techniques. How many of these roots are also solution to 
$\Re[\eta(\sigma+it)]=0$? Such values of $t$ correspond to the non-trivial complex zeros of the Riemann zeta function, on the critical line $\sigma=\frac{1}{2}$. \vspace{1ex}   \\
{\bf Solution} \vspace{1ex}   \\
The challenge here is the slow and \textcolor{index}{chaotic convergence}\index{chaotic convergence} of the two series (real and imaginary parts) representing the function $\eta(\sigma +it)$ in Formula~(\ref{eta1}) and (\ref{eta2}).  I refer to $t$ as the time. The larger $t$, the smaller the time increments required to correctly plot the orbit. These increments can be as small as $0.01$ if $t\approx 10^3$, to not miss any rare value, say $t_0,$ resulting in $\eta(\sigma+it_0)$ unusually close to the origin when $\sigma=0.75$.  A convergence acceleration technique is described in Exercise~\ref{exercise12}.
\end{Exercise}

\begin{Exercise}\label{exercise12}{\bf [S*]} 
{\bf Convergence acceleration}.
Design a basic algorithm for \textcolor{index}{convergence acceleration}\index{convergence acceleration} of 
alternating series [\href{https://en.wikipedia.org/wiki/Alternating_series}{Wiki}]. 
How does it perform, when computing the sum in
Formula~(\ref{eta2})? Try with $s=0.75$ and $t=18265.2$ (the correct value of the sum is about 0.292040897 if you ignore the sign, see Mathematica computation \href{https://bit.ly/32uIwpE}{here}). \vspace{1ex}   \\
{\bf Solution} \vspace{1ex}   \\
If $S_n=a_1+a_2+\dots+a_n$ converges to $S$, and the $a_k$'s are alternating, then one can proceed as follows:

\begin{itemize}
\item Let $S'_n=a'_1+a'_2+\dots +a'_n$ with $a'_k= \alpha a_k + (1-\alpha)a_{k+1}$, and $0\leq \alpha\leq 1$ chosen to maximize the speed of convergence of $S'_n$.  
\item Let $S''_n=a''_1+a''_2 +\dots  +a''_n$ where $a''_k= \alpha' a'_k + (1-\alpha')a'_{k+1}$, and $0\leq \alpha' \leq1$ chosen to maximize the speed of convergence of $S''_n$.  
\end{itemize}
One can continue iteratively with $S'''_n$ and so forth, each new sum converging faster to $S$ than the previous one. Also, the sequence $a_1,a'_1,a''_1$ and so on, rapidly converges to $S$.  However, it fails to work in our example.

The reason is because, despite the appearance, the series in Formula~(\ref{eta2}) is not an alternating one. Indeed, hundreds, and even trillions of trillions of consecutive terms, depending on $t$, can have the same sign despite the $(-1)^k$ factor attached to each term. This behavior creates  \textcolor{index}{numerical instability}\index{numerical stability}. The explanation is as follows: If for some large $k$ in Formula~(\ref{eta1}) or (\ref{eta2}),  the quantity $t \log(k+1)-t \log k \approx t/k$ 
is close to an odd multiple of $\pi$, then around that $k$, a lot of terms in the series will have the same sign and similar value (as opposed to the regular alternating behavior). As a result, if $k$ is not large enough (but not too small) when this happens, a sum that seems to have converged, will suddenly experience a huge shift. This is what happens here, most strikingly when $k=5814$ and $t=18265.2$, leading to $t/k =3.141589\dots$ very close to $\pi$, and resulting in the odd behavior around $k=5814$, illustrated in Figure~\ref{fig:fcc}. The X-axis represents $k$ and the Y-axis represents the value of the partial sum computed using $k$ terms.


\begin{figure}[H]
\centering
\includegraphics[width=0.82\textwidth]{PB-accelerate.PNG}
%  \includegraphics[width=\linewidth]{pbx2.PNG}
\caption{Chaotic convergence of partial sums in Formula~(\ref{eta2})}
\label{fig:fcc}
\end{figure}

There are various workarounds to deal with this issue. First, the \textcolor{index}{Dirichlet eta function}\index{Dirichlet eta function} $\eta$ has numerous representations: you can choose one that is more suitable for 
computation purposes. But even if you want to stick to Formula~(\ref{eta2}), you can improve it by splitting the sum into two parts: 
\begin{itemize}
\item One part that deals with the few dips and spikes, easy to identify. Here, the last one occurs at $k=5814$.
\item The second part is to compute the first few hundred terms by traditional means.
\item Then combine both parts to get a good approximation of the final sum, in the end using much fewer operations than brute force, and having a good sense as to when convergence is reached.
\end{itemize} 
To prove the convergence of the series in Formulas~(\ref{eta1}) and (\ref{eta2}) representing the Dirichlet eta function, one can use the Dirichlet test [\href{https://bit.ly/3prDAL1}{Wiki}]. Note that without the factor $(-1)^k$ in Formulas~(\ref{eta1}) and (\ref{eta2}), the series may not converge.
\end{Exercise}

\begin{Exercise}\label{fastfilter}{\bf [S]} 
{\bf Fast image filtering algorithm}.  The filtering algorithm described in  Section~\ref{spa2} requires a large moving window of $21\times 21$ pixels, around each pixel. The size of this window is the bottleneck. How can you make the algorithm about $20$ times faster, still keeping the same window size? \vspace{1ex} \\
{\bf Solution} \vspace{1ex} \\
When filtering the image, the window used at $(x,y)$, and the next one at $(x+1,y)$, both have $21\times 21 = 441$ pixels, but these two windows have $441 - (2\times 21) = 399$ pixels in common. So rather than visiting $441$ pixels each time, the overlapping pixels can be kept in a $21 \times 21$ buffer. To update the buffer after visiting a pixel and moving to the next one to the right, one only has to update $21$ values in the buffer: overwrite the column corresponding to the old $21$ leftmost pixels, by the values derived from the new $21$ rightmost pixels.  
\end{Exercise}

\begin{Exercise}\label{exhotelling}{\bf [M**]} 
{\bf Confidence regions: theory and computations}.  What are the foundations justifying the methodology used to build the 
\glspl{gls:cr}\index{confidence region} 
in Section~\ref{cils1}? In particular, how would you proceed to find the values of $\sigma_p,\sigma_q,\rho_{p,q}$ in Formula~(\ref{gauss2d})? Does $G_\gamma$ depend 
on $n, p$ or $q$? Why not? What justifies the choice of an ellipse for the confidence region? What is the role of 
\textcolor{index}{Hotteling's distribution}\index{Hotelling distribution} in the methodology? How would you tabulate $G_\gamma$ via 
simulations? Can you think of a different type of confidence region? 

The goal here is to identify references that answer the questions, rather than trying to prove everything on your own. There is a considerable amount of tightly packed material in Section~\ref{cils1}, presented at a high level. I only ask you, in this exercise, to dig  just one level beneath the surface. \vspace{1ex} \\
{\bf Solution} \vspace{1ex} \\
Each of the $2n$ observations is  realization of a Bernoulli random vector  
$(U_k, V_k)\in \{(0,0), (0,1), (1,0)\}$, with $k=-n,\dots,n-1$.  In particular, $U_k=1$ if the interval $B_k$ defined by Formula~(\ref{bk01}) contains exactly one point of the Poisson-binomial process, otherwise $U_k=0$. 
Likewise $V_k=1$ if $B_k$ contains exactly two points, otherwise $V_k=0$. The statistic $p$ (a random variable depending on $n$) is the proportion of $1$ in the sequence $(U_k)$, and $q$ is the proportion
of $1$ in $(V_k)$. From this, it follows that $\sigma_p=\sqrt{p(1-p)}$, $\sigma_q=\sqrt{q(1-q)}$, $p+q\leq 1$, and $U_k,V_k$ are negatively correlated. The proportions of $(0,0), (1,0)$ 
and $(0,1)$ among the $(U_k,V_k)$ are respectively 
$1-(p+q), p$ and $q$. From this, it follows that $\rho_{p,q}=-pq/\sqrt{pq(1-p)(1-q)}$.

If the random vectors $(U_k,V_k)$ were identically and independently distributed (iid), things would be easier thanks to
 the \textcolor{index}{multivariate central limit theorem}\index{central limit theorem!multivariate} [\href{https://en.wikipedia.org/wiki/Central_limit_theorem#Multidimensional_CLT}{Wiki}], despite the strong correlation between $U_k$ and $V_k$. Unfortunately, they are neither. Exercise~\ref{exercise4} shows that the 
\glspl{gls:pc}\index{point count distribution} are not identically distributed in general, and Exercise~\ref{exercise43} shows the lack of independence. But the dependencies are local and very weak.
Also a careful choice of non-overlapping $B_k$'s in Formula~(\ref{bk01}) -- inspired by Theorem~\ref{combiexp} -- makes the point counts  almost identically distributed. 
They are in fact asymptotically iid. The length of $B_k$, set by Formula~(\ref{bk00}), is very well approximated by (and asymptotically equal to) $1/\lambda$, to minimize any problem. 
Section~\ref{bbei} provides further reassurance. Finally, when $n$ is large, the $B_k$'s are {\em on average} far away from each other, further dampening dependencies and related issues. And as a bonus,
the bias caused by \glspl{gls:be}\index{boundary effect} tends to zero.  By asymptotically, I mean when $n\rightarrow\infty$.

In the remaining of this discussion, I consider the previous issue as overcome. I proceed as if the $(U_k,V_k)$ were iid. Thus, we can use the central limit theorem (CLT) {\em as is}. 
If we only had one statistic $p$ and $2n$ observations, then  the CLT states that 
$Z=\sqrt{2n}\cdot(p-\mu_p)/\sigma_p\rightarrow \mathcal{N}(0,1)$
 as $n\rightarrow\infty$. In two dimensions, $\sigma_p^2$ is replaced by the $2\times 2$ symmetric covariance matrix, denoted as $\Sigma$ or $\Sigma_{p,q}$. Its inverse (the analogous of $\sigma_p^{-1}$ in one dimension), is denoted as $\Sigma^{-1}$. The multivariate CLT implies that
$Z = \sqrt{2n} \cdot (p-\mu_p, q-\mu_q)\Sigma^{-1/2}\rightarrow \mathcal{N}(0,I)$. That is, we have convergence to a 
\textcolor{index}{bivariate normal distribution}\index{normal distribution}\index{normal distribution!multivariate} [\href{https://en.wikipedia.org/wiki/Multivariate_normal_distribution}{Wiki}]
(also called 
\textcolor{index}{multivariate Gaussian}) \index{Gaussian distribution}\index{Gaussian distribution!multivariate}\index{distribution!Gaussian} of zero mean and identity 
\textcolor{index}{covariance matrix}\index{covariance matrix} $I$ [\href{https://en.wikipedia.org/wiki/Covariance_matrix}{Wiki}].  Also,
$$\Sigma^{-1}= \frac{1}{1-\rho_{p,q}^2} \cdot 
\begin{bmatrix}
\sigma_p^{-2} & \rho_{p,q} \sigma_p^{-1} \sigma_q^{-1}\\
-\rho_{p,q} \sigma_p^{-1}\sigma_q^{-1}  & \sigma_q^{-2}
\end{bmatrix}
$$
In one dimension, $Z^2$ has a chi-squared distribution with one degree of freedom at the limit as $n\rightarrow\infty$. The 
\textcolor{index}{Berry-Esseen theorem}\index{Berry-Esseen theorem} [\href{https://bit.ly/3Jsq0Ow}{Wiki}]
quantifies the stochastic error (that is, the quality of the approximation) when $n$ is not infinite. In two dimensions,  $Z^2$ is replaced by 
\begin{align}
Z\cdot Z^t & =\Big[\sqrt{2n}\mbox{ }(p-\mu_p, q-\mu_q)\mbox{ }\Sigma^{-1/2}\Big] \times \Big[\sqrt{2n}\mbox{ }(p-\mu_p, q-\mu_q)\mbox{ }\Sigma^{-1/2}\Big]^t  \nonumber \\
  & = 2n\mbox{ } (p-\mu_p, q-\mu_q)\mbox{ }\Sigma^{-1}(p-\mu_p, q-\mu_q)^t \nonumber \\
 & = \frac{2n}{1-\rho_{p,q}^2}\cdot 
\Big[\Big( \frac{p-\mu_p}{\sigma_p}\Big)^2 
-2\rho_{p,q}\Big(\frac{p-\mu_p}{\sigma_p}\Big)\Big(\frac{q-\mu_q}{\sigma_q}\Big) 
+ \Big(\frac{q-\mu_q}{\sigma_q}\Big)^2\Big]  \nonumber
\end{align}
and still has a \textcolor{index}{chi-squared distribution}\index{chi-squared distribution}\index{distribution!chi-squared} 
[\href{https://en.wikipedia.org/wiki/Chi-squared_distribution}{Wiki}], but this time with two 
\textcolor{index}{degrees of freedom}\index{degrees of freedom} [\href{https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)}{Wiki}]. This explains the choice of $H_n(x,y,p,q)$ in Formula~(\ref{gauss2d}), and why $G_\gamma$ does not depend on $p, q$ and quickly converges as $n\rightarrow\infty$.
Here the symbol $^t$ denotes the transposition operator [\href{https://en.wikipedia.org/wiki/Transpose}{Wiki}], transforming a row vector into a column vector. Also, $Z^{-1/2} \cdot (Z^{-1/2})^t=Z^{-1}$. The chi-squared limit 
is a particular case of 
\textcolor{index}{Cochran's theorem}\index{Cochran's theorem} [\href{https://bit.ly/3JzreHS}{Wiki}]. It assumes that the exact values of $\mu_p,\mu_q,\sigma_p,\sigma_q,\rho_{p,q}$ are known. Unfortunately,
 this is not the case here: these values are replaced by their estimates based on $p$ and $q$. As a result, in two dimensions, the chi-squared must be replaced 
by
\textcolor{index}{Hotteling's distribution}\index{Hotelling distribution} [\href{https://bit.ly/3uQ4a3t}{Wiki}].  The proof of these results (the fact that the square of a Gaussian is a chi-squared, and so on) 
is based on the \textcolor{index}{characteristic functions}\index{characteristic function} of these distributions.

The ellipse is the best possible shape for the confidence region: given a confidence level $\gamma$, it is the one of minimum area. To see why, start building a tiny, almost empty confidence region with $\gamma\approx 0$. You need to start at the maximum of the density function (here, a bivariate Gaussian by virtue of the central limit theorem). As you increase $\gamma$, the confidence region expands. But to keep it expanding at the slowest possible rate (keeping its area minimum at all times), you need to follow the contour lines of the density: the curves where the density is constant. For the Gaussian distribution, these contour lines are ellipses. But the same principle is true for any continuous bivariate density. In general, the shape will  not be an ellipse.

There is an alternative definition of confidence region, 
called \textcolor{index}{dual} \gls{gls:cr}\index{dual confidence region}\index{confidence region!dual region}, 
leading to non-elliptic shapes even for Gaussian distributions. It consists of computing the confidence region for all $(p,q)$ in the proxy space, 
rather than for your estimate $(p_0,q_0)$ only. If the confidence region of some $(p,q)$ contains $(p_0,q_0)$, then it is part of $(p_0,q_0)$'s newly defined confidence region. The boundary of the newly defined confidence region of $(p_0,q_0)$ consists of the points $(x,y)$ satisfying 

\begin{equation}
\frac{2n}{1-\rho_{x,y}^2}\cdot 
\Big[\Big( \frac{p-x}{\sigma_x}\Big)^2 
-2\rho_{x,y}\Big(\frac{p-x}{\sigma_x}\Big)\Big(\frac{q-y}{\sigma_y}\Big) 
+ \Big(\frac{q-y}{\sigma_y}\Big)^2\Big] = H_\gamma,\label{gauss3d}
\end{equation}
 with $(p,q)$ set to $(p_0,q_0)$. Compare Formula~(\ref{gauss2d}) with (\ref{gauss3d}). Clearly, the latter does not correspond to the equation of an ellipse; the former does. The roles of $(p,q)$ and $(x,y)$ have been swapped. Also note the use of a different ``scale" $H_\gamma$
 instead of $G_\gamma$. Yet in practice, the two methods yield almost identical results. Both the standard and newly defined confidence regions are implemented in the 
spreadsheet.  An example using the standard region is featured on the left plot in Figure~\ref{fig:pbcr}. Tables for $G_\gamma$ and $H_\gamma$ are provided
in the \texttt{Confidence\_Region} tab in the spreadsheet in question 
(\href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_Independence.xlsx}}): see columns \texttt{F}, \texttt{G}, and \texttt{K}. I produced them via simulations, based on the code
in column \texttt{Y}.

Last but not least, in the end the goal is to obtain confidence regions for the parameter $(\lambda,s)$ in the parameter space, not for the proxy 
vector $(p,q)$ in the proxy space . The final step consists of using the inverse mapping defined in Section~\ref{cils1}, to map the confidence region built in the proxy space, onto the parameter space. The challenge here is to
prove that the mapping is one-to-one. This is still an open question. Most likely, the final confidence region in the parameter space won't be an ellipse. There is an easy formula to do the mapping from the parameter space to the proxy space, see Section~\ref{bbei}. But the inverse mapping, needed here, is a bit less easy to perform.
\end{Exercise}

\begin{Exercise}\label{exercise92}{\bf [M*]} 
 {\bf Minimum set covering 90\% of a distribution}. This is related to the 
\glspl{gls:cr}\index{confidence region} discussed 
in Exercise~\ref{exhotelling}. It consists of (1) finding the shape of the 2D set of minimum area, covering a 
proportion $\gamma$ of the mass of a specific 2D probability distribution, and (2) determining its area. \vspace{1ex}   \\
{\bf Solution} \vspace{1ex}   \\
Let  $S_\gamma$ be the set in question, and $f(x,y)$ be the density attached to the distribution. I assume 
that the density has one maximum only, and that it is continuous everywhere on $\mathbb{R}^2$. Thus the problem consists of finding 
the set $S_\gamma$ of minimum area, such that
\begin{equation}
\int\int_{S_\gamma} f(x,y) dxdy = \gamma.\label{zinal}
\end{equation}
It is easy to see that the boundary of $S_\gamma$ is a contour line of $f(x,y)$. To build $S_\gamma$, you start at the maximum of the density, and to keep the area minimum, the set must progressively be expanded, strictly following contour lines, until (\ref{zinal}) is satisfied. So 
$$S_\gamma = \{(x, y) \in\mathbb{R}^2 \mbox{ such that } f(x,y)\leq G_\gamma\},$$
where $G_\gamma$ must be chosen so that (\ref{zinal}) is satisfied. Assuming $\max f(x,y)=M$, the volume covered by $S_\gamma$
 is 
\begin{equation}
\gamma = z_\gamma \cdot |S_\gamma| + \int_{z_\gamma}^M |R(z)| dz, \label{zinal2}
\end{equation}
where $R(z) = \{(x, y) \in\mathbb{R}^2 \mbox{ such that } f(x,y) =z\}$, and $|\cdot|$ denotes the area of a 2D domain. Clearly, 
$|S_\gamma|=|R(z_\gamma)|$. So there is only one unknown in Equation~(\ref{zinal2}), namely $z_\gamma$. Finally, $G_\gamma=z_\gamma$, and thus the value of $G_\gamma$ is found by solving (\ref{zinal2}). The area of $S_\gamma$ is
thus $|S_\gamma|=|R(G_\gamma)|$. 
\end{Exercise}

\pagebreak

%=====================================
\section{Source Code, Data, Videos, and Excel Spreadsheets}\label{s:code}
%=====================================


My source code is available online at \href{https://github.com/VincentGranville/Point-Processes}{github.com/VincentGranville/Point-Processes}, as well
as in this textbook. It is written using only basic data structures and manipulations available in all programming languages, such as
strings, arrays, stacks, subroutines, regular expressions and \textcolor{index}{hash tables}\index{hash table},
to make it easy to read and rewrite in Java, C++ or other languages. The visualizations are performed either in R with the 
\href{https://www.cairographics.org/}{Cairo graphics library} 
[\href{https://en.wikipedia.org/wiki/Cairo_(graphics)}{Wiki}] to create better scatterplots, or Python with the Pillow library to create PNG images pixel by pixel, 
including density estimation and clustering via image filtering. 

My source code is designed to bring as much educational value as possible, without jeopardizing efficiency. It includes algorithms useful in many other contexts, such as the generation of random deviates from a logistic, Cauchy or Laplace distribution,  and a fast, compact algorithm to detect 
\glspl{gls:cc}\index{connected components}\index{graph!connected components} in a graph.
The textbook version has detailed explanations. The source code
repository is organized according to Table~\ref{tabpgr}; to access the code online, click on the filename:

%\begin{center}
\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
  Filename & Textbook code & Purpose  & Language \\ %\hline \\
\hline
  \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_main.py}{\texttt{PB\_main.py}} & Section \ref{s:codeperl} & one-dimensional stats &  Python \\ 

 \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_radial.py}{\texttt{PB\_radial.py}} & Section \ref{s:code2} & radial clusters &  Python \\ 

 \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN.py}{\texttt{PB\_NN.py}} & Section \ref{nnsc} & NN distances &  Python \\ 

\href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN_graph.py}{\texttt{PB\_NN\_graph.py}} &  Section \ref{scc} & NN connected components &  Python \\ 

\href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PP_NN_arrows.r}{\texttt{PB\_NN\_arrows.r}}  &  Section \ref{rrr4} & NN graph visualization &    R \\ 

 \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/GD_util.py}{\texttt{GD\_util.py}} &  Section \ref{plgd} & maps creation (PNG images) &  Python \\ 

 \href{https://github.com/VincentGranville/Point-Processes/blob/main/Videos/av_demo.r}{\texttt{av\_demo.r}} &  Section \ref{etavis} & video -- Dirichlet eta function &  R \\ 


 \href{https://github.com/VincentGranville/Point-Processes/blob/main/Videos/PB_clustering_video.py}{\texttt{PB\_clustering\_video.py}} &  Section \ref{codefr} & video frames -- fractal clustering &  Python \\ 
\hline


\end{tabular}
\caption{\label{tabpgr}List of programs -- NN stands for nearest neighbors, PB for Poisson-binomial}
\end{center}
\end{table} 
\noindent Below is a short description. 

\begin{itemize}
\item \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_main.py}{\texttt{PB\_main.py}}: Computes the expectation, variance, and $P[N(B)=0]$ of  the \gls{gls:pc}\index{point count distribution} $N(B)$ for any interval $B=[a,b]$,
using respectively Formulas~(\ref{eq:f1}), (\ref{eq:f2}) and (\ref{eq:f3}), for various $F$'s (uniform, logistic, Cauchy). Also computes the expectation, variance and higher moments $E[T^r]$ ($r>0$) of the \gls{gls:ia}\index{interarrival times}\index{point process!interarrival times}, using simulations.
The main parameters are the \gls{gls:sf}\index{scaling factor} $s$, and the \gls{gls:intensity1} $\lambda$ \index{intensity function}\index{point process!intensity}.
\item \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_radial.py}{\texttt{PB\_radial.py}}: Generates a realization of a radial cluster process as described in Section~\ref{s:clp}. Used to produce
Figures~ \ref{fig:pbr4b}, \ref{fig:pbr}, \ref{fig:pbr4}, and \ref{fig:pbr4c}.
\item \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN.py}{\texttt{PB\_NN.py}}: Generates points and computes \textcolor{index}{nearest neighbor distances}\index{nearest neighbors!nearest neighbor distances} and related statistics, for a realization of a superimposition
of $m$ shifted stretched Poisson-binomial processes (also called \gls{gls:mip}\index{$m$-interlacing}), as defined in Section~\ref{sm1}, using Formulas~\ref{simm1} and
\ref{simm2} for the simulation of each individual point process. The output data consists of text files with tab-separated columns; they are used to study the distribution of nearest neighbor distances and statistical testing,
and as input files for 
    \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PP_NN_arrows.r}{\texttt{PB\_NN\_arrows.r}}, 
    \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN_graph.py}{\texttt{PB\_NN\_graph.py}} and 
    \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/GD_util.py}{\texttt{GD\_util.py}}. 
\item \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PP_NN_arrows.r}{\texttt{PB\_NN\_arrows.r}}: Based on output data from 
       \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN.py}{\texttt{PB\_NN.py}}, produces an image featuring the nearest neighbor points
     across $m$ superimposed point processes (or a mixture of point processes), as defined in Section~\ref{sm1}. Each arrow points from a point of the process, to its nearest neighbor(s). 
    The color attached to each point indicates the process it belongs to, among the $m$ processes used to generate the superimposition. See
    Figure~\ref{fig:hexa}.
\item \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN_graph.py}{\texttt{PB\_NN\_graph.py}}: Creates the list of all 
\glspl{gls:cc}\index{graph!connected components}\index{connected components} of an 
\textcolor{index}{undirected graph}\index{graph!undirected}, 
for instance the  \gls{gls:nng}\index{graph!nearest neighbor graph}\index{nearest neighbors!nearest neighbor graph}
 of a point process. Two points are considered connected if one of the two points is the nearest neighbor to the other one. 
\item \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN_graph.py}{\texttt{GD\_util.py}}: Small, easy-to-use home-made graphics library
consisting of one function \texttt{GD\_Maps}, relying on the Pillow library, to produce PNG images (bitmaps). Produces
density and cluster maps such as Figure~\ref{fig:map}, as an alternative to traditional contour plots [\href{https://scipython.com/book/chapter-7-matplotlib/examples/a-simple-contour-plot/}{Wiki}], using image filtering and enhancing techniques including
equalization. This library is used in
 \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN.py}{\texttt{PB\_NN.py}}, at the very end.

\item \href{https://github.com/VincentGranville/Point-Processes/blob/main/Videos/av_demo.r}{\texttt{av\_demo.r}}: Creates the video for the 
\textcolor{index}{Dirichlet eta function}\index{Dirichlet eta function}, showing
convergence of its series in the complex plane. Uses input data from 
\href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference.xlsx}} to produce the video
\href{https://www.youtube.com/watch?v=FUxAeW4JEXA}{\texttt{av\_demo2c.mp4}}. See Section~\ref{videocluster1}.

\item \href{https://github.com/VincentGranville/Point-Processes/blob/main/Videos/PB_clustering_video.py}{\texttt{PB\_clustering\_video.py}}: Generates the frames for
the video  \href{https://www.youtube.com/watch?v=dNPSEh-X6uw}{\texttt{imgPB.mp4}} featuring \textcolor{index}{fractal supervised clustering}\index{fractal clustering}\index{clustering!fractal clustering}. See Section~\ref{videocluster2}.

\end{itemize}
Detailed descriptions are included in the relevant subsections. Table~\ref{tabpgr2} lists the data sets produced by the various programs, as well as the 
interactions between these programs. All these files are standard text files, with tab-separated columns. They are also available on my
GitHub repository: click on the filename to find an example of the corresponding data set. The fields attached to each data set are described in the section
covering the source code that produces it: for instance, Section~\ref{nnsc} for the data set 
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Data/PB_NN_dist_full.txt}{\texttt{PB\_NN\_dist\_full.txt}}, 
produced by 
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN.py}{\texttt{PB\_NN.py}}.


\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|l|}
\hline
  Filename & Output of & Input for & Description\\ %\hline \\
\hline
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Data/PB_main.txt}{\texttt{PB\_main.txt}} &
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_main.py}{\texttt{PB\_main.py}}& \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference.xlsx}} & one-dimensional stats  \\

\href{https://github.com/VincentGranville/Point-Processes/blob/main/Data/PB_radial.txt}{\texttt{PB\_radial.txt}} &
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_radial.py}{\texttt{PB\_radial.py}}& \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference.xlsx}} & points of radial process  \\

\href{https://github.com/VincentGranville/Point-Processes/blob/main/Data/PB_cc.txt}{\texttt{PB\_cc.txt}} &
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN_graph.py}{\texttt{PB\_NN\_graph.py}}&  & connected components  \\
 
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Data/PB_r.txt}{\texttt{PB\_r.txt}} &
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN.py}{\texttt{PB\_NN.py}}& 
     \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PP_NN_arrows.r}{\texttt{PB\_NN\_arrows.r}}  &  nearest neighbor graph  \\

\href{https://github.com/VincentGranville/Point-Processes/blob/main/Data/PB_NN.txt}{\texttt{PB\_NN.txt}} &
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN.py}{\texttt{PB\_NN.py}}& 
     \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference.xlsx}}  &  points of the process \\

\href{https://github.com/VincentGranville/Point-Processes/blob/main/Data/PB_NN_mod.txt}{\texttt{PB\_NN\_mod.txt}} &
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN.py}{\texttt{PB\_NN.py}}& 
     \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference.xlsx}}  &  points of the process ($\bmod{\frac{2}{\lambda}}$)  \\

\href{https://github.com/VincentGranville/Point-Processes/blob/main/Data/PB_NN_dist_full.txt}{\texttt{PB\_NN\_dist\_full.txt}} &
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN.py}{\texttt{PB\_NN.py}}& 
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN_graph.py}{\texttt{PB\_NN\_graph.py}}
     &  nearest neighbor distances  \\

\href{https://github.com/VincentGranville/Point-Processes/blob/main/Data/PB_NN_dist_small.txt}{\texttt{PB\_NN\_dist\_small.txt}} &
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN.py}{\texttt{PB\_NN.py}}& 
     \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference.xlsx}}  &  nearest neighbor distances\\

\href{https://github.com/VincentGranville/Point-Processes/blob/main/Images/PB-map.PNG}{\texttt{PB-map.PNG}} &
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/GD_util.py}{\texttt{GB\_util.py}}  &
     Figure \ref{fig:map} &  density and cluster map \\

\href{https://github.com/VincentGranville/Point-Processes/blob/main/Images/PB-hexa.png}{\texttt{PB-hexa.png}} &
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PP_NN_arrows.r}{\texttt{PB\_NN\_arrows.r}}  &
     Figure \ref{fig:hexa} &  nearest neighbor graph \\

\href{https://www.youtube.com/watch?v=FUxAeW4JEXA}{\texttt{av\_demo2c.mp4}} &
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Videos/av_demo.r}{\texttt{av\_demo.r}}  &
      &  video, Dirichlet eta function \\

\hline
\end{tabular}
\caption{\label{tabpgr2}Source code architecture: input/output data flow between modules}
\end{center}
\end{table} 

The spreadsheets accompanying this textbook are discussed in Section~\ref{spr}. They are also accessible from the same GitHub repository, 
\href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{here}.

%-----------------------------------
\subsection{Interactive Spreadsheets and Videos}\label{spr}
%-----------------------------------

Here I provide a brief overview of the spreadsheets and data animations (MP4 videos) referenced in the textbook. Most of the figures come from these documents. 
The content (columns, cells, formulas, graphs) is documented in details in the relevant material in the textbook. 
In Table~\ref{tabspread1}, I provide references to the sections where the corresponding material is discussed, for each tab of the two spreadsheets. %\pagebreak
  
\noindent  General guidelines:
\nopagebreak \begin{itemize}
\item Parameters highlighted in light yellow can be fine-tuned. Any change will be immediately visible on the graphs that are included in the same tab.
\item Do not change the other parameters.
\item Data produced directly or indirectly with the \texttt{RAND()} Excel function is updated each time you save the spreadsheet or change some parameters. New random deviates are automatically generated. This includes realizations of Poisson-binomial processes used for simulation purposes, and the resulting graphs.
\item Most of the time, tabs are self-contained: all the required data (for instance, to generate a chart) is produced from within the same tab. On some occasions, raw data requiring of lot of storage is not provided.
Instead, summary data is included in the spreadsheet. The source code to produce the summary data  is in the same tab where it is used. This allows for full
 replication. 
\item The spreadsheets are available on my GitHub repository. It is best to download them rather than view them locally on my repository. GitHub, Google Drive and other sharing websites have poor Excel viewing capabilities. They may be good for basic spreadsheets, but mine include a lot of Excel features that are poorly rendered (if at all) on these platforms.
\item The labels, headers and parameter names in the spreadsheet are compatible with those used in the textbook. 
\item I do not use macros, pivot tables, plug-ins, or other non-basic Excel features. A standard version of Excel 2013 (or above) is all you need.
\end{itemize}

\noindent The spreadsheets, \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_independence.xlsx}} and \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference.xlsx}}, are 
summarized in Table~\ref{tabspread1}.

\begin{table}[H]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
  Spreadsheet & Tab & Description & Section\\
\hline 
\href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_independence}} & \texttt{Periodicity} & periodicity of point count expectations & \ref{bbei} \\
\href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_independence}} & \texttt{Confidence\_Region} & standard and dual confidence regions & \ref{cils1}\\
\href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_independence}} & \texttt{MC\_Estimation} & minimum contrast estimation & \ref{cils1} \\
\href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_independence}} & \texttt{Summary} & testing independence of point counts & \ref{indep1}\\
\href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_independence}} & \texttt{Dataset\_A}, \texttt{B}, \texttt{C} & raw datasets used in \texttt{Summary} tab & \ref{indep1} \\
%\hline
\href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference}} & \texttt{N\_k} & expectation and variance of point count & \ref{estim1} \\
\href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference}}& \texttt{E(T\^}\texttt{2)} & second moment of interarrival times & \ref{estim1}\\
\href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference}} & \texttt{Rayleith\_Test} & model fitting, two dimensions & \ref{spa1} \\
\href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference}} & \texttt{Elbow\_Brownian} & elbow rule, Figure~\ref{fig:pbelbow1} & \ref{bbcl}\\
\href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference}}& \texttt{Elbow\_Riemann} &  elbow rule, Figure~\ref{fig:pbelbow2} & \ref{bbcl}\\
\href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference}}& \texttt{Video\_Riemann} & data animation, Dirichlet function & \ref{videocluster}\\
\hline
\end{tabular}
\caption{\label{tabspread1}Spreadsheet structure and references to textbook sections}
\end{center}
\end{table} 
\noindent {\bf Note About the Videos}

\noindent The material about the videos (data animation, MP4 files) and how to create them, is described in Section~\ref{videocluster}.


%----------------------------------
\subsection{Source Code: Point Count, Interarrival Times}\label{s:codeperl}

{On GitHub}: \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_main.py}{\texttt{PB\_main.py}}. The code below is used to compute two sets of statistics:
\begin{itemize}
\item Point count: $\mbox{E}[N(B)], \mbox{Var}[N(B)]$ and $P[N(B)=0]$, where $B=[a, b]$
\item Interarrival times: $\mbox{E}[T], \mbox{Var}[T]$ and $\mbox{E}[T^r]$, with $T=T(\lambda,s)$
\end{itemize}
The input parameters are $a, b, r>0$ and $\lambda>0$. The code below tests various values of $s$, between $s=0.05$ and $s\leq 0.40$,  with increments equal to 0.2. For the \gls{gls:pc}\index{point count distribution}, the theoretical Formulas (\ref{eq:f0}),  (\ref{eq:f1}) and (\ref{eq:f2}) are used, with 
the infinite sums truncated to $k$ between $-n_1$ and $n_1$. The default value is $n_1=10^3$. For the 
\gls{gls:ia}\index{interarrival times}, simulations are used instead, by generating one instance (realization) of a Poisson-binomial process consisting of $n_2=3\times 10^3$ points. Both $n_1$ and $n_2$ can be changed, but $s$ should always be much smaller than $n_2$. The computation of the main statistics use a subset of the $n_2$ points, to minimize
\textcolor{index}{boundary effects}.

Three options (Logistic, Cauchy and Uniform) are offered for the CDF (cumulative distribution function) $F$. These three options are stored in the \texttt{model} list. Two useful functions are provided: one computing the CDF (see Section \ref{scdf}), and one generating the corresponding deviates (see Section \ref{deviates}). The parameter \texttt{type} specifies which distribution $F$ is used at any time. The main program performs a loop on $s$, with an inner loop on the three CDF. The output is stored in a text file named \texttt{PB\_main.txt}. Table~\ref{tab124} is produced with this code. \vspace{1ex} 

The full program consists of all the pieces of code in Section~\ref{s:codeperl}, in the same order. You also need to add one instruction at the very bottom: \texttt{main()}. The reason is because in Python, functions must be defined above the main code that calls them. A workaround is to define the main code as a function, listed above all the other functions. This is what I did here: the main code is in the \texttt{main()} function, which must be called after all the other functions have been defined. This is also how it is implemented in the GitHub version.

\begin{lstlisting}
# PB_main.py

import math
import random
random.seed(100)

model=("Uniform","Logistic","Cauchy")
pi= 3.1415926535897932384626433
seed=4565   # allows for replicability (to produce same random numbers each time)

llambda=1   # represents lambda [lambda is reserved keyword in Python]
aa =-0.75   # B=[aa, bb] is the interval used to compute exact Expectation and var
bb = 0.75   # see aa
r  = 0.50   # to compute E[T^r], r>0
            # E[T^r] tends to r!/(lambda)^r as s tends to infinity
n1 = 10000  # compute E[N(B)], Var[N(B)]: k between -n1 and +n1
            # n1 much larger than s (if F has thick tail)
            # reduce n1 if program too slow [speed ~ O(n1 log n1
n2 = 30000  # Simulation: Xk with index k between -n2 and +n2

#---
def main():

  OUT=open('PB_main.txt',"w")    # computations saved in file pb.txt
  line = "Type\tlambda\ts\ta\tb\tr\tE[N]\tVar[N]\tP[N=0]\t";
  line = line+"E[T]\tVar[T]\tE[T^r]\n";
  OUT.write(line)

  for type in model:
    s=0.05
    while s <= 40:
      line=str(type)+"\t"+str(llambda)+"\t"+str(s)+"\t"+str(aa)+"\t"+str(bb)+"\t"+str(r)+"\t"
      print("F = ",type," | lambda = ",llambda," | s=",s)  # show progress on the screen
  
      # Compute E[N(B)], Var[(B)], P[B=0] via formula
      (exp,var,prod)=E_and_Var_N(type,llambda,s,aa,bb,n1)
      line=line+str(exp)+"\t"+str(var)+"\t"+str(prod)+"\t"

      # Compute E[T], Var[T] via simulations
      random.seed(seed)  # to produce same random deviates each time (for replicability)
      (exp,var,moment)=var_T(type,llambda,s,r,n2)
      line=line+str(exp)+"\t"+str(var)+"\t"+str(moment)+"\n"

      OUT.write(line)
      s=s+0.2

  OUT.close()

\end{lstlisting}
\subsubsection{Compute $\mbox{E}[N(B)], \mbox{Var}[N(B)]$ and $P[N(B)=0]$}\label{enpi}
This computation is done using the exact Formulas (\ref{eq:f0}),  (\ref{eq:f1}) and (\ref{eq:f2}). The input parameters are $\lambda,s, a, b$, with $B=[a,b]$. See code below.

\begin{lstlisting}
def E_and_Var_N(type,llambda,s,aa,bb,n):

  # Return E[N(B)], Var[N(B)] and P[N(B)=0] with B=[aa, bb]
  # expectation -> E[N(B)]
  # variance    -> Var[N(B)]
  # product     -> P[N(B)=0]
  # Type specifies the distribution F, lambda the intensity, s the scaling factor

  variance=0
  expectation=0
  product=0
  flag=0

  for k in range(-n1,n1+1): 
    f1=CDF(type,llambda,s,k,bb)
    f2=CDF(type,llambda,s,k,aa)
    if 1-f1+f2 == 0:  
      flag=1 
    else:
      product=product+math.log(1-f1+f2)
    expectation=expectation+(f1-f2)
    variance=variance+((f1-f2)*(1-f1+f2))
  if flag==1: 
    product=0 
  else:
    product=math.exp(product)
  return[expectation,variance,product]
\end{lstlisting}
\subsubsection{Compute $\mbox{E}[T], \mbox{Var}[T]$ and $\mbox{E}[T^r]$} \label{evm}
This computation is done via simulations. The input parameters are $\lambda,s, r$. See code below. 

\begin{lstlisting}
def var_T(type,llambda,s,r,n):

  # Return var(T) and E(T^r) computed on simulated data (2n+1 points)
  # Type specifies the distribution F, lambda the intensity, s the scaling factor
  # r=1 yields the expectation

  xs=[]
  m=0

  for k in range(-n,n+1): 
    ranx=random.random()
    xs.append(deviate(type,llambda,s,k))   
    m=m+1
  xs.sort() 
  expectation=0
  variance=0
  moment_r=0
  k1=int(m/4)
  k2=int(3*m/4)
  for k in range(k1,k2+1): 
    dist=(xs[k]-xs[k-1])
    expectation=expectation+dist
    variance=variance+(dist*dist)
    moment_r=moment_r+(dist**r)
  expectation=expectation/(k2-k1+1)
  variance=(variance/(k2-k1+1))-(expectation*expectation)
  moment_r=moment_r/(k2-k1+1)
  return[expectation,variance,moment_r]

\end{lstlisting} 
\subsubsection{Produce random deviates for various $F$'s} \label{deviates} 
Below is the code to generate \textcolor{index}{deviates}\index{deviate} from selected distributions (\textcolor{index}{uniform}\index{distribution!uniform}, \textcolor{index}{logistic}\index{distribution!logistic}, \textcolor{index}{Cauchy}\index{distribution!Cauchy}), using \textcolor{index}{inverse transform sampling} [\href{https://en.wikipedia.org/wiki/Inverse_transform_sampling}{Wiki}]\index{inverse transform sampling}. Note that these deviates are centered at $k$, which is an input parameter. To produce standard deviates (centered at the origin), set $k$ to zero. The \gls{gls:sf}\index{scaling factor} $s$ is a function of the variance $\sigma^2$. Table \ref{tab123} provides the conversion table between $s$ and $\sigma^2$. A standard reference on this topic is Ripley \cite{ripley}. See also \cite{hbsimul}.

\begin{lstlisting}
def deviate(type,llambda,s,k):

  # Generate random deviate for F determined by type
  # centered at k/lambda, scaling factor s

  ranx=random.random()
  if type == "Logistic":
    z=k/llambda+s*math.log(ranx/(1-ranx))
  elif type == "Uniform":
    z=k/llambda+2*s*(ranx-1/2)
  elif type == "Cauchy":
    z=k/llambda+s*math.tan(pi*(ranx-1/2))
  return(z)
\end{lstlisting}
\subsubsection{Compute $F(x)$ for Various $F$}\label{scdf}
Below is the code to compute $F(x)$, the value of the cumulative distribution (CDF) for selected distributions, given $x, \lambda$ and $s$. Note that these CDF's are centered at $k$, which is an input parameter. To use for the standard CDF (centered at the origin), set $k$ to zero. The scaling factor $s$ is a function of the variance $\sigma^2$. Table \ref{tab123} provides the conversion table between $s$ and $\sigma^2$.  

\begin{lstlisting}
def CDF(type,llambda,s,k,x):

  # Returns F((x-k/lambda)/s), with F determined by type

  if type == "Logistic":
    z= 1/2+ (1/2)*math.tanh((x-k/llambda)/(2*s))
  elif type == "Uniform":
    z= 1/2 + (x-k/llambda)/(2*s)
    if z<=0: 
      z=0
    if z>1: 
      z=1
  elif type == "Cauchy":
    z= 1/2 +math.atan((x-k/llambda)/s)/pi;
  return(z)
\end{lstlisting}

\subsection{Source Code: Radial Cluster Simulation}\label{s:code2}

{On GitHub}: \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_radial.py}{\texttt{PB\_radial.py}}. Simulates realizations of the cluster processes featured in Sections~\ref{s:clp} and \ref{illus}, to produce 
Figures~\ref{fig:pbr4b} and \ref{fig:pbr}. The \textcolor{index}{parent process}\index{parent process} is Poisson-binomial with $F$ being uniform, $\lambda=1$, and consisting of the blue crosses in Figure \ref{fig:pbr}. The points of this process are called the centers. A radial process -- the \textcolor{index}{child process}\index{child process} -- with up to 15 points, is attached to each random center $(X, Y)$. It shows up as green dots in Figure~\ref{fig:pbr}. 
Each  point $(X', Y')$ of this child process is generated as follows:
\begin{align}
X' & = X + \log\Big(\frac{U}{1-U}\Big) \cos(2\pi V)\nonumber\\
Y' & = Y + \log\Big(\frac{U}{1-U}\Big) \sin(2\pi V)\nonumber
\end{align}
where $U,V$ are independent uniform deviates on $[0,1]$. 

\begin{lstlisting}
# PB_radial.py 

import math
import random
random.seed(100)

s=10

pi=3.14159265358979323846264338

file=open('PB_radial.txt',"w")
for h in range(-30,31): 
    for k in range(-30,31): 

         # Create the center (parent Poisson-binomial process, F uniform)

         ranx=random.random()
         rany=random.random()
         x=h+2*s*(ranx-1/2)
         y=k+2*s*(rany-1/2)
         line=str(h)+"\t"+str(k)+"\tCenter\t"+str(x)+"\t"+str(y)+"\n"
         file.write(line)

        # Create the child, radial process (up to 15 points per center)

         M=int(15*random.random())

         for m in range(M): 
             ran1=random.random()
             ran2=random.random()
             factor=math.log(ran2/(1-ran2))
             x1=x+factor*math.cos(2*pi*ran1);
             y1=y+factor*math.sin(2*pi*ran1);
             line=str(h)+"\t"+str(k)+"\tLocal\t"+str(x1)+"\t"+str(y1)+"\n"
             file.write(line)
file.close()
\end{lstlisting}

%-----------------------------------------------------
\subsection{Source Code: Nearest Neighbor Distances}\label{nnsc}

{On GitHub}: \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN.py}{\texttt{PB\_NN.py}}. Generates points and computes \textcolor{index}{nearest neighbor distances}\index{nearest neighbors!nearest neighbor distances} and related statistics, for a realization of a superimposition
of $m$ shifted stretched Poisson-binomial processes (also called \gls{gls:mip}\index{$m$-interlacing}), as defined in Section~\ref{sm1}, using Formulas~\ref{simm1} and
\ref{simm2} for the simulation of each individual point process. The output data consists of text files with tab-separated columns. They are used to study the distribution of nearest neighbor distances and statistical testing,
and as input files for 
    \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PP_NN_arrows.r}{\texttt{PB\_NN\_arrows.r}}, 
    \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN_graph.py}{\texttt{PB\_NN\_graph.py}} and 
    \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/GD_util.py}{\texttt{GD\_util.py}}. 

\noindent The source code is divided into 5 parts.  \\
\quad \\
\noindent {\bf Part 1} consists of initializing the following global variables:
\begin{itemize}
\item \texttt{Nprocess}: The number $m$ of processes used to create the superimposed process.
\item \texttt{seed}: To initialize the pseudo-random number generator, so that the same data is produced each time the program is run, for easy replication.
\item \texttt{s}: The \gls{gls:sf}\index{scaling factor} $s$ (note that $\lambda = 1$) 
\item \texttt{ShiftX[i]}, \texttt{ShiftY[i]}: X- and Y-coordinates of the \gls{gls:sv}\index{shift vector} (arrays)
\item \texttt{StretchX[i]}, \texttt{StretchY[i]}:  \textcolor{index}{Stretching factors}\index{stretching (point process)} for the X and Y axes; set to $1$ here.
\item \texttt{epsilon}: Used for numerical stability.
\item \texttt{processID}: Index of the point process (among the $m$ point processes) currently accessed, in any loop.
\item \texttt{bitmap}: $400 \times 400$ array to store and process an image in memory.
\item \texttt{string}: Used for Excel compatibility to easily create scatterplots with multiple colors: one color for each \texttt{processID}. Should be replaced 
by a single TAB character if you don't use Excel; otherwise it consists of multiple TABs.
\end{itemize}
\noindent The fields of the output files are provided in Parts 2 to 5, where they are used.

\begin{lstlisting}
# PB_NN.py  
# lambda = 1

import numpy as np
import math
import random

# PART 1: Initialization

Nprocess=5                  # number of processes in the process superimposition
s=0.15                      # scaling factor
method=0        # method=0 is fastest
NNflag=False   # set to True if you need to compute NN distances
window=10     # determines size of local filter [the bigger, the smoother]
nloop=3       # number of times the image is filtered [the bigger, the smoother]

epsilon=0.0000000001 # for numerical stability
seed=82431                  # arbitrary number
random.seed(seed)           # initialize random generator 

sep="\t"      # TAB character 
shiftX=[]
shiftY=[]
stretchX=[]
stretchY=[]
a=[]
b=[]
process=[]
sstring=[]   # string in Perl version

for i in range(Nprocess) :
  shiftX.append(random.random())
  shiftY.append(random.random())
  stretchX.append(1.0)
  stretchY.append(1.0)
  sstring.append(sep)   
  # i TABs separating x and y coordinates in output file for points
  # originating from process i; Used to easily create a scatterplot in Excel 
  # with a different color for each process.
  sep=sep + "\t"

processID=0
m=0  # number of points generated
height,width = (400, 400)

bitmap = [[255 for k in range(height)] for h in range(width)]
\end{lstlisting}
\quad \\
\noindent {\bf Part 2} generates a realization of $m$ superimposed stretched shifted Poisson-binomial point processes, called \gls{gls:mip}\index{$m$-interlacing}; 
$m$ is represented by the variable \texttt{Nprocess}.
The \gls{gls:index1}\index{index!index space} is limited to $(h,k)\in \{-25,\dots,25\} \times \{-25,\dots,25\}$. The points of the process,
along with their \textcolor{index}{lattice index}\index{lattice!lattice index} $(h,k)$ and the individual process they belong to (\texttt{processID}),
are saved in the output file \texttt{PB\_NN.txt}. A subset of these points, those with coordinates in
$[-20,20]\times [20,20]$, this time taken \gls{gls:modulo}\index{modulo operator (point processes)} $2/\lambda$ (with $\lambda=1$), are saved in the 
$\texttt{bitmap}$ array for further processing as well as in the output file \texttt{PB\_NN\_mod.txt}. 

The restriction to a subset is to mitigate 
\glspl{gls:be}\index{boundary effect}. Taking the modulo allows you to magnify the patterns in the 
\gls{gls:pb55}\index{point distribution}, to make
statistical inference easier and to make the underlying shift-induced clustering structure visible to the naked eye.  The modulo function is defined as follows: 
$x \mod \frac{2}{\lambda} = x - \frac{2}{\lambda}\lfloor x \cdot \frac{\lambda}{2}\rfloor$ 
where the brackets represent the integer function, also called floor function.

\begin{lstlisting}
# PART 2: Generate point process, its modulo 2 version; save to bitmap and output files.

OUT  = open("PB_NN.txt", "w")                # the points of the process 
OUT2 = open("PB_NN_mod.txt", "w") # the same points modulo 2/lambda both in x and y directions

for h in range(-25,26):   
    for k in range(-25,26):  
        for processID in range(Nprocess): 
            ranx=random.random()
            rany=random.random()
            x=shiftX[processID]+stretchX[processID]*h+s*math.log(ranx/(1-ranx)) 
            y=shiftY[processID]+stretchY[processID]*k+s*math.log(rany/(1-rany))
            a.append(x)  # x coordinate attached to point m
            b.append(y)  # y coordinate attached to point m
            process.append(processID) # processID attached to point m
            m=m+1
            line=str(processID)+"\t"+str(h)+"\t"+str(k)+"\t"+str(x)+sstring[processID]+str(y)+"\n"
            OUT.write(line)
            # replace sstring[processID] by \t if you don't care about Excel

            if x>-20 and x<20 and x>-20 and x<20:
                xmod=1+x-int(x)   # x modulo 2/lambda
                ymod=1+y-int(y)   # y modulo 2/lambda
                pixelX=int(width*xmod/2)   
                pixelY=int(height*(2-ymod)/2) # pixel (0,0) at top left corner
                bitmap[pixelX][pixelY]=processID
                line=str(xmod)+sstring[processID]+str(ymod)+"\n"
                OUT2.write(line)  
                # replace sstring[processID] by \t if you don't care about Excel
OUT2.close()
OUT.close()
\end{lstlisting}
\quad \\
\noindent {\bf Part 3} detects the \textcolor{index}{nearest neighbor(s)}\index{nearest neighbors} to each point of the
process, and compute the \textcolor{index}{nearest neighbor distances}\index{nearest neighbors!nearest neighbor distances}. Only points in $[-20,20]\times[20,20]$ are considered, to mitigate boundary effects. The main loop is over all points of the process. Per convention, variables with the keyword ``hash" in their name, represent
\textcolor{index}{hash tables}\index{hash table}. The output file \texttt{PB\_NN\_dist\_small.txt} contains all that is needed to study the distribution of nearest neighbor distances for model-fitting purposes
(see Section~\ref{ssnn}). 

The output file \texttt{PB\_NN\_dist\_full.txt} contains more fields, including the points and their nearest neighbor(s); this information is used to
compute the \glspl{gls:cc}\index{connected components}\index{graph!connected components}
in the program \texttt{PB\_NN\_graph.py}. Here the variable \texttt{m} represents the number of points of the process. For each point \texttt{i},
\begin{itemize}
\item \texttt{a[i]}, \texttt{b[i]} are the X and Y coordinate of point \texttt{i}.
\item \texttt{NNidx[i]} is a nearest neighbor to point \texttt{i} (usually unique unless $s=0$), and \texttt{NNx[i]}, \texttt{NNy[i]} are its X and Y coordinates.
\item \texttt{mindist} is the distance between point \texttt{i} and its nearest neighbor point \texttt{NNidx[i]}.
\item \texttt{NNidxHash[i]} is the list of points having \texttt{i} as nearest neighbor (separated by the character ``\textasciitilde")
\end{itemize}

\begin{lstlisting}
# PART 3: Find nearest neighbor points, and compute nearest neighbor distances.

if NNflag:

  OUT  = open("PB_NN_dist_small.txt", "w")     # the points of the process 
  OUTf = open("PB_NN_dist_full.txt", "w") # the same points modulo 2/lambda both in x and y directions

  NNx=[]
  NNy=[]
  NNidx=[]
  NNidxHash={}

  for i in range(m):
    NNx.append(0.0)
    NNy.append(0.0)
    NNidx.append(-1)
    mindist=99999999
    flag=-1
    if a[i]>-20 and a[i]<20 and b[i]>-20 and b[i]<20: 
      flag=0;
      for j in range(m):
        dist=math.sqrt((a[i]-a[j])**2 + (b[i]-b[j])**2)  # taxicab distance faster to compute
        if dist<=mindist+epsilon and i!=j: 
          NNx[i]=a[j]  # x-coordinate of nearest neighbor of point i
          NNy[i]=b[j]  # y-coordinate of nearest neighbor of point i
          NNidx[i]=j      # indicates that point j is nearest neighbor to point i
          #  NNidxHash[i] is the list of points having point i as nearest neighbor;
          #  these points are separated by "~" (usually only one point in NNidxHash[i]
          #  unless the simulated points are exactly on a lattice, e.g. if s = 0)
          if abs(dist-mindist) < epsilon: 
            NNidxHash[i]=NNidxHash[i]+"~"+str(j) 
          else:    
            NNidxHash[i]=str(j) 
          mindist=dist 
      if i % 100 == 0: 
        print("Finding Nearest neighbors of point",i)
      line=str(i)+"\t"+str(mindist)+"\n"
      OUT.write(line) 
      line=str(i)+"\t"+str(NNidx[i])+"\t"+str(NNidxHash[i])+"\t"+str(a[i])+"\t" 
      line=line+str(b[i])+"\t"+str(NNx[i])+"\t"+str(NNy[i])+"\t"+str(mindist)+"\n"
      OUTf.write(line) 

  OUTf.close()
  OUT.close()
\end{lstlisting}
\quad \\
\noindent {\bf Part 4} produces the output file \texttt{PB\_r.txt} used by \texttt{PB\_NN\_arrows.r} to generate Figure~\ref{fig:hexa}.
This file consists of the points of the process, with for each point \texttt{idx}:  
its X and Y coordinates \texttt{a[idx]}, \texttt{b[idx]}, its nearest neighbor point \texttt{NNindex}, the X and Y coordinates
\texttt{a[NNindex]}, \texttt{b[NNindex]} of point \texttt{NNindex}, and the individual process \texttt{process[idx]} that \texttt{idx} belongs to (for coloring purposes).

\begin{lstlisting}
# PART 4: Produce data to use in R code that generates the nearest neighbors picture.

if NNflag:

  OUT  = open("PB_r.txt","w")     
  OUT.write("idx\tnNN\tNNindex\ta\tb\taNN\tbNN\tprocessID\tNNprocessID\n")

  for idx in NNidxHash:
    NNlist=NNidxHash[idx]
    list=NNlist.split("~")
    nelts=len(list)
    for n in range(nelts): 
      NNindex=int(list[n])
      line=str(idx)+"\t"+str(n)+"\t"+str(NNindex)+"\t"+str(a[idx])+"\t"+str(b[idx])
      line=line+"\t"+str(a[NNindex])+"\t"+str(b[NNindex])+"\t"+str(process[idx])
      line=line+"\t"+str(process[NNindex])+"\n"
      OUT.write(line)  
                
  OUT.close()
\end{lstlisting}
\quad \\
\noindent {\bf Part 5} consists of a single call the the function \texttt{GD\_Maps} in 
\href{https://bit.ly/3vmEGek}{\texttt{GD\_util.py}} (see Section~\ref{plgd}) to produce two images:
one representing the point density of the point process (to identify cluster centers, corresponding to the darkest gray level), and one representing (by a color) how each future, unobserved point should
be classified based on its X and Y coordinates in the context of supervised clustering. See Figure~\ref{fig:residues} (original point process), and 
\ref{fig:map} after clustering / density estimation.

The X and Y coordinates are taken 
 \gls{gls:modulo}\index{modulo operator (point processes)} $2/\lambda$; here $\lambda=1$ (see Part 2 of this source code)
and thus cover the entire, infinite 2-D space. The choice of the modulus (here $2/\lambda$, rather than $1/\lambda$) is dictated by the granularity of the underlying
\gls{gls:lattice1}\index{lattice!lattice space}. The image is first processed
in memory (the \texttt{bitmap} array) before being saved to PNG files (\texttt{pb-cluster3.png} and \texttt{pb-density3.png}). An
high-pass (sharpening) kernel-based filter is applied 
\texttt{nloop} times to the entire bitmap image, using a $p\times p$ pixels filtering window. For a large image of fixed size, filtering the entire image once is $O(p^2)$
 but can be reduced to $O(p)$ with a smarter implementation (see Exercise~\ref{fastfilter}). The variable \texttt{window} represents $p$.
See section~\ref{ssnn} for details. 

\begin{lstlisting}
# PART 5: Creates density and cluster images.

img_cluster="PB-cluster"  # use for output image filenames
img_density="PB-density"  # use for output image filenames

from GD_util import *
GD_Maps(method,bitmap,Nprocess,window,nloop,height,width,img_cluster,img_density)
\end{lstlisting}

%-----------------------------------------------------
\subsection{Source Code: Detection of Connected Components}\label{scc}

{On GitHub}: \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN_graph.py}{\texttt{PB\_NN\_graph.py}}. Creates the list of all 
\glspl{gls:cc}\index{graph!connected components}\index{connected components} of an 
\textcolor{index}{undirected graph}\index{graph!undirected}, 
for instance the  \gls{gls:nng}\index{graph!nearest neighbor graph}\index{nearest neighbors!nearest neighbor graph} of a point process. Two points are considered connected if one of the two points is the nearest neighbor to the other one. 
See Exercise~\ref{exercise14g}.  

\noindent The source code is divided into 3 parts. \\
\quad \\
\noindent {\bf Part1} reads the first two columns of \texttt{PB\_dist\_full.txt} produced by 
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN.py}{\texttt{PP\_NN.py}} (see Section~\ref{nnsc}). The first column represents the index 
\texttt{idx} of a point, and \texttt{NNidx[idx]} (in the second column) is the index of a point that has point \texttt{idx} as nearest neighbor. 

\noindent Then, it creates the undirected graph \texttt{hash}, as follows:  if a point with index \texttt{k} is nearest neighbor to a point with index 
\texttt{idx}, add point \texttt{idx} to 
\texttt{hash[k]}, and add point \texttt{k} to \texttt{hash[idx]}. 
Thus \texttt{hash[idx]} contains all the points (their indices) directly connected to point \texttt{idx}; the points are separated
by the character ``\textasciitilde". 

\begin{lstlisting}
# PB_NN_graph.py
#
# Compute connected components of nearest neighbor graph
# Input file has two tab-separated columns: idx and idx2
#   idx is the index of of point, idx2 is the index of a nearest neighbor to idx
# Output file has two fields, for each principal component:
#   the list of points it is made up (separated by ~), and the number of points

# Example. 

# Input:

# 100	101
# 100	103
# 101	100
# 101	102
# 103	100
# 103	102
# 102	101
# 102	100
# 102	103
# 102	104
# 104	102
# 106	105
# 105	107

# Output:

# ~100~103~102~104~101    5
# ~106~105~107    3

# PART 1: Initialization. 

point=[]
NNIdx={}
idxHash={}

n=0
file=open('PB_dist_full.txt',"r") # input file 
lines=file.readlines()
for aux in lines:
  idx =int(aux.split('\t')[0])
  idx2=int(aux.split('\t')[1])
  if idx in idxHash:
    idxHash[idx]=idxHash[idx]+1
  else:
    idxHash[idx]=1
  point.append(idx)
  NNIdx[idx]=idx2
  n=n+1
file.close()

hash={}
for i in range(n):
  idx=point[i]
  if idx in NNIdx:
    substring="~"+str(NNIdx[idx])
  string="" 
  if idx in hash:
    string=str(hash[idx])
  if substring not in string: 
    if idx in hash:
      hash[idx]=hash[idx]+substring 
    else:
      hash[idx]=substring  
  substring="~"+str(idx)
  if NNIdx[idx] in hash: 
    string=hash[NNIdx[idx]]
  if substring not in string: 
    if NNIdx[idx] in hash:
      hash[NNIdx[idx]]=hash[NNIdx[idx]]+substring 
    else:
      hash[NNIdx[idx]]=substring 
\end{lstlisting}
\quad \\
\noindent {\bf Part 2}: Find the connected components. The algorithm is as follows. Browse the list of points. If a point \texttt{idx} has not yet been assigned to a connected component, 
create a new connected component \texttt{cliqueHash[idx]} containing \texttt{idx}; find the points connected to \texttt{idx}, 
add them to the stack (\texttt{stack}). Find the points connected to the points connected to \texttt{idx}, and so on recursively, until no more points can be added. Each time a point is added to \texttt{cliqueHash}, decrease the stack size by one. It takes
about $2n$ steps to find all the connected components, where $n$ is the number of points. This algorithm does not use recursive functions; it uses a stack instead, which emulates recursivity. 

\begin{lstlisting}
# PART 2: Find the connected components 

i=0;
status={}
stack={}
onStack={}
cliqueHash={}

while i<n:

  while (i<n and point[i] in status and status[point[i]]==-1):  
    # point[i] already assigned to a clique, move to next point
    i=i+1

  nstack=1
  if i<n:
    idx=point[i]
    stack[0]=idx;   # initialize the point stack, by adding idx 
    onStack[idx]=1;
    size=1  # size of the stack at any given time

    while nstack>0:  
      idx=stack[nstack-1]
      if (idx not in status) or status[idx] != -1: 
        status[idx]=-1  # idx considered processed
        if i<n:  
          if point[i] in cliqueHash:
            cliqueHash[point[i]]=cliqueHash[point[i]]+"~"+str(idx)
          else: 
            cliqueHash[point[i]]="~"+str(idx)
        nstack=nstack-1 
        aux=hash[idx].split("~")
        aux.pop(0)  # remove first (empty) element of aux
        for idx2 in aux:
          # loop over all points that have point idx as nearest neighbor
          idx2=int(idx2)
          if idx2 not in status or status[idx2] != -1:   
            # add point idx2 on the stack if it is not there yet
            if idx2 not in onStack: 
              stack[nstack]=idx2
              nstack=nstack+1
            onStack[idx2]=1
\end{lstlisting}
\quad \\
\noindent {\bf Part 3} saves the result to output text file \texttt{PB\_cc.txt}. Each row corresponds to a connected component. The
first column stores the connected component, as a string of point indices, separated by the character ``\textasciitilde". The second column is the size (number of points)
in the connected component in question.

\begin{lstlisting}
# PART 3: Save results.

file=open('PB_cc.txt',"w")
for clique in cliqueHash:
  count=cliqueHash[clique].count('~') 
  line=cliqueHash[clique]+"\t"+str(count)+"\n"
  file.write(line)
file.close()
\end{lstlisting}

%-------------------------------------------------------
\subsection{Source Code: Visualizations, Density Maps}\label{visusc}

The code here produces the PNG images related to the clustering algorithms, and for Figure~\ref{fig:hexa}. Also, it is used to produce the PNG frames for some of the videos (MP4 files) in
Section~\ref{visusc}.  This section requires some familiarity with basic image processing techniques such as
color allocation, equalization or filtering, applied to clustering problems. The code still remains basic, and can be used as an introduction to image processing techniques for software engineers and scientists. It will teach you how to create an image pixel by pixel in some automated way, if you never tried before.  

\subsubsection{Visualizing the Nearest Neighbor Graph}\label{rrr4}

{On GitHub}: \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PP_NN_arrows.r}{\texttt{PB\_NN\_arrows.r}}. Based on output data from 
       \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN.py}{\texttt{PB\_NN.py}}, produces an image showing the nearest neighbor points
     across $m$ superimposed point processes (or a mixture of point processes), as defined in Section~\ref{sm1}. Each arrow points from a point of the process, to its nearest neighbor(s). 
    The color attached to each point indicates the process it belongs to, among the $m$ processes used to generate the superimposition. See
    Figure~\ref{fig:hexa}. 

The code below handles a superimposition of up to 5 point processes, but can easily be generalized to more than 5. Choosing many colors that are well contrasted
and well rendered on the screen, is a science. It will be discussed in one of my upcoming books. The input file here is \texttt{PB\_r.txt}, produced by \texttt{PB\_NN.py}. Only a small window is displayed on the screen: $(x, y)\in [0, 5] \times [0, 5]$,
to avoid cluttering. Try with $[-5, 5] \times [-5,5]$, using the same input file and modifying the \texttt{c()} parameter accordingly in the 
\texttt{plot} function. The result is still nice. 

My R code uses the \href{https://www.cairographics.org/}{Cairo graphics library} [\href{https://en.wikipedia.org/wiki/Cairo_(graphics)}{Wiki}] to 
produce better, smoother graphics: this library uses anti-aliasing techniques [\href{https://en.wikipedia.org/wiki/Spatial_anti-aliasing}{Wiki}], making R
plots look much nicer. For details, see \href{https://www.rdocumentation.org/packages/Cairo/versions/1.5-14/topics/Cairo}{here}. The output image is 
\texttt{PB\_hexa2.png}, integrated in Figure~\ref{fig:hexa} in this textbook.  The \texttt{arrows} function and its parameters are discussed
\href{https://stat.ethz.ch/R-manual/R-devel/library/graphics/html/arrows.html}{here}.

\begin{lstlisting}
# install.packages('Cairo')
library('Cairo');
# CairoWin(6,6);
CairoPNG(filename = "c:/Users/vince/tex/PB-hexa2.png", width = 600, height = 600);

data<-read.table("c:/Users/vince/tex/PB_r.txt",header=TRUE);
a<-data$a;  # x coordinate of point of the superimposed/mixture process 
b<-data$b;  # y coordinate of point of the superimposed/mixture process 
aNN<-data$aNN;  # x coordinate of nearest neighbor point to (a,b) across all processes
bNN<-data$bNN;  # y coordinate of nearest neighbor point to (a,b) across all processes
processID<-data$processID;

plot(a,b,xlim=c(0,5),ylim=c(0,5),pch=20,cex=0,
                    col=rgb(0,0,0),xlab="",ylab="",axes=TRUE  );
arrows(a, b, aNN, bNN, length = 0.10, angle = 10, code = 2,col=rgb(0.7,0.7,0.7));

aa<-data$a[processID == 0];
bb<-data$b[processID == 0];
points(aa,bb,col=rgb(1,0,0),pch=20,cex=1.75);

aa<-data$a[processID == 1];
bb<-data$b[processID == 1];
points(aa,bb,col=rgb(0,0,1),pch=20,cex=1.55);

aa<-data$a[processID == 2];
bb<-data$b[processID == 2];
points(aa,bb,col=rgb(1,0.7,0),pch=20,cex=1.75);

aa<-data$a[processID == 3];
bb<-data$b[processID == 3];
points(aa,bb,col=rgb(0,0,0),pch=20,cex=1.75);

aa<-data$a[processID == 4];
bb<-data$b[processID == 4];
points(aa,bb,col=rgb(0,0.7,0),pch=20,cex=1.75);

dev.off();

\end{lstlisting}

\subsubsection{Clustering and Density Estimation via Image Filtering}\label{plgd}

{On GitHub}: \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/GD_util.py}{\texttt{GD\_util.py}}. Small, easy-to-use home made graphics library
consisting of one function \texttt{GD\_Maps}, relying on the Pillow library, to produce PNG images (bitmaps). Produces
density and cluster maps such as Figure~\ref{fig:map}, as an alternative to traditional contour plots [\href{https://scipython.com/book/chapter-7-matplotlib/examples/a-simple-contour-plot/}{Wiki}], using image filtering and enhancing techniques including
equalization. This library is used in
 \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN.py}{\texttt{PB\_NN.py}}, at the very end. \\
\quad \\
\noindent {\bf Part 1} initializes the color palette for the cluster image. The input parameters of the function \texttt{GD\_Maps} are
 \texttt{window}, the size of the filtering window (see Section~\ref{spa2}), \texttt{nloop}, the number of times the image is filtered,
\texttt{img\_cluster} and \texttt{img\_density}, the names of the output PNG images, and \texttt{bitmap}, a two-dimensional $400\times 400$
array representing the point process in a format suitable for image processing.  

Before describing \texttt{bitmap}, 
let's quickly summarize the observed data. It consists of a simulation of $m$ superimposed stretched shifted Poisson-binomial point processes  $P_1,\cdots,P_m$, as described in Exercise~\ref{exercise14e} and Sections~\ref{sm1}, \ref{sm2} and \ref{ssnn}. An observed point $(x,y)=(X_{ih}, Y_{ik})$
in the \gls{gls:state1}  \index{state space} is a point such that
$(x,y)\in P_i$, and $(h, k)$ is the \textcolor{index}{index}
in the \gls{gls:index1}\index{index!index space}, with $h,k\in\{-n,\dots,n\}$. I used $n=25$ and $m=5$ in \texttt{PB\_NN.py}, the parent Python script that calls \texttt{GD\_Maps}.  For the simulation, see source code \texttt{PB\_NN.py} (Section~\ref{nnsc}, Part 2), or Formulas (\ref{simm1}) and (\ref{simm2}).

Now I can describe \texttt{bitmap}.  Initially, \texttt{bitmap[pixelX][pixelY]=255}, unless there is a point of the process,
 say $(x,y)\in P_i$,  such that  \texttt{pixelX=}$\lfloor 200 \times (x \bmod \frac{2}{\lambda}) \rfloor$  and 
\texttt{pixelY=}$\lfloor 200 \times (y \bmod \frac{2}{\lambda}) \rfloor$.
In that case, \texttt{bitmap[pixelX][pixelY]=processID}, where \texttt{processID} is the variable representing $i-1$  in the
source code. The brackets represent the floor function (also called integer function).

\begin{lstlisting}
import math
from PIL import Image, ImageDraw           # ImageDraw to draw rectangles etc.

def GD_Maps(method,bitmap,Nprocess,window,nloop,height,width,img_cluster,img_density):

  # PART 1: Allocate first image (clustering), including colors (palette)

  img1  = Image.new( mode = "RGBA", size = (width, height), color = (0, 0, 0) )
  pix1  = img1.load()   # pix[x,y]=col[n] to modify the RGB color of a pixel
  draw1 = ImageDraw.Draw(img1,"RGBA")

  col1=[] 
  col1.append((255,0,0,255))
  col1.append((0,0,255,255))
  col1.append((255,179,0,255))
  col1.append((0,0,0,255))
  col1.append((0,179,0,255))
  for i in range(Nprocess,256):
    col1.append((255,255,255,255))
  oldBitmap = [[255 for k in range(height)] for h in range(width)]
  densityMap= [[0.0 for k in range(height)] for h in range(width)]
  for pixelX in range(0,width): 
    for pixelY in range(0,height): 
      processID=bitmap[pixelX][pixelY]
      pix1[pixelX,pixelY]=col1[processID] 
  draw1.rectangle((0,0,width-1,height-1), outline ="black",width=1)
  fname=img_cluster+'.png'
  img1.save(fname)
\end{lstlisting}
\quad \\
\noindent {\bf Part 2} performs supervised clustering in bitmap by filtering the entire bitmap \texttt{nloop} times. It also creates and filters
\texttt{densityMap}, another bitmap with same dimensions, this time for unsupervised clustering, and using a slightly different filter. Both filters take place within the same loop.
The contribution $g(u,v)$ of a point $(u, v)$, in the small moving window (the local filter), is function of its distance to the center of the
window: $g(u,v)=(1+u^2+v^2)^{-1/2}$. Also, for unsupervised clustering, successive applications of the filter to the entire image are increasingly dampened. The purpose is to get the algorithm to converge to a meaningful solution. For details, see Section~\ref{ssnn}.
Finally, boundary effects\index{boundary effect} are taken care of.

\begin{lstlisting}
  # PART 2: Filter bitmap and densityMap 

  for loop in range(nloop): #  

    print("loop",loop,"out of",nloop)
    for pixelX in range(0,width): 
      for pixelY in range(0,height): 
        oldBitmap[pixelX][pixelY]=bitmap[pixelX][pixelY]

    for pixelX in range(0,width): 
      for pixelY in range(0,height):   
        count=[0] * Nprocess
        density=0
        maxcount=0
        topProcessID=255 # dominant processID near (pixelX, pixelY)
        for u in range(-window,window+1): 
          for v in range(-window,window+1):
            x=pixelX+u
            y=pixelY+v
            if x<0: 
              x+=width      # boundary effect correction
            if y<0: 
              y+=height     # boundary effect correction
            if x>=width: 
              x-=width      # boundary effect correction
            if y>=height: 
              y-=height     # boundary effect correction
            if method == 0:
              dist2=1
            else: 
              dist2=1/math.sqrt(1+u*u + v*v)
            processID=oldBitmap[x][y]
            if processID < 255: 
              count[processID]=count[processID]+dist2
              if count[processID]>maxcount: 
                maxcount=count[processID]
                topProcessID=processID
              density=density+dist2 
        density=density/(10**loop)   # 10 at power loop (dampening)
        densityMap[pixelX][pixelY]=densityMap[pixelX][pixelY]+density
        bitmap[pixelX][pixelY]=topProcessID
\end{lstlisting}
\quad \\
\noindent {\bf Part 3} assigns the right color to each pixel of the supervised clustering image and generates the associated PNG output file. It also generates a highly granular histogram of the density values observed in the unsupervised clustering image. The histogram, used in Part 4,  is stored in the hash table \texttt{densityCountHash}. 
\begin{lstlisting}
    # PART 3:  Some pre-processing; output cluster image

    densityCountHash={}  # use to rebalance gray levels
    for pixelX in range(0,width): 
      for pixelY in range(0,height):   
        topProcessID=bitmap[pixelX][pixelY]
        density=densityMap[pixelX][pixelY]
        if density in densityCountHash:
          densityCountHash[density]=densityCountHash[density]+1
        else:
          densityCountHash[density]=1
        pix1[pixelX,pixelY]=col1[topProcessID]

    draw1.rectangle((0,0,width-1,height-1), outline ="black",width=1)
    fname=img_cluster+str(loop)+'.png'
    img1.save(fname)
\end{lstlisting}
\quad \\
\noindent {\bf About the cluster image} (see right plot in Figure~\ref{fig:map}): each point in the \gls{gls:state1}\index{state space} (\gls{gls:modulo}\index{modulo operator (point processes)} $2/\lambda$) colored in red, must be assigned to the red cluster, or in other words, classified to red. The same applies to the other colors, and the assignment mechanism is extremely fast. Each color is attached to one of the individual processes of the model; each process (its points generated via simulation) can be seen as a particular cluster of a training set 
(see right plot in Figure~\ref{fig:residues}).  So the code performs a very fast supervised clustering of the entire state space; the clustering algorithm is represented by the cluster image itself. See Section~\ref{ssnn} for details. \\
\quad \\
\noindent {\bf Part 4} equalizes the density levels in the unsupervised clustering image, then allocates the image, allocates the gray levels in the palette, and save the density image 
(corresponding to unsupervised clustering) as a PNG file. I manually selected the thresholds in the equalizer algorithm for best visual
impact; this needs to be automated.  The result of the equalizer is this: the darkest areas in the image (left plot, Figure~\ref{fig:map}) correspond to the highest 
concentration of points in the \gls{gls:state1}\index{state space} modulo $2/\lambda$. This is where the mass of each cluster is concentrated. The cluster centers (darkest in the image) are the estimators of the \glspl{gls:sv}\index{shift vector} used to build the superimposed point process (one shift vector per individual process).
The unsupervised clustering is performed on the observed data shown in the right plot in Figure~\ref{fig:residues}, assuming the colors are not known. Detailed explanations are in Section~\ref{ssnn}. 

\begin{lstlisting}
    # PART 4: Equalize gray levels in the density image; output image as a PNG file 
    # Also try https://www.geeksforgeeks.org/python-pil-imageops-equalize-method/

    densityColorHash={} 
    col2=[]
    size=len(densityCountHash)  # number of elements in hash
    counter=0

    for density in sorted(densityCountHash):
      counter=counter+1
      quant=counter/size   # always between zero and one
      if quant < 0.08: 
        densityColorHash[density]=0
      elif quant < 0.18:
        densityColorHash[density]=30 
      elif quant < 0.28:
        densityColorHash[density]=55
      elif quant < 0.42:
        densityColorHash[density]=90
      elif quant < 0.62:
        densityColorHash[density]=120
      elif quant < 0.80:
        densityColorHash[density]=140
      elif quant < 0.95:
        densityColorHash[density]=170
      else:
        densityColorHash[density]=254

    # allocate second image (density image)

    img2  = Image.new( mode = "RGBA", size = (width, height), color = (0, 0, 0) )
    pix2  = img2.load()   # pix[x,y]=col[n] to modify the RGB color of a pixel
    draw2 = ImageDraw.Draw(img2,"RGBA")

    # allocate gray levels (palette)
    for i in range(0,256):
        col2.append((255-i,255-i,255-i,255))

    # create density image pixel by pixel
    for pixelX in range(0,width): 
      for pixelY in range(0,height):   
        density=densityMap[pixelX][pixelY] 
        color=densityColorHash[density]
        pix2[pixelX,pixelY]=col2[color]  

    # output density image
    draw2.rectangle((0,0,width-1,height-1), outline ="black",width=1)
    fname=img_density+str(loop)+'.png'
    img2.save(fname)

  return()
\end{lstlisting}
\quad \\
\noindent {\bf Conclusion} We accomplished the whole purpose: estimating the unknown shift vectors (or cluster centers) associated to the observations, and inventing a new, very fast clustering technique (supervised or unsupervised) that
can be performed in GPU [\href{https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units}{Wiki}]. See also \cite{vgar} (available online, \href{https://arxiv.org/abs/0804.1448}{here}) for a similar use of GPU in the context of nearest neighbor clustering.

%-------------------------------------------------------
\subsection{Source Code: Production of the Videos}\label{visusc}

The purpose here is to show you how to build high quality data videos, also called data animations. Emphasis is on automation and ease of implementation. 
The material, besides its educational value, also illustrates interesting aspects of the theory presented in this textbook. In particular, the video featuring 
fractional supervised clustering is an example of a neural network in action: each frame represents an hidden layer. It also illustrates machine learning performed in GPU (graphics processing unit) using image filtering techniques.

\subsubsection{Dirichlet Eta Function}\label{etavis}
{On GitHub}: \href{https://github.com/VincentGranville/Point-Processes/blob/main/Videos/av_demo.r}{\texttt{av\_demo.r}}. R code using the AV library [\href{https://ropensci.org/blog/2018/10/06/av-release/}{Wiki}] to produce the video related to the Dirichlet eta function $\eta(z)$, with $z=\sigma+it\in\mathbb{C}$, based on Formula~(\ref{eta1b}) for the X coordinate (the real part), and 
Formula~(\ref{eta2b}) for the Y coordinate (the imaginary part). The input file 
\href{https://github.com/VincentGranville/Point-Processes/blob/main/Videos/av_demo_vg2cb.txt}{\texttt{av\_demo\_vg2cb.txt}}, with $\num{20000}$ rows, is generated in the 
\href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{\texttt{PB\_inference.xlsx}} spreadsheet, in columns \texttt{T} to \texttt{Y} in the 
\texttt{Video\_Riemann} tab. The R code generates $\num{1000}$ PNG images 
\texttt{av\_demo001.png}, \texttt{av\_dem002.png} and so on
(the frames of the video) in the directory \texttt{c:/Users/vince/tex/}. Each frame is based on 20 consecutive rows from the input file. Each row of the input file has 6 fields:
\begin{itemize}
\item \texttt{k}: the index (the row number),
\item \texttt{x}, \texttt{y}: the sum (a 2D vector) of the first $k$ terms of the series defining $\eta(z)$,
\item \texttt{x2}, \texttt{y2}:  the sum (a 2D vector) of the first $k+1$ terms of the series defining $\eta(z)$,
\item \texttt{col}: specifies the color used to draw the arrow between $(x,y)$ and $(x_2,y_2)$.
\end{itemize}
The output video is \href{https://www.youtube.com/watch?v=FUxAeW4JEXA}{\texttt{av\_demo\_vg2cb.mp4}}, with a $1200 \times 800$ resolution and 12 frames per second.
The initial conditions, that is the parameters $\sigma$ and $t$, are specified in the spreadsheet. The above description corresponds to the basic version.
The new version allows you to draw two orbits at the same time, that is, to work with two sets of values for $\sigma,t$. This allows you to 
simultaneously compare the convergence for two different sets of 
initial conditions. In that case, the first 10,000 rows correspond to the first set, and the other 10,000 to the second one. For more information,
see Sections~\ref{rh} and \ref{videocluster1}.

The RGB (red/green/blue) color system attached to the \texttt{arrows} R function relies on sinusoids: $0.9\times |\sin(0.00100 \times \mbox{col})|$ for red,
 $0.6\times |\sin(0.00075 \times\mbox{col})|$ for green, and  $|\sin(0.00150 \times\mbox{col})|$ for blue. Note that in this data set, \texttt{col} and \texttt{k} are
identical. The parameters in the sine functions have simple ratios, creating periodicity and harmonic waves [\href{https://en.wikipedia.org/wiki/Harmonic}{Wiki}]. They are responsible for the harmonious display of colors.
\quad 
\begin{lstlisting}
# install.packages('Cairo')
library('Cairo');

CairoPNG(filename = "c:/Users/vince/tex/av_demo%03d.png", width = 1200, height = 800); 
# https://www.rdocumentation.org/packages/Cairo/versions/1.5-14/topics/Cairo

data<-read.table("c:/Users/vince/tex/av_demo_vg2cb.txt",header=TRUE);

k<-data$k;
x<-data$x;   
y<-data$y;  
x2<-data$x2;   
y2<-data$y2; 
col<-data$col; 

for (n in 1:1000) {

    plot(x,y,pch=20,cex=0,col=rgb(0,0,0),xlab="",ylab="",axes=FALSE  );
    rect(-60, -60, 90, 30, density = NULL, angle = 45,
       col = rgb(0,0,0), border = NULL);
    # You need to adjust the size of the rectangle to your data

    a<-x[k <= n*20];
    b<-y[k <= n*20];
    a2<-x2[k <= n*20];
    b2<-y2[k <= n*20];
    c<-col[k <= n*20];
    arrows(a, b, a2, b2, length = 0, angle = 10, code = 2,
      col=rgb(  0.9*abs(sin(0.00100*col)),0.6*abs(sin(0.00075*col)),
      abs(sin(0.00150*col))  ));
}
dev.off();

png_files <- sprintf("c:/Users/vince/tex/av_demo%03d.png", 1:1000)
av::av_encode_video(png_files, 'c:/Users/vince/tex/av_demo_vg2cb.mp4', framerate = 12)
\end{lstlisting}

%------------------
\subsubsection{Fractal Supervised Clustering}\label{codefr} 
{\bf On GitHub}: \href{https://github.com/VincentGranville/Point-Processes/blob/main/Videos/PB_clustering_video.py}{\texttt{PB\_clustering\_video.py}}. This code is a minimalist version of the \texttt{PB\_NN.py} / \texttt{GD\_util.py} combination, with both blended together. It does not compute nearest neighbor distances, does not
perform unsupervised clustering, and does not use the equalizer. Rather it focuses on supervised clustering only, using a very small (atomic) filter window and a large number of passes (the variable \texttt{nloop}, set to $250$). This
allows you to create a large number of frames for the video. In a nutshell, the code generates $251$ PNG images named 
\texttt{img\_0.png} (the initial image corresponding to the training set), \texttt{img\_1.png}, \texttt{img\_2.png} and so: these are the frames of the video. 
 Once created, they are combined into an mp4 video
(in this case \href{https://www.youtube.com/watch?v=dNPSEh-X6uw}{\texttt{imgPB.mp4}}), for instance using the last two lines of the R code in Section~\ref{etavis}. The methodology for the clustering algorithm is discussed in Sections~\ref{videocluster2} and \ref{spa2}.\\
\quad \\
\noindent {\bf Part 1} creates the training set consisting of 4 groups, via simulation. The variable \texttt{ProcessID} represents the group label. It is transformed into an image and stored in memory as \texttt{bitmap} (a 2D array), for easy image processing.
\begin{lstlisting}
# PB_clustering_video.py

import math
import random
from PIL import Image, ImageDraw    # ImageDraw to draw rectangles etc.
import moviepy.video.io.ImageSequenceClip  # to produce mp4 video

Nprocess=4       # number of processes in the process superimposition
seed=82431      # arbitrary number
random.seed(seed) # initialize random generator 
s=0.15  # scaling factor
shiftX=[]
shiftY=[]

for i in range(Nprocess) :
  shiftX.append(random.random())
  shiftY.append(random.random())
processID=0
height,width = (600, 600)
bitmap = [[255 for k in range(height)] for h in range(width)]

for h in range(-25,26):   
  for k in range(-25,26):  
    for processID in range(Nprocess): 
      ranx=random.random()
      rany=random.random()
      ranID=random.random()
      if ranID < 0.20:
        processID=0
      elif ranID < 0.60:
        processID=1
      elif ranID < 0.90:
        processID=2 
      else:
        processID=3
      x=shiftX[processID]+h+s*math.log(ranx/(1-ranx)) 
      y=shiftY[processID]+k+s*math.log(rany/(1-rany))
      if x>-3 and x<3 and x>-3 and x<3:
        xmod=1+x-int(x)   # x modulo 2/lambda
        ymod=1+y-int(y)   # y modulo 2/lambda
        pixelX=int(width*xmod/2)   
        pixelY=int(height*(2-ymod)/2) # pixel (0,0) at top left corner
        bitmap[pixelX][pixelY]=processID
\end{lstlisting}
\quad \\
\noindent {\bf Part 2} generates the first frame \texttt{img\_0.png} corresponding to the training set stored in the the \texttt{bitmap} array.
\begin{lstlisting}
img1  = Image.new( mode = "RGBA", size = (width, height), color = (255, 255, 255) )
pix1  = img1.load()   # pix[x,y]=col[n] to modify the RGB color of a pixel
draw1 = ImageDraw.Draw(img1,"RGBA")

col1=[] 
col1.append((255,0,0,255))
col1.append((0,0,255,255))
col1.append((255,179,0,255))
col1.append((0,179,0,255))
col1.append((0,0,0,255))
for i in range(Nprocess,256):
  col1.append((255,255,255,255))
 
for pixelX in range(0,width): 
  for pixelY in range(0,height): 
        topProcessID=bitmap[pixelX][pixelY]
        pix1[pixelX,pixelY]=col1[topProcessID]

draw1.rectangle((0,0,width-1,height-1), outline ="black",width=1)
fname="img_0.png"
img1.save(fname)
\end{lstlisting}
\quad \\
\noindent {\bf Part 3} filters the \texttt{bitmap} image \texttt{nloop} times, generating the output frames \texttt{img\_1.png}, \texttt{img\_2.png} and so on, up to \texttt{img\_251.png}.
\begin{lstlisting}
nloop=250       # number of times the image is filtered 

oldBitmap = [[255 for k in range(height)] for h in range(width)]
flist=[]

for loop in range(1,nloop+1): 
  print("loop",loop,"out of",nloop+1) 
  for pixelX in range(0,width): 
    for pixelY in range(0,height): 
      oldBitmap[pixelX][pixelY]=bitmap[pixelX][pixelY]
  for pixelX in range(1,width-1): 
    for pixelY in range(1,height-1):   
      x=pixelX
      y=pixelY
      topProcessID=oldBitmap[x][y]
      if topProcessID==255 or loop>50: 
        r=random.random()
        if r<0.25: 
          x=x+1 
          if x>width-2: 
            x=x-(width-2)
        elif r<0.5:
          x=x-1 
          if x<1: 
            x=x+width-2
        elif r<0.75:
          y=y+1 
          if y>height-2: 
            y=y-(height-2)
        else:
          y=y-1 
          if y<1: 
            y=y+height-2         
        if loop>=50 and oldBitmap[x][y]==255:
          x=pixelX
          y=pixelY
      topProcessID=oldBitmap[x][y]  
      bitmap[pixelX][pixelY]=topProcessID
      pix1[pixelX,pixelY]=col1[topProcessID]
  draw1.rectangle((0,0,width-1,height-1), outline ="black",width=1)
  fname="img_"+str(loop+1)+'.png'
  flist.append(fname)   
  img1.save(fname)

clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(flist, fps=20) 
clip.write_videofile('img.mp4')
\end{lstlisting}

%-------------------------------------------------------------------------------------------------------------------------------------


%\renewcommand{\glsnamefont}[1]{\textbf{#1}}
\setlength{\glsdescwidth}{0.75\hsize}
\pagebreak
\printnoidxglossary[type=gloss,style=long,title={Glossary},sort=def] %%%%
%\printnoidxglossary[type=symbols,style=long,title={List of Symbols and Abbreviations}] %%% remove ??
\listoffigures
%\listoftables %%%% remove
\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{refs} % Entries are in the refs.bib file in same directory as the tex file
\printindex

\end{document}