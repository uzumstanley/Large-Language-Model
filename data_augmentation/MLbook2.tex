\documentclass[oneside,10pt]{book}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
\pagestyle{plain}
\newcommand\Chapter[2]{
  %\chapter[#1: {\itshape#2}]{#1\\[2ex]\Large\itshape#2}
  \chapter[#1]{#1\\[2ex]\Large\itshape#2}
}

%\usepackage{fontspec}
%\setmainfont{Times New Roman} %Times New Roman
%\setmonofont{Consolas}

%\usepackage{selinput}
%\SelectInputMappings{Euro={€}}

%\usepackage[utf8]{inputenc}
\usepackage{amsmath}    % need for subequations
\usepackage{amsfonts}
\usepackage{amssymb}  % needed for mathbb  OK
\usepackage{bigints}
\usepackage{graphicx}   % need for figures
\usepackage{subfig}
\usepackage{verbatim}   % useful for program listings
\usepackage{color}      % use if color is used in text
%\usepackage{subfigure}  % use for side-by-side figures
\usepackage{parskip}
\usepackage{float}
\usepackage{courier}
%\usepackage{artemisia} %%%
\usepackage{exercise}
\usepackage{sistyle}
\usepackage{textcomp} 
%

%%%\usepackage[utf8]{luainputenc}
%\usepackage{luatextra}
%
%%\usepackage[utf8]{inputenc}
%%\usepackage[T1]{fontenc}
%%\usepackage{textcomp,upgreek}
%\usepackage{fontspec} %,xltxtra}
%\usepackage{unicode}
\usepackage[euler]{textgreek}
%%\DeclareUnicodeCharacter{3B8}{\ensuremath{\uptheta}}
%

\SIthousandsep{,}
%\usepackage{numprint}
\setlength\parindent{0pt}

\newtheorem{prop}{Proposition}

\renewcommand{\DifficultyMarker}{}
\newcommand{\AtBeginExerciseHeader}{\hspace{-21pt}}  %-0.2pt
\renewcommand{\ExerciseHeader}{\AtBeginExerciseHeader\textbf{\ExerciseName~\ExerciseHeaderNB} \ExerciseTitle}
\renewcommand{\AnswerHeader}{\large\textbf{\AnswerName~\ExerciseHeaderNB}\smallskip\newline}
\setlength\AnswerSkipBefore{1em}

\usepackage{xspace}
\usepackage{imakeidx}
\makeindex

\usepackage[nottoc]{tocbibind}

\usepackage[colorlinks = true,
          linktocpage=true,
            pagebackref=true, % add back references to bibliography
            linkcolor = red,
            urlcolor  = blue,
            citecolor = red,
 %           refcolor  =red,
            anchorcolor = blue]{hyperref}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{gray2}{rgb}{0.35,0.35,0.35}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{index}{rgb}{0.88,0.32,0}

%------- source code settings
\usepackage{listings} 
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

%-----------------------------------------------------------------

\usepackage{blindtext}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }


\setlength{\baselineskip}{0.0pt} 
\setlength{\parskip}{3pt plus 2pt}
\setlength{\parindent}{20pt}
\setlength{\marginparsep}{0.0cm}
\setlength{\marginparwidth}{0.0cm}
\setlength{\marginparpush}{0.0cm}
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.4} %%%
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

\usepackage[symbols,nogroupskip,acronym]{glossaries-extra}
%\usepackage[xindy,symbols,nogroupskip,sort=def,acronym]{glossaries}
\makenoidxglossaries %%%%%%%%%%%%%%%
%\setlength{\glsdescwidth}{1.3\hsize}

\newglossary*{gloss}{Glossary}

\newglossaryentry{gls:armodels}{type=gloss,name={Autoregressive process},description={\textcolor{index}{Auto-correlated time series}\index{auto-regressive process}, as described in section~\ref{linearar}. 
 Time-continuous versions include \textcolor{index}{Gaussian processes}\index{Gaussian process} and 
\textcolor{index}{Brownian motions}\index{Brownian motion}, while \textcolor{index}{random walks}\index{random walk} are a discrete example; two-dimensional versions exist. These processes are essentially integrated \textcolor{index}{white noise}\index{white noise}.  See pages },text={auto-regressive}}

\newglossaryentry{gls:binning}{type=gloss,name={Binning},description={Feature binning consists of aggregating the values of a feature into a small number of bins, to avoid \gls{gls:overfitting}\index{overfitting} and reduce the number of 
\textcolor{index}{nodes}\index{node (decision tree)} in methods such as \textcolor{index}{naive Bayes}\index{naive Bayes}, 
\glspl{gls:neuralnet},  or \glspl{gls:decisiontree}. Binning can be applied to two or more features simultaneously. I discuss \textcolor{index}{optimum binning}\index{binning!optimum binning} in this book. See pages },text={binning}}

\newglossaryentry{gls:boosted}{type=gloss,name={Boosted model},description={Blending of several models to get the best of each one, also referred to as \glspl{gls:ensembles}. The concept is illustrated with 
\textcolor{index}{hidden decision trees}\index{hidden decision trees} in this book. Other popular examples are \textcolor{index}{gradient boosting}\index{gradient boosting} and \textcolor{index}{AdaBoost}\index{AdaBoost}. See pages },text={boosting}}

\newglossaryentry{gls:bootstrap}{type=gloss,name={Bootstrapping},description={A data-driven, model-free technique to estimate parameter values, to optimize \gls{gls:goodnessoffit} metrics. Related to resampling in the context of \gls{gls:crossvalid}. In this book, I discuss \textcolor{index}{parametric bootstrap}\index{parametric bootstrap} on \gls{gls:syntheticdata} that mimics the actual observations. See pages },text={bootstrapping}}

\newglossaryentry{gls:cr}{type=gloss,name={Confidence Region},description={A confidence region of  level $\gamma$ is a 2D set of minimum area covering a proportion $\gamma$ of the mass of a bivariate probability distribution. It is a 2D generalization of 
\textcolor{index}{confidence intervals}\index{confidence interval}. In this book, I also discuss \textcolor{index}{dual confidence regions}\index{confidence region!dual region} -- the analogous of \textcolor{index}{credible regions}\index{credible region (Bayesian)} in Bayesian inference. See pages },text={confidence region}}

\newglossaryentry{gls:crossvalid}{type=gloss,name={Cross-validation},description={Standard procedure used in \gls{gls:bootstrap}, and to test and validate a model, by splitting your data into training and \glspl{gls:validset}. Parameters are estimated based on \gls{gls:trainingset} data. An alternative to cross-validation is testing your model on \gls{gls:syntheticdata} with known response. See pages },text={cross-validation}}

\newglossaryentry{gls:decisiontree}{type=gloss,name={Decision trees},description={A simple, intuitive non-linear modeling techniques used in classification problems. It can handle missing and categorical data, as well as a large number of features, but requires appropriate feature binning. Typically one blends multiple binary trees each with a few \textcolor{index}{nodes}\index{node (decision tree)}, to boost performance. See pages },text={decision tree}}

\newglossaryentry{gls:dimreduct}{type=gloss,name={Dimension reduction},description={A technique to reduce the number of features in your dataset while minimizing the loss in predictive power. The most well known are \textcolor{index}{principal component analysis}\index{principal component analysis} and \gls{gls:featureselection} to maximize \gls{gls:goodnessoffit} metrics. See pages },text={dimensionality reduction}}

\newglossaryentry{gls:empdistr}{type=gloss,name={Empirical distribution},description={Cumulative frequency histogram attached to a statistic (for instance, nearest neighbor distances), and based on observations. When the number of observations tends to infinity and the bin sizes tend  to zero, this step function tends to the theoretical cumulative distribution function of the statistic in question. See pages },text={empirical distribution}}

\newglossaryentry{gls:ensembles}{type=gloss,name={Ensemble methods},description={A technique consisting of blending multiple models together, such as many \glspl{gls:decisiontree} with \gls{gls:logreg}, to get the best of each method and outperform each method taken separately. Examples include \gls{gls:boosted}, bagging, and AdaBoost. In this book, I discuss \textcolor{index}{hidden decision trees}\index{hidden decision trees}. See pages },text={ensemble method}}

\newglossaryentry{gls:explainableai}{type=gloss,name={Explainable AI},description={Automated machine learning techniques that are easy to interpret are referred to as interpretable machine learning or explainable artificial intelligence. As much as possible, the methods discussed in this book belong to that category. The goal is to design black-box systems less likely to generate unexpected results with unintended consequences. See pages },text={explainable AI}}

\newglossaryentry{gls:featureselection}{type=gloss,name={Feature selection},description={Features -- as opposed to the model response -- are also called independent variables or predictors. Feature selection, akin to \gls{gls:dimreduct}, aims at finding the minimum subset of variables with enough \gls{gls:predictivepower}. It is also used to eliminate redundant features and find \textcolor{index}{causality}\index{causality} (typically using \textcolor{index}{hierarchical Bayesian models}\index{Bayesian inference!hierarchical models}), as opposed to mere correlations. Sometimes, two features have poor predictive power when taken separately, but provide improved predictions when combined together. See pages },text={feature selection}}

\newglossaryentry{gls:goodnessoffit}{type=gloss,name={Goodness-of-fit},description={A \textcolor{index}{model fitting}\index{model fitting} criterion or metric to assess how a model or sub-model fits to a dataset, or to measure its \gls{gls:predictivepower} on a \gls{gls:validset}. Examples include \gls{gls:rsquared}, \textcolor{index}{Chi-squared}\index{Chi-squared}, \textcolor{index}{Kolmogorov-Smirnov}\index{Kolmogorov-Smirnov}, error rate such as false positives and other metrics discussed in this book. See pages },text={goodness-of-fit}}

\newglossaryentry{gls:gradient}{type=gloss,name={Gradient methods},description={Iterative optimization techniques to find the minimum of maximum of a function, such as the \textcolor{index}{maximum likelihood}\index{maximum likelihood estimation}. When there are numerous local minima or maxima, use \textcolor{index}{swarm optimization}\index{swarm optimization}. Gradient methods (for instance, stochastic gradient descent or Newton's method) assume that the function is differentiable. If not, other techniques such as \textcolor{index}{Monte Carlo simulations}\index{Monte Carlo simulations} or the 
\textcolor{index}{fixed-point algorithm}\index{fixed-point algorithm} can be used. Constrained optimization involves using 
\textcolor{index}{Lagrange multipliers}\index{Lagrange multiplier}. See pages },text={gradient}}

\newglossaryentry{gls:graphmodel}{type=gloss,name={Graph structures},description={Graphs are found in \glspl{gls:decisiontree}, in  \glspl{gls:neuralnet} (connections between \textcolor{index}{neurons}\index{neural network!neuron}), in \textcolor{index}{nearest neighbors methods}\index{nearest neighbors method}  (NN graphs), in \textcolor{index}{hierarchical Bayesian models}\index{Bayesian inference!hierarchical models}, and more. See pages },text={graph}}

\newglossaryentry{gls:hyperparam}{type=gloss,name={Hyperparameter},description={An hyperparameter is used to control the learning process: for instance, the dimension, the number of features, parameters, layers (neural networks) or clusters (clustering problem), or the width of a  filtering window in image processing. By contrast, the values of other parameters (typically node weights in \glspl{gls:neuralnet} or regression coefficients) are derived via training. See pages },text={hyperparameter}}

\newglossaryentry{gls:linkf}{type=gloss,name={Link function},description={A link function maps a nonlinear relationship to a linear one so that a linear model can be fit, and then mapped back to the original form using the inverse function. For instance, the \textcolor{index}{logit link function}\index{logit function} is used in \gls{gls:logreg}. Generalizations include \textcolor{index}{quantile}\index{quantile} functions and inverse \textcolor{index}{sigmoids}\index{sigmoid function} in \gls{gls:neuralnet} to work with additive (linear) parameters. See pages },text={link function}}


\newglossaryentry{gls:logreg}{type=gloss,name={Logistic regression},description={A generalized linear \gls{gls:regression} method where the binary response  (fraud/non-fraud or cancer/non-cancer) is modeled as a probability via the logistic link function. Alternatives to the iterative maximum likelihood solution are discussed in this book. See pages },text={logistic regression}}


\newglossaryentry{gls:neuralnet}{type=gloss,name={Neural network},description={A blackbox system used for predictions, optimization,  or pattern recognition especially in computer vision. It consists of layers, neurons in each layer, \glspl{gls:linkf} to model non-linear interactions, parameters (weights associated to the connections between neurons) and \glspl{gls:hyperparam}. Networks with several layers are called \textcolor{index}{deep neural networks}\index{deep neural network}. Also, \textcolor{index}{neurons}\index{neural network!neuron} are sometimes called nodes. See pages },text={neural network}}

\newglossaryentry{gls:nlp}{type=gloss,name={NLP},description={Natural language processing is a set of techniques to deal with unstructured text data, such as emails, automated customer support, or webpages downloaded with a crawler. The example discussed in section~\ref{nlp21} deals with creating a keyword taxonomy based on parsing Google search results pages. See pages },text={natural language processing}}


\newglossaryentry{gls:numericalstability}{type=gloss,name={Numerical stability},description={This issue occurring in unstable optimization problems typically with multiple minima or maxima, is frequently overlooked and leads to poor predictions or high volatility. It is sometimes referred to as \textcolor{index}{ill-conditioned problems}\index{ill-conditioned problem}. I explain how to fix it in several examples in this book, for instance in section~\ref{vandervg}. Not to be confused with numerical precision. See pages },text={numerical stability}}

\newglossaryentry{gls:overfitting}{type=gloss,name={Overfitting},description={Using too many unstable parameters resulting in excellent performance on the \gls{gls:trainingset}, but poor performance on future data or on the \gls{gls:validset}. It typically occurs with numerically unstable procedures such as regression (especially polynomial regression) when the training set is not large enough, or in the presence of \textcolor{index}{wide data}\index{wide data} (more features than observations) when using a method not suited to this situation. The opposite is underfitting. See pages },text={overfitting}}

\newglossaryentry{gls:predictivepower}{type=gloss,name={Predictive power},description={A metric to assess the \gls{gls:goodnessoffit} or performance of a model or subset of features, for instance in the context of \gls{gls:dimreduct} or \gls{gls:featureselection}. Typical metrics include \gls{gls:rsquared}, or \textcolor{index}{confusion matrices}\index{confusion matrix} in classification. See pages },text={predictive power}}

\newglossaryentry{gls:rsquared}{type=gloss,name={R-squared},description={A \gls{gls:goodnessoffit} metric to assess the predictive power of a model, measured on a \gls{gls:validset}. Alternatives include adjusted R-squared, mean absolute error and other metrics discussed in this book. See pages },text={R-squared}}

\newglossaryentry{gls:prng}{type=gloss,name={Random number},description={Pseudo-random numbers are sequences of binary digits, usually grouped into blocks, satisfying properties of independent Bernoulli trials. In this book, the concept is formally defined, and strong pseudo-number generators are built and used in computer-intensive simulations. See pages },text={pseudo-random number}}


\newglossaryentry{gls:regression}{type=gloss,name={Regression methods},description={I discuss a unified approach to all regression problems in chapter 1. Traditional techniques include linear, logistic, Bayesian, polynomial and \textcolor{index}{Lasso regression}\index{Lasso regression} (to deal with numerical instability and \gls{gls:overfitting}), solved using optimization techniques, \textcolor{index}{maximum likelihood}\index{maximum likelihood estimation} methods, linear algebra (\textcolor{index}{eigenvalues}\index{eigenvalue} and \textcolor{index}{singular value decomposition}\index{singular value decomposition}) or stepwise procedures. See pages },text={regression}}

\newglossaryentry{gls:supervisedlearning}{type=gloss,name={Supervised learning},description={Techniques dealing with labeled data (\textcolor{index}{classification}\index{classification}) or when the response is known (\gls{gls:regression}). The opposite is \textcolor{index}{unsupervised learning}\index{unsupervised learning}, for instance \textcolor{index}{clustering}\index{clustering} problems. In-between, you have \textcolor{index}{semi-supervised learning}\index{semi-supervised learning} and \textcolor{index}{reinforcement learning}\index{reinforcement learning} (favoring good decisions). The technique described in chapter 1 fits into unsupervised regression. \textcolor{index}{Adversarial learning}\index{adversarial learning} is testing your model against extreme cases intended to make it fail, to build better models. See pages },text={supervised learning}}

\newglossaryentry{gls:syntheticdata}{type=gloss,name={Synthetic data},description={Artificial data simulated using a \textcolor{index}{generative model}\index{generative model}, typically a \textcolor{index}{mixture model}\index{mixture model}, to enrich existing datasets and improve the quality of \glspl{gls:trainingset}. Called \textcolor{index}{augmented data}\index{augmented data} when blended with real data. See pages },text={synthetic data}}

\newglossaryentry{gls:tensor}{type=gloss,name={Tensor},description={Matrix generalization with three of more dimensions. A matrix is a two-dimensional tensor. A triple summation with three indices is represented by a three-dimensional tensor, while a double summation involves a standard matrix. See pages },text={tensor}}

\newglossaryentry{gls:trainingset}{type=gloss,name={Training set},description={Dataset used to train your model in \gls{gls:supervisedlearning}. 
Typically, a portion of the training set is used to train the model, the other part is used as \gls{gls:validset}. See pages },text={training set}}

\newglossaryentry{gls:validset}{type=gloss,name={Validation set},description={A portion of your \gls{gls:trainingset}, typically $20\%$, used to measure the actual performance of your predictive algorithm outside the training set. In cross-validation and bootstrapping, the training and validation sets are split into multiple subsets to get a better sense of variations in the predictions. See pages },text={validation set}}

%----------------
% Top KW


% DISCUSS in LAST SECTION
%
% dummy variable
% exploratory analysis
% SVM
% recursivity
% computational complexity
% markov chain
% distributed architecture
% bayesian inference [dual confidence regions]
%  test of hypothesis, 
% *** prediction interval [?? in bayesian stats], 
% *** hash table / dictionary / associative array / list
% *** monte carlo methods, 

% give a fish... teach how to fish.. teach how to learn... install python... pip install... search the web / stackoverflow, post on forum questions, look for answer reviews/date, search doc on librarry functions...known instructors will all my varied experience, provide recommendations

% clustering, classification,  

% IGNORE
% robust method, covariance matrix
% black box system, nosql

% Create courses page on MLT


%% Add section "automated data cleaning/exploratory analysis" -- Well, not something I did overnight but after years of cleaning data and noticing I was always facing the same issues. The first step is to create a summary table for all the features: type (numbers, categories, text, mixed), for each feature get number of unique values and list top 5 popular values with count for each of these values  (could be a wrong zip-code like 99999). Compute min, max, median some percentiles if numeric feature. Compute cross-correlations b/w features. Check misalignments. Look for special stuff like NaN, N/A etc. Check for duplicate IDs or same IDs with 2 names (suggesting typos). Look for special character (accented and so on). Some negative or out-of-range or non-numeric or missing values when it should not happen? Duplicate features? The list is not short, indeed long enough that it feels there's a new problem each time, but in the end, yes there are several dozens (not several hundreds) of things to look for and fix, but it is manageable. Automation can help fix most of them. 
%structure unstructure data or use dictionary of typos or guided field capturing [KW: exploratory analysis] xxx Wells fargo broken sessions  simulations

%add NBCi , polynomial regression? data_cleaning, density estimation on the grid, long-range autocorrel time series ?? other concepts [adversarial models, change point detection]

% Other KW

%curve fitting,similarity metric, fixed point algorithm, stochastic processes
%shape recognition, lagrange multiplier, interpolation, augmented data, PCA, data video [and sound]

%--------




\newglossaryentry{gls:cc}{type=gloss,name={Connected Component},description={A set of vertices in a graph that are connected to each other by paths. 
See also \gls{gls:nng}. See pages },text={connected component}}


\newglossaryentry{gls:ergo}{type=gloss,name={Ergodicity},description={A statistic such as the interarrival times is ergodic if it has the same asymptotic distribution, whether it is computed on many observations from a single realization of the process, or averaged across many realizations, each with few observations. See pages },text={ergodicity}}

\newglossaryentry{gls:homo1}{type=gloss,name={Homogeneity},description={A property of a point process, characterized by an homogeneous intensity function, that is, constant or independent of the location. See pages },text={homogeneous}}

\newglossaryentry{gls:mi}{type=gloss,name={Identifiability},description={A models is identifiable if it is uniquely defined by its parameters. Then it is possible to estimate each parameter separately. A trivial example of non-identifiability is when we have two parameters, say $\alpha,\beta$, but they only occur in a product $\alpha\beta$. In that case, if $\alpha\beta=6$, it is impossible to tell whether $\alpha=2,\beta=3$ or $\alpha=1,\beta=6$. See pages },text={model identifiability}}



\newglossaryentry{gls:ia}{type=gloss,name={Interarrival Time},description={In one dimension, random variable measuring the distance between a point of the process and its closest neighbor to the right, on the real axis. Interarrival times are also called {\em increments}. See pages },text={interarrival times}}

\newglossaryentry{gls:lattice1}{type=gloss,name={Lattice Space},description={In two dimensions, it consists of the locations $(h/\lambda,k/\lambda)$ with $h,k\in\mathbb{Z}$. The distribution of a point $(X_h,Y_k)$ is centered at $(h/\lambda,k/\lambda)$. The concept can be extended to any dimension. See pages },text={lattice space}}

\newglossaryentry{gls:lsc}{type=gloss,name={Location-scale},description={A random variable $X$ has a location-scale distribution with two parameters, the scale $s$ and location $\mu$, if any linear transformation $a+bX$ has a distribution of the same family, with parameters respectively $b^2s$ and $\mu+a$. Here $\mu$ is the expectation and $s$ is proportional to the variance of the distribution. See pages },text={location-scale distributions}}

\newglossaryentry{gls:modulo}{type=gloss,name={Modulo Operator},description={Sometimes, it is useful to work with point ``residues" modulo $\frac{1}{\lambda}$, instead of the original points, due to the nature of the underlying lattice. It magnifies the patterns of the point process. By definition, 
$X_k \bmod{\frac{1}{\lambda}}=X_k-\frac{1}{\lambda}\lfloor \lambda X_k \rfloor$ where the brackets represent the integer part function. See pages },text={modulo}}

\newglossaryentry{gls:nng}{type=gloss,name={NN Graph},description={Nearest neighbor graph. The vertices are the points of the process. Two vertices (the points they represent) are connected if at least one of the two points is nearest neighbor to the other one. This graph is undirected. See pages },text={nearest neighbor graph}}

\newglossaryentry{gls:pc}{type=gloss,name={Point Count},description={Random variable, denoted as $N(B)$, counting the number of points of the process in a particular set $B$, typically an interval $[a, b]$ in one dimension, and a square or circle in two dimensions. See pages },text={point count}}

\newglossaryentry{gls:pb55}{type=gloss,name={Point Distribution},description={Random variable representing how a point of the process is distributed in a domain $B$; for instance, for a stationary Poisson process, points are uniformly distributed on any compact domain $B$ (say, an interval in one dimension, or a square in two dimensions). See pages },text={point distribution}}

\newglossaryentry{gls:quant}{type=gloss,name={Quantile function},description={Inverse of the cumulative distribution function (CDF) $F$, denoted as $Q$. 
Thus if $P(X<x)=F(x)$, then $P(X<Q(x))=x$. See pages },text={quantile function}}


\newglossaryentry{gls:sf}{type=gloss,name={Scaling Factor},description={Core parameter of the Poisson-binomial process. Denoted as $s$, proportional to the variance of the distribution $F$ attached to the points of the process. It measures the level of repulsion among the points (maximum if $s=0$, minimum if $s=\infty$). In $d$ dimensions, the process is stationary Poisson of intensity $\lambda^d$ if $s=\infty$, and coincides with the fixed {\em lattice space} if $s=0$. See pages },text={scaling factor}}


\newglossaryentry{gls:statio}{type=gloss,name={Stationarity},description={Property of a point process: the point distributions in two sets of same shape and area, are identical. The process is stochastically invariant under translations. See pages },text={stationary}}


\begin{document}

\hypersetup{linkcolor=blue}
%inserting a glossary entry in gloss: \gls{gls:keyword1} \\



\baselineskip=2\baselineskip
\thispagestyle{empty}
\hspace{0pt}
\vfill
%\hrulefill
\begin{center}
\rule{0.75\textwidth}{.4pt}
\end{center}

\begin{center}
{\Huge \bf{Intuitive Machine Learning} }  
\end{center}


\baselineskip=0.5\baselineskip
\addvspace{1cm}
\begin{center}
%\includegraphics[width=0.7\textwidth]{linear.png}  \\
%\addvspace{1cm}
%\includegraphics[width=0.6\textwidth]{imgpyRiemannFinalOrbits-v2-small2.jpg} 
\includegraphics[width=0.75\textwidth]{fc.png}  
\end{center}
\addvspace{1cm}
\begin{center}
\rule{0.75\textwidth}{.4pt}
\end{center}
\begin{center}
Vincent Granville, Ph.D. $|$ \href{https://mltechniques.com/}{www.MLTechniques.com} $|$ Version 1.0, September 2022 
\end{center}
%\hrulefill

\hypersetup{linkcolor=red} % red %

\vfill
\hspace{0pt}
\pagebreak

\chapter*{Preface} %\clearpage

This book covers the foundations of machine learning, with modern approaches to solving complex problems. Emphasis is on scalability, automation, testing, optimizing, and interpretability (explainable AI). For instance, regression techniques -- including logistic and Lasso -- are presented as a single method, without using advanced linear algebra. There is no need to learn $50$ versions when one does it all and more. Confidence regions and prediction intervals are built using parametric bootstrap, without statistical models or probability distributions. Models (including generative models and mixtures) are mostly used to create rich synthetic data to test and benchmark various methods.

Topics covered include clustering and classification, GPU machine learning, ensemble methods including an original boosting technique, elements of graph modeling, deep neural networks, auto-regressive and non-periodic time series, Brownian motions and related processes, simulations, interpolation, random numbers, natural language processing (smart crawling, taxonomy creation and structuring unstructured data), computer vision (shapes generation and recognition), curve fitting, cross-validation, goodness-of-fit metrics, feature selection, curve fitting, gradient methods, optimization techniques and numerical stability. 

Methods are accompanied by enterprise-grade Python code, replicable datasets and  visualizations, including data animations (gifs, videos, even sound done in Python). The code
 uses various data structures and library functions sometimes with advanced options. It constitutes a Python tutorial in itself, and  an introduction to scientific computing. Some data animations and chart enhancements are done in R. The code, datasets, spreadsheets and data visualizations are also on GitHub, \href{https://github.com/VincentGranville/Machine-Learning}{here}.

Chapters are mostly independent from each other, allowing you to read in random order. A glossary, index and numerous cross-references make the navigation easy and unify all the chapters. The style is very compact, getting down to the point quickly, and suitable to business professionals eager to learn a lot of useful material in a limited amount of time. Jargon and arcane theories are absent, replaced by simple English to facilitate the reading by non-experts, and to help you discover topics usually made inaccessible to beginners. 

While state-of-the-art research is presented in all chapters, the prerequisites to read this book are minimal: an analytic professional background, or a first course in calculus and linear algebra. The original presentation avoids all unnecessary math and statistics, yet without eliminating advanced topics. 


Finally, this book is the main reference for my upcoming course on intuitive machine learning. For details about the classes, see \href{https://mltechniques.com/courses/}{here}. \vspace{1ex}

\noindent {\bf About the Author}\vspace{1ex}

\noindent Vincent Granville is a pioneering data scientist and machine learning expert, co-founder of Data Science Central (acquired by a publicly traded company), former VC-funded executive, author and patent owner. Vincent’s past corporate experience includes Visa, Wells Fargo, eBay, NBC, Microsoft, CNET, InfoSpace. Vincent is also a former post-doc at Cambridge University, and the National Institute of Statistical Sciences (NISS).  

Vincent published in {\em Journal of Number Theory}, {\em Journal of the Royal Statistical Society} (Series B), and {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}. He is also the author of multiple books, available \href{https://mltechniques.com/resources/}{here}. He lives  in Washington state, and enjoys doing research on stochastic processes, dynamical systems, experimental math and probabilistic number theory.

\hypersetup{linkcolor=red}
\tableofcontents 
%\hypersetup{linkcolor=blue}

\Chapter{Machine Learning Cloud Regression and Optimization}{The Swiss Army Knife of Optimization} 
%%%%%%%%%%%%5\begin{abstract} 
%The Swiss Army Knife of Optimization

This chapter is not about \gls{gls:regression} performed in the cloud. It is about considering your data set as a cloud of points or observations, where the
 concepts of dependent and independent variables (the response and the features) are blurred. It is a very general type of regression, offering
 backward-compatibility with existing methods. Treating a variable as the response amounts to setting a constraint on the multivariate parameter, and results in an optimization algorithm with  Lagrange multipliers. The originality comes from unifying and bringing under a same umbrella, a number of disparate methods each solving a part of the general problem and originating from various fields. I also 
 propose a novel approach to logistic regression, and a generalized \gls{gls:rsquared} adapted to shape fitting, model fitting, \gls{gls:featureselection} 
and  \gls{gls:dimreduct}.  In one example, I show how the technique can perform unsupervised clustering, with \glspl{gls:cr} for
 the cluster centers obtained via parametric bootstrap.

Besides ellipse fitting and its importance in \textcolor{index}{computer vision}\index{computer vision}, an interesting application is non-periodic sum of periodic time series. While rarely discussed in machine learning circles, such models explain many phenomena, for instance ocean tides. It is particular useful in time-continuous situations where the error is not a white noise, but instead smooth and continuous everywhere. For instance, granular temperature forecast.  Another curious application is modeling meteorite shapes. Finally, my methodology is model free and data driven, with a focus on \gls{gls:numericalstability}. Prediction intervals and confidence regions
 are obtained via bootstrapping. I provide Python code and \gls{gls:syntheticdata} generators for replication purposes. 
%%%%%%%%%%%%%%%%%%\end{abstract}

\hypersetup{linkcolor=red}


%\listoffigures

\section{Introduction: circle fitting}\label{circ1}

The goal is to unify all regression techniques and present a simple, generic  framework to solve
 most problems dealing with fitting an equation to a data set. Currently, there are dozens of types of regressions, each with its own
 methodology and algorithm. Here I propose a single methodology and a single algorithm to solve all these problems.

The originality of my technique resides in my approach and methodology, rather than in the type of math or algorithm being used. Like all generic methods, it is rather abstract and  one would think more difficult to learn and describe. To the contrary, I believe it is 
 actually more intuitive and easier to grasp. First,
 the dependent variable and independent features are interchangeable: the concept of dependent variables is even absent in my methodology. Thus I call it ``cloud regression", as the data set is viewed as a cloud of points, with no particular axis or dimension being privileged unless explicitly required. 
 Then the technique is model-free: it uses resampling and bootstrap to build prediction intervals, or confidence intervals for the 
 regression coefficients. 

A judicious choice of notations makes my methodology backward-compatible with all existing techniques. The concept of \gls{gls:rsquared} is slightly modified  
 to offer extra possibilities: measuring the quality of the fit for the full model versus a sub-model of your choice. 
 In standard regression, the sub-model is a constant and referred to as the base model. Here the sub-model could be fitting a circle if the full model 
 is about ellipses, or fitting a plane versus an hyperplane in standard linear regression.

All regression books and chapters for beginners start with fitting a line. Here the easiest example -- the first one to be taught -- is fitting a circle centered at the origin. Think about it for a moment: intuitively, the estimated radius is the average radius computed on the data points. Indeed, this is the solution
 produced by my technique. The second easiest case is then fitting a line involving a slope and an intercept. Both examples are a particular case of fitting a quadratic form (ellipsoid). 

This presentation is intended to beginners. There are examples, just as in standard regression, where the solution is not unique. In my opinion, non-uniqueness should be embraced rather than avoided: in real life one would expect that multiple, different shapes can fit to a particular data set. Finding several of them provides more insights about your data. However, conditions needed for uniqueness are not discussed here: this is the topic of a more advanced presentation. 

In many cases, thanks to an appropriate re-parameterization, the solution is obtained using simple constrained optimization and Lagrange multipliers. It has more to do with elementary calculus than advanced matrix algebra. In particular, 
there is no explicit mention of \textcolor{index}{eigenvalues}\index{eigenvalue} 
[\href{https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors}{Wiki}] or 
 \textcolor{index}{singular value decomposition}\index{singular value decomposition} [\href{https://en.wikipedia.org/wiki/Singular_value_decomposition}{Wiki}]. Also, the shape does not need to have derivatives, though if it does, a faster implementation is possible, with a Newton-like algorithm. Indeed, the shape may be differentiable nowhere: think about fitting a Brownian motion to a set of observations. 

I provide examples using \gls{gls:syntheticdata}\index{synthetic data} [\href{https://en.wikipedia.org/wiki/Synthetic_data}{Wiki}] and Python code. One of them involves time series forecasting with two periods $p,q$ where $p/q$ is not a rational number. Since $p$ and $q$ are among the parameters to be estimated, this is a true non-linear problem that can not be transformed into something linear via a 
 \gls{gls:linkf}\index{link function} [\href{https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function}{Wiki}], unlike (say) logistic regression.  
A real life application, to benchmark the performance of the method, is predicting ocean tides: large, granular geospatial data sets are available to test
 the prediction algorithm in this non-linear context. 


Finally, ``cloud regression" encompasses the \textcolor{index}{general linear model}\index{general linear model} [\href{https://en.wikipedia.org/wiki/General_linear_model}{Wiki}], 
the \textcolor{index}{generalized linear model}\index{generalized linear model} [\href{https://en.wikipedia.org/wiki/Generalized_linear_model}{Wiki}] (and thus logistic regression), 
 as well as \textcolor{index}{weighted least squares}\index{weighted least squares} [\href{https://en.wikipedia.org/wiki/Generalized_least_squares#Weighted_least_squares}{Wiki}]
 (and thus Bayesian regression). Via the mapping $z\mapsto w$ discussed in section~\ref{prevmeth}, it can accommodate splines as in  
\textcolor{index}{adaptive regression splines}\index{regression splines} [\href{https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline}{Wiki}].   
 Both cloud regression and the \textcolor{index}{total least squares}\index{total least squares} method [\href{https://en.wikipedia.org/wiki/Total_least_squares}{Wiki}]  minimize the sum of the squared distances between the data points and the shape, though my method does not give the  response (called 
 the dependent variable by statisticians) a particular status: in other words, it also works in the standard situation where there is no response, but just a cloud of points instead. 
 In addition, my technique handles truly non-linear situations, unlike the generalized linear model. For that reason, I call it the mother of all regressions.

This is not the first time a regression technique does not discriminate between dependent and independent variables: \textcolor{index}{partial least squares}\index{partial least squares} [\href{https://en.wikipedia.org/wiki/Partial_least_squares_regression}{Wiki}] 
  also allows for that.  See also~\cite{fit2015}. 


\subsection{Previous versions of my method}\label{prevmeth}\label{mapmp}

The current version is much more general, simpler and radically different from the first implementation. However, it may help to provide
 some historical context. Initially, the goal was to compute the sum of the squared distances between a set of points (the observations, or the ``cloud"), and
 a pre-specified shape $\Gamma_\theta$ (a line, plane or circle) belonging to a parametric family driven by a multidimensional parameter $\theta$.   

The idea was as follows. Let $P_\infty$ be a fixed point located extremely far away from the shape, and $P$ be a point of the \gls{gls:trainingset}. Draw the line
 $L_P$ that goes through $P$ and $P_\infty$, and find the intersection $\Gamma_\theta \cap \L_P$ closest to $P$, between the shape and the line. Let $Q_\theta$ be this point. The point in question may not be unique or may not exist (depending on $\theta$), but the distance $\Delta_\theta(P)=||P-Q_\theta||$ is, assuming there is an intersection. Then find $\theta^*$ that 
minimizes the sum of $\Delta_\theta(P)$ computed over all training set points $P$. This $\theta^*$, if unique, determines the shape that best fits the data. Traditional projection-based techniques compute the exact distance between a point and a shape, and therefore require the shape to be differentiable. The method based on
 $P_\infty$ works with shapes that are not differentiable. Some particular cases in the new implementation produce similar or identical results to those obtained with the $P_\infty$ method.

If the shape in question is an hyperplane and the context is traditional multivariate linear \gls{gls:regression}, then the shape is defined by 
$g(w,\theta)=0$ where $w=(y,x_1,\dots,x_m)$ and $g(w,\theta)=\theta_0 y+(\theta_1 x_1+\cdots +\theta_m x_m)$. Here $y$ corresponds to the dependent variable, $x_1,\dots, x_m$ to the features, and $\theta_0, \theta_1,\dots,\theta_m$ are the regression coefficients, with the constraint 
$\theta_0=-1$. In the new methodology, the constraint $\theta_0=-1$ is handled using a Lagrange multiplier, but other than that, it leads to the same traditional solution. If there is an intercept, then $x_1=1$. In the end, the goal is to propose a technique that is both general and intuitive,
 following the modern trend of \gls{gls:explainableai}\index{explainable AI} [\href{https://en.wikipedia.org/wiki/Explainable_artificial_intelligence}{Wiki}].

In a second version of my methodology (not the current version), I introduced a mapping system, which essentially is a change of coordinates 
 associated to a link function. The
 goal was to transform the data so that after the mapping, it is more amenable to a simple solution. Also, it is an attempt at
 obtaining a scale-independent solution: whether your unit is a mile or a kilometer should have no impact on the solution. In its most general form, the observations and parameters are denoted as $z$ and $\varphi$. The shape satisfies the equation $h(z,\varphi)=0$. The mapping is defined as 
$g(w,\theta)=\xi(h(z,\varphi))$ where $\xi : \mathbb{R} \rightarrow \mathbb{R}$ is a strictly monotonic function, with $w=\nu(z)$ and $\theta
 = \phi(\varphi)$, for some multivariate one-to-one mappings $\nu$ and $\phi$. All the computations are done in the $(w,\theta)$-space, thought it is possible to revert back to the original $(z,\varphi)$ when computations are done, if ever needed. 

I eventually dropped both $\xi$ and simply ignored $\varphi$ and $\phi$, leading to a less abstract model, yet covering all practical cases. 
Thus in the current version, $h(z,\varphi)=g(w,\theta)$, and we don't care about $\varphi$. We may as well use $\varphi=\theta$. The mapping $\nu$ gives rise to spline regression in the new method. However, when splines are used, they are pre-specified rather than estimated, to avoid over-fitting. Typically, they are chosen to simplify the computations.

Finally, I was interested in some original dimension reduction technique. Not a true data reduction technique, but it allows you to reduce the number of parameters by a factor two: consider $w$ and $\theta$ to be complex, rather than real numbers, for instance via a mapping $z \mapsto w$ from $\mathbb{R}^2$ to 
$\mathbb{C}$, with $w=\nu(z)$. A benefit is the possibility to use
 a \textcolor{index}{conformal map}\index{conformal map} [\href{https://en.wikipedia.org/wiki/Conformal_map}{Wiki}] for $\nu$, thus preserving angles. Such an example is the \textcolor{index}{log-polar map}\index{log-polar map} [\href{https://en.wikipedia.org/wiki/Log-polar_coordinates}{Wiki}] $z=\exp(w)$ with
$g(w,\theta)=z^\theta=\exp(\theta w)$, which corresponds to using the polar coordinate system with $\theta,z,w\in\mathbb{C}$: it makes things easier when dealing with circular data. The next step was to look at quaternions to reduce the number of parameters by a factor four, but there are a number of challenges. Anyway, I promised to keep things simple in this introductory 
 presentation, so I won't discuss complex or quaternion mappings here. This is the topic of future research. 

It is interesting to note that the problem of circle fitting has been quite extensively studied, see~\cite{ieee200y}. Essentially, these
 methods solve the problem using $\varphi$ and they are not trivial. The solution based on my method 
 involves working with $\theta$ and leads to a very classic algorithm with a simple solution. The price to pay is that the $\theta$
 parameters are less obvious to interpret: they are the coefficients of a quadratic form. To the contrary, the direct solution 
 involves $\varphi$ parameters that have obvious meaning: the radius of the circle, its center and (in case of an ellipse) the rotation angle. 
However, my approach makes it a lot easier to generalize to ellipses
 and even far more complicated shapes, or hyperplanes for that matter, while at the same time having a solution that is even simpler than those discussed in~\cite{ieee200y} and applicable to
 the circle only. Of course, in this case there is a one-to-one mapping between $\varphi$ and $\theta$, see 
 \href{https://math.stackexchange.com/questions/1810677/center-and-axis-of-quadratic-surface}{here}. So you can always retrieve
 $\varphi$ from $\theta$. 

  
\section{Methodology, implementation details and caveats}

I encourage you to first read section~\ref{prevmeth}, as it provides a good amount of context. This section describes the details of the methodology. 
For simplicity, I do not describe the most general case, but a case that is general enough to cover all practical applications. I start by introducing the concept of data (called point cloud), parameter, and shape.

The {\bf data} set is denoted 
 as $W$, and consists of $m+1$ variables and $n$ observations. Thus $W$ is a $n \times (m+1)$ matrix as usual. The $k$-th row 
 corresponds to the $k$-th observation $W_k=(W_{k,0},W_{k,1},\dots,W_{k, m+1})$. For backward compatibility with traditional models, I use the notation $W_{k,0}=Y_k$ for the dependent variable or response (if there is one), and $(X_{k,1},\dots,X_{k,m})=(W_{k,1},\dots,W_{k, m+1})$ for
 the independent variables of features. The column vector corresponding to the response is denoted as $Y$, and the $n\times m$ matrix 
 representing the independent variables is denoted as $X$. The whole data set $W$ is referred to as the point cloud. 

The {\bf parameter} is a multivariate
 column vector denoted as $\theta=(\theta_0,\theta_1,\dots,\theta_d)$, with $d+1$ components. Typically, $d=m$ and $\theta$ satisfies some  constraint, specified by $\eta(\theta)=0$
 for some function $\eta$. The most common functions are $\eta(\theta)=\theta^T\theta-1$, $\eta(\theta)=\theta_0+1$, and
  $\eta(\theta)=(\theta_0+\cdots + \theta_d) - 1$. Here $^T$ denotes the matrix/vector transposition operator.

The purpose is to fit a {\bf shape} to the point cloud. The most typical shapes, after proper mapping, are hyperplanes or quadratic forms (ellipsoids). The
 former is a particular case of the latter. The shape belongs to a parametric family of equations driven by the multivariate parameter $\theta$. The equation of the shape is $g(w,\theta)=0$, for some function $g$. Typical examples include $g(w,\theta)=w\theta$ and $g(w,\theta)=w\theta-1$, with $d=m$. 
The former usually involves an intercept: $X_{k,1}=1$ for all $k=1,\dots,n$. Keep in mind that $w$ and $\theta$ are vectors, but $g(w,\theta)$ is a real number, not a vector. Thus $w\theta$ represents a \textcolor{index}{dot product}\index{dot product} [\href{https://en.wikipedia.org/wiki/Dot_product}{Wiki}]. 

\subsection{Solution, R-squared and backward compatibility}

The shape that best fits the data corresponds to $\theta=\theta^*$, obtained by minimizing the squares:
\begin{equation}
\theta^* = \underset{\theta}{\arg\min} \sum_{k=1}^n g^2(W_k,\theta).\label{tyrefd}
\end{equation}
The solution may not be unique. Uniqueness and \gls{gls:numericalstability} will be addressed in a future article, but the basics are covered in this document. The constraint $\eta(\theta)=0$ guarantees that the solution requires solving a (sometimes non-linear) 
 system of $d+2$ equations with $d+2$  unknowns. In some cases, $d\leq m$ to avoid \textcolor{index}{model identifiability}\index{model identifiability} issues
 [\href{https://en.wikipedia.org/wiki/Identifiability}{Wiki}]. Also, a large $d$ may result in \gls{gls:overfitting}\index{overfitting} 
 [\href{https://en.wikipedia.org/wiki/Overfitting}{Wiki}].  Then, you want $n > d$ otherwise the solution may  not be unique unless you add more 
 constraints on $\theta$. The solution $\theta^*$ is obtained by solving the system
\begin{equation}
\centering
\left\{\begin{split}
 \sum_{k=1}^n \nabla_\theta [g^2(W_k,\theta)] & =\lambda \nabla_\theta [\eta(\theta)],\\
 \eta(\theta) & =0 \\
\end{split}\right. \label{bgvcx}
\end{equation}
where $\nabla_\theta$ is the \gls{gls:gradient}\index{gradient operator} operator with respect to $\theta$ [\href{https://en.wikipedia.org/wiki/Gradient}{Wiki}], and $\lambda$ is called the \textcolor{index}{Lagrange multiplier}\index{Lagrange multiplier} [\href{https://en.wikipedia.org/wiki/Lagrange_multiplier}{Wiki}]. This is a classic constrained convex optimization problem. The top part of~(\ref{bgvcx}) consists of a system of $d+1$ equations with $d+2$ unknowns 
  $\theta_0,\dots\theta_d$ and $\lambda$. The bottom part is a single equation with $d+1$ unknowns  $\theta_0,\dots\theta_d$. Combined together,
 it constitutes a system of $d+2$ equations with $d+2$ unknowns. Note the analogy with \textcolor{index}{Lasso regression}\index{Lasso regression} 
 [\href{https://en.wikipedia.org/wiki/Lasso_(statistics)}{Wiki}] when $\eta(\theta)=\theta^T\theta - 1$, that is, when $\theta^T\theta=1$. 

\noindent The \textcolor{index}{mean squared error}\index{mean squared error} (MSE) relative to a particular $\theta$ is defined as  
\begin{equation}
\text{MSE}(\theta)=\frac{1}{n}\sum_{k=1}^n g^2(W_k,\theta) \geq \text{MSE}(\theta^*). \label{vgbe4} 
\end{equation}
The inequality in~(\ref{vgbe4})  is an immediate consequence of~(\ref{tyrefd}). Now define the 
%\textcolor{index}{R-squared}
\gls{gls:rsquared}
\index{R-squared} with respect to 
 $\theta$ as 
\begin{equation}
R^2(\theta)=1 - \frac{\text{MSE}(\theta^*)}{\text{MSE}(\theta)}. \label{rsqwa}
\end{equation}
It follows immediately that $0\leq R^2(\theta)\leq 1$. A perfect fit corresponds to $\text{MSE}(\theta^*)=0$ (the whole cloud residing on the shape). In that case,  if $\theta\neq \theta^*$ and the optimum $\theta^*$ is unique, then $R^2(\theta)=1$.

In traditional linear \gls{gls:regression}, the R-squared is defined as $R^2(\theta_*)$ where $\theta_*$ is the optimum $\theta$ for the base model. 
The base model corresponds to all the coefficients $\theta_i$ attached to the independent variables set to zero, except the one attached to the intercept. In other words, in the base model, the predicted $Y$ is constant, equal to the empirical mean of $Y$. As a result,  $\text{MSE}(\theta_*)=\text{Var}[Y]$, the empirical variance of $Y$. A consequence is that $R^2(\theta_*)$ is the square of the
 correlation between the observed response $Y$, and the predicted response of the full model. 


Backward compatibility with traditional linear regression works as follows. The standard univariate regression corresponds to 
$g(w,\theta) = w \theta =\theta_0 y +\theta_1 x + \theta_2$, with the constraint $\theta_0=-1$.  Thus $g(w,\theta)=0$ if and only if
 $y= \theta_1 x + \theta_2$. This generalizes to multivariate regression as well.
A more elegant formulation in the new methodology is to replace the constraint $\theta_0=-1$ by the symmetric constraint $\theta_0^2+\theta_1^2+\theta_2^2=1$. 
 Note that $w$ is a row vector and $\theta$ is a column vector. 

\subsection{Upgrades to the model}

By model, I mean the general setting of the method: there is no probabilistic model involved in this discussion. 
\textcolor{index}{Prediction intervals}\index{prediction interval} [\href{https://en.wikipedia.org/wiki/Prediction_interval}{Wiki}] for the individual error $g(W_k,\theta^*)$ at each data point $W_k$ (or for the estimated response attached to $Y_k$, if there is an independent variable) and 
 \glspl{gls:cr}\index{confidence region} [\href{https://en.wikipedia.org/wiki/Confidence_region}{Wiki}] for $\theta^*$ can be obtained via re-sampling and \gls{gls:bootstrap}\index{bootstrapping} [\href{https://en.wikipedia.org/wiki/Bootstrapping_(statistics)}{Wiki}]. This is also true for points outside the training set. 

Also, the squares 
 can be replaced by absolute values, as in \textcolor{index}{quantile regression}\index{quantile regression} [\href{https://en.wikipedia.org/wiki/Quantile_regression}{Wiki}], to minimize the impact of outliers and for scale preservation: if a variable is measured in years, then squares are expressed in squared years, a metric that is meaningless. This leads to a modified, better metric to assess the quality of the fit, replacing the R-squared.  See the section~\ref{pasr} about ``performance
 assessment" in chapter~\ref{chapterregression}, for alternatives to the R-squared. 
The \gls{gls:goodnessoffit} (say, the R-squared) should be measured on the \gls{gls:validset}\index{validation set}
 [\href{https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets}{Wiki}] even though $\theta^*$ is computed on a subset of the \gls{gls:trainingset}: this is a standard practice, called \gls{gls:crossvalid}\index{cross-validation} [\href{https://en.wikipedia.org/wiki/Cross-validation_(statistics)}{Wiki}], 
 illustrated on \gls{gls:syntheticdata} in chapter~\ref{chapterfuzzy} about fuzzy regression.

Now, let's get back to the 
%\textcolor{index}{R-squared}
\gls{gls:rsquared}\index{R-squared}. In standard linear regression, the R-squared is defined as $R^2(\theta_*)$ via 
 Formula~(\ref{rsqwa}), where 
 $\theta_*$ is the optimum $\theta$ for the base model (the predicted response is constant, equal the mean of $Y$ for the base model). In the
 new methodology, there may be no response. Still, the definition of $R^2$ extends to that situation, and is compatible with the traditional version.
 What's more, it leads to many possible $R^2$, one for each sub-model (not just the base model), and this is true too for the standard regression. 
A sub-model corresponds to adding constraints on the parameter vector $\theta$, or in other words, working with a subset of the parameter space. Let $\theta_*$ be the optimum for a specific sub-model, while $\theta^*$ is the optimum for the full model. Then the definition of $R^2$, depending on $\theta_*$, is unchanged. It could not be any simpler! 

Now you can use $R^2$ for model comparison purposes and even for \gls{gls:featureselection}\index{feature selection} [\href{https://en.wikipedia.org/wiki/Feature_selection}{Wiki}]. You can test the improvement obtained by using the full model over a sub-model,  
 with the metric $S(\theta_*)= R^2(\theta^*)-R^2(\theta_*)$. Here $\theta_*$ is the optimum $\theta$ attached to the sub-model. Obviously, $0\leq S(\theta_*)\leq 1$. The larger $S(\theta_*)$, the bigger the improvement. Conversely, the smaller, the better the performance of the sub-model. Examples include fitting an ellipse (full model) versus fitting a circle (sub-model) or using all the features (full model) versus using a subset (sub-model). 
You can compare sub-models and rank them according to $S(\theta_*)$. This allows you to identify the smallest set of features that achieve a good
 enough $S(\theta_*)$, for 
%\textcolor{index}{dimensionality reduction}
\gls{gls:dimreduct}
\index{dimensionality reduction} purposes [\href{https://en.wikipedia.org/wiki/Dimensionality_reduction}{Wiki}]. 


Finally, another update consists of using positive weights $\psi_k(\theta)$ in Formula~(\ref{tyrefd}). This amounts to performing
 \textcolor{index}{weighted regression}\index{weighted regression} [\href{https://en.wikipedia.org/wiki/Generalized_least_squares#Weighted_least_squares}{Wiki}].
 For instance, data points far away from the optimum shape, that is observations with a large $g^2(W_k,\theta^*)$, may be discarded to reduce the
 impact of outliers. Or the weights can be used to balance the coefficients $\theta_i$, in an effort to achieve scale-invariance in the expression
 $w\theta$.  Then the top system in~(\ref{bgvcx}) becomes
\begin{equation}
 \sum_{k=1}^n \psi_k(\theta)\nabla_\theta [g^2(W_k,\theta)] +\sum_{k=1}^n g^2(W_k,\theta)\nabla_\theta[\psi_k(\theta)]  =\lambda \nabla_\theta [\eta(\theta)].  \label{bgvcx2_1228}
\end{equation}



\section{Case studies}

In section~\ref{2ways}, I show how to solve the logistic regression. The first version is standard least squares, to further illustrate
backward compatibility with the traditional method. The second one illustrates how it could be done if you want to follow the spirit of the new methodology.  Then I discuss two fundamental examples based on synthetic data.

\subsection{Logistic regression, two ways}\label{2ways}

In the traditional setting, $w=(y,x)$ where $y$ is the response, and $x$ the features. For the 
%\textcolor{index}{logistic regression}
\index{logistic regression} 
\gls{gls:logreg}
 [\href{https://en.wikipedia.org/wiki/Logistic_regression}{Wiki}],
we have 
$$
g(w,\theta)=g(y,x)= y-F(x\theta), \quad \text{with } \text{ } x\theta=\theta_1 x_1 + \dots \theta_m x_m.
$$
Here $x_1=1$ corresponds to the intercept, thus we have $m-1$ actual features $x_2,x_3,\dots,x_m$. There is no constraint on the parameter $\theta$,
 thus there is no function $\eta(\theta)$. In Formula~(\ref{bgvcx}), $\eta(\theta)=0$ should be ignored, and $\lambda=0$. The function $F$ is
 a cumulative distribution function with a symmetric density around the origin. In this case, $F(x\theta)=1/(1+\exp[-x\theta])$ is the
  standard \textcolor{index}{logistic distribution}\index{logistic distribution}\index{probability distribution!logistic} [\href{https://en.wikipedia.org/wiki/Logistic_distribution}{Wiki}].

In the new methodology, one would proceed as follows. First, the original data is denoted as $z=(v,u)$. The logistic regression applies to the original data. Here $v$ is the response, 
 and $u$ the feature vector. The parameter $\theta$ is unchanged (not subject to a mapping), and still denoted as $\theta$.  This \gls{gls:regression} can be stated as
$$
g(z,\theta)=g(v,u)= v-F(u\theta), \quad \text{with } \text{ } u\theta=\theta_1 u_1 + \dots \theta_m u_m.
$$

The first step is to map $z=(v,u)$ onto $w=(y,x)$, with the hope of simplifying the problem, as discussed in section~\ref{mapmp}. This is done
 via the \gls{gls:linkf}\index{link function} $y=F^{-1}(v)=\log[v/(1-v)]$ and $u=x$. Now we are back to 
$$
g(z,\theta)=g(w,\theta)=g(y,x;\theta)= y-x\theta, \quad \text{with } \text{ } x\theta=\theta_1 x_1 + \dots \theta_m x_m.
$$
This is how standard linear regression is expressed in the new framework. But it is still the traditional linear regression, with nothing new. The final step
 consists in extending $\theta$, adding one component $\theta_0$ to $\theta_1,\dots,\theta_m$. With the new $\theta$ (still denoted as $\theta$) we have $g(w,\theta)=w\theta=\theta_0 w_0+\cdots + \theta_m w_m$. You need to add one constraint on $\theta$. The constraint $\theta_0=-1$, 
 that is $\eta(\theta)=\theta_0+1$, yields the exact same solution as traditional linear regression. But $\theta^T\theta=1$, that is $\eta(\theta)=\theta^T\theta-1$, makes the problem somehow symmetric, and more elegant.

However, in many applications, the response $v$ in the original space is either $0$ or $1$, such as cancer versus non-cancer, or fraud versus non-fraud.  In this case, the link function is undefined. The mapping with the link function works if the response is a proportion, strictly between zero and one. Otherwise, the standard \gls{gls:logreg} is the best approach. 
 A possible workaround is to use for $F$ a distribution with a finite support, such as uniform on $[a,b]$. Afterall, the observed values (the features) are always bounded anyway. Then, intuitively, given $\theta$, estimates of $a$ and $b$ are proportional respectively to the minimum and maximum of $U_k\theta$, over $k=1,\dots,n$. 

This suggests a new approach to logistic regression. First, use the model $v=F_\theta(u\theta)$ in the $(v,u)$-space, where $0\leq v\leq 1$ and 
$F_\theta$ is the
 %\textcolor{index}{empirical distribution}
\gls{gls:empdistr}
\index{empirical distribution} [\href{https://en.wikipedia.org/wiki/Empirical_distribution_function}{Wiki}] of $u\theta$ given $\theta$. Then choose $\theta^*$ that minimizes the sum of squared residuals:
$$
\theta^* =\underset{\theta}{\arg\min}\sum_{k=1}^n g^2(V_k,U_k;\theta)=\underset{\theta}{\arg\min}\sum_{k=1}^n (V_k - F_\theta(U_k\theta))^2.
$$
 Remember, $U_k$ is a row vector, and $\theta$ is a column vector; the dot product $U_k\theta$ is a real number. Also,
 $V_k$ is the binary response attached to the $k$-th observation, while $U_k$ is the corresponding $m$-dimensional feature vector, both in the original 
 $(v,u)$-space. The empirical distribution $F_\theta$ is computed as follows: $F_\theta(t)$ is the proportion of observed feature vectors, among $U_1,\dots,U_n$, satisfying  $U_k\theta\leq t$.
Such a method could be called \textcolor{index}{CDF regression}\index{CDF regression}. You can use the methodology presented here to solve it, but it would be very computer-intensive, because $F_\theta$ depends on $\theta$
 in a non-obvious way. The predicted value for $V_k$, is $F_{\theta^*}(U_k\theta^*)$ in this case.
 

\subsection{Ellipsoid and hyperplane fitting}

This is a fundamental example, with hyperplanes being a particular case of ellipsoids. I illustrate the methodology with an example based on \gls{gls:syntheticdata}, in a small dimension. The idea is to represent the shape with a quadratic form. In two dimensions, the equations is 
$$
\theta_0 x^2 + \theta_1 xy + \theta_2 y^2 + \theta_3 x + \theta_4 y + \theta_5=0.
$$
The trick is to re-write it with artificial variables $w_0=x^2, w_1=xy,w_2=y^2, w_3=x,w_4=y,w_5=1$, so that we can use the general framework
 with $g(w,\theta)=w\theta$. Again, $w\theta$ is the dot product. To avoid the trivial solution $\theta^*=0$, let's add the constraint
 $\theta^T\theta=1$, that is, $\eta(\theta)=\theta^T\theta-1$. Then, $\theta^*$ is solution of  the system
\begin{equation}
\centering
\left\{\begin{split}
( W^TW -\lambda I)\theta & =0, \\
 \theta^T\theta & =1. \\
\end{split}\right. \label{bgvcx2} %xxxxxx bgvcx2b
\end{equation}
The above solution is correct in any dimension. It is a direct application of~(\ref{bgvcx}). Here $W$ is the $n \times 6$ matrix containing the $n$ observations. Thus, $W_{k0}=X_k^2, W_{k1}=X_kY_k, W_{k2}=Y_k^2,  W_{k3}=X_k, W_{k4}=Y_k, W_{k5}=1$. The Python code and additional details, for a slightly different version with a slightly different $\eta(\theta)$, can be found \href{https://scipython.com/blog/direct-linear-least-squares-fitting-of-an-ellipse/}{here}. I use it in my own code, available on my GitHub repository, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/fittingEllipse.py}{here}, under the name \texttt{fittingEllipse.py}.  It is based on Halir's article about fitting ellipses~\cite{Halir98numericallystable}.

The Python code checks if the fitted shape is actually an ellipse. However, in the spirit of my methodology, it does not matter if it is an ellipse, a parabola, an hyperbola or even a line. The uniqueness of the solution is unimportant: indeed, if two very different solutions (say an ellipse and a 
 parabola) yield the same minimum mean squared error and are thus both optimal, it says something about the data set, 
 something interesting to know.   However, it would be interesting to compute $R^2(\theta_*)$ using
 Formula~(\ref{rsqwa}), where $\theta_*$ corresponds to a circle. It would tell
 whether the full model (ellipse) over a significant improvement over the circle sub-model.

\noindent Ellipsoid fitting shares some similarities with multivariate polynomial regression~\cite{vaccari2007}.  The differences are: 
\begin{itemize}
\item Ellipse fitting is a ``full" model;  in the polynomial regression $y=\theta_1+\theta_2 x + \theta_3 x^2$, the terms $y^2$ and $xy$ are always missing.
\item Polynomial regression fits a curve that is unbounded such as $y=x^2$, resulting in poor fitting; to the contrary in ellipse fitting (if the solution is actually an ellipse) the solution is bounded.
 \item To get as many terms in polynomial regression as in ellipse fitting, the only way is to increase the degree of the polynomial, which further increases the instability of the solution.
\end{itemize}  

\noindent Finally, Umbach~\cite{ieee200y} proposes a different approach to ellipse fitting. It is significantly more complicated, and indeed, they stopped at the circle. In short, their method directly estimates the center, semi-axis lengths and rotation angle via least squares, as opposed to estimating the coefficients in the quadratic form that represents the ellipse. More on parametric curves can be found in my article on shape recognition in chapter~\ref{chaptershapes}. 

\subsubsection{Curve fitting: 250 examples in one video}

Ellipse fitting is performed by setting \texttt{mode='CurveFitting'} in the Python code. The program automatically creates a number of ellipses, specified by their parameters (center, lengths of semi-axes, and rotation angle), then generates a different training set for each ellipse, and outputs the result of the fitting procedure as an image. The images are then bundled together to produce a video, and an animated gif. Each image features a particular 
 ellipse and \gls{gls:trainingset}, as well as the fitted ellipse based on the training set. The ellipses parameters are set by the variable \texttt{params} in the code: it is an array with five components. The number of ellipses is set by the parameter \texttt{nframes}, which also determines the number of frames in the output video.

%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.8\textwidth]{ellipse1c.png}  
\caption{Fitted ellipse (blue), given the training set (red) distributed around a partial arc}
\label{fig:ellipse11b}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

Actually, the program does a little more: it works with ellipse arcs. Using the centroid of the training set to estimate the center of the ellipse does not work in this case. So the program retrieves the original, unknown ellipse even if the training set consists of points randomly distributed around a portion of that ellipse. The arc in question is determined by a lower and upper angle in polar coordinates,
 denoted respectively as \texttt{tmin} and \texttt{tmax} in the code, with \texttt{tmin=0} and \texttt{tmax=2$\pi$} corresponding to the full ellipse.  

The training set consists of $n$ observations generated as follows. 
First sample $n$ points on the ellipse (or the arc you are interested in). Then perturb these points by adding some noise. You have two options:
 \texttt{noise\_CDF='Uniform'} and \texttt{noise\_CDF='Normal'}. The amount of noise is specified by the parameter 
 \texttt{noise} in the code. For the uniform distribution on the square $[-a,a] \times [-a,a ]$, \texttt{noise} represents $a$. For the bivariate normal distribution with covariance matrix $\sigma^2 I$ where $I$ is the identity matrix, it represents $\sigma^2$. There are various ways of sampling points on an ellipse. Three options are offered here, set by the parameter \texttt{sampling}. 
They are described in section~\ref{pipybv}, in the paragraph ``Sampling points on an ellipse arc". The option \texttt{'Enhanced'} is the only one performing  stochastic sampling (points randomly picked up on the ellipse), and used in Figure~\ref{fig:meteor}. 


In Figure~\ref{fig:meteor}, the size of the training set is $n=30$ while in Figure~\ref{fig:ellipse11b}, $n=250$. In the code, $n$ is represented by the variable \texttt{npts}. The training set is colored in red, the fitted ellipse in blue, and if present on the image as in Figure~\ref{fig:meteor}, the
 true ellipse is in black. The latter appears as a polygon rather than an ellipse because the sampled points on the true ellipse are joined by segments, 
 and $n$ is small. Typically, the true and fitted ellipses are very close to each other, although there is a systematic bias too small to be noticed to the naked eye unless the ellipse eccentricity is high. More on this soon.

Table~\ref{ellipar} compares the exact parameter values (set by the user) of the true ellipse in
 Figure~\ref{fig:meteor},  to a few sets of estimated values obtained by least squares. Each set of estimates is computed using
 a different training set. All training sets are produced with same amount and type of noise, to give an idea of the variance of the parameter estimates  
 at a specific level of noise.  The five parameters are the ellipse center $(x_0,y_0)$, the lengths of the semi axes $(a_p, b_p)$, and the ellipse orientation
 (the rotation angle $\phi$). 

In some cases, the solution may not be unique, or could be an hyperbola or parabola rather than an ellipse. For instance, if the ellipse is reduced to a circle, any value for the rotation angle is de facto correct, though the estimated curve is still unique and correctly identified. Also, if the true ellipse has a high eccentricity, the generated white (unbiased) noise 
 forces the training set points inside the ellipse more often than they should, as opposed to outside the boundary. This is because inside the ellipse, the noise from the North side strongly overlaps with the noise from the South side, assuming the long axis is the horizontal one. The result is biased estimates for $a_p$ and $b_p$, smaller than the actual ones. In the end, the fitted curve has a higher eccentricity than the true one. The effect is more pronounced the higher the eccentricity. If the variance of the noise is small enough, there is almost no bias.  

I posted a video featuring $250$ fitted ellipses with the associated training sets, \href{https://youtu.be/ReyA9NWyjso}{here}  on YouTube. 
It is also on GitHub, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Images/ellipseFitting300dpi.mp4}{here}. The accompanying animated gif is also on GitHub, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Images/ellipse100dpi.gif}{here}. All were produced with the Python code. In the video, the transition from one ellipse to the next one is very smooth. While I use $250$ different combinations of arcs, rotation angles, eccentricities and noises to feature a large collection of very different cases, these configurations slowly progress from one frame to the next one in the video. But the $250$ frames eventually cover a large spectrum of situations. The last one shows a perfect fit, where the training set points are all on the true ellipse.


\subsubsection{Confidence region for the fitted ellipse}

The computation of \glspl{gls:cr} is performed by setting \texttt{mode='ConfidenceRegion'} in the Python code. This time the program automatically creates a number of training sets (determined by the parameter \texttt{nframes}), for the same ellipse  
specified by its parameters \texttt{params}: center, lengths of semi-axes, and rotation angle. 
 Then it estimates the ellipse parameters, and thus the true ellipse, uniquely determined by the parameters in question. Figure~\ref{fig:meteor} shows the confidence region for the example outlined in Table~\ref{ellipar}.

\renewcommand{\arraystretch}{1.2} %%%
\begin{table}[H]
\[
\begin{array}{lccccc}
\hline
   & x_0 & x_1  & a_p & b_p & \phi  \\
\hline
\text{Exact values} & 3.00000 & -2.50000 & 7.00000 & 4.00000 & 0.78540\\
\text{Training set } 1 & 2.61951 & -2.41818 & 6.44421 & 3.82838 & 0.72768 \\
\text{Training set } 2 & 2.77270 & -2.32346 & 6.59185 & 4.24624 & 0.59971 \\
\text{Training set } 3 & 3.29900 & -2.60532 & 6.71834 & 4.15181 & 0.87760 \\
\text{Training set } 4 & 2.71936 & -2.42349 & 7.15562 & 4.52900 & 0.80404 \\
\hline
\end{array}
\]
\caption{\label{ellipar} Estimated ellipse parameters vs true values ($n=30$), for shape in Figure~\ref{fig:meteor}}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%

Actually I decided to display a polygon instead of the fitted ellipse, by selecting the option \texttt{sampling=} \texttt{'Enhanced'}. The polygon consists of the predicted locations of the $n=30$ training set points on the fitted ellipse. These locations are obtained in the exact same way that predicted values are obtained in a linear \gls{gls:regression} problem and then shown on the fitted line. After all, ellipse fitting as presented in this article is a particular case of the general cloud regression technique. I then joined these points using segments, resulting in one polygon per training set. The superimposition of these polygons is the confidence region. 

The reason for using polygons rather than ellipses is for a particular application: estimating the shape of a small, far away celestial body based on a low resolution image.  This is particularly useful when creating a taxonomy of these bodies: the shape parameters are used to classify them and understand their history as well as gravity interactions, and can be used as features
 in a machine learning algorithm. Then, for a small meteorite, people expect to see it as a polyhedron (the 3D version of a polygon) rather than an ellipsoid. Of course, if the number $n$ of points in the training set is large, then the polyhedron is indistinguishable  from the fitted ellipsoid. But in practice, with low resolution images,  $n$ is usually pretty small. 


%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{meteorite.png}  
\caption{Confidence region in blue, $n=30$ training set points; $50$ training sets (left) vs $150$ (right)}
\label{fig:meteor}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------




\subsubsection{Python code}\label{pipybv}

The main parameters in the code are highlighted in red in this high level summary. The program 
  is listed in this section and also available on GitHub 
\href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/fittingEllipse.py}{here},
 under the name \texttt{fittingEllipse.py}.

The least square optimization is performed using an implementation of  the Halir and Flusser algorithm~\cite{Halir98numericallystable}, adapted from a version posted 
 \href{https://scipython.com/blog/direct-linear-least-squares-fitting-of-an-ellipse/}{here} by Christian Hill, the author of the book
``Learning Scientific Programming with Python" \cite{chsp2016}. The optimization -- minimizing the sum of squared errors between the observed points and the fitted ellipse -- is performed on the coefficients of the quadratic equation representing the ellipse.
 This is the easiest way to do it, and it is also the approach that I use elsewhere in this article. 
 The function \texttt{fit\_ellipse} does the job, while \texttt{cart\_to\_pol} converts these coefficients into meaningful features: the center, rotation angle, eccentricity and the major and minor semi-axes of the ellipse [\href{https://simple.wikipedia.org/wiki/Semi-major_and_semi-minor_axes}{Wiki}]. \vspace{1ex}

%\pagebreak
 
\noindent {\bf Sampling points on an ellipse arc}\vspace{1ex}

\noindent The Python code also integrates other components written by various authors. First, it offers three options to sample points on an ellipse or a partial arc of an ellipse, via the parameter \textcolor{red}{\texttt{sampling}} in the main section of the code: 
\begin{itemize}
\item Evenly spaced on the perimeter, via the function \texttt{sample\_from\_ellipse\_even}. The code is adapted from an anonymous version posted 
\href{https://math.stackexchange.com/questions/3710402/generate-random-points-on-perimeter-of-ellipse}{here}. It requires the evaluation of
 \textcolor{index}{elliptic integrals} [\href{https://en.wikipedia.org/wiki/Elliptic_integral}{Wiki}]. The technique is identical to that described in the section~\ref{centr1}  on ``weighted centroid" in chapter~\ref{chaptershapes}. 
\item Randomly chosen on the perimeter, in such a way that on average, the distance between two consecutive sampled points on the ellipse is constant. 
 It involves sampling from a multinormal distribution, rescaling the points and then sorting the sampled points so that they are ordered on the perimeter. This 
 also requires sorting an array according to another array.  It is done in the function
 \texttt{sample\_from\_ellipse}.
\item The standard, easiest but notoriously skewed sampling. It consists of choosing equally spaced angles in the polar representation of the ellipse. For curve fitting, it is good enough with very little differences compared to the two other methods.
\end{itemize}

\noindent For sampling on a partial arc rather then the full ellipse, set the parameters 
\textcolor{red}{\texttt{tmin}} and \textcolor{red}{\texttt{tmax}} to the appropriate values, in the main loop.
These are angles in the polar coordinate system, and should lie between $0$ and $2\pi$. The full ellipse corresponds to
 \texttt{tmin} set to zero, and \texttt{tmax} set to $2\pi$. \vspace{1ex}

\noindent{\bf Training set and ellipse parameters}\vspace{1ex}

\noindent Then, to create the training set, perturb the sampled points on the ellipse via uniform or Gaussian noise. 
 The choice is set by the parameter \textcolor{red}{\texttt{noise\_CDF}} in the main section of the code. The parameter 
\textcolor{red}{\texttt{noise}} determines the amount of noise, or in other words, the noise variance. Points, be it on the ellipse or in the training set,
 are arrays with names \texttt{x} and \texttt{y} (respectively for the X and Y coordinates). The number of points in the \gls{gls:trainingset} is determined
 by the parameter \textcolor{index}{\texttt{npts}}.

The shape of the ellipse is set by the $5$-dimensional parameter vector \textcolor{red}{\texttt{params}}. Its components, denoted
 as \texttt{x0}, \texttt{y0}, \texttt{ap}, \texttt{bp}, \texttt{phi} throughout the code, are respectively the center of the ellipse, the length of the semi-axes, and the orientation of the ellipse (the rotation angle). \vspace{1ex}

\noindent {\bf Confidence regions versus curve fitting} \vspace{1ex}

\noindent The program creates \textcolor{red}{\texttt{nframes}} ellipses, one at a time in the main loop.
 At each iteration, the created ellipse and training set is saved as a PNG image, for inclusion in a video or animated gif (see next paragraph). This is why the variable controlling the main loop is called  \texttt{frame}. At each iteration the true parameters of the ellipse (the ones you chose), and their least squares estimates are displayed on the screen. 

If the parameter \textcolor{red}{\texttt{mode}} is set to \texttt{'ConfidenceRegion'}, then the amount of noise and all ellipse parameters are kept constant throughout the iterations. The fitted shapes varies from one iteration to the next depending on the training set (itself depending on the noise), creating a \gls{gls:cr} for a specific ellipse, given a specific amount of noise. New fitted ellipses keep being added to the image without erasing older ones, to display the confidence region under construction. Highly eccentric ellipses result in biased confidence regions.
 The method used to build the confidence region is known as \textcolor{index}{parametric bootstrap}\index{parametric bootstrap} [\href{https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Parametric_bootstrap}{Wiki}]. 

To the contrary, if \texttt{mode} is set to \texttt{'FittingCurves'}, a different ellipse with different parameters and different amount of noise is generated at each iteration, erasing the previous one in the new image. The purpose in this case is to assess the quality of the fit depending on the amount of noise and 
 the shape of the ellipse (the eccentricity and whether you use a full or partial arc for training, in particular).\vspace{1ex}

\noindent {\bf Creating videos and animated gifs} \vspace{1ex}

\noindent At each iteration in the main loop, the program creates and saves an image in your local folder, featuring the training set in red (a cloud of dots distributed around the true ellipse arc) and the fitted ellipse in blue. The name of the image is 
\texttt{ellipsexxx.png}, where \texttt{xxx} is
 the current frame number.  At the last iteration (the last frame in the video), the true ellipse -- the one with the parameters set in the main loop -- is added to the image, in black:  it allows you to assess the bias when choosing
 the option \texttt{mode='ConfidenceRegion'}. 

The video is saved as
  \texttt{ellipseFitting.mp4}, and the animated gif as \texttt{ellipseFitting.gif}.
The parameter \textcolor{red}{\texttt{DPI}} (dots per inch) sets the resolution of the images. For videos, I recommend to set it to $300$. For animated gifs, I recommend using $100$. At the bottom of the code, when creating the video with a Moviepy function, you are free  
 to change \texttt{fps=20} to any other value. This parameter sets the number of frames per second. 
\textcolor{index}{Color transparency}\index{color transparency} [\href{https://en.wikipedia.org/wiki/Alpha_compositing}{Wiki}] is used throughout the plots, to improve the rendering when multiple curves overlap. The transparency level is denoted as \texttt{alpha} in the code. You are not supposed to play with it unless
 you don't like my choice. I mention it just in case you are wondering what \texttt{alpha} represents.

Finally, if the parameter \textcolor{red}{\texttt{ShowImage}} is set to \texttt{True}, each 
 frame is also displayed on your screen. The default value is \texttt{False}. Turn it on only
 if you produce a very small number of frames, say \texttt{nframes=10} or less.\\


\begin{lstlisting}
import numpy as np
import matplotlib.pyplot as plt
import moviepy.video.io.ImageSequenceClip  # to produce mp4 video
from PIL import Image  # for some basic image processing

def fit_ellipse(x, y):

    # Fit the coefficients a,b,c,d,e,f, representing an ellipse described by
    # the formula F(x,y) = ax^2 + bxy + cy^2 + dx + ey + f = 0 to the provided
    # arrays of data points x=[x1, x2, ..., xn] and y=[y1, y2, ..., yn].

    # Based on the algorithm of Halir and Flusser, "Numerically stable direct
    # least squares fitting of ellipses'.

    D1 = np.vstack([x**2, x*y, y**2]).T
    D2 = np.vstack([x, y, np.ones(len(x))]).T
    S1 = D1.T @ D1
    S2 = D1.T @ D2
    S3 = D2.T @ D2
    T = -np.linalg.inv(S3) @ S2.T
    M = S1 + S2 @ T
    C = np.array(((0, 0, 2), (0, -1, 0), (2, 0, 0)), dtype=float)
    M = np.linalg.inv(C) @ M
    eigval, eigvec = np.linalg.eig(M)
    con = 4 * eigvec[0]* eigvec[2] - eigvec[1]**2
    ak = eigvec[:, np.nonzero(con > 0)[0]]
    return np.concatenate((ak, T @ ak)).ravel()

def cart_to_pol(coeffs):

    # Convert the cartesian conic coefficients, (a, b, c, d, e, f), to the
    # ellipse parameters, where F(x, y) = ax^2 + bxy + cy^2 + dx + ey + f = 0.
    # The returned parameters are x0, y0, ap, bp, e, phi, where (x0, y0) is the
    # ellipse centre; (ap, bp) are the semi-major and semi-minor axes,
    # respectively; e is the eccentricity; and phi is the rotation of the semi-
    # major axis from the x-axis.

    # We use the formulas from https://mathworld.wolfram.com/Ellipse.html
    # which assumes a cartesian form ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0.
    # Therefore, rename and scale b, d and f appropriately.
    a = coeffs[0]
    b = coeffs[1] / 2
    c = coeffs[2]
    d = coeffs[3] / 2
    f = coeffs[4] / 2
    g = coeffs[5]

    den = b**2 - a*c
    if den > 0:
        raise ValueError('coeffs do not represent an ellipse: b^2 - 4ac must'
                         ' be negative!')

    # The location of the ellipse centre.
    x0, y0 = (c*d - b*f) / den, (a*f - b*d) / den

    num = 2 * (a*f**2 + c*d**2 + g*b**2 - 2*b*d*f - a*c*g)
    fac = np.sqrt((a - c)**2 + 4*b**2)
    # The semi-major and semi-minor axis lengths (these are not sorted).
    ap = np.sqrt(num / den / (fac - a - c))
    bp = np.sqrt(num / den / (-fac - a - c))

    # Sort the semi-major and semi-minor axis lengths but keep track of
    # the original relative magnitudes of width and height.
    width_gt_height = True
    if ap < bp:
        width_gt_height = False
        ap, bp = bp, ap

    # The eccentricity.
    r = (bp/ap)**2
    if r > 1:
        r = 1/r
    e = np.sqrt(1 - r)

    # The angle of anticlockwise rotation of the major-axis from x-axis.
    if b == 0:
        phi = 0 if a < c else np.pi/2
    else:
        phi = np.arctan((2.*b) / (a - c)) / 2
        if a > c:
            phi += np.pi/2
    if not width_gt_height:
        # Ensure that phi is the angle to rotate to the semi-major axis.
        phi += np.pi/2
    phi = phi % np.pi

    return x0, y0, ap, bp, phi

def sample_from_ellipse_even(x0, y0, ap, bp, phi, tmin, tmax, npts):

    npoints = 1000
    delta_theta=2.0*np.pi/npoints
    theta=[0.0]
    delta_s=[0.0]
    integ_delta_s=[0.0]
    integ_delta_s_val=0.0
    for iTheta in range(1,npoints+1):
        delta_s_val=np.sqrt(ap**2*np.sin(iTheta*delta_theta)**2+ \
                            bp**2*np.cos(iTheta*delta_theta)**2)
        theta.append(iTheta*delta_theta)
        delta_s.append(delta_s_val)
        integ_delta_s_val = integ_delta_s_val+delta_s_val*delta_theta
        integ_delta_s.append(integ_delta_s_val)
    integ_delta_s_norm = []
    for iEntry in integ_delta_s:
        integ_delta_s_norm.append(iEntry/integ_delta_s[-1]*2.0*np.pi)    
    
    x=[]
    y=[] 
    for k in range(npts):
        t = tmin + (tmax-tmin)*k/npts
        for lookup_index in range(len(integ_delta_s_norm)):
            lower=integ_delta_s_norm[lookup_index]
            upper=integ_delta_s_norm[lookup_index+1]
            if (t >= lower) and  (t < upper):
                t2 = theta[lookup_index]
                break    
        x.append(x0 + ap*np.cos(t2)*np.cos(phi) - bp*np.sin(t2)*np.sin(phi))
        y.append(y0 + ap*np.cos(t2)*np.sin(phi) + bp*np.sin(t2)*np.cos(phi))

    return x, y

def sample_from_ellipse(x0, y0, ap, bp, phi, tmin, tmax, npts): 

    x=np.empty(npts)
    y=np.empty(npts)
    x_unsorted=np.empty(npts)
    y_unsorted=np.empty(npts)
    angle=np.empty(npts)

    # sample from multivariate normal, then rescale 
    cov=[[ap,0],[0,bp]]
    count=0
    while count < npts:
        u, v = np.random.multivariate_normal([0, 0], cov, size = 1).T
        d=np.sqrt(u*u/(ap*ap) + v*v/(bp*bp))
        u=u/d
        v=v/d
        t = np.pi + np.arctan2(-ap*v,-bp*u)   
        if t >= tmin and t <= tmax:
            x_unsorted[count] = x0 + np.cos(phi)*u - np.sin(phi)*v
            y_unsorted[count] = y0 + np.sin(phi)*u + np.cos(phi)*v
            angle[count]=t
            count=count+1

    # sort the points x, y for nice rendering with mpl.plot
    hash={}
    hash = dict(enumerate(angle.flatten(), 0)) # convert array angle to dictionary
    idx=0
    for w in sorted(hash, key=hash.get):
        x[idx]=x_unsorted[w]
        y[idx]=y_unsorted[w]
        idx=idx+1

    return x, y

def get_ellipse_pts(params, npts=100, tmin=0, tmax=2*np.pi, sampling='Standard'):

    # Return npts points on the ellipse described by the params = x0, y0, ap,
    # bp, e, phi for values of the parametric variable t between tmin and tmax.

    x0, y0, ap, bp, phi = params
    
    if sampling=='Standard':
        t = np.linspace(tmin, tmax, npts)
        x = x0 + ap * np.cos(t) * np.cos(phi) - bp * np.sin(t) * np.sin(phi)
        y = y0 + ap * np.cos(t) * np.sin(phi) + bp * np.sin(t) * np.cos(phi)
    elif sampling=='Enhanced':
        x, y = sample_from_ellipse(x0, y0, ap, bp, phi, tmin, tmax, npts) 
    elif sampling=='Even':
        x, y = sample_from_ellipse_even(x0, y0, ap, bp, phi, tmin, tmax, npts) 

    return x, y

def vgplot(x, y, color, alpha, npts, tmin, tmax):

    plt.plot(x, y, linewidth=0.2, color=color,alpha=alpha) # plot exact ellipse 
    # fill gap (missing segment in the ellipse plot) if plotting full ellipse
    if tmax-tmin > 2*np.pi - 0.01:
        gap_x=[x[npts-1],x[0]]
        gap_y=[y[npts-1],y[0]]
        plt.plot(gap_x, gap_y, linewidth=0.2, color=color,alpha=alpha)
    return()

def main(npts, noise, seed, tmin, tmax, params, sampling):

    # params = x0, y0, ap, bp, phi (input params for ellipse)

    # Get points x, y on the exact ellipse and plot them
    x, y = get_ellipse_pts(params, npts, tmin, tmax, sampling)
    if frame == nframes-1 and mode == 'ConfidenceRegion':
        vgplot(x, y,'black', 1, npts, tmin, tmax)

    # perturb x, y on the ellipse with some noise, to produce training set
    np.random.seed(seed)
    if noise_CDF=='Normal':
      cov = [[1,0],[0,1]]  
      u, v = np.random.multivariate_normal([0, 0], cov, size = npts).T
      x += noise * u
      y += noise * v
    elif noise_CDF=='Uniform':
      x += noise * np.random.uniform(-1,1,size=npts) 
      y += noise * np.random.uniform(-1,1,size=npts)

    # get and print exact and estimated ellipse params
    coeffs = fit_ellipse(x, y) # get quadratic form coeffs
    print('True ellipse    :  x0, y0, ap, bp, phi = %+.5f %+.5f %+.5f %+.5f %+.5f' % params)
    fitted_params = cart_to_pol(coeffs)  # convert quadratic coeffs to params
    print('Estimated values:  x0, y0, ap, bp, phi = %+.5f %+.5f %+.5f %+.5f %+.5f' % fitted_params)
    print()

    # plot training set points in red
    if mode == 'ConfidenceRegion':
      alpha=0.1  # color transparency for Confidence Regions
    elif mode == 'CurveFitting':
      alpha=1
    plt.scatter(x, y,s=0.5,color='red',alpha=alpha)   
 
    # get points on the fitted ellipse and plot them
    x, y = get_ellipse_pts(fitted_params,npts, tmin, tmax, sampling) 
    vgplot(x, y,'blue', alpha, npts, tmin, tmax)

    # save plots in a picture [filename is image]
    plt.savefig(image, bbox_inches='tight',dpi=dpi)  
    if ShowImage:
        plt.show()
    elif mode=='CurveFitting':
        plt.close() # so, each video frame contains one curve only
    return()

#--- Main Part: Initializationa

noise_CDF='Normal'       # options:  'Normal' or 'Uniform'
sampling='Enhanced'      # options: 'Enhanced', 'Standard', 'Even'
mode='ConfidenceRegion'  # options: 'ConfidenceRegion' or 'CurveFitting' 
npts = 25                # number of points in training set

ShowImage = False # set to False for video production
dpi=100     # image resolution in dpi (100 for gif / 300 for video)
flist=[]    # list of image filenames for the video
gif=[]      # used to produce the gif image
nframes=50  # number of frames in video

# intialize plotting parameters
plt.rcParams['axes.linewidth'] = 0.5
plt.rc('axes',edgecolor='black') # border color
plt.rc('xtick', labelsize=6) # font size, x axis  
plt.rc('ytick', labelsize=6) # font size, y axis

#--- Main part: Main loop

for frame in range(0,nframes): 

    # Global variables: dpi, frame, image
    image='ellipse'+str(frame)+'.png' # filename of image in current frame
    print("Creating image",image) # show progress on the screen

    # params = (x0, y0, ap, bp, phi) : first two coeffs is center of ellipse, last one 
    #  is rotation angle, the two in the middle are the semi-major and semi-minor axes

    if mode=='ConfidenceRegion':
        seed=frame      # new set of random numbers for each image 
        noise=0.8       # amount of noise added to to training set
        # 0 <= tmin < tmax <= 2 pi
        tmin=0          # training set: ellipse arc starts at tmin
        tmax = 2*np.pi  # training set: ellipse arc ends at tmax
        params = 3, -2.5, 7, 4, np.pi/4 # ellipse parameters
    elif mode=='CurveFitting':
        seed = 100          # same seed (random number generator) for all images
        p=frame/(nframes-1) # assumes nframes > 1
        noise=3*(1-p)*(1-p) # amount of noise added to to training set
        # 0 <= tmin < tmax <= 2 pi
        tmin= (1-p)*np.pi   # training set: ellipse arc starts at tmin 
        tmax= 2*np.pi       # training set: ellipse arc ends at tmax  
        params = 4, -3.5, 7, 1+6*(1-p), 2*(p+np.pi/3) # ellipse parameters 

    # call to main function 
    main(npts, noise, seed, tmin, tmax, params, sampling)

    # processing images for video and animated gif production (using pillow library)
    im = Image.open(image)
    if frame==0:  
      width, height = im.size  # determines the size of all future images
      width=2*int(width/2)
      height=2*int(height/2)
      fixedSize=(width,height) # even number of pixels for video production 
    im = im.resize(fixedSize)  # all images must have same size to produce video
    gif.append(im)       # to produce Gif image [uses lots of memory if dpi > 100] 
    im.save(image,"PNG") # save resized image for video production
    flist.append(image)

# output video / fps is number of frames per second
clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(flist, fps=20) 
clip.write_videofile('ellipseFitting.mp4')

# output video as gif file 
gif[0].save('ellipseFitting.gif',save_all=True, append_images=gif[1:],loop=0)    
\end{lstlisting}

\subsection{Non-periodic sum of periodic time series}

In this section I consider the problem of fitting a \textcolor{index}{non-periodic trigonometric series}\index{time series!non-periodic} via least squares. One  well known example 
is the Dirichlet eta function with an infinite number of superimposed periods. Its modulus is pictured in Figure~\ref{fig:rhs3v2}.
Practical application are also numerous. A good exercise
 is to download ocean tide data, and use the methodology described in this section to predict low and high tides, at various locations: the tides are influenced mostly by two factors --
 gravitation from the moon and from the sun -- each with its own period. But the combination is not periodic. The fitting technique allows you
 to quantify the effect of each hidden component (the sun and the moon in this case) and retrieve their respective periods. 
The tide data is available 
 for free, \href{https://tidesandcurrents.noaa.gov/}{here}.

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.81\textwidth]{rhs3.png}  
\caption{Three non-periodic time series made of periodic terms. Source: \cite{vggrh2022}.}
\label{fig:rhs3v2}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------


With my notation, the problem is 
 defined by $w=(y,x)$ and
 \begin{equation}
g(y,x;\theta)= y - \Big[\theta_1 \cos(\theta_2 x + \theta_3) + \theta_4\cos (\theta_5 x + \theta_6)\Big]. \label{rtfer}
\end{equation}
There is no constraint on $\theta$, and thus no $\eta$ function and no $\lambda$ in~(\ref{bgvcx}). Here $y$ is the response, and $x$ represents the time. For this reason, I also use the notation $y=f_\theta(x)$, equivalent to $g(y,x;\theta)=0$. This type of problem is called \textcolor{index}{curve fitting}\index{curve fitting} 
 [\href{https://en.wikipedia.org/wiki/Curve_fitting}{Wiki}] in scientific computing. There are libraries to solve it: in Python,
 one can use the \texttt{optimize.curve\_fit} function from the Scipy library. For more details, see the
 \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html}{Python documentation}. 



Finally, if you are only interested in predicting $y$ given $x$ , but not in estimating the parameter vector $\theta$, then the following interpolation formula 
is sometimes useful:
\begin{equation}
y=f(x)\approx \frac{\sin\pi x}{\pi}\cdot \Bigg[ \frac{f(0)}{x} + 2x\sum_{k=1}^n (-1)^k \frac{f(k)}{x^2-k^2}\Bigg] \label{apcfv}
\end{equation}
I used it in Exercise 9 in my article on the Riemann Hypothesis~\cite{vggrh2022}, to get a good approximation of $y=f_\theta(x)$ when $y$ is known (that is, observed) at
integer increments $x=0,1$ and so on, even though $\theta$ is not known. I applied it to a function $f_\theta(x)$ closely related
to those pictured in Figure~\ref{fig:rhs3v2}. The function in question (namely, the real part of the Dirichlet eta function) can be expressed as an infinite sum of cosine terms with different amplitudes and different periods.
 Thus, in this case, the dimension of the unobserved $\theta$ is infinite, and $\theta$ remains an hidden parameter in the prediction experiment, hidden to the experimenter (as in a blind test). Its components are the various period and amplitude coefficients. 
 The approximation formula~(\ref{apcfv}) works under certain conditions: see Exercise~9 in~\cite{vggrh2022} for details. 

\subsubsection{Numerical instability and how to fix it}

Consider the simpler case where $\theta_3=\theta_6=0$: let's drop these two coefficients from the model. Even then, 
the problem is \textcolor{index}{ill-conditioned}\index{ill-conditioned problem} [\href{https://en.wikipedia.org/wiki/Condition_number}{Wiki}]. In particular if $\theta^*=(\theta_1^*, \theta_2^*,\theta_4^*,\theta_5^*)$ is an optimum solution, so is $(\theta_4^*, \theta_5^*,\theta_1^*,\theta_2^*)$. At the very least, 
 without loss of generality, you need to add the
 constraint $|\theta_1|\geq |\theta_4|$. 

In Python, I used the \texttt{curve\_fitting} function from Scipy. It does a poor job for this problem, even if you specify bounds for the coefficients 
 $\theta_1, \theta_2,\theta_4,\theta_5$ and start the algorithm with a vector $\theta$ close to an optimum $\theta^*$. My test involved
\begin{itemize}
\item Finding an optimum $\theta$ if the fitting function is $y=f_\theta(x) =\theta_1 \cos \theta_2 x +  \theta_4 \cos \theta_5 x$,
\item Using a synthetic training set where  $y=a_1 \cos a_2 x +  a_4 \cos a_5 x + a_7 \cos a_8 x$.
\end{itemize}
 The gray curve in  Figure~\ref{fig:pyplot1}, called the ``model'', is $y=a_1 \cos a_2 x +  a_4 \cos a_5 x$, 
 while the blue one is the fitted curve (not necessarily unique), and the dots represent the observations (training set in red, validation set in orange). 
The observations points do not lie exactly on the gray curve because I introduced some noise: the third term $a_7 \cos a_8 x$ between the model and the data. Note that the observations are equally spaced with respect to the X-axis, but absolutely not with respect to the Y-axis. It is possible
 to use a different sampling mechanism to address this issue. 
 The figure was produced with the Python code in section~\ref{poihgf}. The values of $a_1,\dots,a_8$ are pre-specified. Evidently, if $a_7=0$, then an obvious optimum solution is
 $\theta_i^*=a_i$ for $i=1,2,4,5$. It provides a perfect fit. Also, the coefficient $a_7$ specifies the amount of noise in the data. 


Unfortunately,  \texttt{curve\_fitting} fails or performs very poorly in most cases. Figure~\ref{fig:pyplot1} shows one of the relatively rare cases where it
 works well in the presence of noise. This Python function is still very useful in many contexts, but not in our example. The default setting 
(\texttt{method='lm'}, to be avoided) uses a supposedly robust version of the Levenberg-Marquardt algorithm, dating back to 1977, 
 see \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html}{here}. Essentially, it gets stuck in
 local minima or fails to converge, and may even reject a manually chosen initial condition close to an optimum as ``not feasible". Surprisingly,
 increasing the amount of noise in the data, can provide improvements. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{cosines.png}  
\caption{Training set (red), validation set (orange), fitted curve (blue) and model (gray)}
\label{fig:pyplot1}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

To fix the numerical instability problem, one can use a more modern technique, such as \textcolor{index}{swarm optimization}\index{swarm optimization} [\href{https://en.wikipedia.org/wiki/Particle_swarm_optimization}{Wiki}]. The \texttt{PySwarms} library documented \href{https://pyswarms.readthedocs.io/en/latest/}{here} 
 is the Python solution. For a tutorial, see 
 \href{https://machinelearningmastery.com/a-gentle-introduction-to-particle-swarm-optimization/}{here}. A simpler home-made solution taking 
advantage of the fact that the fitting curve $f_\theta(x)$ (called \gls{gls:regression} function by statisticians) is linear both in $\theta_1$ and
 $\theta_4$,  consists in splitting the problem as follows.

\begin{itemize}
\item Step 1: Sample $(\theta_2,\theta_5)$ over a region large enough to encompass the optimum values.
\item Step 2:  Given $\theta_2,\theta_5$, find $\theta_1,\theta_4$ that minimizes $E(\theta)=\sum_{k=1}^n g^2(Y_k,X_k;\theta)$. 
\end{itemize}
Here $(Y_k,X_k)$ is the $k$-th observation in the training set, and $\theta=(\theta_1,\theta_2, \theta_4,\theta_5)$. Both steps are straightforward.  
You repeat them until you reach a point where the minimum $E(\theta)$ computed over the past iterations almost never decreases anymore. Step 2 is just a simple standard two-dimensional linear regression with no intercept, with an exact solution. One of the benefits is that if there are
 multiple minima, you are bound to find them all, without facing convergence issues. It also reduces a 4-D Monte-Carlo simulation to a 2-D one.



\subsubsection{Python code}\label{poihgf}

Despite the issues previously described, I decided to include the Python code. It shows how the \texttt{curve\_fitting} function works, beyond using the default settings.
 It is still a good optimization technique for many problems such as polynomial regression. Unfortunately, not for our problem. If you decrease
 the amount of noise from \texttt{a7=0.2} to \texttt{a7=0.1} in the code below, there is a dramatic drop in performance. Likewise, if you change the  initial $\theta_5$ (the last component in  \texttt{\textcolor{gray2}{θ}\_start}) 
%\texttt{{\fontspec{GFS Artemisia} θ}\_start} 
  from 
$1.8$ to $1.7$, the performance collapses. The exact value in this example is $\theta_5=2$ by construction; the estimated
 (final) value produced by the Python code is about $1.995$. 


\renewcommand{\arraystretch}{1.2} %%%
\begin{table}[H]
\[
\begin{array}{lrrrr}
\hline
   & \theta_1 & \theta_2  & \theta_4 & \theta_5  \\
\hline
\text{Start}	&	0.00000	&	1.00000	& 0.00000	 &  1.80000\\
\text{End}	&	0.52939	&	1.42184	&-0.67571	 &  1.99526\\
\text{Model}	&	0.50000	&	1.41421	& -0.70000	 & 2.00000  \\
\hline
\end{array}
\]
\caption{\label{tablibvc} First and last step of \texttt{curve\_fitting}, approaching the model.}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%

Table~\ref{tablibvc} shows the quality of the estimation, for the parameter vector $\theta=(\theta_1,\theta_2,\theta_4,\theta_5)$.
The procedure \texttt{curve\_fitting} starts with an initial guess \texttt{\textcolor{gray2}{θ}\_start} labeled ``Start" in the table, and
 ends  with the entry marked as ``End" in the table: supposedly, close to an optimum $\theta^*$. Because of the way the 
\gls{gls:syntheticdata}\index{synthetic data} is generated (within the Python code), the row marked ``Model" and consisting of the value 
 $a_1,a_2,a_4,a_5$ is always close to an optimum $\theta^*$,
 unless the amount of noise introduced in the training set is too large. The ``End" solution (the output of
 \texttt{curve\_fitting}) is based exclusively on the training set points (the red dots 
 in Figure~\ref{fig:pyplot1}), not on the \gls{gls:validset} (the orange dots). Yet the approximation is unusually good, given the amount of noise.

 By noise, I don't mean a random or Gaussian noise. Here the noise is deterministic: the purpose of this test is to check how well we can predict a phenomena modeled by a superimposition of multiple cosine terms of arbitrary periods, phases and amplitudes -- for instance ocean tides over time -- if we only use a sum of two cosine terms as an approximation. This model (its generalization with more terms)  is particular useful in situations where the error is not a \textcolor{index}{white noise}\index{white noise} [\href{https://en.wikipedia.org/wiki/White_noise}{Wiki}], but instead smooth and continuous everywhere: for instance in granular temperature forecast.  

The curve fitting code, also producing Figure~\ref{fig:pyplot1}, is on my GitHub repository,
 \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/fittingCurve.py}{here},
 under the name \texttt{fittingCurve.py}, and also listed below. I use Greek letters in the code  to represent the $\theta$ vector and its
 components, for consistency reasons. Python digests them with no problem.  \\
 
\renewcommand{\arraystretch}{1.0} %%%
\renewcommand{\arraystretch}{1.4} %%%


%\lstinputlisting{fittingCurve3.py}

\begin{lstlisting}[escapechar=@]
import numpy as np
import matplotlib as mpl
from scipy.optimize import curve_fit
from matplotlib import pyplot, rc

# initializations, define functions

def fpred(x, @\textcolor{gray2}{θ}@1, @\textcolor{gray2}{θ}@2, @\textcolor{gray2}{θ}@4, @\textcolor{gray2}{θ}@5):
  y = @\textcolor{gray2}{θ}@1*np.cos(@\textcolor{gray2}{θ}@2*x)+ @\textcolor{gray2}{θ}@4*np.cos(@\textcolor{gray2}{θ}@5*x) 
  return y

def fobs(x, a1, a2, a4, a5, a7, a8):   
  y = a1*np.cos(a2*xobs)+a4*np.cos(a5*xobs)+a7*np.cos(a8*xobs)  
  return y

n=800
n_training=200  # first n_training points is training set
x=[]
y_obs=[]
y_pred=[]
y_exact=[]

# create data set (observations)

a1=0.5 
a2=np.sqrt(2)
a4=-0.7 
a5=2
a7=0.2 # noise (e=0 means no noise)
a8=np.log(2)

for k in range(n):
  xobs=k/20.0
  x.append(xobs)
  y_obs.append(fobs(xobs, a1, a2, a4, a5, a7, a8)) 

# curve fitting between f and data, on training set

@\textcolor{gray2}{θ}@_bounds=((-2.0, -2.5, -1.0, -2.5),(2.0, 2.5, 1.0, 2.5))
@\textcolor{gray2}{θ}@_start=(0.0, 1.0, 0.0, 1.8)
popt, _ = curve_fit(fpred, x[0:n_training], y_obs[0:n_training],\
    method='trf',bounds=@\textcolor{gray2}{θ}@_bounds,p0=@\textcolor{gray2}{θ}@_start) 
@\textcolor{gray2}{θ}@1, @\textcolor{gray2}{θ}@2, @\textcolor{gray2}{θ}@4, @\textcolor{gray2}{θ}@5 = popt
print('Estimates       : @\textcolor{mauve}{θ}@1=%.5f @\textcolor{mauve}{θ}@2=%.5f @\textcolor{mauve}{θ}@4=%.5f @\textcolor{mauve}{θ}@5=%.5f' % (@\textcolor{gray2}{θ}@1, @\textcolor{gray2}{θ}@2, @\textcolor{gray2}{θ}@4, @\textcolor{gray2}{θ}@5))
print('True values: @\textcolor{mauve}{θ}@1=%.5f @\textcolor{mauve}{θ}@2=%.5f @\textcolor{mauve}{θ}@4=%.5f @\textcolor{mauve}{θ}@5=%.5f' % (a1, a2, a4, a5))
print('Initial val: @\textcolor{mauve}{θ}@1=%.5f @\textcolor{mauve}{θ}@2=%.5f @\textcolor{mauve}{θ}@4=%.5f @\textcolor{mauve}{θ}@5=%.5f' % \
   (@\textcolor{gray2}{θ}@_start[0], @\textcolor{gray2}{θ}@_start[1], @\textcolor{gray2}{θ}@_start[2], @\textcolor{gray2}{θ}@_start[3]))

# predictions  

for k in range(n):
  xobs=x[k]
  y_pred.append(fpred(xobs, @\textcolor{gray2}{θ}@1, @\textcolor{gray2}{θ}@2, @\textcolor{gray2}{θ}@4, @\textcolor{gray2}{θ}@5))
  y_exact.append(fpred(xobs, a1, a2, a4, a5)) 

# show plot

mpl.rcParams['axes.linewidth'] = 0.5
rc('axes',edgecolor='black') # border color
rc('xtick', labelsize=6) # font size, x axis 
rc('ytick', labelsize=6) # font size, y axis
pyplot.scatter(x[0:n_training],y_obs[0:n_training],s=0.5,color='red')
pyplot.scatter(x[n_training:n],y_obs[n_training:n],s=0.5,color='orange')
pyplot.plot(x, y_pred, color='blue',linewidth=0.5)
pyplot.plot(x, y_exact, color='gray',linewidth=0.5)
pyplot.show()
\end{lstlisting}

\subsection{Fitting a line in 3D, unsupervised clustering, and other generalizations}

In three dimensions, a line is the intersection of two planes $A$ and $B$, respectively with equations
$g_1(w,\theta_A)=0$ and $g_1(w,\theta_B)=0$. For instance, $g_1(w,\theta_A)=\theta_0 w_0 + \theta_1 w_1 +\theta_2 w_2 
- \theta_3$. To fit the line, \vspace{1ex}
\begin{itemize}
\item let $\theta_A=(\theta_0,\theta_1,\theta_2,\theta_3)^T$ and $\theta_B=(\theta_4,\theta_5,\theta_6,\theta_7)^T$, 
\item use $\theta=(\theta_A,\theta_B)$ and $g(w,\theta)=g_1^2(w,\theta_A)+g_1^2(w,\theta_B)$ in Formula~(\ref{tyrefd}), 
\item use the constraints  $\theta_A^T\theta_A + \theta_B^T\theta_B=1$, or two constraints: $\theta_A^T\theta_A=1$ and $\theta_B^T\theta_B=1$. 
\end{itemize}
 With two constraints, we have two 
Lagrange multipliers $\lambda_A$ and $\lambda_B$ in Formula~(\ref{bgvcx}).

Likewise, if the data points are either in plane $A$ or plane $B$ and you want to find these planes based on unlabeled training set observations, proceed exactly as for fitting a line in 3D (the
 previous paragraph), but this time use $g(w,\theta)=g_1(w,\theta_A)g_1(w,\theta_B)$ instead. By ``unlabeled", I mean that you don't know which plane a training set point is assigned to.   This is actually an unsupervised clustering problem. The \gls{gls:trainingset} points (called cloud) don't have to reside 
 in two separate flat planes: the cloud consists of two sub-clouds $A$ and $B$, possibly overlapping, each with its own thickness.

This generalizes in various ways: replacing planes by ellipsoids, working in higher dimensions, or with multiple sub-clouds
 $A, B, C$ and so on. One interesting example is as follows. Training set points are distributed in two clusters $A$ and $B$, and you want to find the centers of these clusters. Typically, one uses a \textcolor{index}{mixture}\index{mixture model} [\href{https://en.wikipedia.org/wiki/Mixture_model}{Wiki}] to model this situation.
 In our model-free framework, with the convention that $w$ is a row vector and $\theta_A,\theta_B$ are column vectors, the problem is stated as 
\begin{equation}
g(w,\theta)=||w^T-\theta_A||^{p/2} \cdot ||w^T-\theta_B||^{p/2} \quad \label{gerdsa}
\end{equation}
where $w, \theta_A, \theta_B$ have same dimensions, and there is no constraint on $\theta=(\theta_A,\theta_B)$. Here, $p>0$ is an \gls{gls:hyperparam}\index{hyperparameter} [\href{https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)}{Wiki}]. 
If you use an iterative algorithm to find an optimum solution $\theta^*=(\theta_A^*,\theta_B^*)$, that is, the two centers $\theta_A^*,\theta_B^*$, it makes sense to start with
 $\theta_A=\theta_B$ being the centroid of the whole cloud. The solution may not be unique. Obviously, the problem is symmetric in $\theta_A$ and 
$\theta_B$, but there may be more subtle types of non-uniqueness. 

A more general formulation, not discussed here, is to replace $w^T-\theta_A$ and $w^T-\theta_B$ respectively by 
$\Lambda(w^T-\theta_A)$ and $\Lambda(w^T-\theta_B)$, where $\Lambda$ is a square invertible matrix, considered and treated as an extra parameter,
 part of the general parameter $\theta=(\theta_A,\theta_B,\Lambda)$. As a pre-processing step, one can normalize the data, so that its center is the origin and its covariance matrix -- after a suitable rotation -- is diagonal. My method preserves the norm $||\cdot||$, under such transformations.  

\subsubsection{Example: confidence region for the cluster centers}\label{reserse}

I tested model~(\ref{gerdsa}) in one dimension with $n=1000$ observations, 
 $p=1$ and \gls{gls:syntheticdata}\index{synthetic data} generated as a mixture of two normal distributions. The purpose was to identify the cluster centers. The results are pictured in Figure~\ref{fig:screen2}.  The centers are correctly identified, despite the huge overlap between the two clusters (the purple area in the histogram). 
The histogram shows the point distribution in cluster $A$ (blue) and $B$ (red), here for the test labeled ``Sample~$39$" in the screenshot.

I computed confidence intervals for the centers, using \textcolor{index}{parametric bootstrap}\index{parametric bootstrap} [\href{https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Parametric_bootstrap}{Wiki}]. The theoretical values for the center
 locations are $0.50$ and $1.00$. The $95\%$ confidence intervals are $[0.46,0.53]$ and $[1.00, 1.04]$. The small bias is due to the uneven point counts and variances in the generated clusters: $400$ points and $\sigma=0.3$ in $A$, versus $600$ points and $\sigma=0.2$ in $B$. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.56\textwidth]{screen2g.png}   
\caption{Finding the two centers $\theta_A^*, \theta_B^*$ in sample 39; $n=1000$}
\label{fig:screen2}
\end{figure}
%imgpy9979_2and3.PNG screen2e.png  
%-------------------------

The bias visible in Figure~\ref{fig:cr} could be exacerbated by the
\textcolor{index}{Mersenne twister}\index{Mersenne twister} pseudo-random number generator [\href{https://en.wikipedia.org/wiki/Mersenne_Twister}{Wiki}] used in \texttt{numpy.random}, especially in extensive simulations such as this one:
 see chapter~\ref{chapterPRNG}. In this experiment, the twister was called $800$ million times.  Then, I used the most extreme estimates based on $40$ tests, to get the upper and lower bounds of the confidence intervals. This could have contributed to the bias as well, as it is not the most robust approach: running $400$ tests and building the confidence intervals based on test percentiles is more robust. But it requires $10$ times more computations.

Out of curiosity, I decided to plot the \gls{gls:cr}\index{confidence region} [\href{https://en.wikipedia.org/wiki/Confidence_region}{Wiki}] for $(\theta_A^*,\theta_B^*)$, this time using $\num{5000}$ tests, 
 based on one trillion \glspl{gls:prng}: $\num{5000}$ tests $\times$ $1000$ points per test $\times$ $\num{100000}$ iterations per test $\times$ two 
 coordinates. It took about two hours of computing time on my laptop. The result is displayed in Figure~\ref{fig:cr}. Not surprisingly, the confidence
 region is elliptic: see section~\ref{generi} for the explanation.

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.73\textwidth]{CR3.png}  
\caption{Biased confidence region for $(\theta_A^*,\theta_B^*)$;  same example as in Figure~\ref{fig:screen2}; true value is $(0.5,1.0)$}
\label{fig:cr}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

The implementation details are in the short
 Python code in section~\ref{pyclustrw}.  This unsupervised center-searching algorithm is told that there are two clusters, but it does not know what proportion of points belong to $A$ or $B$, nor the variances attached to
 each distribution, or which one is labeled $A$ or $B$. If the number of clusters is not specified, try different values. In section~\ref{bbcl}, I describe
 a blackbox solution to find the optimum number of clusters. 

\subsubsection{Exact solution and caveats}\label{exact5}

Let $W_k$ be the $k$-th observation ($k=1,\dots,n$) stored as a row vector, $W$ the data set (a matrix with $n$ rows) and $\theta=(\theta_A,\theta_B)$ where $\theta_A,\theta_B$ are column vectors, each with the same dimension as $W_k$. Then, according to~(\ref{tyrefd}),  any optimum solution satisfies
\begin{equation}
(\theta_A^*,\theta_B^*) = \underset{\theta}{\arg\min} \sum_{k=1}^n g^2(W_k,\theta)= \underset{\theta_A,\theta_B}{\arg\min} \sum_{k=1}^n ||W_k^T-\theta_A||^{p} \cdot ||W_k^T-\theta_B||^{p}. \label{optim1}
\end{equation}
 As usual, the \textcolor{index}{mean squared error}\index{mean squared error} (MSE) is the sum in~(\ref{optim1}) computed at $\theta^*=(\theta_A^*,\theta_B^*)$, and divided by $n$. 
It follows immediately that if $h$ is a distance-preserving mapping (rotation, symmetry or translation) and $\theta^*$ is an optimum solution for the data set $W$, then $h(\theta^*$) is optimum for $h(W)$, since MSE is invariant under such transformations. Thus, without loss of generality, one can assume that the data set $W$ is centered at the origin.

You can choose a different $p$ for each cluster -- say $p_A,p_B$ -- or a weighted sum as in Formula~(\ref{bgvcx2_1228}). 
If $p=2$ and there is only one cluster (thus no $\theta_B$), then $\theta_A^*$ is the centroid of the point
 cloud (the $W_k$'s). Now let the clusters be well separated to the point that each observation $W_k$ coincides either with the center of $A$, or the center of $B$. 
Then there is only one unique optimum: $\theta_A^*$ is the centroid of one cluster, $\theta_B^*$ is the centroid of the other cluster, and the MSE is zero.

If there are three clusters, formula (\ref{optim1})  becomes
\begin{equation}
(\theta_A^*,\theta_B^*,\theta_C^*) = \underset{\theta_A,\theta_B,\theta_C}{\arg\min} \sum_{k=1}^n ||W_k^T-\theta_A||^{p} \cdot ||W_k^T-\theta_B||^{p} \cdot ||W_k^T-\theta_C||^{p}. \label{optim2}
\end{equation}

If $p>0$ is an even integer (or both $p_A,p_B$ are even integers), then finding the optimum in~(\ref{optim1}) or~(\ref{optim2}) consists in solving a system of 
 multivariate polynomials, where the 
 unknowns are the components of $\theta_A$ and $\theta_B$. The more clusters, the higher the degrees of the polynomials. In particular, in one dimension with $p=p_A=p_B=2$, if the data set (the point cloud $W$) is centered at the origin, then the optimum $(\theta_A^*,\theta_B^*)$ satisfies 
\begin{equation}
\theta_A^* \theta_B^* =-\sigma^2_W, \quad \theta_A^*+\theta_B^*=\frac{1}{n\sigma^2_W}\sum_{k=1}^n W_k^3, 
\quad \text{with } \sigma^2_W=\frac{1}{n}\sum_{k=1}^n W_k^2. \label{hhggffd}
\end{equation}
Formula~(\ref{hhggffd}) can easily be generalized to any dimension. It is obtained by vanishing the \gls{gls:gradient} to find the minimum in~(\ref{optim1}). Unfortunately, no exact formula exists for $p=1$. However, in one dimension for $p=1$, we have
$$
|W_k-\theta_A|\cdot |W_k-\theta_B| = \frac{1}{2}\cdot | (W_k-\theta_A)^2 + (W_k-\theta_B)^2-(\theta_A -\theta_B)^2 |.
$$

Based on the few tests done so far in one dimension, in general $p=1$ works better than $p=2$. If the two clusters are moderately unbalanced as in Figure~\ref{fig:screen2}, then $p=1$ still does well. 
However, if they are strongly unbalanced as in Figure~\ref{fig:hard}, the method fails.  It is still fixable, by choosing two different $p$'s,  denoted as $p_A$ and $p_B$. Then the optimum 
corresponds to the larger $p$ attached to
 the smaller cluster: the blue one, in Figure~\ref{fig:hard}.  In this case $p_A=3,p_B=1$ works just as well  as $p_A=1, p_B=1$ does
 in Figure~\ref{fig:screen2}.
The blue cluster has $1500$ points in Figure~\ref{fig:hard}, the red one $8500$. The two centers are $0.5$ and $1.0$,
   and the standard deviations are $0.1$ and $0.2$ respectively for the small and large cluster.

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.72\textwidth]{hard2.png}  
\caption{Challenging mixture, requiring $p_A=3,p_B=1$ to identify the two cluster centers}
\label{fig:hard}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------


 It is a good practice to try $(1,1), (2,1)$ and $(3,1)$ for $(p_A,p_B)$, to see which one provides the best fit as illustrated
 in section~\ref{kmeans}. This is particularly useful in blackbox systems, when automatically processing thousands of datasets without a human being ever looking at any of them.  Because the 
 ``best" solution -- from a visual point of view -- is sometimes a local rather than a global minimum, I recommend to list all the local minima found during the search for an 
 optimum $(\theta_A^*,\theta_B^*)$. 

Of course, there is a limit to this methodology, as well as to any other classifiers or mode-searching algorithms: if the mixture
 has just one mode, it is impossible to find two distinct meaningful centers, no matter what method you use. This happens when the cluster centers are truly distinct, but the variances 
are huge, or if one cluster contains very few points. Another example is when the clusters have irregular, non-convex shapes, with multiple centers and holes. In the latter case,
 the methodology is still useful to find the local modes.

 \subsubsection{Comparison with K-means clustering}\label{kmeans}

I included the \textcolor{index}{K-means clustering}\index{K-means clustering} method [\href{https://en.wikipedia.org/wiki/K-means_clustering}{Wiki}] in the Python code in section~\ref{pyclustrw}. Here I compare Kmeans with my method, on two datasets, each 
 with $n=1000$ points and two overlapping clusters. The theoretical cluster centers based on the underlying mixture model are $0.5$ and $1.0$ respectively.  
The datasets are pictured in Figure~\ref{fig:screen2} and~\ref{fig:hard}.

On challenging data with significant cluster overlapping, my method frequently outperforms Kmeans. However, on very skewed data, you 
 need two exponents $p_A, p_B$ in Formula~(\ref{optim1}), rather than just $p$ to get the best performance. When using $(p_A, p_B)=(3,1)$, my method is denoted as $(3,1)$. Likewise, with $(p_A, p_B)=(1,1)$ or $(p_A, p_B)=(2,1)$, my method is denoted respectively as $(1, 1)$ and $(2,1)$.
Intuitively, model $(3, 1)$ -- compared to the default version $(1, 1)$ -- allows you to  reduce the influence of a highly dominant cluster $A$, by penalizing its contribution to MSE, with a small $p_A$. Due to the symmetry of the problem, model $(3, 1)$ and $(1, 3)$ yield the same optimum centers 
with labels swapped,
and the same MSE at the optimum.  
  The reference model with one 
 single cluster coinciding with the centroid of the whole dataset, is called the ``base" model. An alternative is to use 
the \textcolor{index}{medoid}\index{medoid} [\href{https://en.wikipedia.org/wiki/Medoid}{Wiki}] rather than the centroid in the base model. 

The remaining of this section, besides model comparison, focuses on automatically detecting whether the default model $(1, 1)$ is good enough for a 
specific dataset,   or whether you should use the cluster centers generated by $(2,1)$ or $(3, 1)$ instead. The decision is based on 
 the MSE defined at the beginning of section~\ref{exact5}. However comparing MSE$(1,1)$  and
MSE$(1, 3)$ even for the same $\theta$ and on the same dataset is meaningless. This is the challenge that we face. 

To solve this problem, I start by computing MSE$(1,1)$ and MSE$(3,1)$ for all methods and both data sets. 
I skipped MSE$(2,1)$ as it yields similar solutions to MSE$(3,1)$. The results are summarized in 
Table~\ref{mse111} for the first dataset, and Table~\ref{mse112} for the second dataset. The vector   
$\theta^*(1, 1)=(\theta_A^*(1, 1),\theta_B^*(1, 1))$ contains the two optimum centers according to model $(1,1)$, given a data set. The same notation  
$\theta^*(3, 1)$ 
is used for model $(3,1)$.

\renewcommand{\arraystretch}{1.2} %%%
\begin{table}[H]
\[
\begin{array}{lcccc}
\hline
  \text{Model} &  \theta_A  & \theta_B & \text{MSE}(1,1) & \text{MSE}(3,1)  \\
\hline
\theta^*(1,1)	&	\textcolor{red}{0.53554}&	\textcolor{red}{1.02221}&	\textcolor{red}{0.09804}&	0.02981\\
\theta^*(3,1)	&	\textcolor{red}{0.20602}&	\textcolor{red}{0.94284}&	0.12986&	\textcolor{red}{0.02147}\\
\text{Kmeans}	&	0.42525&	1.01235&	0.10086&	0.03049\\
\text{Base}	&	0.80392&	0.80392&	0.11673&	0.03661\\
\hline
\end{array}
\]
\caption{\label{mse111} MSE for different methods and $\theta$s, same data set as in Figure~\ref{fig:screen2}}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%
\renewcommand{\arraystretch}{1.2} %%%



\begin{table}[H]
\[
\begin{array}{lrrrr}
\hline
 \text{Model} &  \theta_A  & \theta_B & \text{MSE}(1,1) & \text{MSE}(3,1)  \\
\hline
\theta^*(1,1)	&	\textcolor{red}{0.72435} & \textcolor{red}{1.09296} & \textcolor{red}{0.05743} & 0.01092\\
\theta^*(3,1)	&	\textcolor{red}{1.06020} & \textcolor{red}{0.52378} & 0.06871 & \textcolor{red}{0.00686}\\
\text{Kmeans}	&	0.65856 & 1.09833 & 0.05857 & 0.01340\\
\text{Base}	&	0.92682 & 0.92682 & 0.06812 & 0.01156\\
\hline
\end{array}
\]
\caption{\label{mse112} MSE for different methods and $\theta$s, same data set as in Figure~\ref{fig:hard}}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%

The red entries in Tables~\ref{mse111} and \ref{mse112} correspond to an optimum for models $(1,1)$ and $(3,1)$. The centers for M$(1,1)$ in the first dataset (Table~\ref{mse111}), and for M$(3,1)$ in the second dataset (Tables~\ref{mse112}), are much closer to the true values 
$0.5$ and $1.0$ than those produced by Kmeans. However, to claim that my method is better than Kmeans, you need a mechanism to decide
 when M$(1,1)$ or M$(3,1)$ is the best fit.  This is still a work in progress. 
As a starting point, the following arguments provide empirical rules to decide.
\begin{itemize}
\item For the first data set, MSE$(1,1)$ evaluated at the centroid of the whole data set (the base model) 
 is better (lower) than when evaluated at $\theta^*(3,1)$, suggesting that $\theta^*(3,1)$ is not a great solution here. Thus the default model $(1,1)$ 
 should be
preferred to $(3,1)$ in this case.
\item For the second data set, MSE$(1,1)$ evaluated at the centroid 
 is about the same as when evaluated at $\theta^*(3,1)$, suggesting that $\theta^*(3,1)$ is not that bad, at least not as bad as in the previous case. Thus model $(3,1)$ should not automatically be ruled out in this case. It is also an indicator that this data set is more challenging than the previous one.
\item For the second data set, MSE$(3,1)$ evaluated at  $\theta^*(3,1)$ is better than when evaluated at $\theta^*(1,1)$. This does not mean
anything: of course MSE$(3,1)$ is always best at $\theta^*(3,1)$, by design. 
However the ratio of these two MSE's,  $\rho = 0.01092 / 0.00686 \approx 1.59$, is quite high here. To the contrary, for the first data set  
$\rho \approx 1.39$ is much smaller. Thus model $(3,1)$ is more justified for the second dataset than for the
 first one.
\end{itemize} 

\noindent Note that he computation of MSE$(1,1)$ or MSE$(3,1)$ is performed without knowing which cluster a point is assigned to. Indeed, point allocation is discussed nowhere in my method: you find the centers without allocating points to specific clusters. Once the two centers 
$\theta_A^*,\theta_B^*$ have been computed, each point $W_k$ is assigned to the closest cluster. Proximity is measured as the distance between the point and the cluster center. 
Then choose the model -- $(1,1)$ or $(3, 1)$ -- minimizing the sum of these distances. 


It is my guess that replacing $||W_k-\theta_A||^2$ and $||W_k-\theta_B||^2$ 
by $||W_k-\theta_A||^{p_A}$ and $||W_k-\theta_B||^{p_B}$ 
 in the Kmeans procedure will yield better results similar to my method. Again, $\theta_A, \theta_B$ are the two cluster centers, and $W_k$ is 
 the $k$-th observation. Even $p_A=p_B=1$ could lead to significant improvements in Kmeans in the presence of outliers (for instance outliers from
 a large cluster spilling over to a nearby smaller cluster). 
This approach is somewhat similar to \textcolor{index}{K-medians clustering}\index{K-means clustering} [\href{https://en.wikipedia.org/wiki/K-medians_clustering}{Wiki}]. Using linear rather than power weights may have the same effect. \vspace{1ex}

\noindent {\bf Conclusions}

\noindent My method frequently works better than Kmeans to detect the centers when clusters strongly overlap and are unbalanced. However, this assumes that you have a 
mechanism to choose between model $(1, 1)$ and $(2, 1)$ or $(3, 1)$.  If you know beforehand that your data is highly skewed as in
 Figure~\ref{fig:hard}, then model $(3, 1)$ is a good candidate to begin with. When the clusters are well separated and one or two of the distributions is asymmetric,
 model $(1, 1)$ tends to correctly identify the cluster medians, rather than the standard centers (the mean). 

The method is simple and fast:  it does not perform point allocation. You can use it to find initial center configurations as first
 approximations in more complex algorithms, or in the context of ``unsupervised" 
%\textcolor{index}{logistic regression}
\gls{gls:logreg}
\index{logistic regression!unsupervised} to detect the two clusters when there is no independent variable.  Exact solutions such as (\ref{hhggffd}) are available in any dimension for two and more clusters, for instance for the $(2, 2)$, $(2,2,2)$ and $(4, 2)$ models. Other original clustering algorithms are described in my book on stochastic processes~\cite{vgsimulnew}.

Next steps: test on asymmetric \gls{gls:syntheticdata} modeled as a mixture of normal and gamma distributions with unequal cluster sizes and variances, reduce volatility, investigate the model $(1, \frac{1}{3})$ -- the sister of $(3, 1)$ -- and generalize the method to two or three dimensions and more than two clusters. One of the goals is to identify when my method performs better than Kmeans, to learn more about Kmeans and further improve it. 


\subsubsection{Python code}\label{pyclustrw}

The Python code \texttt{mixture1D.py} for the one-dimensional case in section~\ref{reserse}, is also on GitHub \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/mixture1D.py}{here}. 
 Note that $W_A, W_B$ and $W$ are vectors, respectively with $n_A,n_B$ and $n$ elements. The vector operations (multiplications and so on) are  implicitly performed component-wise, without using a loop on the elements. When $p=2$, \texttt{Error} corresponds to the mean squared error. I use color transparency -- the parameter \texttt{alpha} in 
 the histogram function \texttt{plt.hist} -- to visualize the overlap between the two components of the Gaussian mixture: see result in left plot,
 Figure~\ref{fig:screen2}. 

The optimum $\theta_A^*,\theta_B^*$ (the cluster centers) are obtained via Monte-Carlo simulations, using
 $\num{100000}$ sampled $\theta_A,\theta_B$ per test. On the screenshot showing convergence to the optimum, you can see
 that $\theta_A$ and $\theta_B$ are randomly flipped back and forth within each test. The algorithm senses that there are two distinct centers; however, it can't tell which one is labeled $A$ or $B$.  Afterall, this is \textcolor{index}{unsupervised learning}\index{unsupervised learning}. To address this issue, when computing the confidence intervals, I use the notation $\theta_A$ for the center on the left, and $\theta_B$ for the other one. That is, $\theta_A < \theta_B$. 
Finally, I run $40$ tests to determine a $95\%$ confidence interval for the 
 optimum values. Results are displayed in the screenshot in Figure~\ref{fig:screen2}. Zoom in for a better view.  \\ 

\begin{lstlisting}[escapechar=@]
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm 
from sklearn.cluster import KMeans

N_tests = 5    # number of data sets being tested
n_A = 1500  # number of points in cluster A
n_B = 8500  # number of points in cluster B
n = n_A + n_B
Ones = np.ones((n)) # array with 1's
p_A = 3
p_B = 1
np.random.seed(438713)
min_@\textcolor{gray2}{θ}@_A =  99999999
min_@\textcolor{gray2}{θ}@_B =  99999999
max_@\textcolor{gray2}{θ}@_A = -99999999
max_@\textcolor{gray2}{θ}@_B = -99999999
CR_x=[]  # confidence region for (best_@\textcolor{dkgreen}{θ}@_A, best_@\textcolor{dkgreen}{θ}@_A), 1st coordinate  
CR_y=[]  # confidence region for (best_@\textcolor{dkgreen}{θ}@_A, best_@\textcolor{dkgreen}{θ}@_A), 2nd coordinate 

def compute_MSE(@\textcolor{gray2}{θ}@_A, @\textcolor{gray2}{θ}@_B, p_A, p_B, W):
    n = W.size
    MSE = (1/n) * np.sum((abs(W - @\textcolor{gray2}{θ}@_A * Ones)**p_A) * (abs(W - @\textcolor{gray2}{θ}@_B * Ones)**p_B))
    return MSE

for sample in range(N_tests):   # new dataset at each iteration

    # W_A  = np.random.normal(0.5, 2, size=n_A)
    # W_B  = 1 + np.random.gamma(8, 5, size=n_B)/4
    W_A  = np.random.normal(0.5, 0.1, size=n_A)
    W_B  = np.random.normal(1.0, 0.2, size=n_B)
    W    = np.concatenate((W_A, W_B))
    min_MSE=99999999
    print('Sample %1d:' %(sample))

    for iter in range(10000):

        @\textcolor{gray2}{θ}@_A = np.amin(W) + (np.amax(W)-np.amin(W))*np.random.rand()
        @\textcolor{gray2}{θ}@_B = np.amin(W) + (np.amax(W)-np.amin(W))*np.random.rand()
        MSE = compute_MSE(@\textcolor{gray2}{θ}@_A, @\textcolor{gray2}{θ}@_B, p_A, p_B, W)   # MSE for my method
        if MSE < min_MSE:
            min_MSE=MSE
            print('Iter = %5d  @\textcolor{mauve}{θ}@_A = %+8.4f  @\textcolor{mauve}{θ}@_B = %+8.4f  MSE = %+12.4f' \
                    %(iter,@\textcolor{gray2}{θ}@_A ,@\textcolor{gray2}{θ}@_B, MSE))
            best_@\textcolor{gray2}{θ}@_A = min(@\textcolor{gray2}{θ}@_A, @\textcolor{gray2}{θ}@_B)
            best_@\textcolor{gray2}{θ}@_B = max(@\textcolor{gray2}{θ}@_A, @\textcolor{gray2}{θ}@_B)

    if best_@\textcolor{gray2}{θ}@_A < min_@\textcolor{gray2}{θ}@_A:
        min_@\textcolor{gray2}{θ}@_A = best_@\textcolor{gray2}{θ}@_A
    if best_@\textcolor{gray2}{θ}@_A > max_@\textcolor{gray2}{θ}@_A:
        max_@\textcolor{gray2}{θ}@_A = best_@\textcolor{gray2}{θ}@_A
    if best_@\textcolor{gray2}{θ}@_B < min_@\textcolor{gray2}{θ}@_B:
        min_@\textcolor{gray2}{θ}@_B = best_@\textcolor{gray2}{θ}@_B
    if best_@\textcolor{gray2}{θ}@_B > max_@\textcolor{gray2}{θ}@_B:
        max_@\textcolor{gray2}{θ}@_B = best_@\textcolor{gray2}{θ}@_B
    CR_x.append(best_@\textcolor{gray2}{θ}@_A) 
    CR_y.append(best_@\textcolor{gray2}{θ}@_B) 
    print()

    # get centers and MSE from Kmeans method (for comparison purposes)  
    V    = W.copy()  
    km = KMeans(n_clusters=2) 
    km.fit(V.reshape(-1,1))   
    centers_kmeans=km.cluster_centers_ 
    kmeans_A=min(centers_kmeans[0,0],centers_kmeans[1,0])
    kmeans_B=max(centers_kmeans[0,0],centers_kmeans[1,0])
    MSE_kmeans = compute_MSE(centers_kmeans[0,0], centers_kmeans[1,0], p_A, p_B, V)  

    # get cluster centers, medians, global centroid and compute MSE on those, 
    # for comparison with my method and with Kmeans
    centroid=(1/n)*np.sum(W) 
    centroid_A=(1/n_A)*np.sum(W_A) 
    centroid_B=(1/n_B)*np.sum(W_B) 
    median_A=np.median(W_A)
    median_B=np.median(W_B)
    MSE_base = compute_MSE(centroid, centroid, p_A, p_B, W)  # MSE for base model
    MSE_tc1 = compute_MSE(centroid_A, centroid_B, p_A, p_B, W)  
    MSE_tc2 = compute_MSE(centroid_B, centroid_A, p_A, p_B, W)  
    MSE_true_centers = min(MSE_tc1,MSE_tc2)  
    MSE_tm1 = compute_MSE(median_A, median_B, p_A, p_B, W)  
    MSE_tm2 = compute_MSE(median_B, median_A, p_A, p_B, W)  
    MSE_true_medians = min(MSE_tm1,MSE_tm2)  # MSE for base model

    print('True centers  @\textcolor{mauve}{θ}@_A = %+8.4f  @\textcolor{mauve}{θ}@_B = %+8.4f  MSE = %+12.4f' \
           %(centroid_A,centroid_B,MSE_true_centers)) 
    print('model (%1d,%1d)   @\textcolor{mauve}{θ}@_A = %+8.4f  @\textcolor{mauve}{θ}@_B = %+8.4f  MSE = %+12.4f' \
           %(p_A,p_B,best_@\textcolor{gray2}{θ}@_A,best_@\textcolor{gray2}{θ}@_B,min_MSE)) 
    print('Kmeans        @\textcolor{mauve}{θ}@_A = %+8.4f  @\textcolor{mauve}{θ}@_B = %+8.4f  MSE = %+12.4f' \
           %(kmeans_A,kmeans_B,MSE_kmeans)) 
    print('True medians  @\textcolor{mauve}{θ}@_A = %+8.4f  @\textcolor{mauve}{θ}@_B = %+8.4f  MSE = %+12.4f' \
           %(median_A,median_B,MSE_true_medians)) 
    print('Base          @\textcolor{mauve}{θ}@_A = %+8.4f  @\textcolor{mauve}{θ}@_B = %+8.4f  MSE = %+12.4f' \
           %(centroid,centroid,MSE_base)) 
    print()
 
print('95 %% range for min(@\textcolor{mauve}{θ}@_A, @\textcolor{mauve}{θ}@_B): [%+8.4f, %+8.4f]' %(min_@\textcolor{gray2}{θ}@_A ,max_@\textcolor{gray2}{θ}@_A))
print('95 %% range for max(@\textcolor{mauve}{θ}@_A, @\textcolor{mauve}{θ}@_B): [%+8.4f, %+8.4f]' %(min_@\textcolor{gray2}{θ}@_B ,max_@\textcolor{gray2}{θ}@_B))

# intialize plotting parameters
plt.rcParams['axes.linewidth'] = 0.2
plt.rc('axes',edgecolor='black') # border color
plt.rc('xtick', labelsize=7) # font size, x axis  
plt.rc('ytick', labelsize=7) # font size, y axis

# plotting histogram and density
bins=np.linspace(min(W), max(W), num=100)
plt.hist(W_A, color = "blue", alpha=0.2, edgecolor='blue',bins=bins) 
plt.hist(W_B, color = "red", alpha=0.3, edgecolor='red',bins=bins) 
plt.hist(W, color = "green", alpha=0.1, edgecolor='green',bins=bins) 
# plt.plot(bins, 8*norm.pdf(bins,0.5,0.3),color='blue',linewidth=0.6) 
# plt.plot(bins, 12*norm.pdf(bins,1,0.2),color='red',linewidth=0.6) 
plt.show()

# plotting confidence region
if N_tests > 50:
    plt.scatter(CR_x,CR_y,s=6,alpha=0.3) 
    plt.show() 
\end{lstlisting}

%--------------------------------------------------------------------------------------------------------
\Chapter{A Simple, Robust and Efficient Ensemble Method}{Application to Natural Language Processing}

The method described here illustrates the concept of \glspl{gls:ensembles}, applied to a real life NLP problem: ranking articles published on a website to 
predict performance of future blog posts yet to be written, and help decide on title and other features to maximize traffic volume and quality, and thus revenue.  
The method, called hidden decision trees (HDT), implicitly builds a large number of small usable (possibly overlapping) \glspl{gls:decisiontree}. Observations that 
don't fit in any usable node are classified with an alternate method, typically simplified \gls{gls:logreg}. 

This hybrid procedure offers the best of both worlds: decision tree combos  and \gls{gls:regression} models.  It is intuitive and simple to implement. The code is written in Python, and I also offer a light version in basic Excel. The interactive Excel version is targeted to analysts interested in learning Python or machine learning. HDT fits in the same category as bagging, \gls{gls:boosted}, stacking and \textcolor{index}{AdaBoost}\index{AdaBoost}.  This article encourages you to understand all the details, upgrade the technique if needed, and play with the full code or spreadsheet as if you wrote it yourself. This is in contrast with using blackbox Python functions without understanding their inner workings and limitations. Finally, I discuss how to build model-free confidence intervals for the predicted values.

 \section{Introduction}

The technique presented here, called \textcolor{index}{hidden decision trees}\index{hidden decision trees}, blends non-standard, robust versions of 
 \textcolor{index}{decision trees}\index{decision tree} and regression. Compared to \textcolor{index}{adaptive boosting}\textcolor{index}{adaptive boosting} [\href{https://en.wikipedia.org/wiki/AdaBoost}{Wiki}], it is simpler to implement. I used it in credit card fraud detection while working at Visa, as well as for scoring Internet traffic quality and search keyword scoring and bidding at eBay. It is an \textcolor{index}{ensemble method}\index{ensemble methods} [\href{https://en.wikipedia.org/wiki/Ensemble_learning}{Wiki}], in the sense that it 
blends multiple techniques to get the best of each one, to make predictions.  Here I describe an NLP (\gls{gls:nlp}\index{natural language processing}) case study: optimizing website content. 
The purpose is to to predict the performance of articles published in media outlets or blogs, in particular to predict which types of articles do well.


Here the response (that is, what we are trying to predict, also called dependent variable by statisticians) is the traffic volume, measured in page views, unique page views, or number of users who read the article over some time period. Page view counts
can be influenced by robots, and ``unique page views" is a more robust metric. Also, older articles have accumulated more page views over time, while the most recent ones
 have yet to build traffic. We need to correct for this bias. 
Correcting for time is explained in section~\ref{timeab}. A simple approach is to use articles published within the last two years but that are at least six month old, in the \gls{gls:trainingset}. Due to
 the highly skewed distribution, I use the logarithm of unique page views as the core metric. 

The features, also called predictors or independent variables, are:\vspace{1ex} 

\renewcommand{\arraystretch}{1.2} %%%

\begin{table}%[H]
%\[
\begin{center}
\small
\begin{tabular}{lc}
\hline
   Feature & Comment \\
\hline
Title keywords &  binary\\
Article category & blog, event, forum\\
Publisher website or category & \\
Creation date & year/month\\
The title contains numbers & yes/no\\
The title is a question & yes/no \\
The title contains special characters & yes/no \\
Length of title & \\
Size of article & number of words \\
The article contains pictures & yes/no\\
Body keywords & binary \\
Author popularity & \\
First few words in the body & \\
\hline
\end{tabular}
%]
\caption{\label{fffdsa}List of potential features to use in the model}
%\end{array}
\end{center}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%


\noindent Each keyword is a binary feature in itself, also called \textcolor{index}{dummy variable}\index{dummy variable} [\href{https://en.wikipedia.org/wiki/Dummy_variable_(statistics)}{Wiki}]: it is set to ``yes" if the keyword is found (say) in the title, and to ``no" otherwise. I used
 a shortlist of top keywords (by volume) found in all the articles combined. Also, I used a subset, narrowed version  
of the features in Table~\ref{fffdsa}: for instance, title keywords only, and whether the article is a blog or not.

 The method takes into account at all potential \textcolor{index}{key-value pair}\index{key-value pair} combinations, where ``key" is a subset of features, and ``value" is the vector of corresponding values.
For instance \texttt{key=(keyword1,keyword2,category)} and \texttt{value=('Python','tutorial','Blog')}. It is important to appropriately bin the features 
 to prevent the number of key-value pairs from exploding, using \textcolor{index}{optimum binning}\index{binning!optimum binning} [\href{http://gnpalencia.org/optbinning/}{Python}]. 
See recent article \cite{binh2020} on this topic. Another mechanism described later  is also used to keep the key-value table, stored as an hash table or associate array, manageable. Finally, this can easily be implemented in a distributed environment.
A key-value pair is also called a \textcolor{index}{node}\index{node (decision tree)}, and plays the same role as a node in a decision tree. 


\section{Methodology} 

You want to predict $p$, the logarithm of unique page views for an article (over some time period), as a function of keywords found in the title, and whether the article in question is a blog or not.  You start by creating a list of all one-token and two-token keywords found in all the article titles, with the article category (blog versus non-blog), after cleaning the titles and eliminating some stop word such as ``that", ``and" or ``the". Do not eliminate all keywords made up of one or two letters: the one-letter keyword ``R", corresponding to the programming language R, has a high \textcolor{index}{predictive power}\index{predictive power}.
For each key-value pair, get the number of articles matching it, as well as the average, minimum and maximum $p$ across these articles.

For instance, say the  key-value pair \texttt{(keyword1='R',keyword2 ='Python',category='Blog')} has $6$ articles and  the following statistics: average $p$ is $8.52$, minimum 
 is $7.41$, and maximum is $10.45$. If the average $p$ across all articles in the training set is  $6.83$, then this specific key-value pair (also called node) generates $\exp(8.52 - 6.83) = 5.42$ times more traffic than an average article. It is thus a large node in terms of traffic. 

Even the worst article among the $6$ ones belonging to this node, with a $p$ of $7.41$, outperforms the average $6.83$ across all nodes. So not only this is a large node, but a stable one. Some nodes have a higher variance $\text{Var}[p]$, for instance when one of the keywords has different meanings, such as the word ``training" in "training set"  and in
 ``courses and training".


\subsection{How hidden decision trees (HDT) work}\label{algoaba}

The nodes are overlapping, allowing considerable flexibility. In particular, nodes with two keywords are sub-nodes of nodes with one keyword.  
The general idea behind this technique is to group articles into buckets that are large enough to provide good predictions, without explicitly building \glspl{gls:decisiontree}.  The nodes are simple and easy to interpret, and unstable ones (with high variance) can be discarded. There is no splitting/pruning involved as with classical decision trees, making this methodology simple and robust, and thus fit for artificial intelligence and black-box implementation. The method is called 
 \textcolor{index}{hidden decision trees}\index{hidden decision trees} and abbreviated as \textcolor{index}{HDT} because you don't create decision trees, but you indirectly rely on a large number of small ones that are hidden in the
 algorithm.



Whether you are dealing with predicting the popularity of an article, or the risk for a client to default on a loan, the basic methodology is identical. It involves training sets, \gls{gls:crossvalid}, \gls{gls:featureselection}, \gls{gls:binning}, and populating hash tables of key-value pairs, referred to as the nodes.
When you process a new observation outside the training set, you check which node(s) it belongs to. If the ``ideal" nodes it belongs to are stable and not too small, you use a weighted average score computed over these nodes, as predictor.  If this score (defined as $p$ here) is significantly above the global average, and other constraints are met, then you classify the observation -- in this case a potential article  you want to write -- as good. An ideal node has strong \gls{gls:predictivepower}: its $p$ is either
  very high or very low.

Also, you need to update your training set and the nodes table, including automatically discovered new nodes, every six months or so.
Parameters must be calibrated to guarantee that the proportion of false positives  remains small enough. Ideally, you want to end up with
 less than $\num{3000}$ stable nodes, each with at least $10$ observations (articles), covering $80\%$ of the articles or more. 
I discuss the parameters of the technique, and how to fine-tune them, in section~\ref{parambana}. Fine-tuning can be automated or made more robust by testing (say) 
$\num{2000}$ sets of parameters and identify regions of stability minimizing the error rate in the parameter space.  Error rate is defined as the proportion of misclassification, and false positives in particular.  

A big question is what to do with observations not belonging to any usable node: they cannot be classified. A \textcolor{index}{usable node}\index{node (decision tree)!usable node} is one with enough articles, with average $p$ within the mode far away from the global mean of $p$ computed across all nodes.
One way to address this issue is to use two algorithms: the one described so far, applied to usable or ideal nodes (let's call it algorithm A) and another one called algorithm B that classifies all observations. Observations that can't be classified or scored with algorithm A are classified/scored with algorithm B. 
The resulting hybrid algorithm is called Hidden Decision Trees. 

%---

\subsection{NLP Case study: summary and findings}


If you run the Python script listed in section~\ref{pythourew}, besides producing the table of key-value pairs (the nodes) as a text file for further automated processing, it displays summary statistics that look like the following:

\begin{lstlisting}[frame=none] 
    Average log pageview count (pv): 6.83
    Avg pv, articles marked as Good: 8.09
    Avg pv, articles marked as Bad : 5.95

    Number of articles marked as Good: 223 (real number is 1079)
    Number of articles marked as Bad : 368 (real number is 919)
    Number of false positives             : 25 (Bad marked as Good)
    Number of false negatives             : 123 (Good marked as Bad)
    Total number of articles              : 2616

    Proportion of False Positives:  11.2%
    Proportion of Unclassified            :  77.4%

    Aggregation factor (Good node): 29.1
    Number of feature values: 16711 (marked as good: 49)

    Execution time:  0.0630 seconds
\end{lstlisting}

\noindent In the code, $p$ -- the logarithm of the pageview count -- is represented by  \texttt{pv}. The number of nodes is the total number of key-value pairs found, including the small unstable ones, regardless as to whether they are classified as good, bad, or unclassified. An article with $p$ above the arbitrary  \texttt{pv\_threshold\_good=7.1} (see source code) is considered as good. This corresponds to articles having about $1.3$ times more traffic than average, since I use a log scale and the average $p$ is $6.83$. Articles classified as good have an average $p$ of $8.09$, that is, about $3.3$ times more traffic than average. 

\noindent Two important metrics are:
\begin{itemize}
\item Aggregation factor: it is an indicator of the average size of a useful node, in this case classified as Good. A value above $5$ is highly desirable.
\item The most important error rate is measured here as the number of bad articles wrongly classified as good. The goal is to detect very good articles and find the reasons that make  them popular, to be able to increase the proportion of good articles in the future. Avoiding bad articles is the second most important goal, so I am also interested in identifying what makes them bad.
\end{itemize}

\noindent Also note that the method correctly identifies a proportion of the good articles, but leaves many unclassified. I explain in section~\ref{pythourew} 
 how to improve this. Finally an article is marked as good if it meets some criteria specified  in section~\ref{parambana}. 

Now I share some interesting findings revealed by these hidden decision tress, on the data set investigated in this study. First, articles with the following title features do well:
	contains a number as in ``10 great deep learning articles",
	contains the current year,
	is a question  (how to),
	is a blog post or belongs to the book category. 

Then the following title keywords are a good predictor of popularity:
	everyone (as in ``10 regression techniques everyone should know"),
	libraries,
	infographic,
	explained, 
	algorithms,
	languages, 
	amazing, 
	must read,
	R Python,
	job interview questions,
	should know (as in ``10 regression techniques everyone should know"),
	NoSQL databases,
	versus, 
	decision trees,
	logistic regression,
	correlations,
	tutorials,
	code, 
	free. 

\subsection{Parameters}\label{parambana} % parameter tuning

Besides \texttt{pv\_threshold\_good} and \texttt{pv\_threshold\_bad}, the algorithm uses $12$ parameters to identify a usable, stable node classified as good. You can see them in action in the Python code
 in section~\ref{pythourew}, in the instruction

\begin{lstlisting}[frame=none]
     if n > 3 and n < 8 and Min > 6.9 and Avg > 7.6 or \
         n >= 8 and n < 16 and Min > 6.7 and Avg > 7.4 or \
         n >= 16 and n < 200 and Min > 6.1 and Avg > 7.2:
\end{lstlisting}

%if ( ((\$n > 3)&&(\$n < 8)&&(\$min > 6.9)&&(\$avg > 7.6)) || 
% ((\$n >= 8)&&(\$n < 16)&&(\$min > 6.7)&&(\$avg > 7.4)) ||
% ((\$n >= 16)&&(\$n < 200)&&(\$min > 6.1)&&(\$avg > 7.2)) ) 

\noindent Here, \texttt{n} represents the size (number of observations) of a node, while \texttt{Avg} and \texttt{Min} are the average and minimum \texttt{pv} for the node in question.  I tested many combinations of values for these parameters. Increasing the required size to qualify as usable node will do the following:\vspace{1ex}
\begin{itemize}
	\item[-] Decrease the number of good articles correctly identified as good
	\item[-] Increase the error rate
	\item[-] Increase the stability of the system
	\item[-] Decrease the predictive power
	\item[-] Increase the aggregation factor
\end{itemize}\vspace{1ex}



\subsection{Improving the methodology}\label{ccvcx}

Some two-token keywords should be treated as one-token. For instance ``San Francisco" must be treated as a one-token keyword. It is easy to automatically detect this: when you analyze the text data, ``San" and ``Francisco" are lumped together far more frequently than dictated by pure chance. 
Also, I looked at nodes  where the two keywords are adjacent in the text. If you allow the two keywords not to be adjacent, the number of key-value pairs (the nodes) increases significantly, but you don't get much more additional predictive power in return, and there is a risk of over-fitting. 


Another improvement consists of favoring nodes containing articles spread over several years, as opposed to concentrated on a few weeks or months. The latter category may be popular articles at some time, that faded away.
Finally, you cannot exclusively focus on articles with great potential. It is important to have many, less popular articles as well: they constitute the long tail. Without these less popular articles, you face excessive content concentration and readership attrition in the long term. 

\section{Implementation details}

This section contains the Python code and details about the Excel spreadsheet. Both the \glspl{gls:decisiontree} and the regression part of HDT (referred to as ``algorithm B" in section~\ref{algoaba}) are implemented in the spreadsheet. The Python version, though more comprehensive in many regards, is limited to the decision trees. But first
I start by discussing a possible improvement of the methodology: bias correction.

\subsection{Correcting for bias}\label{timeab}

In online rankings, the most popular books, authors, articles, restaurants, products and so on are usually those that have been around for a long time. 
Here I address this issue by creating adjusted scores. It allows you to make fair comparisons between new and old items.

For top time-insensitive articles, page views peak in the first three days, but popularity remains high for many years.  In short, page view decay is very low over time. Finally, the most popular topics (keywords) change over time; this type of analysis helps find the trends. It is also a good idea to use two different sources of data for pageview
 measurements, see how they differ, understand why, and check whether the discrepancy worsens over time.

The articles scored here span over a three-year period, covering over $\num{2600}$ pieces of content totaling $6$ million pageviews across three websites. 
The summary data is on GitHub, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/ArticlePopularity.txt}{here}. It features the top $46$
 articles ranked according to the time-adjusted score. The number in parenthesis attached to each article is the non-adjusted (old) score. The difference between the time-adjusted score and the old one, is striking. 

\subsubsection{Time-adjusted scores}

You measure the page view count for a specific article, and your time period is  $[t_0, t_1]$. Typical models use 
 an \textcolor{index}{exponential decay}\index{exponential decay} of rate $\lambda$. The adjustment factor is then
 $$q = \frac{1}{\lambda}\cdot \Big[\exp(-\lambda t_0) -  \exp(-\lambda t_1)\Big] > 0.$$


Now define the adjusted score as $p / q$, where $p$ is the observed page view count in $[t_0, t_1]$. If $\lambda = 0$ (no decay) then $q = t_1-t_0$. 

\subsection{Excel spreadsheet}\label{excerds}

The interactive spreadsheet named \texttt{HDTdata4Excel.xlsx} is on my GitHub repository, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Spreadsheets/HDTdata4Excel.xlsx}{here}. It uses a subset of $9$ binary features. The first three are respectively ``published after 2014", 
``article is a forum discussion", and ``article is a blog post". The next six ones are indicators of whether or not the title contains a specific character string. The six strings in question are ``python", ``r",  ``machine learning", ``data science", ``data",  and ``analy". The last string captures words such as ``analytic" or
 ``analyst". These strings must be surrounded by spaces, so ``r" clearly represents the R programming language. True/false are encoded as $1$ and $0$ respectively. 



\renewcommand{\arraystretch}{1.2} %%%
\begin{table}%[H]
%\[
\begin{center}
%\small
\begin{tabular}{lrcc}
\hline
node & size & pv & index \\
\hline
N-000-000000 & 8 & 7.12 & 1.33 \\
N-000-000001 & 5 & 6.87 & 1.04 \\
N-000-000010 & 8 & 6.86 & 1.02 \\
N-000-000011 & 3 & 6.49 & 0.71 \\
N-000-000110 & 3 & 7.18 & 1.42 \\
N-001-000000 & 313 & 6.88 & 1.05 \\
N-001-000001 & 75 & 6.71 & 0.88 \\
N-001-000010 & 276 & 7.14 & 1.35 \\
N-001-000011 & 44 & 7.16 & 1.38 \\
N-001-000110 & 130 & 7.68 & 2.34 \\
N-001-000111 & 5 & 8.05 & 3.37 \\
N-001-001000 & 5 & 8.07 & 3.45 \\
N-001-001010 & 1 & 7.58 & 2.11 \\
N-001-001110 & 1 & 7.35 & 1.67 \\
\hline
\end{tabular}
%]
\caption{\label{fffnode}Statistics for selected HDT nodes (Excel version)}
%\end{array}
\end{center}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%

For instance, node \texttt{N-001-001110} in Table~\ref{fffnode} corresponds to blog posts published in 2014, containing the keywords ``machine learning", ``data science" and ``data" in the title, but not ``python", ``r" or ``analy". The column ``size" tells us that the node in question has only one article.


Nodes with fewer than 10 articles are classified using the \gls{gls:regression} method via the \texttt{LINEST} Excel function, rather than the mini decision trees. Instead of standard regression, you can use a simplified \gls{gls:logreg}, 
 as described in section~\ref{2ways}. There are $\num{2616}$ observations (articles) and $74$ nodes in the \gls{gls:trainingset}. By grouping all nodes with less than $10$ observations into one node, we get down to $24$ nodes. Correlations between individual features and the response $p$ (logarithm of pageviews, denoted as
 \texttt{pv} in Table~\ref{fffnode}) is very low. Thus individual features have no \gls{gls:predictivepower}. They must be combined together
 to gain predictive power. The full HDT method is superior to either the mini decision trees, or the regression model taken separately.

The index in Table~\ref{fffnode} is the \texttt{pv} of the node in question, divided by the average \texttt{pv} across all nodes. It measures the performance of the node. Finally, an article is classified as good or bad depending on whether its index is significantly larger or lower than one. The thresholds are user-defined. 


\begin{figure}%[H]
\centering
\includegraphics[width=0.85\textwidth]{hdt.png}
%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Output from the Excel version of HDT}
\label{fig:hdt}
\end{figure}



\subsection{Python code and dataset}\label{pythourew}


The input dataset \texttt{HDTdata4.txt} is on my GitHub repository, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/HDTdata4.txt}{here}. The Python program \texttt{HDT.py} is listed below and can also be found on my GitHub repository, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/HDT.py}{here}. The output file \texttt{hdt-out2.txt} contains the usable key-value pairs (nodes) corresponding to popular articles, and the list of article IDs for each of these nodes. Finally, the variable \texttt{pv} represents $p$, the logarithm of the pageview count. The bivariate combinations (title keyword, article category)  constitute the
 keys of the hash table \texttt{list\_pv}, while the \texttt{pv} are the hash table values. Keywords are either one- or two-token. For one-token keywords, the second token is marked as N/A. In short, keyword is a bivariate entity.

As for the error rate, since the focus is on producing good articles, I am interested only in minimizing the number of bad articles flagged as good: the false positives.  To reduce error rates or the proportion of unclassified nodes, use more features (for instance, more of those listed in Table~\ref{fffdsa}), three-token keywords, a larger training set, a better keyword cleaning mechanism, and fine-tune the parameters. 

Of course, if you choose the option \texttt{mode='perfect\_fit'} in the program, your false positive rate drops to $0\%$ on the training set, but doing may lower the
 performance on the \gls{gls:validset}, and may leave many nodes unclassified.  On the plus side, you have much fewer parameters to fine-tune:
 \texttt{pv\_threshold\_good}, \texttt{pv\_threshold\_bad}, and the minimum size of a usable node (the variable \texttt{n} in the code). The first two can be set respectively to $5\%$ above and 
$10\%$ below 
 the global average \texttt{pv}. The minimum node size should be set above $2$, and ideally above $5$, though a large value results in more unclassified nodes.
 For \texttt{mode='robust method'}, the
 parameters in the conditional statements defining \texttt{good\_node} and \texttt{bad\_node} were set manually based on an average \texttt{pv} of $6.83$. These choices can be automated.

In the end, unclassified nodes are classified via regression in the spreadsheet (see section~\ref{excerds}), but this has not yet been implemented in the Python code.
 But for my purpose (identifying what makes an article good), I did not need to add the regression part, as the mini \glspl{gls:decisiontree} alone (the nodes) provide enough valuable insights. \\


\begin{lstlisting}
from math import log
import time

start = time.time()

# This method updates the dictionaries based on given ID, pv and word
def update_pvs(word, pv, id, word_count_dict, word_pv_dict, min_pv_dict, max_pv_dict, ids_dict):
    if word in word_count_dict:
        word_count_dict[word] += 1
        word_pv_dict[word] += pv
        if min_pv_dict[word] > pv:
            min_pv_dict[word] = pv
        if max_pv_dict[word] < pv:
            max_pv_dict[word] = pv
        ids_dict[word].append(id)
    else:
        word_count_dict[word] = 1
        word_pv_dict[word] = pv
        min_pv_dict[word] = pv
        max_pv_dict[word] = pv
        ids_dict[word] = [id]
# dictionaries to hold count of each key words, their page views, and the ids of the article in which used.
List = dict()
list_pv = dict()
list_pv_max = dict()
list_pv_min = dict()
list_id = dict()
articleTitle = list() # Lists to hold article id wise title name and pv
articlepv = list()
sum_pv = 0
ID = 0
in_file = open("HDTdata4.txt", "r")

for line in in_file:
    if ID == 0: # excluding first line as it is header
        ID += 1
        continue
    line = line.lower()
    aux = line.split('\t') # Indexes will have: 0 - Title, 1 - URL, 2 - data and 3 - page views
    url = aux[1]
    pv = log(1 + int(aux[3]))
    if "/blogs/" in url:
        type = "BLOG"
    else:
        type = "OTHER"
#   #--- clean article titles, remove stop words
    title = aux[0]
    title = " " + title + " " # adding space at the ends to treat stop words at start, mid and end alike
    title = title.replace('"', ' ')
    title = title.replace('?', ' ? ')
    title = title.replace(':', ' ')
    title = title.replace('.', ' ')
    title = title.replace('(', ' ')
    title = title.replace(')', ' ')
    title = title.replace(',', ' ')
    title = title.replace(' a ', ' ')
    title = title.replace(' the ', ' ')
    title = title.replace(' for ', ' ')
    title = title.replace(' in ', ' ')
    title = title.replace(' and ', ' ')
    title = title.replace(' or ', ' ')
    title = title.replace(' is ', ' ')
    title = title.replace(' in ', ' ')
    title = title.replace(' are ', ' ')
    title = title.replace(' of ', ' ')
    title = title.strip()
    title = ' '.join(title.split()) # replacing multiple spaces with one
    #break down article title into keyword tokens
    aux2 = title.split(' ')
    num_words = len(aux2)
    for index in range(num_words):
        word = aux2[index].strip()
        word = word + '\t' + 'N/A' + '\t' + type
        update_pvs(word, pv, ID - 1, List,list_pv, list_pv_min, list_pv_max, list_id) # updating single words

        if (num_words - 1) > index:
            word = aux2[index] + '\t' + aux2[index+1] + '\t' + type
            update_pvs(word, pv, ID - 1, List, list_pv, list_pv_min, list_pv_max, list_id) # updating bigrams

    articleTitle.append(title)
    articlepv.append(pv)
    sum_pv += pv
    ID += 1
in_file.close()

nArticles = ID - 1  # -1 as the increments were done post loop
avg_pv = sum_pv/nArticles
articleFlag = ["NA" for n in range(nArticles)]
nidx = 0
nidx_Good = 0
nidx_Bad  = 0
pv_threshold_good = 7.1
pv_threshold_bad = 6.2
mode = 'robust method'  # options are 'perfect fit' or 'robust method'
OUT = open('hdt-out2.txt','w')
OUT2 = open('hdt-reasons.txt','w')
for idx in List:
    n = List[idx]
    Avg = list_pv[idx]/n
    Min = list_pv_min[idx]
    Max = list_pv_max[idx]
    idlist = list_id[idx]
    nidx += 1
    if mode == 'perfect fit':
      good_node = n > 2 and Min > pv_threshold_good
      bad_node  = n > 2 and Max < pv_threshold_bad
    elif mode == 'robust method': 
        # below values are chosen based on heuristics and experimenting 
        good_node = n > 3 and n < 8 and Min > 6.9 and Avg > 7.6 or \
                n >= 8 and n < 16 and Min > 6.7 and Avg > 7.4 or \
                n >= 16 and n < 200 and Min > 6.1 and Avg > 7.2
        bad_node =  n > 3 and n < 8 and Max < 6.3 and Avg < 5.4 or \
                n >= 8 and n < 16 and Max > 6.6 and Avg < 5.9 or \
                n >= 16 and n < 200 and Max > 7.2 and Avg < 6.2 
    if good_node:
        OUT.write(idx + '\t' + str(n) + '\t' + str(Avg) + '\t' + str(Min) + '\t' + str(Max) + '\t' + str(idlist) + '\n')
        nidx_Good += 1
        for ID in idlist:
            title=articleTitle[ID]
            pv = articlepv[ID]
            OUT2.write(title + '\t' + str(pv) + '\t' +  idx + '\t' + str(n) + '\t' + str(Avg) + '\t' + str(Min) + '\t' + str(Max) + '\n')
            articleFlag[ID] = "GOOD"
    elif bad_node:
        nidx_Bad += 1
        for ID in idlist:
            articleFlag[ID] = "BAD"
# Computing results based on Threshold values
pv1 = 0
pv2 = 0
n1 = 0
n2 = 0
m1 = 0
m2 = 0
FalsePositive = 0
FalseNegative = 0
for ID in range(nArticles):
    pv = articlepv[ID]
    if articleFlag[ID] == "GOOD":
        n1 += 1
        pv1 += pv
        if pv < pv_threshold_good:
            FalsePositive += 1
    elif articleFlag[ID] == "BAD":
        n2 += 1
        pv2 += pv
        if pv > pv_threshold_bad:
            FalseNegative += 1
    if pv > pv_threshold_good: 
        m1 += 1
    elif pv < pv_threshold_bad:  
        m2 += 1
#
# Printing results
avg_pv1 = pv1/n1
avg_pv2 = pv2/n2
errorRate = FalsePositive/n1
UnclassifiedRate = 1 - (n1 + n2) / nArticles
aggregationFactor = (nidx/nidx_Good)/(nArticles/n1)
print ("Average log pageview count (pv):","{0:.2f}".format(avg_pv))
print ("Avg pv, articles marked as Good:","{:.2f}".format(avg_pv1))
print ("Avg pv, articles marked as Bad :","{:.2f}".format(avg_pv2))
print()
print ("Number of articles marked as Good: ", n1, " (real number is ", m1,")", sep = "" )
print ("Number of articles marked as Bad : ", n2, " (real number is ", m2,")", sep = "")
print ("Number of false positives        :",FalsePositive,"(Bad marked as Good)")
print ("Number of false negatives        :", FalseNegative, "(Good marked as Bad)")
print ("Total number of articles         :", nArticles)
print()
print ("Proportion of False Positives: ","{0:.1%}".format(errorRate))
print ("Proportion of Unclassified   : ","{0:.1%}".format(UnclassifiedRate))
print()
print ("Aggregation factor (Good node):","{:.1f}".format(aggregationFactor))
print ("Number of feature values: ", nidx," (marked as good: ", nidx_Good,")", sep = "")
print ()
print("Execution time: ","{:.4f}".format(time.time() - start), "seconds")
\end{lstlisting}

\section{Model-free confidence intervals and perfect nodes}


Node \texttt{N-100-000000} in the spreadsheet has an average 
\texttt{pv} of $5.85$. It consists of $10$ articles with  
the following \texttt{pv}: $5.10, 5.10, 5.56, 5.56, 5.66, 5.69, 6.01,  6.19, 6.80, 6.80$. The $15$th and $85$th percentiles are $5.26$ and $6.68$ respectively, when computed with the \texttt{Percentile} function in Excel. Thus, $[5.26, 6.68]$ is a $70\%$ \textcolor{index}{confidence interval}\index{confidence interval} (CI) for \texttt{pv}, for the node
 in question. 


The whole CI including its upper bound is below the average \texttt{pv} of $6.83$. In fact this node corresponds to articles posted after 2014, not a blog or forum question (it could be a video or event announcement), and with a title containing none of the keyword features in the spreadsheet 
(columns \texttt{K:P} in the \texttt{data} tab). This node  has a maximum \gls{gls:predictivepower}, in the sense that $100\%$ of the articles that it contains are bad, and $0\%$ 
 are good. This would also be true if it was the other way around, with Good swapped with Bad. Such a node is called a \textcolor{index}{perfect node}\index{node (decision tree)!perfect node}. When selecting the option 
\texttt{mode='Perfect fit'} in the Python code,
the method looks at perfect nodes only. The concept of \textcolor{index}{predictive power}\index{predictive power} is further discussed in section~\ref{secdr}.

\subsection{Interesting asymptotic properties of confidence intervals}

I focus here on traditional model-free confidence intervals, as computed in the above paragraphs. The reader should be aware that there are other ways to define them, for instance \textcolor{index}{credible intervals}\index{credible interval} in the context of
 \textcolor{index}{Bayesian inference}\index{Bayesian inference} [\href{https://en.wikipedia.org/wiki/Bayesian_inference}{Wiki}], or 
 Bayesian-like \textcolor{index}{dual confidence intervals}\index{confidence region!dual region} as in section~\ref{dualcr1wqa}.


\renewcommand{\arraystretch}{1.2} %%%
\begin{center}
\begin{table}[H]
\[
\begin{array}{lccc}
\hline 
\text{pv distribution} & \text{Type} &  \text{E}[R_n] & \text{Stdev}[R_n] \\
\hline 
\text{Uniform} & \text{short tail} & 1 & 1/n \\
\text{Gaussian} & \text{medium tail} & \sqrt{\log n} & 1/\sqrt{n} \\
\text{Exponential} & \text{fat tail} & \log n & 1\\
\hline
\end{array}
\]
\caption{\label{ffraged}Order of magnitude for the expectation and standard deviation of the range $R_n$}
\end{table}
\end{center}

\renewcommand{\arraystretch}{1.0} %%%

In almost all cases, as the number $n$ of observations becomes large within a node, the length of the confidence interval, in this case for the expected \texttt{pv}, is asymptotically
 $L_n\sim\alpha n^\beta$. I discuss in an upcoming paper how to estimate $\alpha$ and $\beta$. The order of magnitude of the range $R_n =\max(\texttt{pv}) - \min(\texttt{pv})$
  computed on a node with $n$ observations, depends on the distribution of \texttt{pv}, and more specifically, on the type of this distribution. The result is summarized in Table~\ref{ffraged}, and discussed in the same upcoming article. In practice, \texttt{pv} may have a \textcolor{index}{mixture  distribution}\index{mixture model}.


%------------------------------------------------------------------------------------------------------------
\Chapter{Gentle Introduction to Linear Algebra}{with Spectacular Applications}\label{chapterlinear}

This simple introduction to matrix theory offers a refreshing perspective on the subject. Using a basic concept that leads to a simple formula for the power of a matrix, I show how it can solve time series, Markov chains, linear \gls{gls:regression}, linear recurrence equations, pseudo-inverse and square root of a matrix, data compression, principal components analysis (PCA) or dimension reduction, and other machine learning problems. These problems are usually solved with more advanced matrix algebra, including eigenvalues, diagonalization, generalized inverse matrices, and other types of matrix normalization. My approach is more intuitive and thus appealing to professionals who do not have a strong mathematical background, or who have forgotten what they learned in math textbooks. It will also appeal to physicists and engineers, and to professionals more familiar or interested in calculus, than in matrix algebra. Finally, it leads to simple algorithms, for instance for matrix inversion. The classical statistician or data scientist will find my approach somewhat intriguing. The core of the methodology is the characteristic polynomial of a matrix, and in particular, the roots with lowest or largest moduli. It leads to a numerically stable method to solve Vandermonde systems, and thus, many linear algebra problems. Simulations include a curious fractal time series that looks incredibly smooth.


%\hypersetup{linkcolor=red}
%\tableofcontents 




\section{Power of a matrix}

This is not a traditional tutorial on linear algebra. The material presented here, in a compact style, is rarely taught in college classes. It covers a wide range of topics, while avoiding excessive use of jargon or advanced math. The fundamental tool is the power of a matrix, and its byproduct, the characteristic polynomial. It can solve countless problems, as discussed later in this article, with illustrations. It has more to do with calculus, than matrix algebra.

For simplicity, in this section, I illustrate the methodology for a $2 \times$ 2 matrix denoted as $A$. The generalization is straightforward. I provide a simple formula for the $n$-th power of $A$, where $n$ is a positive integer. I then extend the formula 
to $n = -1$ (the most useful case) and to non-integer values of $n$. 
Using the notation

\renewcommand{\arraystretch}{1.0} %%%

\begin{equation*}
A = 
\begin{pmatrix}
a & b \\
c & d 
\end{pmatrix},
\quad 
\text{ with }
A^n = 
\begin{pmatrix}
a_n & b_n \\
c _n & d_n 
\end{pmatrix}
=
\begin{pmatrix}
a & b \\
c & d 
\end{pmatrix}
\cdot
\begin{pmatrix}
a_{n-1} & b_{n-1} \\
c_{n-1} & d_{n-1} 
\end{pmatrix},
\end{equation*}

\noindent we obtain

\begin{equation*}
  \left\{
    \begin{aligned}
      a_n & =a\cdot a_{n-1}+b\cdot c_{n-1}\\
      b_n & =a\cdot b_{n-1}+b\cdot d_{n-1}\\
      c_n & =c\cdot a_{n-1}+d\cdot c_{n-1}\\
      d_n & =c\cdot b_{n-1}+d\cdot d_{n-1}
    \end{aligned}
  \right.
\end{equation*}
Using elementary substitutions, this leads to the following system:

\begin{equation*}
  \left\{
    \begin{aligned}
      a_n & =(a+d)\cdot a_{n-1}-(ad-bc)\cdot a_{n-2}\\
      b_n & =(a+d)\cdot b_{n-1}-(ad-bc)\cdot b_{n-2}\\
      c_n & =(a+d)\cdot c_{n-1}-(ad-bc)\cdot c_{n-2}\\
      d_n & =(a+d)\cdot d_{n-1}-(ad-bc)\cdot d_{n-2}
    \end{aligned}
  \right.
\end{equation*}
We are dealing with identical linear homogeneous recurrence relations. Only the initial conditions corresponding to $n = 0$ and $n = 1$, are different for these four equations. The solution to such equations is obtained as follows [\href{https://en.wikipedia.org/wiki/Linear_recurrence_with_constant_coefficients}{Wiki}]. First, solve the quadratic equation
\begin{equation}\label{cplr}
x^2 - (a+d)x + (ad-bc)=0.
\end{equation}
The two solutions $r_1,r_2$ are
$$
\begin{aligned}
      r_1 & =\frac{1}{2}\Big[a+d+ \sqrt{(a+d)^2-4(ad-bc)}\Big],\\
      r_2 & =\frac{1}{2}\Big[a+d-\sqrt{(a+d)^2-4(ad-bc)}\Big].
\end{aligned}
$$
If the quantity under the square root is negative, then the roots are complex numbers. The final solution depends on whether the roots are distinct or not:
\begin{equation}\label{linea1}
A^n=\left\{
\begin{aligned}
      & r_1^n Q_1 + r_2^n Q_2 & \quad \text{ if } r_1\neq r_2,\\
     & r_1^n Q_1 + n r_1^{n-1} Q_2 & \quad \text{ if } r_1= r_2,\\
\end{aligned}
\right.
\end{equation}
with 
\begin{equation}\label{linea2}
\left\{
\begin{aligned}
      & Q_1 = (A-r_2 I)/(r_1-r_2) & \quad \text{ if } r_1\neq r_2,\\  
     &  Q_2 = (A-r_1 I)/(r_2-r_1) & \quad \text{ if } r_1\neq r_2,\\  
    & Q_1 = I & \quad \text{ if } r_1= r_2,\\
   & Q_2 = A-r_1 I & \quad \text{ if } r_1= r_2.\\
\end{aligned}
\right.
\end{equation}

Here the symbol $I$ represents the $2 \times 2$ identity matrix. The last four relationships were obtained by applying formula~(\ref{linea1}) to $A^n$, with $n = 0$ and $n = 1$. 
It is easy to prove (by recursion on $n$) that~\ref{linea1}, together with~\ref{linea2}, is the correct solution. If none of the roots is zero, then the formula are still valid for $n = -1$, and thus it can be used to compute the inverse of $A$.

\section{Examples, Generalization, and Matrix Inversion}\label{slin1}

For a $p \times p$ matrix, the methodology generalizes as follows. The quadratic polynomial becomes a polynomial of degree $p$, known as the 
\textcolor{index}{characteristic polynomial}\index{characteristic polynomial}, and linked to the \textcolor{index}{Cayley-Hamilton theorem}\index{Cayley-Hamilton theorem} [\href{https://en.wikipedia.org/wiki/Cayley\%E2\%80\%93Hamilton_theorem}{Wiki}]. If its roots are distinct, we have

\begin{equation}\label{linea11}
A^n=\sum_{k=1}^p r_k^n Q_k, \quad\text{with } \left(
\begin{aligned}
      Q_1\\
     Q_2 \\
     \vdots \\
    Q_p
\end{aligned}
\right) = V^{-1} \left(
\begin{array}{c}
      I \\
     A \\
     \vdots \\
    A^{p-1}
\end{array}
\right)  =\left(
\begin{array}{cccc}
      1 & 1 & \cdots & 1 \\
     r_1 & r_2 & \cdots & r_p \\
     \vdots & \vdots &  & \vdots \\
    r_1^{p-1} &  r_2^{p-1} &\cdots  &  r_p^{p-1}
\end{array}
\right)^{-1} \left(
\begin{array}{c}
      I \\
     A \\
     \vdots \\
    A^{p-1}
\end{array}
\right)
\end{equation}
The matrix $V$ is a \textcolor{index}{Vandermonde matrix}\index{Vandermonde matrix} [\href{https://en.wikipedia.org/wiki/Vandermonde_matrix}{Wiki}], so there is an explicit formula to compute its inverse, 
see [\href{https://proofwiki.org/wiki/Inverse_of_Vandermonde_Matrix}{Wiki}]. 
For a fast algorithm for the computation of its inverse, see \cite{ieeevander}. However, inverting $V$ should be avoided as it is
a \textcolor{index}{numerically unstable}\index{numerical stability} procedure. Instead, approximations methods based on selected roots of the characteristic polynomial -- those with  highest or lowest moduli -- are preferred. They are discussed in section~\ref{vandervg}.

The determinants of $A$ and $V$ are respectively equal to
$$
\begin{aligned}
      |A| & = (-1)^p r_1 r_2 \cdots r_p\\
     |V| & = \prod_{1\leq k < l\leq p} (r_k - r_l)
\end{aligned}
$$
Note that the roots can be real or complex numbers, simple or multiple, or equal to zero. Usually the roots are ordered by decreasing modulus, that is
$$
|r_1|\geq |r_2|\geq |r_3| \geq \cdots \geq |r_p|.
$$

\noindent That way, a good approximation for $A^n$ is obtained by using the first three or four roots if $n > 0$, and the last three or four roots if $n < 0$. In the context of linear regression, one of the main problems  consists of inverting a matrix, that is, using $n = -1$ in formula~(\ref{linea11}). Working with first three roots only  is equivalent to performing a 
\textcolor{index}{principal component analysis}\index{principal component analysis} (PCA) [\href{https://en.wikipedia.org/wiki/Principal_component_analysis}{Wiki}] as well as PCA-induced dimension reduction.  This technique can be used for data compression.

If some roots have a multiplicity higher than one, the formulas must be adjusted. The solution can be found by looking at how to solve an homogeneous linear recurrence equation. See theorem 4 in section 8.2 of ``Math 55 Lecture Notes" \cite{arashfa}, taught at Berkeley University.

\subsection{Example with a non-invertible matrix}\label{slin1b}

Even if $A$ is non-invertible, some useful quantities can still be computed when $n = -1$, not unlike using a 
\textcolor{index}{pseudo-inverse matrix}\index{pseudo-inverse matrix} [\href{https://en.wikipedia.org/wiki/Moore\%E2\%80\%93Penrose_inverse}{Wiki}]
in the \textcolor{index}{generalized linear model}\index{generalized linear model} [\href{https://en.wikipedia.org/wiki/Generalized_linear_model}{Wiki}] in regression analysis. Let's look at the following example, using the methodology previously discussed:

 

\begin{equation*}
A=\left(
\begin{array}{cc}
      1 & 2\\
     2 & 4
\end{array}
\right) \Rightarrow A^n = 5^n \cdot \frac{1}{5}\left(
\begin{array}{cc}
 1 & 2 \\
 2 & 4  
\end{array}
\right)
+ 0^n \cdot \frac{1}{5}
\left(
\begin{array}{rr}
 4 & -2 \\
 -2 &  1
\end{array}
\right).
\end{equation*}

Note that $0^n=1$ if $n=0$. The rightmost matrix attached to the second root $0$ is of particular interest, and plays the role of a pseudo-inverse matrix for $A$. If that second root was very close to zero rather than exactly zero, then the term involving the rightmost matrix would largely dominate in the expression of $A^n$, when $n = -1$. At the limit, some ratios involving the (non-existent!) inverse of $A$ still make sense. For instance:
\begin{itemize}
\item The sum of the elements of the inverse of $A$, divided by its trace, is $(4 - 2 - 2 + 1) / (4 + 1) = 1 / 5$.
\item The arithmetic mean divided by the geometric mean of its elements, is $1 / 2$.
\end{itemize}

\subsection{Fast computations}

If $n$ is large, one way to efficiently compute $A^n$ is as follows. Let's say that $n = 100$. Do the following computations:
\begin{align*}
 A^2=A\cdot A, A^4 = A^2\cdot A^2, A^8=A^4\cdot A^4,A^{16}=A^8\cdot A^8, \\
A^{32}=A^{16}\cdot A^{16}, A^{64}=A^{32}\cdot A^{32}, A^{100}=A^{64}\cdot A^{32}\cdot A^4.
\end{align*}
\noindent This can be useful to quickly get an approximation of the largest root of the characteristic polynomial, by eliminating all but the first (largest) root $r_1$ in formula~(\ref{linea11}), and using $n = 100$. Once the first root has been found, it is easy to also get an approximation for the second one, and then for the third one. If instead, you are interested in approximating the smallest roots, you can proceed the other way around, by using the formula for $A^n$, with $n = -100$ this time. Note that $A^{-100}=(A^{-1})^{100}$.

\subsection{Square root of a matrix}

Formula~(\ref{linea11}) works not only for integer values of $n$, but can be extended to fractional arguments, such as $n=1/2$. Here, I illustrate how it works on an example. Thus, I show how to compute the \textcolor{index}{square root of a matrix}\index{square root (matrix)} [\href{https://en.wikipedia.org/wiki/Square_root_of_a_matrix}{Wiki}], using the methodology developed so far. This problem has many applications, especially when the matrix $A$ is 
symmetric \textcolor{index}{positive semidefinite}\index{positive semidefinite (matrix)} [\href{https://en.wikipedia.org/wiki/Definite_matrix}{Wiki}]: then it has only one symmetric 
positive semidefinite square root, denoted as $A^{1/2}$. See for instance chapter~\ref{chapterregression} on linear regression and \gls{gls:syntheticdata}, especially section~\ref{c6correlstr} entitled ``correlation structure". The square root is needed to generate synthetic data with a pre-specified correlation matrix.

\noindent Now, using formula~(\ref{linea11}) with $n=1/2$, let us compute the square root of the $2\times 2$ matrix $A$ defined as
$$
A = \left(
\begin{array}{rr}
 3 & 1 \\
 1 & 1
\end{array}
\right).
$$
\noindent
Its characteristic polynomial, according to formula~(\ref{cplr}), is $x^2 -4x +2$. The roots are $r_1=1+\sqrt{2}$ and $r_2=1-\sqrt{2}$ and are both real and positive. According
to formula~(\ref{linea1}), the matrices $Q_1,Q_2$ are respectively equal to 
$$
  Q_1 = \frac{A-r_2 I}{r_1-r_2}=
\frac{1}{4}\cdot\left(
\begin{array}{cc}
 2+\sqrt{2} & \sqrt{2} \\[6pt]
 \sqrt{2} & 2-\sqrt{2}
\end{array}
\right), 
\quad    Q_2 = \frac{A-r_1 I}{r_2-r_1}=
\frac{1}{4}\cdot\left(
\begin{array}{cc}
 2-\sqrt{2} & -\sqrt{2} \\[6pt]
 -\sqrt{2} & 2+\sqrt{2}
\end{array}
\right).  
$$
The matrix $A$ has one positive definite square root, namely
$$
A^{1/2}=\sqrt{r_1}\cdot Q_1 + \sqrt{r_2}\cdot Q_2 = \frac{1}{2}
\left(
\arraycolsep=1.4pt\def\arraystretch{1.4}
\begin{array}{ll}
\sqrt{10+\sqrt{2}} \text{ }& \sqrt{2-\sqrt{2}} \text{ }\\[6pt]
\sqrt{2-\sqrt{2}}  \text{ }& \sqrt{2+\sqrt{2}} \text{ }
\end{array}
\right).
$$ 

\noindent It is easy to show that $A^{1/2}\cdot A^{1/2}= A$. In higher dimensions, the Vandermonde system (\ref{linea11}) is solved using the method described in section~\ref{vandervg}.


\section{Application to Machine Learning problems}

I discussed principal component analysis (PCA), data compression via PCA, and pseudo-inverse matrices in section~\ref{slin1}. Here I focus on applications to time series, Markov chains, and linear regression.

\subsection{Markov chains}

A \textcolor{index}{Markov chain}\index{Markov chain} [\href{https://en.wikipedia.org/wiki/Markov_chain}{Wiki}] is a particular type of  time series or stochastic process. At iteration or time $n$, a system is in a particular state $i$ with probability $P_n(i)$. The probability to move from state $i$ at time $n$, to state $j$ at time $n + 1$ is called a transition probability and denoted as $p_{ij}$. It does not depend on $n$, but only on $i$ and $j$. The Markov chain is governed by its initial conditions $P_0(i), 1\leq i\leq p$, and the transition probability matrix denoted as $A$, containing the elements $p_{ij}$. The size of the transition matrix is 
$p \times p$, where $p$ is the number of potential states in the system. Thus $1\leq i,j\leq p$. As $n$ tends to infinity, $A^n$ and the whole system reaches an equilibrium distribution. This is because

\begin{itemize}
\item	The \textcolor{index}{characteristic polynomial}\index{characteristic polynomial} attached to $A$ has a root equal to $1$. 
\item	The absolute value of any root is less than or equal to $1$.
\end{itemize}

\subsection{Time series: auto-regressive processes}\label{tslineas}

\Gls{gls:armodels} processes\index{auto-regressive process} (AR) [\href{https://en.wikipedia.org/wiki/Autoregressive_model}{Wiki}] represent another basic type of time series. Unlike Markov chains, the number of potential states is infinite, and a state can any real value, not just an integer. Yet the time is still discrete. Time-continuous AR processes such as \textcolor{index}{Gaussian processes}\index{Gaussian process} [\href{https://en.wikipedia.org/wiki/Gaussian_process}{Wiki}], are not included in this discussion. An AR($p$) process is defined as follows:
$$
X_n=a_1 X_{n-1}+ \cdots+ a_p X_{n-p} + e_n.
$$
Its \textcolor{index}{characteristic polynomial}\index{characteristic polynomial} is
\begin{equation}\label{rootcp}
x^p=a_1 x^{p-1}+a_2 x^{p-2}+\cdots + a_{p-1}x +a_p.
\end{equation}

\noindent Here $\{ e_n \}$ is a \textcolor{index}{white noise}\index{white noise} process (typically uncorrelated Gaussian variables with same variance) [\href{https://en.wikipedia.org/wiki/White_noise}{Wiki}]. We assume that all expectations are zero. We are dealing here with a non-homogeneous linear (stochastic) recurrence relation. The most interesting case is when all the roots of the characteristic polynomial have absolute value less than $1$. Processes satisfying this condition are called 
\textcolor{index}{stationary}\index{stationary process}. In that case, the \textcolor{index}{auto-correlations}\index{auto-correlation} are decaying exponentially fast.

\noindent The lag-$k$ covariances satisfy the relation 
$$
\gamma_k=\text{Cov}[X_n,X_{n-k}]=\left\{
\begin{array}{ll}
a_1 \gamma(k-1)+\cdots + a_p\gamma(k-p) &   \text{if } k\neq 0 \\ [6pt]
a_1   \gamma(k-1)+\cdots + a_p\gamma(k-p) + \sigma^2 & \text{if } k= 0
\end{array}
\right.
$$
with
$$
\sigma^2=\text{Var}[e_n],\quad \text{Var}[X_n]=\gamma(0),\quad \rho(k)=\text{Corr}[X_n,X_{n-k}]=\gamma(k)/\gamma(0).
$$
\noindent Thus the auto-correlations can be explicitly computed, and are also related to the characteristic polynomial. This fact can be used for model fitting, as the auto-correlation structure uniquely characterizes the (stationary) time series. Note that if the white noise is Gaussian, then the $X_n$'s are also Gaussian. 

More about the auto-correlation structure can be found in lecture notes from Marc-Andreas Muendler \cite{mamu}, who teaches economics at UCSD. His material is very similar to what I discuss here, but more comprehensive. See also Barbara Bogacka's lecture notes on time series \cite{bbog}, especially chapter 6. Finally, section~\ref{linearar} in this article explores the mathematical aspects in more details.

\subsection{Linear regression}

Linear \gls{gls:regression} problems can be solved using the \textcolor{index}{ordinary least squares}\index{ordinary least squares} (OLS) method [\href{https://en.wikipedia.org/wiki/Ordinary_least_squares}{Wiki}]. 
The framework involves a response $y$, a data set $X$ consisting of $p$ features or variables and $m$ observations, and $p$ regression coefficients (to be determined) stored in a vector $b$. In matrix notation, the problem consists of finding $b$ that minimizes 
the distance $||y - Xb||$ between $y$ and $Xb$. Here $X$ is an $m\times p$ matrix, and $y,b$ are column vectors. The solution is 
$$
b= A^{-1}X^T y, \quad \text{with } A = X^TX.
$$
The techniques discussed in section~\ref{vandervg} can be used to compute the inverse of $A$ using formula~(\ref{linea11}) with $n=-1$, either exactly using all the roots of its 
\textcolor{index}{characteristic polynomial}\index{characteristic polynomial}, or approximately using the $2$--$3$ roots with the lowest moduli. You need to use the recursion~(\ref{itervv}) backward when implementing the methodology in section~\ref{vandervg}, that is, for $n=-1,-2$ and so on, rather than for $n=1,2$ and so on.  

If $A$ is not invertible, the methodology described in section~\ref{slin1b} can be useful: it amounts to working with a pseudo inverse of $A$. Note that $A$ is a $p \times p$ matrix as in section~\ref{slin1}. Questions regarding confidence intervals can be addressed using model-free techniques as discussed in chapter~\ref{chapterfuzzy}. 
  
\section{Mathematics of auto-regressive time series}\label{linearar}

Here I connect the dots between the auto-regressive \textcolor{index}{time series}\index{time series} described in section~\ref{tslineas}, and the material in section~\ref{linea1}. For the AR($p$) process in section~\ref{tslineas}, we have
$$
X_n = g(e_p,e_{p+1},\dots,e_n)+\sum_{k=1}^p r_k^n q_k, \quad \text{with }
\left(
\begin{array}{c}
q_1 \\
q_2 \\
\vdots \\
q_p
\end{array}
\right)
=V^{-1} 
\left(
\begin{array}{c}
X_0 \\
X_1 \\
\vdots \\
X_{p-1}
\end{array}
\right)
$$
where $V$ is the same matrix as in formula~(\ref{linea11}), the $r_k$'s are the roots (assumed distinct here) of the 
\textcolor{index}{characteristic polynomial}\index{characteristic polynomial} defined by~(\ref{rootcp}), and $g$ is a linear function of $e_p, e_{p+1}, ..., e_n$. For instance, if $p = 1$, we have
$$
g(e_p,e_{p+1},\dots,e_n)=\sum_{k=0}^{n-p}a_1^k e_{n-k}.
$$
This allows you to compute $\text{Var}[X_n]$ and $\text{Cov}[X_n, X_{n-k}]$, conditionally to  $X_0, ..., X_{p-1}$. The limit, when $n$ tends to infinity, allows you to compute the unconditional variance and auto-correlations attached to the process, in the stationary case. For instance, if $p = 1$, we have
$$
\text{Var}[X_\infty]=\lim_{n\rightarrow\infty}\sum_{k=0}^{n-p} a_1^{2k} \text{Var}[e_{n-k}] =\sigma^2\sum_{k=0}^\infty a_1^{2k}=\frac{\sigma^2}{1-a_1^2}
$$
where $\sigma^2=\text{Var}[e_0]$ is the variance of the white noise $\{e_n\}$, and $|a_1| < 1$ because we assumed stationarity.  For the general case (any $p$) the formula, if $n$ is larger than or equal to $p$, is 
$$
g(e_p,e_{p+1},\dots,e_n)=\sum_{k=0}^{n-p}\alpha_k e_{n-k},  \text{ } \text{with }\alpha_0 =1,  \text{ } \alpha_k=\sum_{t=1}^p a_t \alpha_{k-t} \text{ }  \text{if } k>0, \text{ } \alpha_k=0 \text{ } \text{if } k<0.
$$
In this case, we have
$$
\text{Var}[X_\infty]=\sigma^2\sum_{k=0}^\infty \alpha_k^{2}.
$$
The initial conditions for the coefficients $\alpha_k$ correspond to $k = 0, -1, -2, ..., -(p -1)$. Thus $\alpha_0=1$, and the remaining $\alpha_k$'s ($k<0)$ are zero. Thus, the recurrence relation for $\alpha_n$ can be solved 
using the same roots $r_1,\dots,r_p$ solution of equation~(\ref{rootcp}). Assuming the roots are distinct, we have
\begin{equation}\label{recurr}
\alpha_n = \sum_{k=1}^p r_k^n q'_k, \quad \text{with }
\left(
\begin{aligned}
      q'_1\\
     q'_2 \\
     \vdots \\
    q'_p
\end{aligned}
\right) = W^{-1} \left(
\begin{array}{c}
      1 \\
     0\\
     \vdots \\
    0
\end{array}
\right), 
\quad \text{and }
W=\left(
\begin{array}{cccc}
      1 & 1 & \cdots & 1 \\
     r_1^{-1} & r_2^{-1} & \cdots & r_p^{-1} \\
     \vdots & \vdots &  & \vdots \\
    r_1^{-(p-1)} &  r_2^{-(p-1)} &\cdots  &  r_p^{-(p-1)}
\end{array}
\right).
\end{equation}

\noindent Again, a direct computation of $W^{-1}$ is numerically unstable, except in some special cases. Typically, a few of the $r_k$'s are dominant. The other ones can be ignored, leading to approximations and increased stability. 
Finally, if two time series models, say an ARMA and an AR models, have the same variance and covariance structure, they are actually identical. 

\subsection{Simulations: curious fractal time series}

To simulate the \textcolor{index}{auto-regressive}\index{time series!auto-regressive} time series described in section~\ref{tslineas}, I proceed backwards: first, pick up the
roots $r_1,\dots,r_p$ of the \textcolor{index}{characteristic polynomial}\index{characteristic polynomial}, then compute the coefficients $a_1,\dots,a_p$. 
This leads to interesting discoveries. I want at least one of the roots to have its modulus equal to one. The other roots must have a modulus strictly smaller than one. Roots can have a multiplicity greater than one. Depending on the roots, whether they are all real, whether some are complex, or whether some have a multiplicity greater than one, the pattern is very different. It falls into three categories:
\begin{itemize}
\item Type 1: A very smooth time series if the root(s) with largest modulus has a mutiplicity greater than one.
\item Type 2: A Brownian-like appearance if all the roots are real and distinct.
\item Type 3: An highly oscillating time series that fills a dense area when complex roots with modulus equal to one, are present. 
\end{itemize}
In Figure~\ref{fig:linearbv}, I have re-scaled the horizontal and vertical axes so that the time series look time-continuous. With this transformation, Type 2 actually corresponds to a 1-D \textcolor{index}{Brownian motion}\index{Brownian motion} [\href{https://en.wikipedia.org/wiki/Brownian_motion}{Wiki}].
Type 3 has a curve that exhibits a \textcolor{index}{fractal dimension}\index{fractal dimension} 
[\href{https://en.wikipedia.org/wiki/Fractal_dimension}{Wiki}] strictly between $1$ and $2$. Type 1 looks very smooth. It seems like it has derivatives of any order, everywhere. Thus it can not be a Brownian motion. Yet if you zoom in or out, the same statistical properties (smoothness, expected numbers of bumps and so on) repeat themselves. This means that we are dealing with a strange mathematical function: one that can not be approximated by a Taylor series, yet very smooth and chaotic with self-replicating features, at the same time. Also, we need to keep in mind that it is a \textcolor{index}{stochastic function}\index{stochastic function}.

The smoothness of a time series is typically measured using its \textcolor{index}{Hurst exponent}\index{Hurst exponent}\index{time series!Hurst exponent} [\href{https://en.wikipedia.org/wiki/Hurst_exponent}{Wiki}]. The extreme, most chaotic case is a white noise, and the ``middle case" is a Brownian motion. Here Type 3 seems more extreme than a white noise, and Type 1 is unusually smooth. Except for Type 3, the discrete version of these time series all have long-range auto-correlations.

\subsubsection{White noise: Fréchet, Weibull and exponential cases}

I use one of the simplest distributions to sample from, to generate the white noise $\{e_n\}$. Using independent uniform deviates $U_1,U_2,\dots$ on $[0, 1]$, I first generate the deviates 
\begin{equation}\label{eq1lin}
v_n=\tau\Big(-\log(1-U_n)\Big)^\gamma, \quad n=1,2,\dots
\end{equation}
Then the noise $e_n$ is produced using 
$$e_n=v_n-\text{E}[v_n],\quad \text{with } \text{E}[v_n]=\tau\Gamma(1+\gamma) \text{ and }
\text{Var}[e_n] = \text{Var}[v_n]= \tau^2\Big[\Gamma(1+2\gamma)-\Gamma^2(1+\gamma)\Big].$$

\noindent Here $\tau,\gamma$ are parameters, with $\gamma>-\frac{1}{2}$, and $\Gamma$ is the \textcolor{index}{Gamma function}\index{Gamma function} [\href{https://en.wikipedia.org/wiki/Gamma_function}{Wiki}]. We have the following cases:
\begin{itemize}
\item If $\gamma=1$, then $v_n$ has an exponential distribution.
\item If $-1<\gamma<0$, then $v_n$ has a \textcolor{index}{Fréchet distribution}\index{Fréchet distribution}\index{probability distribution!Fréchet}. If in addition, $\gamma>-\frac{1}{2}$, then its variance is finite. 
\item If $\gamma>0$, then $v_n$ has a \textcolor{index}{Weibull distribution}\index{Weibull distribution}\index{probability distribution!Weibull}, with finite variance. 
\end{itemize}
It is surprising that this distribution has different names, depending on whether $\gamma<0$ or $\gamma>0$. This distribution is discussed in my book
on stochastic processes \cite{vgsimulnew}, pages 41--42.

\renewcommand{\arraystretch}{1.4} %%%
\begin{table}[H]
\[
\begin{array}{lrrrrl}
\hline
  \text{Category} & a_1 & a_2  & a_3 & a_4  & \text{Factored version}\\
\hline
\text{Type 1 (a)}	&	\frac{11}{4}	&	-\frac{21}{8}	&	1 & -\frac{1}{8}	&	(x-1)^2(x-\frac{1}{2})(x-\frac{1}{4}) \\
\text{Type 1 (b)}	&	2	&	-2	&	2	&	-1	&	(x-1)^2(x^2+1)\\
\text{Type 2}	&	\frac{15}{8}	& -\frac{35}{32} & \frac{15}{64} & -\frac{1}{64} &	(x-1)(x-\frac{1}{2})(x-\frac{1}{4})(x-\frac{1}{8}) \\
\text{Type 3 (a)}	&	0	&	-\frac{1}{2}	&	0	&	\frac{1}{2}	&	(x^2+1)(x^2-\frac{1}{2}) \\
\text{Type 3 (b)} 	&	0	&	2	&	0	&	-1	&	(x-1)^2(x-\frac{1}{2})^2\\ 
\hline
\end{array}
\]
\caption{\label{tablin1} Characteristic polynomials used in the simulations}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%


\subsubsection{Illustration}

The simulation results shown in Figure~\ref{fig:linearbv} come from my spreadsheet \texttt{linear2-small.xlsx}, available on my GitHub repository, 
\href{https://github.com/VincentGranville/Machine-Learning/blob/main/Spreadsheets/README.md}{here}. The cells highlighted in light yellow 
correspond to the model parameters, such as $\tau,\gamma, a_1,\dots,a_4$: you can modify them, and it will automatically update the picture.
A much larger spreadsheet \texttt{linear2.xlsx} (about 30MB), containing many interesting simulations, each with 50,000 observations,
 is available \href{https://ln5.sync.com/dl/4c157bf10/5be4uhcd-w6g99e8h-96ndb9t4-jshr672n}{here}.

\begin{figure}%[H]
\centering
\includegraphics[width=0.94\textwidth]{linear.png}
\caption{AR models, classified based on the types of roots of the characteristic polynomial}
\label{fig:linearbv}
\end{figure}


The simulations use $\tau=\gamma=0.5$, and the characteristic polynomials $x^4-(a_1x^3+a_2 x^2 +a_3 x + a_4)$ listed 
in Table~\ref{tablin1}. The ``type 2" plot is a classic, while ``type 3 (a)" looks like an ordinary audio signal. 
In types 3 (a) and 3 (b), the frequency of oscillations is extremely high: this explains while the curve seems to completely fill an area. 
The ``type 3 (b)" time series is rather original. Its characteristic polynomial has two distinct real roots, each with multiplicity $2$. Thus, you are
unlikely to encounter it in college classes or textbooks. 

But the truly spectacular plot, surprisingly, is ``type 1". These curves are very smooth,
 but no matter how much you zoom in, it never becomes a flat line, unlike polynomials or well-behaved math functions. The ``type~1" curves  in
 Figure~\ref{fig:linearbv} have this property: when differentiated, they become a ``type 2" curve. In short, 
these curves are the integral of Brownian motions, and called \textcolor{index}{Itô integrals}\index{Itô integral} [\href{https://www.robots.ox.ac.uk/~lsgs/posts/2018-09-30-ito-strat.html}{Wiki}]. The ``type 2" curves  are continuous, and their derivatives are white noises.
If the multiplicity of the root with largest modulus is $3$, then the corresponding curve is even smoother and can be differentiated three times. It's first derivative is a ``type~1" curve, and its second derivative is a ``type~2" curve. 

Other unusual Brownian motions, this time in two dimensions, are discussed in section~\ref{lvfgf}.  It includes a 
family of Brownian motions, exhibiting an extraordinary strong clustering structure, depending on model parameters.  Some Gaussian processes 
can have a behavior similar to ``type 1" depending on parameters, see \href{https://www.r-bloggers.com/2019/07/sampling-paths-from-a-gaussian-process/}{here}. For Gaussian processes, see also \cite{cras2006}, especially chapter 4.

%xxxx update github to mention spreadsheet link to MLT

\subsection{Solving Vandermonde systems: a numerically stable method}\label{vandervg}

\textcolor{index}{Vandermonde systems}\index{Vandermonde matrix}  [\href{https://en.wikipedia.org/wiki/Vandermonde_matrix}{Wiki}] are notoriously 
\textcolor{index}{ill-conditioned}\index{ill-conditioned problem} [\href{https://en.wikipedia.org/wiki/Condition_number}{Wiki}]. This explains why nobody really solve them, and instead, use other techniques. Yet they could potentially be used to solve many problems,
including linear \gls{gls:regression} and \textcolor{index}{eigenvalues}\index{eigenvalue} [\href{https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors}{Wiki}] via
 the characteristic polynomial, \textcolor{index}{Lagrange interpolation}\index{Lagrange interpolation} [\href{https://en.wikipedia.org/wiki/Runge\%27s_phenomenon}{Wiki}], Markov chains (computation of the \textcolor{index}{stationary distribution}\index{stationary distribution}), linear recurrence equations, principal component analysis (PCA), auto-regressive time series, square root of a matrix, and more.

Using the methodology presented in this article, I propose a numerically stable algorithm to solve such systems, in a very indirect way. I illustrate my method on a particular example. Let's consider the auto-regressive process 
\begin{equation}\label{itervv}
X_n=\frac{3}{4}X_{n-1} + \frac{7}{8} X_{n-2} -\frac{3}{4} X_{n-3} +\frac{1}{8} X_{n-4},
\end{equation}
with no white noise. In other words, let $\tau=0$ in formula~(\ref{eq1lin}). The characteristic polynomials has roots $r_1=1,r_2=-1,r_3=\frac{1}{2},r_4=\frac{1}{4}$. Let the initial conditions be $X_0=1,X_{-1}=0, X_{-2}=0, X_{-3}=0$ as in formula~(\ref{recurr}). Thus, I will be solving the Vandermonde system
\begin{equation}\label{refrecu}
\left(
\begin{array}{rrrr}
      1 & 1 & 1 & 1 \\
     1 & -1 & 2 & 4 \\
     1 & 1 & 4  & 16 \\
    1 &  -1 & 8  &  64
\end{array}
\right) 
\left(
\begin{aligned}
      q'_1\\
     q'_2 \\
     q'_3 \\
    q'_4
\end{aligned}
\right) =
\left(
\begin{array}{c}
      1 \\
     0\\
     0 \\
    0
\end{array}
\right).
\end{equation}
We know that 
\begin{equation}\label{eqvglin}
X_n=r_1^nq'_1 + r_2^n q'_2 + r_3^n q'_3+r_4^n q'_4.
\end{equation}
Computing $X_n$ iteratively using formula~(\ref{itervv}), we find that
$X_{30}\approx 1.6000$ and $X_{31}\approx 1.0667$. The influence of $r_3, r_4$ is negligible in formula~(\ref{eqvglin}), and we can reasonably ignore these two roots if $n$ is large enough. Thus, if $n=30$ we have $1.6000 \approx q'_1 + q'_2$, according to~(\ref{eqvglin}). And $n=31$ yields $1.0667 \approx q'_1 - q'_2$. These two equations allow us to get a very good approximation both for $q_1'$ and $q_2'$. We don't care about $q_3'$ and $q_4'$ as the associated roots $r_3,r_4$ have almost no impact on $X_n$ when $n$ is large. 

However, we can still compute them if we want to. Consider the sub-recursion $\{Y_n\}$ with characteristic polynomial $(x-r_3)(x-r_4)$, that is $Y_n=\frac{3}{4}Y_{n-1}-\frac{1}{8}Y_{n-2}$. Let the initial conditions be 
\begin{equation}\label{eqvgr1}
Y_n=X_n - (r_1^n q'_1 + r_2^n q'_2), \quad \text{for } n=0,-1.
\end{equation}
 Use the approximated values $q'_1=1.333$ and $q'_2=0.2667$ computed in the previous step. Clearly, by design,
\begin{equation}\label{eqvgr2}
Y_n=r_3^n q'_3 + r_4^n q'_4
\end{equation}
with $r_3=\frac{1}{2},r_4=\frac{1}{4}$. Also, since $r_3$ dominates over $r_4$, for $n$ large enough, we have $Y_n\approx r_3^n q'3$, that is,
$q'_3\approx r_3^{-n}Y_n \approx 2^n Y_n$. Using formulas~(\ref{eqvgr1}) and~(\ref{eqvgr2}) combined with the approximated values of $q'_1, q'_2$
 and $n=15$, one
eventually obtains $q'_3\approx 2^{15}Y_{15} \approx -0.6667$. Now we are left with finding the last coefficient $q'_4$. It can be done using the linear recurrence with characteristic polynomial $x-r_4$. The details are left as an exercise. The exact values are
$$q'_1=\frac{4}{3}=1.3333\dots, q'_2=\frac{4}{15}=0.2666\dots, q'_3=-\frac{2}{3}=-0.6666\dots, q'_4=\frac{1}{15}=0.0666\dots. $$
If we have more than $p=4$ unknowns, we can proceed iteratively in the same manner, obtaining approximate values successively for $q'_1,q'_2$ and so on, assuming the roots are ordered by modulus, with $r_1$ having the largest modulus.  We accumulate inaccuracies at each new iteration, but the loss of accuracy is controlled:
 the biggest losses are on the coefficients with the lowest impact. Roots with same moduli must be treated jointly, as I did here for $r_1$ and $r_2$. If some roots have a multiplicity greater than $1$, formula~(\ref{eqvglin}) must be adapted: see \cite{arashfa}.

\section{Math for Machine Learning: Must-Read Books}

In general, my articles are not traditional. I try to offer original content to the reader, presenting innovative methods explained in simple English, in
a style that is pleasant to read. 
If you are looking for standard textbooks to learn the math of machine learning, I recommend ``Mathematics for Machine Learning" \cite{faisal2020}
and ``Introduction to Mathematical Statistics" \cite{hogg2019}. The book ``Deep Learning" \cite{goodfellow2016} also covers the math of matrix algebra. Lectures notes from Zico Kolter at Stanford University (2015), covering linear algebra, are available \href{https://cs229.stanford.edu/section/cs229-linalg.pdf}{here}. The book ``Linear Algebra for Data Science" by Shaina Bennett (2021) is available online \href{https://shainarace.github.io/LinearAlgebra/}{here},
and features examples in R. See also ``Introduction to Probability for Data Science" by Stanley Chan (2021) available \href{https://probability4datascience.com/}{here}, with Matlab and Python code, especially the last chapter on random processes.

For hands-on references with Python code, \href{https://www.statsmodels.org/stable/user-guide.html}{StatsModels.org} is a vast GitHub repository  covering a lot of linear algebra and time series. Another one is Statistics and Machine Learning in Python, \href{https://duchesnay.github.io/pystatsml/}{here}. See also the Python \href{https://scikit-learn.org/0.15/modules/linear_model.html}{Scikit-learn.org} guide about linear models,
and \href{https://christophm.github.io/interpretable-ml-book/limo.html}{Interpretable Machine Learning}. You won't find the math or type of examples discussed here, in any of these books. However, they are useful, classic yet modern references.


%MLT: spectacular: vandermonde stable, smooth fractal, weibull=frechet

% https://ln5.sync.com/dl/ecf3c0a70/gg26ps3m-xrzavdkb-6wxerziq-jy25dd4g

% Gentle Introduction to Linear Algebra, with Spectacular Applications https://mltblog.com/3M1nfVv

\renewcommand{\arraystretch}{1.4} %%%

%-----------------------------------------------------------------------------------------------------------------
\Chapter{The Art of Visualizing High Dimensional Data}{}\label{chapvisu}


I discuss different techniques  to produce professional data videos, animated GIFs, and other visualizations in Python, 
using the \texttt{pillow} and \texttt{moviepy} libraries. Applications include visualizing prediction intervals regardless of the number of features (also called independent variables), supervised classification applied to an infinite dataset, convergence of machine learning algorithms, and animations featuring objects of various sizes moving at various speeds according to various paths. For instance, I show a video simulation of 300 comets circling the sun, to assess the risk of a collision. 

The Python libraries in question 
allow for low-level image processing at the pixel level. This is particularly useful to build ad-hoc, original visualization algorithms.  I also discuss  optimization: 
amount of memory required, performance of compression techniques, numpy versus math library, anti-aliasing to depixelate an image, and so on. Some of the videos use the RGBA palette format. 
This 4-dimensional color encoding (red, green, blue, alpha) allows you to set the transparency level (also called ``opacity") when objects overlap. It is particularly useful in models involving mixtures or overlapping groups in supervised classification. In that context, not only it helps with visualizations, but it actually solves the classification problem on its own.


\section{Introduction}\label{vizintro}

I start with Figures~\ref{fig:orbit80} and~\ref{fig:orbit81}. It is a simulation of comets orbiting the sun, at various velocities, with various orbit orientations and eccentricities. The goal is to assess the risk of collisions. The pictures do a poor job at rendering all the dimensions involved. Thus I created two videos,
available \href{https://www.youtube.com/watch?v=GD_ZPb48lmk}{here} (showing the orbits) and \href{https://www.youtube.com/watch?v=CeYmsBdfrHM}{here} (featuring comet properties and collisions). The videos add far more than one dimension -- the time -- to understand the mechanics of the system. 

The purpose of this article is to introduce you to enriched visualizations, with a focus on animated gifs and videos built in Python. 
For instance, the comet video can feature several dimensions that are difficult to show in a static picture: the comet locations at any given time, the relative velocity of each comet, 
the change in velocity (acceleration), the change in comet size when approaching the sun,  the comet interactions (the apparent collisions), any change in the orbit (orientation or eccentricity), or any change in composition (the color assigned to a comet). The static images are good at showing the size, orbit path, and comet composition. We could make it a bit more general and represent the movements in 3D. Regardless we can easily display  17 dimensions. The 17 dimensions featured in the comet video are:
\begin{itemize}
\item location in space and time (3 or 4 dimensions)
\item comet composition or type, and change in composition (2 dimensions, categorical variables)
\item comet size and change in size (2 dimensions, categorical/binned variables here)
\item comet velocity and acceleration, including change in acceleration (3 dimensions)
\item orbit orientation (rotation angle) and eccentricity, and change in these metrics (4 dimensions)
\item period of each orbit, and collisions (2 dimensions)
\item the number of comets at any given time (1 dimension)
\end{itemize}
\vspace{1ex}
While it is possible to show all these features in traditional time series plots, the video conveys a more compelling message, providing strong visual insights. Note that in my video, the orbit, size and composition of any given comet, is static.  I use colors to represent the velocity and eccentricity: red = fast, green = high eccentricity, purple = fast + high eccentricity, white = standard. Also, the size (big or small) is related to the maximum distance to the sun: small dots correspond to comets staying permanently close to the sun.

The remaining of this article focuses on four applications: prediction intervals in any dimension, supervised classification, convergence of algorithms such as \gls{gls:gradient} descent when dealing with chaotic functions, and spatial time series (the comets illustration). In some of these cases, using a video helps, and in other cases, it does not. All Python visualizations use the \textcolor{index}{RGB}\index{color model!RGB} (red / green / blue) color model [\href{https://en.wikipedia.org/wiki/RGB_color_model}{Wiki}]. It can represent 3 dimensions.
One of the videos uses the \textcolor{index}{RGBA}\index{color model!RGBA} model [\href{https://en.wikipedia.org/wiki/RGBA_color_model}{Wiki}], allowing you to
add transparency or opacity. This is particularly useful when displaying overlapping clusters: when a red cluster overlaps with a green one, the intersection looks yellow (red + green = yellow). 
I also use 
\textcolor{index}{anti-aliasing}\index{anti-aliasing}
%\ati{anti-aliasing}{anti-aliasing} techniques
 [\href{https://en.wikipedia.org/wiki/Anti-aliasing}{Wiki}] to make contours look smooth instead of pixelated. Finally, I use a compression technique (\textcolor{index}{FFmpeg}\index{video compression!FFmpeg}, [\href{https://en.wikipedia.org/wiki/FFmpeg}{Wiki}]) to reduce the size of the animated gifs.

In a future article, I will add a sound track to the video, related to the behavior of the whole system. The sound (amplitude, frequency, texture) can easily add 3 dimensions. For instance, it can represent the local temperature, density and size of the universe at any given time.

\section{Applications}

I discuss specifics of the code, such as Python instructions and libraries, in section~\ref{pythonviz}. This section focuses on high level concepts, including the math behind the visualizations. As much as possible, I use notations that are compatible with the names of the variables and arrays in the Python code.

\subsection{Spatial time series}\label{ellipser}

Here I discuss the comet visualization introduced in section~\ref{vizintro}. All orbits are elliptic. The orbits are bivariate continuous time series, or in other words, spatial time series. This will become obvious when looking at the parametric equation of the ellipse. The cartesian equation of an unslanted ellipse centered at the origin is 
$$\frac{x^2}{a^2}+\frac{y^2}{b^2}=1,
$$
with $a,b>0$. The eccentricity is defined as $\epsilon=\sqrt{|a^2-b^2|}$. It $\epsilon=0$, the ellipse is a circle. If $\epsilon$ is large, the ellipse is elongated. In all cases, an ``horizontal" ellipse has two focus points: $(g'_x,g'_y)=(0, \epsilon)$ and $(g_x,g_y)=(0,-\epsilon)$. Without loss of generality due to rotational symmetry, I only consider horizontal ellipses, and I ignore the second focus point. The parametric equation of the ellipse is
\begin{align}
x_0(t) & = a\cdot\cos(vt+\tau),\nonumber \\
y_0(t) & = b\cdot\sin(vt+\tau),\nonumber 
\end{align}
where $v$ is the speed of the comet, $t$ represents the time, and $\tau$ determines the initial position of the comet when $t=0$.
I then apply a rotation of angle $\theta$. The parameter $\theta$ is referred to as the orientation of the orbit. Now the parametric equation of the ellipse becomes
\begin{align}
x(t) & = x_0(t) \cos\theta - y_0(t)\sin \theta,\nonumber \\
y(t) & = x_0(t) \sin\theta + y_0(t)\cos \theta,\nonumber 
\end{align}
and its focus point of interest becomes $(g_x,g_y)= (g'_x \cos\theta - g'_y\sin \theta,g'_x \sin\theta + g'_y\cos \theta)$.

Now, I want the sun to be at the origin, and have the comet rotates around the sun. This is accomplished by subtracting the vector $(g_x,g_y)$  
to $(x(t),y(t))$. Finally, there are $m$ comets labeled $0,\dots,m-1$. The notation $(x_n(t),y_n(t))$ denotes the position of the $n$-th comet at time
$t$, with $0\leq n<m$. Time is sampled evenly: each sample value produces a frame in the video, with $m$ dots: one per comet.

The parameters $\tau,\theta,v,a,b$, more precisely $\tau_n,\theta_n,v_n,a_n,b_n$ as there is one set for each orbit, are randomized. However, for a
realistic simulation that complies with the laws of the universe (for instance, Kepler's laws), several constraints should be put on these parameters. For instance, the speed should increase when approaching the sun. Also the gravitational interactions are ignored. In fact, the real orbits are not truly periodic because of this, unlike in the simulations.

\subsection{Prediction intervals in any dimensions}

This application, including the accompanying Python code and Excel spreadsheet, is discussed in detail in chapter~\ref{chapterfuzzy}, using \gls{gls:syntheticdata}. There is no need for a video here: a simple
scatterplot will do. I included this visualization because it works in any dimension, and it is rarely if ever mentioned elsewhere, despite its ease of interpretation. Also, it can be done in Excel: see Figure~\ref{fig:orbit60}, and the 
Excel implementation \texttt{fuzzy4.xlsx}, available \href{https://ln5.sync.com/dl/0caeb8e10/mztnibg9-xrkdks7g-r8bsgabw-3fsizwif}{here}.

The purpose is to compute predictions and prediction intervals for data outside of a \gls{gls:trainingset}, typically in a \gls{gls:regression} problem (linear or not). I  use a \gls{gls:validset}\index{validation set} [\href{https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets}{Wiki}] to assess performance, comparing the true value with the predicted one. The validation set is a subset of the training set, not used to train the model, but rather, to test it. The observed response $Z_\text{obs}$ -- also called true value -- depends on $m$ features $X_1,\dots,X_m$. All are column vectors, with each entry corresponding to an observation.  Thus, the dimension is $m$. The predicted
value at a specific location $x=(x_1,\dots,x_m)$ in the $m$-dimensional feature space is denoted as $Z_\text{pred}(x)$, while the lower and upper bounds of (say) a 90\% prediction interval are denoted as $Z_\text{.05}(x)$ and $Z_\text{.95}(x)$ respectively.

If $m>2$, visualizing the prediction intervals becomes challenging. One way to do it is to create a scatterplot featuring the bivariate vectors 
$(Z_\text{obs}(x), Z_\text{pred}(x))$ colored in blue, for all $x$ in the validation set. Thus $Z_\text{obs}(x)$ is on the horizontal axis, and $Z_\text{pred}(x)$ on the vertical axis. 
Then, add the points $(Z_\text{obs}(x),Z_\text{.95}(x))$ in red, and the points $(Z_\text{obs}(x),Z_\text{.05}(x))$ in green, on the scatterplot. 
The end result is Figure~\ref{fig:orbit60}. Of course, it works regardless of the dimention.

Interpreting the visualization is easy. If the observed and predicted values were identical, the point cloud, that is, the $(Z_\text{obs}(x),Z_\text{pred}(x))$'s, should all be located on the main diagonal. Deviations on the vertical axis from the main diagonal show the individual residual errors. It provides a much better picture of the  \gls{gls:goodnessoffit}\index{goodness-of-fit}\index{model fitting} [\href{https://en.wikipedia.org/wiki/Goodness_of_fit}{Wiki}]
than any single metric such as \gls{gls:rsquared} or root-mean-squared deviation \textcolor{index}{RMSE}\index{root mean squared error} [\href{https://en.wikipedia.org/wiki/Root-mean-square_deviation}{Wiki}]. Note that metrics such as R-squared have several drawbacks, and alternatives are discussed in chapter~\ref{chapterfuzzy}. 

Another important feature of the visualization is the slope of the regression line going through the point cloud. If the fit was perfect and all the points aligned on the main diagonal, the slope would be equal to one. In practice, it is always between zero and one. A low slope, say $0.5$, does not mean that the fit is bad. It means that the regression is smoothing out the spikes in the data, and acts as noise-removing filter. This is actually a good thing. It is easy to rescale the predicted values so that their variance is identical to that computed on the training set. This will restore the higher variations in the original data, while preserving the smoothness and the R-squared. Indeed, the R-squared, defined as the square of the correlation between the observed and predicted values, is invariant under scaling and/or translations.

\subsection{Supervised classification of an infinite dataset}\label{scidf}

In this problem, the visualization displays 9 dimensions: one for the time (in the video), one for the size of the dots, one for the category or group label, two for the physical location (state space), and four for the RGBA colors. The last frame of the video, showing raw data, is pictured in Figure~\ref{fig:orbit101}. 
The horizontal red line is the real axis (the X-axis).  The black dot on the left, on the red line, is the origin $(0,0)$ and the black dot on the right corresponds to $(0,1)$.  A version with bigger dots to actually perform the
\textcolor{index}{fuzzy classification}\index{fuzzy classification} of the whole state space is pictured in Figure~\ref{fig:orbit102}. This is the most insightful visualization in this case. The corresponding videos are found respectively \href{https://youtu.be/HT8e3WsRLZI}{here} and \href{https://youtu.be/rRQbxpZAQ78}{here}. Notice the huge overlap between the three groups (red, blue, yellow). Yet strong patterns emerge: the points are not randomly distributed.

The dataset comes from number theory. It is not synthetic in the sense that it represents a real phenomenon. Yet, if it was possible to use the whole, infinite dataset, the boundary of the clusters, the boundary of the holes, and the extend of the clusters overlap, would be known exactly. Theoretical considerations allow to solve some of the mysteries. But the problem investigated here is related to the Riemann Hypothesis [\href{https://en.wikipedia.org/wiki/Riemann_hypothesis}{Wiki}], one of the most famous unsolved
mathematical problems of all times. So machine learning techniques are still useful to gain more insights, and do a great job here. In math circles, the
methods used here are described as \textcolor{index}{experimental mathematics}\index{experimental math} [\href{https://en.wikipedia.org/wiki/Experimental_mathematics}{Wiki}]. 

\subsubsection{Machine learning perspective}\label{rmlp}

Before diving into the mathematical details, I explain the machine learning aspects of the problem. Color levels in each channel (red, green, blue)  are represented by  integer values between 0 and 255. The use of the \textcolor{index}{RGBA}\index{color model!RGBA} color model helps visualize cluster overlap. Regions with a high density of yellow points and low density of blue points appear somewhat greenish, but with more yellow than blue in the color. Conversely, regions with a low density of yellow points and high density of blue points also appear somewhat greenish, but with more blue than yellow in the color. High point density results in
brighter regions, while low density regions are almost transparent. 

Indeed, the supervised classification is automatically performed based on that mechanism alone, 
with the size of a dot being the main \gls{gls:hyperparam}\index{hyperparameter}. To find the label (red, yellow or blue) assigned to any location, one has to get its RGB color in Figure~\ref{fig:orbit102}. 
For instance, if $\text{RGB} = (155, 105, 100)$ then the probability that the point is red, blue or yellow is respectively 
$50/255, 100/255$ and $105/255$. See Exercise~\ref{exqwas} for this computation. Thus the name fuzzy classification sometimes used, but it is actually quite similar to Bayesian classification. 

This is achieved thanks to the A component in the RGBA color scheme: it stands for the opacity or transparency of the color, allowing for color blending via the \textcolor{index}{$\alpha$-compositing}\index{$\alpha$-compositing} algorithm [\href{https://en.wikipedia.org/wiki/Alpha_compositing}{Wiki}]. The letter A in RGBA is named after the $\alpha$ in question, and this component is sometimes referred to as the $\alpha$ channel. The technique also allows you to easily discriminate between areas of high density and low density, determined by the brightness, that is, the cumulated transparency level computed over overlapping dots. In short, it performs density estimation on its own!  

For similar applications, see the ``glowing plot" in  Figure~\ref{fig:pbcixzas}. Also see the fractal GPU-based classification technique 
described in chapter~\ref{chapterfastclassif}, and its related video \href{https://mltechniques.com/2022/03/31/very-deep-neural-networks-explained-in-40-seconds/}{here}.

Finally, the data points are located on a non-periodic orbit that over time covers a dense area. So, the data points in Figure~\ref{fig:orbit101} 
and~\ref{fig:orbit102} are sampled from that orbit (actually 3 orbits: a red, blue and yellow one), in such a way as to be relatively equally spaced on the orbit. 
It is possible to obtain perfect spacing using a method similar to the re-weighting technique described in section~\ref{centr1}  in my article on shape classification in chapter~\ref{chaptershapes}. Without careful sampling, the points are distributed as in Figure~\ref{fig:orbit102b}: the point density is higher where the curvature of the orbit is more pronounced, or when closer to the related hole. It becomes lower on average as the time increases, that is, as more and more video frames are displayed.

The orbits are displayed in Figure~\ref{fig:orbit102b}. The related video can be found \href{https://www.youtube.com/watch?v=aub5am1YjIs}{here}. The added value provided by the video is that it shows how the points slowly fill a dense area over time. Also note the analogy with the comet video, where the sun plays the role of an attractor, yet comets never cross the sun (at least in the simulation). Here, the hole attached to an orbit plays the role of the sun. 
But a big difference is that the orbits are non-periodic. 


\begin{Exercise}\label{exqwas}{\em Point classification}. A point in Figure~\ref{fig:orbit102} has the RGB components $(155,105,100)$. What is the 
chance that it belongs to the red, blue or yellow cluster? \vspace{1ex} \\
{\bf Solution} \vspace{1ex} \\
Let $p_\text{r},p_\text{b},p_\text{y}$ be the probabilities in question. They satisfy
$$M^T=
\left(
\begin{array}{rrr}
 255 & 0 & 0 \\
 0 & 0 & 255 \\
 255 & 255 & 0 
\end{array}
\right)^T \cdot
\left(
\begin{array}{r}
p_\text{r} \\
p_\text{b} \\
p_\text{y}
\end{array}
\right)=
\left(
\begin{array}{r}
155 \\
105 \\
100
\end{array}
\right).
$$
The solution $p_\text{r}=50/255,p_\text{b}=100/255,p_\text{y}=105/255$ is obtained by solving the above system. The first row in the matrix $M$ corresponds to red, the second one to blue, and the third one to yellow RGB vectors. \qed
\end{Exercise}

\subsubsection{Six challenging problems}

\noindent There are few other interesting machine learning problems worth investigating, raised  by Figure~\ref{fig:orbit102}. I summarize them in the list below.
\begin{itemize}
\item Problem 1: The set of yellow points seems to be bounded. Is that also true for the blue and red dots?
\item Problem 2: Assuming the set of yellow points is bounded, what is the shape of its boundary?
\item Problem 3: There are holes in the blue and yellow point distributions. Can we characterize these holes?
\item Problem 4: On the horizontal axis, some segments have no blue dots, some have no yellow dots. Can we characterize these segments?
\item Problem 5: If we continue adding points indefinitely, will the holes eventually shrink to empty sets?
\item Problem 6: If we continue adding points indefinitely, will the point distributions cover dense areas?
\end{itemize}

\noindent Keep in mind that the set of points is infinite, but only a finite number of points is shown in the picture. Before going into the mathematical details, I
 will mention this: if you solve Problem 4, you will probably win the Fields Medal in mathematics [\href{https://en.wikipedia.org/wiki/Fields_Medal}{Wiki}] (the equivalent of the Nobel Prize of mathematics), and a \$1 million award for solving one of the seven Millennium Problems [\href{https://www.claymath.org/millennium-problems/riemann-hypothesis}{Wiki}].

\noindent Partial solutions to the six problems are discussed in section~\ref{psrr}.


\subsubsection{Mathematical background: the Riemann Hypothesis}

What I call data points are values of the Dirichlet eta function $\eta(\sigma+it)$ [\href{https://en.wikipedia.org/wiki/Dirichlet_eta_function}{Wiki}] computed at sampled valued of $t>0$,  for $m=3$ values of $\sigma$, namely
$\sigma_0=0.50$ corresponding to the red dots, $\sigma_1=0.75$ corresponding to the blue dots, and $\sigma_2=1.25$ corresponding to the yellow dots.
The notation $\sigma+it$ for the complex argument is well established in number theory, and I decided to keep it. There are mathematicians interested in the
problem who will read this article, and using a different notation would make my presentation awkward and possibly confusing to them.

\noindent The $\eta$ function returns a complex value  defined by
\begin{equation}\label{riemss}
\begin{aligned}
 \Re[\eta(\sigma+it)] & =\sum_{k=1}^\infty (-1)^{k+1}\cdot \frac{\cos(t\log k)}{k^\sigma}, \\ %xxxxx\label{riemss} \\
 \Im[\eta(\sigma+it)] & =\sum_{k=1}^\infty (-1)^{k+1}\cdot \frac{\sin(t\log k)}{k^\sigma}, %xxxxx\label{riemss2}
\end{aligned}
\end{equation}
where $\Re,\Im$ represent respectively the real and imaginary parts. For our purpose, no knowledge of complex number theory is required. The real and 
imaginary parts are just the two components of a 2D vector, with the real part on the horizontal axis (X-axis), and the imaginary part on the vertical axis (Y-axis).
The notations used in the Python code in sections~\ref{conv222} and~\ref{class222}, for the real and imaginary part of the $\eta$ function, are
respectively \texttt{etax[n]} and \texttt{etay[n]}. Here $n=0$  corresponds to $\sigma=\sigma_0$, $n=1$ to $\sigma=\sigma_1$ and $n=2$ to $\sigma=\sigma_2$. The time argument $t$ is a global variable in the Python code, incremented at each iteration, starting with $t=0$.

Figures that show the orbit are based on fixed increments $\Delta t=0.04$. Figures showing the points but not the orbit use variable
increments. This is to correct for the fact that fixed increments do not produce points evenly spaced on the orbit. In that case, a separate timer is
used for each value of $\sigma$. In the Python code in section~\ref{class222}, it corresponds 
to \texttt{t[0]}, \texttt{t[1]}, and \texttt{t[2]} respectively for the red, blue and yellow points. The main loop is over the three colors (that is, the three values of $\sigma$), and the
inner loop is over the time. The $\eta$ function is computed by the Python function \texttt{G}, returning both the real and imaginary parts. It uses
the first $\num{10000}$ terms of the sums in formula~(\ref{riemss}). 
These are slow converging series. I discuss convergence acceleration techniques, chaotic convergence, and numerical accuracy in section~\ref{psr55}.
 
\subsubsection{Partial solutions to the six challenging problems}\label{psrr}

In machine learning, typically no mathematical proof is available to show that a model is exact. For instance, statistical models show that smoking increases the chances of getting lung cancer. The arguments are compelling. But there is no formal proof to this. Typically, it is difficult to establish causality. To the contrary, in mathematics, usually formal proofs are
available, and they can be used to test and benchmark statistical models. One would think that there is a precise, mathematical answer to the six problems raised in section~\ref{rmlp}. Unfortunately, this is not the case here. However, some partial answers are available. 
First, let me define what I mean by ``hole".

%\setlength{\leftskip}{0.5cm}
%\setlength{\rightskip}{0.5cm}

{\em \noindent{\bf Definition}.  The T-hole $\Omega_T
=\Omega_T(\sigma)$ is the largest circle centered at some location
$t_T$ on the real (horizontal) axis, for which $\eta(\sigma +it)\notin \Omega_T$ if $0< t\leq T$. The hole $\Omega$ is the limit of $\Omega_T$, as $T\rightarrow \infty$. 
}

%\setlength{\rightskip}{0cm}
%\setlength{\leftskip}{0cm}

\noindent In short, $\eta$'s orbit, given $\sigma$, never enters the hole if $0<t<T$. Here $t_T=t_T(\sigma)$ is a function of $\sigma$, with $0\leq t_T < 2$.
Another important concept (see problem 4) is the largest segment on the real axis, that is never crossed or hit by the orbit. 

Now, let's focus on answering problems 1 -- 6. First, the series in formula~(\ref{riemss})  converge absolutely [\href{https://en.wikipedia.org/wiki/Absolute_convergence}{Wiki}]. Thus the yellow orbit is bounded: this is a trivial fact.  The fact that the blue and red orbits are unbounded was proved long ago. See for instance the classic reference ``The Theory
 of the Riemann-Zeta Function"  \cite{tdr1987}. This provides a full answer to problem 1.

Then, it is also known that the orbits cover dense areas. And as you keep adding more and more points (thus increasing $T$), the holes eventually shrink to a set of Lebesgue measure zero: this is true if $0.5 < \sigma < 1$. It is a consequence of the universality property of the Riemann zeta function [\href{https://en.wikipedia.org/wiki/Zeta_function_universality}{Wiki}]. This provides a partial answer to problems 5 and 6.
If $\sigma$ is fixed and $t$ is bounded, I presume that the hole associated to the blue and yellow orbits always exist. For the blue orbit,
this statement, especially if applied to all $\sigma$ in $]0.5, 1[$,  is stronger than the Riemann Hypothesis (RH), and thus unproven to this day. Some argue that RH may be unprovable, but that's another story. 

The red orbit ($\sigma=0.5$) has no hole. As $\sigma$ is decreased from $1.25$ to $0.5$, the hole shrinks and move to the left on the real axis, towards the origin. It eventually shrinks 
to an empty set and becomes and attractor point when $\sigma=0.5$. This is confirmed by the fact that the Riemann zeta function has infinitely many non-trivial zeros if $\sigma=0.5$. And the roots of the Dirichlet eta and Riemann zeta functions are identical when $0.5 < \sigma < 1$. The hole of the blue orbit seems to
encompass the origin, suggesting that if $\sigma=0.75$, the Riemann zeta function has no zero. To this day, this conjecture, much weaker than RH, is unproven.

The concepts and repulsion or \textcolor{index}{attraction basin}\index{attraction basin} [\href{https://en.wikipedia.org/wiki/Attractor}{Wiki}] is fundamental in dynamical systems.
A hole is a repulsion basin, while the origin, for the red orbit, is an attractor point. One of my upcoming books will discuss these topics in detail, for a wide
range of dynamical systems.

\subsection{Algorithms with chaotic convergence}\label{psr55}

We are all familiar with pictures, even animated gifs, showing the gradient descent or some other optimization algorithm in action, converging to an optimum depending on initial conditions. In all cases, the surface (be it 2D or 3D) is smooth, though there are numerous examples with several local maxima and minima. See example, with Mathematica code, \href{https://commons.wikimedia.org/wiki/File:Gradient_descent.gif}{here}. The Wikipedia entry for the gradient descent
(\href{https://en.wikipedia.org/wiki/Gradient_descent}{here}) also features a nice 3D video. 

Here I visually illustrate the convergence mechanism when the function much less smooth, in a general context. It is not optimization-related, unless you consider 
accelerating the convergence to be an optimization problem. Each new frame in the video shows the progress towards the final solution, given initial conditions. The convergence path, for this 2D problem, for six different initial conditions, is shown in Figure~\ref{fig:orbit103}.
It's hard to tell where it starts and where it ends. This is straightforward if you look at the video, \href{https://www.youtube.com/watch?v=XI5MhyNc7us}{here}.

The algorithm pictured in Figure~\ref{fig:orbit103} computes the successive values of the $\eta$ function in the complex plane, using
formula~(\ref{riemss}). Each iteration adds one new term to the summation, and generates a new frame. It is possible to
significantly improve the speed of convergence, using \textcolor{index}{convergence acceleration}\index{convergence acceleration} techniques 
[\href{https://en.wikipedia.org/wiki/Series_acceleration}{Wiki}] such as Euler's transform [\href{https://mathworld.wolfram.com/EulerTransform.html}{Wiki}], described page 65 in my book~\cite{vgsimulnew}. I explain in my book (same page)
 why the convergence is so chaotic in this case. The same convergence acceleration techniques apply to gradient descent
 and other similar iterative algorithms, when successive iterations generate oscillating values. For the $\eta$ function, 
 Borwein’s method [\href{https://en.wikipedia.org/wiki/Borwein\%27s_algorithm}{Wiki}] may be the best approach.

I tested the \gls{gls:numericalstability} of the computations by introducing stochastic noise in formula~(\ref{riemss}). I describe the methodology in
 the section ``Perturbed version of the Riemann Hypothesis'', page 20 in my book \cite{vgsimulnew}. The holes described in section~\ref{psrr} in this paper are very sensitive to minuscule errors in the computations, and are non-existent when very small changes are introduced. This confirms that there is something really unique to the Riemann zeta function. For solutions to optimize  highly chaotic functions (compute a global maximum or minimum), see my book \cite{vgsimulnew} pages 17--18. I used a diverging
 \textcolor{index}{fixed-point algorithm}\index{fixed-point algorithm} [\href{https://en.wikipedia.org/wiki/Fixed-point_iteration}{Wiki}] that emits a signal when approaching a global optimum. 
The technique will be described in detail in an upcoming article. A quick overview of the methodology is available \href{https://www.datasciencecentral.com/a-new-machine-learning-optimization-technique-part-i/}{here}.

Finally, the visualization (Figure~\ref{fig:orbit103} or the corresponding video \href{https://www.youtube.com/watch?v=XI5MhyNc7us}{here}) uses a very large number of RGB colors. To produce the visual effect, I used sine functions to generate the colors. See section~\ref{conv222}, and page 85 in my book~\cite{vgsimulnew} for more details.
Palette optimization (see \href{https://mathoverflow.net/questions/415618/lattice-like-structure-with-maximum-spacing-between-vertices}{here}) will be the subject of an upcoming article.


\renewcommand{\arraystretch}{1.0} %%%
\renewcommand{\arraystretch}{1.4} %%%

\section{Python code}\label{pythonviz}

The Python code, videos, and animated gifs are available on my GitHub repository, \href{https://github.com/VincentGranville/Visualizations}{here}.
The videos are also on YouTube, \href{https://www.youtube.com/c/VincentGranvilleVideos}{here}. For convenience, the Python code is also included 
in this article. Top variables include \texttt{ShowOrbit} (set to true if you want to display the orbit, not just the points), \texttt{dot} (the size of the dots), \texttt{r} (when iterating over time, it outputs a video frame once every $r$ iterations), \texttt{width} and \texttt{height} (the dimension of the image). The final image is eventually reduced by half due to the \textcolor{index}{anti-aliasing}\index{anti-aliasing} procedure used to depixelate the curves. This is performed
within \texttt{img.resize} in the code, using the \texttt{Image.LANCZOS} parameter [\href{https://en.wikipedia.org/wiki/Lanczos_resampling}{Wiki}]. 

Ellipses and lines are produced using the Pillow library and its \texttt{ImageDraw} functions. Animated gifs are produced either 
with 
\texttt{images[0].save} 
(resulting in massive files), or with \texttt{videoClip.write\_gif}, using the Moviepy library with the \textcolor{index}{ffmpeg}\index{video compression!FFmpeg} parameter [\href{https://en.wikipedia.org/wiki/FFmpeg}{Wiki}] for compression. When working with a large number of colors, ffmpeg causes considerable quality loss, not in the rendering of the shapes, but in the color palette.  I suggest to use libraries other than Pillow to produce animated gifs, for instance openCV. Eventually, I converted some of the MP4 videos to gifs using the online tool \href{https://ezgif.com/video-to-gif}{ezgif}, also based on ffmpeg. 

Reducing the size of the image and the number of frames per second (FPS)  will optimize the code and produce much smaller gifs. The biggest improvement, in terms of speed, is replacing all numpy calls (\texttt{np.log}, \texttt{np.cos} and so on) by math calls 
(\texttt{math.log}, \texttt{math.cos} and so on). If you use numpy for image production rather than Pillow, the opposite may be true (I did not test). Finally, the opacity level in the RGBA color model should be set to 127. Currently, it is set to 80; see the fourth parameter for instance in 
\texttt{colp.append((0,0,255,80))}.


\subsection{Paths simulation}
 
On GitHub: \href{https://github.com/VincentGranville/Visualizations/blob/main/Source-Code/image2.py}{\texttt{image2.py}}. Produces the comet video. Description in section~\ref{ellipser}.  

\begin{lstlisting}
from PIL import Image, ImageDraw           # ImageDraw to draw ellipses etc.
import moviepy.video.io.ImageSequenceClip  # to produce mp4 video
from moviepy.editor import VideoFileClip                # to convert mp4 to gif
import numpy as np
import math
import random
random.seed(100)

#--- Global variables ---

m=300               # number of comets
nframe=1500         # number of frames in video
ShowOrbit=True   # do not show orbit (default)

count=0             # frame counter 

count1=0
count2=0
count3=0
count4=0

width = 1600
height = 1200

a=[]    
b=[]
gx=[]       # focus of ellipse (x coord.)
gy=[]       # focus of ellipse (y coord.)
theta=[]  # rotation angle of ellpise (the orbit)
v=[]        # spped of comet
tau=[]      # position of comet on the orbit path, at t = 0
col=[]      # RGB color of the comet
size=[]     # size of the comet
flist=[]  # filenames of the images representing each video frame

a=list(map(float,a))
b=list(map(float,b))
gx=list(map(float,gx))
gy=list(map(float,gy))
theta=list(map(float,theta))
v=list(map(float,v))
tau=list(map(float,tau))
flist=list(map(str,flist))

#--- Initializing comet parameters ---

for n in range (m):
  a.append(width*(0.1+0.3*random.random()))
  b.append((0.5+1.5*random.random())*a[n])
  theta.append(2*math.pi*random.random())
  tau.append(2*math.pi*random.random())
  if a[n]>b[n]:
    gyy=0.0
    gxx=math.sqrt(a[n]*a[n]-b[n]*b[n]) # should use -gxx 50% of the time
  else:
    gyy=math.sqrt(b[n]*b[n]-a[n]*a[n]) # should use -gyy 50% of the time
    gxx=0.0
  gx.append(gxx*np.cos(theta[n])-gyy*np.sin(theta[n])) 
  gy.append(gxx*np.sin(theta[n])+gyy*np.cos(theta[n])) 
  if random.random() < 0.5:
    v.append(0.04*random.random())
  else:
    v.append(-0.04*random.random())
  if abs(a[n]*a[n]-b[n]*b[n])> 0.15*width*width: 
    if abs(v[n]) > 0.03:  
    # fast comet with high eccentricity 
      red=255
      green=0
      blue=255
      count1=count1+1
    else:                 
    # slow comet with high eccentricity
      red=0
      green=255
      blue=0
      count2=count2+1
  else:
    if abs(v[n]) > 0.03:  
    # fast comet with low eccebtricity
      red=255
      green=0
      blue=0
      count3=count3+1
    else:
    # slow comet with low eccentricity
      red=255
      green=255
      blue=255
      count4=count4+1
  col.append((red,green,blue))
  if ShowOrbit:
     size.append(1)
  else: 
    if min(a[n],b[n]) > 0.3*width:  # orbit with large radius 
      size.append(8)
    else:
      size.append(4)

sunx=int(width/2)        # position of the sun (x)
suny=int(height/2) # position of the sun (y)
if ShowOrbit:
  img  = Image.new( mode = "RGB", size = (width, height), color = (0, 0, 0) )
  pix = img.load()
  draw = ImageDraw.Draw(img)
  draw.ellipse((sunx-16, suny-16, sunx+16, suny+16), fill=(255,180,0))

#--- Main Loop ---

for t in range (0,nframe,1): # loop over time, each t corresponds to a ideo frame
  print("Building frame:",t)
  if not ShowOrbit:
    img  = Image.new( mode = "RGB", size = (width, height), color = (0, 0, 0) )
    pix = img.load()
    draw = ImageDraw.Draw(img)
    draw.ellipse((sunx-16, suny-16, sunx+16, suny+16), fill=(255,180,0))
  for n in range (m):  # loop over asteroid
    x0=a[n]*np.cos(v[n]*t+tau[n]) 
    y0=b[n]*np.sin(v[n]*t+tau[n])
    x=x0*np.cos(theta[n])-y0*np.sin(theta[n])
    y=x0*np.sin(theta[n])+y0*np.cos(theta[n])
    x=int(x+width/2 -gx[n])
    y=int(y+height/2-gy[n])
    if x >= 0 and x < width and y >=0 and y < height:
      draw.ellipse((x-size[n], y-size[n], x+size[n], y+size[n]), fill=col[n]) 
  count=count+1
  fname='imgpy'+str(count)+'.png'

  # anti-aliasing mechanism
  img2 = img.resize((width // 2, height // 2), Image.LANCZOS) # anti-aliasing
  # output curent frame to a png file 
  img2.save(fname,optimize=True,quality=30)
  flist.append(fname)

clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(flist, fps=20)
# output video file
clip.write_videofile('videopy.mp4')
# output gif image [converting mp4 to gif with ffmpeg compression]
videoClip = VideoFileClip("videopy.mp4")
videoClip.write_gif("videopy.gif",program='ffmpeg') #,fps=2)

print("count 1-4:",count1,count2,count3,count4)
\end{lstlisting}

\subsection{Visual convergence analysis in 2D}\label{conv222}

On GitHub: \href{https://github.com/VincentGranville/Visualizations/blob/main/Source-Code/image2R.py}{\texttt{image2R.py}}. Produces the successive approximations to the $\eta$ function. Description in section~\ref{psr55}.  

\begin{lstlisting}
from PIL import Image, ImageDraw           # ImageDraw to draw ellipses etc.
import moviepy.video.io.ImageSequenceClip  # to produce mp4 video
from moviepy.editor import VideoFileClip                # to convert mp4 to gif
import numpy as np
import math
import random
random.seed(100)

#--- Global variables ---

m=6               # number of curves
nframe=4000       # number of images 
count=0           # frame counter 
start=2000        # must be smaller than nframe
r=20              # one out of every r image is included in the video

width = 3200 
height =2400 

images=[]
etax=[]
etay=[]
sigma=[]
t=[]
x0=[]
y0=[]
flist=[]  # filenames of the images representing each video frame

etax=list(map(float,etax))
etay=list(map(float,etay))
sigma=list(map(float,sigma))
t=list(map(float,t))
x0=list(map(float,x0))
y0=list(map(float,y0))
flist=list(map(str,flist))

#--- Initializing comet parameters ---

for n in range (0,m):
  etax.append(1.0)
  etay.append(0.0)
  t.append(5555555+n/10)
  sigma.append(0.75)
sign=1

minx= 9999.0
miny= 9999.0
maxx=-9999.0
maxy=-9999.0

for n in range (0,m):
  sign=1
  sumx=1.0
  sumy=0.0
  for k in range (2,nframe,1):
    sign=-sign
    sumx=sumx+sign*np.cos(t[n]*np.log(k))/pow(k,sigma[n])
    sumy=sumy+sign*np.sin(t[n]*np.log(k))/pow(k,sigma[n])
    if k >= start:
      if sumx < minx:
        minx=sumx
      if sumy < miny:
        miny=sumy
      if sumx > maxx:
        maxx=sumx
      if sumy > maxy:
        maxy=sumy
sign=1
rangex=maxx-minx
rangey=maxy-miny

img  = Image.new( mode = "RGB", size = (width, height), color = (255, 255, 255) )
pix = img.load()
draw = ImageDraw.Draw(img)

red=255
green=255
blue=255
col=(red,green,blue)
count=0

#--- Main Loop ---

for k in range (2,nframe,1): # loop over time, each t corresponds to a ideo frame
  if k%10 == 0:
    print("Building frame:",k)
  sign=-sign
  for n in range (0,m):  # loop over curves
    x0.insert(n,int(width*(etax[n]-minx)/rangex))
    y0.insert(n,int(height*(etay[n]-miny)/rangey))
    etax[n]=etax[n]+sign*np.cos(t[n]*np.log(k))/pow(k,sigma[n])
    etay[n]=etay[n]+sign*np.sin(t[n]*np.log(k))/pow(k,sigma[n])
    x=int(width*(etax[n]-minx)/rangex)
    y=int(height*(etay[n]-miny)/rangey)
    shape = [(x0[n], y0[n]), (x, y)]
    red  = int(255*0.9*abs(np.sin((n+1)*0.00100*k)))
    green= int(255*0.6*abs(np.sin((n+2)*0.00075*k)))
    blue = int(255*abs(np.sin((n+3)*0.00150*k)))

    if k>=start:
      # draw line from (x0[n],y0[n]) to (x_new,y_new)
      draw.line(shape, fill =(red,green,blue), width = 1)

  if k>=start and k%r==0:
    fname='imgpy'+str(count)+'.png'
    count=count+1
    # anti-aliasing mechanism
    img2 = img.resize((width // 2, height // 2), Image.LANCZOS) # anti-aliasing
    # output curent frame to a png file
    img2.save(fname)     # write png image on disk
    flist.append(fname)  # add its filename (fname) to flist
    images.append(img2)  # to produce Gif image

# output video file
clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(flist, fps=20) 
clip.write_videofile('riemann.mp4')
\end{lstlisting}

\subsection{Supervised classification}\label{class222}

GitHub version: \href{https://github.com/VincentGranville/Visualizations/blob/main/Source-Code/image3R_orbit.py}{\texttt{image3R\_orbit.py}}. Produces the orbits of the $\eta$ function. Description in section~\ref{scidf}.  


\begin{lstlisting}
from PIL import Image, ImageDraw           # ImageDraw to draw ellipses etc.
import moviepy.video.io.ImageSequenceClip  # to produce mp4 video
from moviepy.editor import VideoFileClip                # to convert mp4 to gif
import numpy as np
import math
import random
random.seed(100)

#--- Global variables ---

m=3               # number of orbits (one for each value of sigma) 
nframe=20000      # number of images created in memory
ShowOrbit=True 
ShowDots=False
count=0           # frame counter 
r=50              # one out of every r image is included in the video
dot=2             # size of a point in the picture
step=0.04         # time increment in orbit

width = 3200      # width of the image
height =2400      # length of the image

images=[]
etax=[]    # real part of Dirichlet eta function
etay=[]    # real part of Dirichlet eta function
sigma=[]# imaginary part of argument of Dirchlet eta
x0=[]      # value of etax on last video frame
y0=[]      # value of etay on last video frame
#col=[]    # RGB color of the orbit
colp=[]    # RGP points on the orbit
t=[]       # real part of argument of Dirchlet eta (that is, time in orbit)
flist=[]# filenames of the images representing each video frame

etax=list(map(float,etax))
etay=list(map(float,etay))
sigma=list(map(float,sigma))
x0=list(map(float,x0))
y0=list(map(float,y0))
t=list(map(float,t))
flist=list(map(str,flist))

#--- Eta function ---

def G(tau,sig,nterms):
  sign=1
  fetax=0
  fetay=0
  for j in range(1,nterms):
    fetax=fetax+sign*math.cos(tau*math.log(j))/pow(j,sig)
    fetay=fetay+sign*math.sin(tau*math.log(j))/pow(j,sig)
    sign=-sign
  return [fetax,fetay]

#--- Initializing comet parameters ---

for n in range (0,m):
  etax.append(1.0)
  etay.append(0.0)
  x0.append(1.0)
  y0.append(0.0)
  t.append(0.0)       # start with t=0.0
sigma.append(0.50)
sigma.append(0.75) 
sigma.append(1.25) 
colp.append((255,0,0,80))
colp.append((0,0,255,80))
colp.append((255,180,0,80))

if ShowOrbit:
  minx=-2 
  maxx=3
else:
  minx=-1
  maxx=2 
rangex=maxx-minx
rangey=0.75*rangex
miny=-rangey/2
maxy=rangey/2
rangey=maxy-miny

img  = Image.new( mode = "RGB", size = (width, height), color = (255, 255, 255) )
# pix = img.load()   # pix[x,y]=col[n] to modify the RGB color of a pixel
draw = ImageDraw.Draw(img,"RGBA")

gx=width*(0.0-minx)/rangex
gy=height*(0.0-miny)/rangey
hx=width*(1.0-minx)/rangex
hy=height*(0.0-miny)/rangey
draw.ellipse((gx-8, gy-8, gx+8, gy+8), fill=(0,0,0,255)) 
draw.ellipse((hx-8, hy-8, hx+8, hy+8), fill=(0,0,0,255)) 
draw.rectangle((0,0,width-1,height-1), outline ="black",width=1)
draw.line((0,gy,width-1,hy), fill ="red", width = 1)

count=0

#--- Main Loop ---

for k in range (2,nframe,1): # loop over time, each t corresponds to an image
  if k %10 == 0:
    string="Building frame:" + str(k) + "> "
    for n in range (0,m):
      string=string+ " | " + str(t[n])
    print(string)
  for n in range (0,m):  # loop over the m orbits
    if ShowOrbit:
      # save old value of etax[n], etay[n]
      x0.insert(n,width*(etax[n]-minx)/rangex)  
      y0.insert(n,height*(etay[n]-miny)/rangey)
    (etax[n],etay[n])=G(t[n],sigma[n],10000) # 500 -> tau
    x= width*(etax[n]-minx)/rangex
    y=height*(etay[n]-miny)/rangey
    if ShowOrbit:
      if k>2:
        # draw line from (x0[n],y0[n]) to (x,y)
        draw.line((int(x0[n]),int(y0[n]),int(x),int(y)), fill =colp[n], width = 0)
        if ShowDots:
          draw.ellipse((x-dot, y-dot, x+dot, y+dot), fill =colp[n]) 
      t[n]=t[n]+step
    else:
      draw.ellipse((x-dot, y-dot, x+dot, y+dot), fill =colp[n]) 
      t[n]=t[n]+200*math.exp(3*sigma[n])/(1+t[n])  # 0.02  
  if k%r==0:    # this image gets included as a frame in the video
    draw.ellipse((gx-8, gy-8, gx+8, gy+8), fill=(0,0,0,255)) 
    draw.ellipse((hx-8, hy-8, hx+8, hy+8), fill=(0,0,0,255)) 
    fname='imgpy'+str(count)+'.png'
    count=count+1
    # anti-aliasing mechanism
    img2 = img.resize((width // 2, height // 2), Image.LANCZOS) # anti-aliasing
    # output curent frame to a png file
    img2.save(fname)     # write png image on disk
    flist.append(fname)  # add its filename (fname) to flist
    images.append(img2)  # to produce Gif image

# output video file
clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(flist, fps=20) 
clip.write_videofile('riemann.mp4')

# output gif file - commented out because it is way too large
# images[0].save('riemann.gif',save_all=True, append_images=images[1:],loop=0)
\end{lstlisting}

\section{Visualizations}

The videos and animated gifs are available on my GitHub repository, \href{https://github.com/VincentGranville/Visualizations}{here}.
The videos are also on YouTube, \href{https://www.youtube.com/c/VincentGranvilleVideos}{here}. Below are selected frames from these videos: those that
are referenced in this article.


\begin{figure}[H]
\centering
\includegraphics[width=0.73\textwidth]{fuzzybig.png}
\caption{Scatterplot observations vs. predicted values, with prediction intervals (in any dimension)}
\label{fig:orbit60}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[width=0.55\textwidth]{cometOrbit.jpg}
\caption{Comets orbiting the sun: simulation}
\label{fig:orbit80}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.55\textwidth]{cometNoOrbit.jpg}
\caption{Comets orbiting the sun: snapshot in time}
\label{fig:orbit81}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.55\textwidth]{imgpyRiemannFinalOrbits-v2-small2.jpg}
\caption{Three orbits of $\eta(\sigma + it)$: $\sigma=0.5$ (red), $0.75$ (blue) and $1.25$ (yellow)}
\label{fig:orbit100}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.55\textwidth]{imgpyRiemannFinal398-v2-small2.jpg}
\caption{Sample orbit points of $\eta(\sigma + it)$: $\sigma=0.5$ (red), $0.75$ (blue) and $1.25$ (yellow)}
\label{fig:orbit101}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.55\textwidth]{imgpyRiemannFinalConfetti398-small2.jpg}
\caption{Sample orbit points of $\eta(\sigma + it)$: $\sigma=0.5$ (red), $0.75$ (blue) and $1.25$ (yellow)}
\label{fig:orbit102}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.55\textwidth]{imgpy98RiemannOrbitRaw-small2.jpg}
\caption{Raw orbit points of $\eta(\sigma + it)$: $\sigma=0.5$ (red), $0.75$ (blue) and $1.25$ (yellow)}
\label{fig:orbit102b}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[width=0.55\textwidth]{riemannseriesapprox-small2.jpg}
\caption{Convergence of partial sums of $\eta(z)$, for six $z=\sigma+it$ in the complex plane}
\label{fig:orbit103}
\end{figure}

%----------------------------------------------------------------------------------------------------------------
\Chapter{Fast Classification and Clustering via Image Convolution Filters}{}\label{chapterfastclassif}

Here I generate \gls{gls:syntheticdata} using a superimposition of stochastic processes, comparing it to Bayesian generative mixture models (Gaussian mixtures). I explain the benefits and differences. The actual classification and clustering algorithms are model-free, and performed in GPU as image filters, after transforming the raw data into an image. I then discuss the generalization to 3D or 4D, and to higher dimensions with sparse \glspl{gls:tensor}. The technique is particularly suitable when the number of observations is large, and the overlap between clusters is substantial. 

It can be done using few iterations and a large filter window, comparable to a \gls{gls:neuralnet}, with pixels in the local window being the nodes, and their distance to the local center being the weight function. Or you can implement the method with a large number of iterations -- the equivalent of hundreds of layers in a deep neural network -- and a tiny window. This latter case corresponds to a 
\textcolor{index}{sparse network}\index{neural network!sparse} with zero or one connection per node. It is used to implement fractal classification, where point labeling changes at each iteration, around highly non-linear cluster boundaries. This is equivalent to putting a prior on class assignment probabilities in a Bayesian framework. Yet, classification is performed without underlying model. Finally, the clustering (unsupervised) part of the algorithm relies on the same filtering techniques, combined with a color equalizer. The latter can be used to perform hierarchical clustering. 

The Python code, included in this document, is also on my GitHub repository. 
A data animation illustrates how simple the methodology is: each frame in the video represents one iteration, that is, a single application of the filter to all the data points. Indeed, the classifier can be used as a black box system. It follows the modern trend of interpretable machine learning, also called \gls{gls:explainableai}. The video shows how the algorithm converges to an optimum, producing a classification of the entire observation space. Classifying a new point is then immediate: read its color. The whole system is time-efficient. It does not require the computation of all training set point intra-distances. However it is memory-intensive. Large filters can be slow, though they require very few iterations. I discuss a simple technique to make them a lot faster. 


\hypersetup{linkcolor=red}


\section{Introduction}\label{vizintro}

I explain, with Python code and numerous illustrations, how to turn traditional tabular data into images, to perform both clustering and supervised classification using simple image filtering techniques. I also explain how to generalize
the methodology to higher dimensions, using tensors rather than images. In the end, image bitmaps are 2D arrays or matrices, that is, 2D tensors. 
By classifying the entire space (in low dimensions), the resulting classification rule is very fast. I also discuss the convergence of the algorithm, and how to further improve its speed.

This short article covers many topics and can be used as a first introduction to synthetic data generation, mixture models, boundary effects, explainable AI, fractal classification, stochastic convergence, GPU machine learning, deep neural networks, and model-free Bayesian classification. I use very little math, making it 
accessible to the layman, and certainly, to non-mathematicians.  Introducing an original, intuitive approach to general classification problems, I explain in simple English how it 
relates to deep and very deep neural networks. In the process, I make connections to image segmentation, histogram equalization, hierarchical clustering, 
convolution filters, and stochastic processes. I also compare standard neural networks with very deep but sparse ones, in terms of speed and performance. 
The fractal classifier -- an example of very deep neural network -- is illustrated with a Python-generated video. It is useful when dealing with massively overlapping clusters and a large number
of observations. \Glspl{gls:hyperparam} allow you to fine tune the level of cluster overlap in the synthetic data, and the shape of the clusters.


% https://project-archive.inf.ed.ac.uk/msc/20172393/msc_proj.pdf
% https://towardsdatascience.com/how-to-use-a-clustering-technique-for-synthetic-data-generation-7c84b6b678ea

\section{Generating the synthetic data}

The data used in my examples is generated using a technique that bears some resemblance to 
\textcolor{index}{Gaussian mixture models}\index{Gaussian mixture} (GMM)
 in a Bayesian framework [\href{https://en.wikipedia.org/wiki/Mixture_model}{Wiki}]. Instead of mixtures, I used superimposed stochastic processes, also called interlacings. The differences with mixtures are subtle and can be detected with statistical tests, but unimportant and not visible to the naked eye. And
rather than Gaussian distributions, I use arbitrary distributions. The mathematical model is that of 
stochastically perturbed lattice processes, also known as Poisson-binomial point processes. 

I wrote a comprehensive 100-page book on the topic \cite{vgsimulnew}, with numerous references, covering all aspects from simulation, \gls{gls:graphmodel} properties, to theory. These processes have become popular recently, with applications to sensor data, cell networks, crystallography and chemistry. They are flexible and very easy to simulate. The three main parameters are the scale or diffusion factor $s$, the intensity $\lambda$ (linked to the expected number of points per unit area), and the local distribution $F$ which may be Gaussian or not. In this article, I don't discuss the theory.  I only introduce the basic material necessary to generate the synthetic data. The reader is referred to my book \cite{vgsimulnew} for details.

First, I use $\lambda=1$ in the Python code included in this document. Then, if $s=0$, the points are all located on a lattice: there is no randomness anymore. To the contrary, if $s$ is large (say $s>5$) then the points are nearly uniformly distributed, as in a Poisson point process. In that case, the simulated data has no clustering structure. The \gls{gls:syntheticdata} here uses either $s=0.05$ resulting in well separated clusters,
or $s=0.15$ resulting in significant cluster overlap. 

For examples with five clusters, see left plot 
in Figure~\ref{fc015c} (with $s=0.15$) and Figure~\ref{fc005b} (with $s=0.05)$. For four clusters, see left plot in Figure~\ref{fc015a} (with $s=0.15$) and Figure~\ref{fc005a}  
(with $s=0.05$).  The number of clusters, denoted as $m$, is the number of components (stochastic processes) used in the superimposition. 



Thus, the cluster structure is generated by interlacing multiple stretched and shifted perturbed lattices. The stretching factor and intensity may be different depending on the direction. A special case with $s=0$ is the deterministic hexagonal lattice pictured in Figure~\ref{fcsss1}. Numerous examples with various degrees of randomness are pictured in my book \cite{vgsimulnew}. If $s>0$, the points of a single process are independently distributed with multivariate distribution $F$, around each lattice vertex. If $s=0$, the data points are the lattice vertices, as in 
Figure~\ref{fcsss1}.

\subsection{Simulations with logistic distribution}\label{fcsim}

All simulations are in two dimensions. There is no particular reason to choose a logistic distribution for $F$, other than that it is the easiest to sample from. A Gaussian distribution would work too. The distribution $F$ has little impact on the final results. Indeed, it is not easy 
and sometimes impossible to 
reverse-engineer the system to identify the underlying distribution $F$. See my book~\cite{vgsimulnew} page 33 for a discussion on this topic.
 Finally, the algorithm is as follows:

\noindent{\bf Data generation: algorithm}

\noindent The data generated is 2D, but it is easy to generalize to any dimension. For each lattice vertex $(h,k)$, where $h,k$ are integers (positive or negative), and for each stochastic lattice process $M_i$ with $0\leq i < m$, generate the bivariate observation $(x_{ih},y_{ik})$ as follows:

\begin{align}
x_{ih} & =\mu_i + \frac{h}{\lambda_i} +s \cdot \log \Big(\frac{U_{ih}}{1-U_{ih}}\Big) \label{simm1}\\
y_{ik} & =\mu'_i+ \frac{k}{\lambda'_i} +s \cdot \log\Big(\frac{U_{ik}}{1-U_{ik}}\Big) \label{simm2}
\end{align}



\noindent Formulas (\ref{simm1}) and (\ref{simm2}) are from my book~\cite{vgsimulnew}, page 11. The $U_{ih},U_{ik}$ are independent uniform deviates on $[0,1]$. In practice, we generate points in a finite rectangular window, and we only keep those inside a sub-window, to avoid the boundary effects described in my book. This is implemented in the Python code in section~\ref{pythonviz3}, with $-25\leq h,k\leq 25$. In the code, I use the arrays \texttt{stretchX}, \texttt{stretchY} to store the coefficients
$1/\lambda_i,1/\lambda'_i$, and \texttt{shiftX}, \texttt{shiftY} to store the shift vectors $(\mu_i,\mu'_i)$. The shift vectors are the centers of the clusters. In the rectangular window, by design, the 
number of
observed points from the process $M_i$ (which plays the role of a mixture component in a mixture model) is proportional to
$\lambda_i \times \lambda_i'$ in two dimensions. In my examples, I chose $\lambda_i=\lambda'_i=\lambda$, with $\lambda=1$.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{sss1b.PNG}
\caption{Special interlacing of $4$ lattice processes with $s=0$.}
\label{fcsss1}
\end{figure}

\subsection{Mapping the raw observations onto an image bitmap} \label{fcmap}

 Eventually, due to the lattice nature of the stochastic processes involved (with point patterns exhibiting statistical tiling around each vertex), the points are transformed using a modulo operator (see \texttt{xmod} and \texttt{ymod} in the Python code) and then mapped onto an image bitmap (the bivariate array \texttt{bitmap} in the Python code).
From there, image processing techniques are used to perform classification or clustering.
 
\section{Classification and unsupervised clustering}

I describe here a methodology for fast supervised and unsupervised 
classification. The data is first transformed into a 
two-dimensional array called {\em bitmap}. The points are referred to as pixels, and the array represents an image stored in 
\textcolor{index}{GPU}\index{GPU-based clustering} 
(the graphics processing unit) [\href{https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units}{Wiki}]. The functions applied to the bitmap are standard image processing techniques such as high pass filtering or 
\textcolor{index}{histogram equalization}\index{histogram equalization} [\href{https://en.wikipedia.org/wiki/Histogram_equalization}{Wiki}]. 

The input data consists of a realization (obtained by simulation in section~\ref{fcsim} and~\ref{fcmap}) of an 
\textcolor{index}{$m$-interlacing}\index{$m$-interlacing} (that is, a superimposition of $m$ shifted Poisson-binomial processes) with each individual process represented by a different color: see Figure~\ref{fc015b} and~\ref{fc015c}. The left plot shows the data points observed modulo $2/\lambda$. So, the point locations, after the modulo operation, are in 
$[0, 2/\lambda[ \times [0, 2/\lambda[$. I chose $\lambda=1$ for the intensity function, in the simulations. 



The modulo operator  magnifies the cluster structure, which is otherwise invisible to the naked eye.  It is defined as 
$a \bmod b = a -  b \lfloor a/b \rfloor$ where the brackets represent the integer part function. For your own simulations, you can use modulo $1/\lambda$, 
rather than $2/\lambda$: this will remove the apparent stochastic duplication in my pictures. The reason I chose $2/\lambda$ is due to boundary effects, with clusters extending beyond 
the window of observations and truncated because of the window, thus making the cluster structure much harder to see if using modulo $1/\lambda$.  For the mathematically inclined reader, the methodology performs 
\textcolor{index}{classification on the torus} [\href{https://en.wikipedia.org/wiki/Periodic_boundary_conditions}{Wiki}] rather than on the plane: this is a standard technique when facing boundary effects. If all your data fits nicely in the observation window, you can ignore the modulo transformation.

The middle and right plots in Figure~\ref{fc015c} correspond to \textcolor{index}{unsupervised clustering}\index{unsupervised clustering}. The centers of the darkest areas provide an approximation to the unknown shift vectors
$(\mu_i,\mu'_i)$ of formulas~(\ref{simm1}) and~(\ref{simm2}), with $i=0,\dots,m$ indicating the (unknown) cluster label. The shift vectors are the theoretical cluster centers. The approximation is far from perfect due to massive cluster overlapping. 
The situation is much better in Figure~\ref{fc005b} (right plot), where cluster overlapping is much less pronounced. The methodology is described in section~\ref{fc12323}. 

The middle and right plots in Figure~\ref{fc015b}  correspond to \textcolor{index}{supervised classification}\index{supervised classification} of the entire space:  the color of a point represents the individual point process or cluster it belongs to. In this case the data set is the training set. The methodology is described in section~\ref{fc12324}. 


\subsection{Supervised classification based on convolution filters}\label{fc12323}

Here the synthetic dataset represents the \gls{gls:trainingset}. The algorithm consists of filtering the whole  bitmap $N$ times. 
Each time, a local filter is applied 
to each pixel $(x,y)$. Initially, the color $c(x,y)$ attached to the pixel represents the cluster it belongs to, in the training set (or in other words, the individual point process it originates from in the $m$-mixture): its value
is an integer between $0$ and $m-1$ if it is in the training set, and $255$ otherwise.  The new color assigned to $(x,y)$ is
\begin{equation}
c'(x,y)=\underset{j}{\arg \max} \sum_{u=-w}^{w}\sum_{v=-w}^{w}\frac{\chi[c(x-u,y-v)=j]}{\sqrt{1+u^2+v^2}}, \label{filt786}
\end{equation}
that is, the value of $j$ that maximizes~(\ref{filt786}). Here $\chi[A]$ is the indicator function [\href{https://en.wikipedia.org/wiki/Indicator_function}{Wiki}]: $\chi[A]=1$ if $A$ is true, and $0$ otherwise.
The boundary problem (when $x-u$ or $y-v$ is outside the bitmap) is handled in the source code. 

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{fc015b.PNG}
\caption{Classification of left dataset; $s=0.15$, $w=10$. One loop (middle) vs $3$ (right).}
\label{fc015b}
\end{figure}

In the python code, $N$ is the parameter \texttt{nloop}, and $w$ is the parameter
\texttt{window}. It is also referred to as $w$ and {\em loops} in the figures. In particular, I used $N=3$ and $w=20$. Formula~(\ref{filt786}) corresponds
to \texttt{method = 1} in the Python code. While slow, it provides granular cluster boundaries. A faster version, namely \texttt{method = 0}, does not make the division by
$\sqrt{1+u^2+v^2}$. It is faster not only because it avoids the square root computations, but also because it can be implemented very efficiently: 
see section~\ref{fctav}. Yet, the loss of accuracy when using the fast method, while noticeable, is smaller than expected.  See Figure~\ref{fc4picb} for comparisons.

Note that each cluster, even when the overlap is small, extends to the entire plane. So there is always some degree of overlap. But the overlap is much smaller when the diffusion 
factor $s$ (the variable \texttt{s} in the Python code) is small. This is evident when comparing Figure~\ref{fc015b} with Figure`\ref{fc005b}. Both figures have the same number
of clusters, and the same cluster centers; only $s$ -- and thus the amount of cluster overlap -- is different.

After filtering the whole bitmap $N=3$ times, thanks to the large size of the local filtering window ($w=20$), all pixels are assigned to a cluster. This means that any future point (not in the training set) can easily and efficiently be classified: first, find its location on the bitmap; then its cluster is the color assigned to that location. 
It is worth asking whether convergence occurs (and to what solution) if you were to filter the bitmap many times.  I studied convergence for a similar type of filter, in my paper 
``Simulated Annealing: A Proof of Convergence" \cite{vgieee}. Empirical evidence suggests that additional loops (increasing $N$ beyond $N=3$) barely makes any difference.  


\subsection{Clustering based on histogram equalization}\label{fc12324}


I use the same filter for unsupervised clustering.  Indeed, both supervised and unsupervised clustering are implemented in parallel in the source code, within the same loop. The main difference is that the color (or cluster) $c(x,y)$ attached to a pixel $(x,y)$ is not known. Instead of colors, I use gray levels representing the density of points at any location on the bitmap: the darker, the higher the density. I start with a bitmap where $c(x,y)=1$ if $(x,y)$ corresponds to the location of an observed point on the bitmap, and $c(x,y)=0$ otherwise. Again, I filter the whole  bitmap $N=3$ times with the same 
 filter size $w=20$. The new gray level assigned to pixel $(x,y)$ at loop $t$ is now
\begin{equation}
c'(x,y)=\underset{j}{\arg \max} \sum_{u=-w}^{w}\sum_{v=-w}^{w}\frac{c(x-u,y-v)\cdot 10^{-t}}{\sqrt{1+u^2+v^2}}. \label{filt787}
\end{equation}
The first time this filter is applied to the whole bitmap, I use $t=0$ in Formula~(\ref{filt787}); the second time I use $t=1$, and the third time I use $t=2$. The purpose is to dampen the effect of successive filtering, otherwise the image (rightmost plots in Figure~\ref{fc015c}) would turn almost black everywhere after a few loops, making it impossible to visualize the cluster structure. The second and third loops, with the damping factor, provide an improvement over using a single loop only. 

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{fc015c.PNG}
\caption{Clustering of left dataset; $s=0.15$, $3$ loops, $w=10$ (middle) vs $20$ (right).}
\label{fc015c} 
\end{figure}

After filtering the image, I use a final post-processing step to enhance the gray levels: see Part 4 of the source code in the \texttt{GD\_maps} function. It  consists of \gls{gls:binning} and rescaling the histogram of gray levels to make the image sharper and easier to interpret, with 8 gray levels only. This step, called 
\textcolor{index}{histogram equalization}\index{histogram equalization},  can be automated. The successive gray levels, starting with the darkest one, correspond to successive levels in an 
\textcolor{index}{hierarchical clustering}\index{hierarchical clustering} algorithm [\href{https://en.wikipedia.org/wiki/Hierarchical_clustering}{Wiki}]. To finalize the clustering procedure, one may use \textcolor{index}{image segmentation}\index{image segmentation} techniques [\href{https://en.wikipedia.org/wiki/Image_segmentation}{Wiki}] to identify the boundary of the clusters.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{fc005b.PNG}
\caption{Classification ($w=10$) and clustering ($w=20$); $s=0.05$, three loops.}
\label{fc005b}
\end{figure}

The equalizer used in my code works on all the images tested. However, you may want to use one that is image-specific. The Python pillow library offers an easy way to do it, see
\href{https://www.geeksforgeeks.org/python-pil-imageops-equalize-method/}{here}. You can also write your own Python code for full control. See the histogram equalization
 code in the gigantic Algorithms repository on GitHub, \href{https://github.com/TheAlgorithms/Python/blob/master/digital_image_processing/histogram_equalization/histogram_stretch.py}{here}. 


\subsection{Fractal classification: deep neural network analogy}\label{fcfract}



The filtering system is essentially a \gls{gls:neuralnet}\index{neural network} [\href{https://en.wikipedia.org/wiki/Neural_network}{Wiki}]. The image before the first loop (Figures~\ref{fc015b} and~\ref{fc005b}, left plot), consisting of the training set,  is the input layer. The final image obtained after 3 loops
is the output layer. The intermediate iterations correspond to the \textcolor{index}{hidden layers}\index{hidden layer}\index{neural network!hidden layer}. In each layer, the pixel color is a function of quantities computed on neighboring pixels, in the previous layer.
The pre-processing step consists of transforming the data set into an image bitmap. In section~\ref{fc12324} about unsupervised clustering, the post-processing step called ``equalizer" plays the role of the sigmoid function in neural networks.
 See Luuk Spreeuwers' PhD thesis ``Image Filtering with Neural Networks" defended in 1992 \cite{luuk} (available online, \href{https://ris.utwente.nl/ws/portalfiles/portal/255169420/Thesis_L_Spreeuwers.pdf}{here}), for more about image filters used as neural networks. 

In my video posted \href{https://www.youtube.com/watch?v=dNPSEh-X6uw}{here} (YouTube) and \href{https://github.com/VincentGranville/Point-Processes/blob/main/Videos/fractal005.gif}{here} (animated gif on GitHub), each frame represents a layer in a \textcolor{index}{very deep neural network}\index{neural network!very deep}\index{deep neural network}. In my methodology, I use the term ``loop" or ``iteration" instead of layer. It is represented by the Python variable \texttt{loop} in \texttt{PB\_clustering\_video.py} (section~\ref{pyfc1}).
A pixel plays the role of a \textcolor{index}{neuron}\index{neural network!neuron}, and the weight attached to the link between a pixel and one of its neighbors -- as in formula~(\ref{filt786}) --  is also called ``weight", or parameter, in neural network  terminology.  


\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{fc015ax.png}
\caption{Fractal classification, $s=0.15$. Loop $6$, $250$ and $400$.}
\label{fc015a}
\end{figure}

The fractal classifier described here, displayed in the video and also pictured in Figures~\ref{fc015a} and~\ref{fc005a}, is in some sense the opposite of the one described in section~\ref{fc12323}: instead of using $N=3$ loops (that is, 3 layers), it uses hundreds of them. But the local filter is extremely small, with $w\leq1$, compared to $w=20$ in section~\ref{fc12323}. Thus, each neuron (pixel) is connected to one neuron at most. Such neural networks are called ``sparse". In the end, it produces similar results, compared to using few loops and a large local filtering window. The main difference is that cluster boundaries are less smooth, and appear fractal-like. The video (\href{https://github.com/VincentGranville/Point-Processes/blob/main/Videos/fractal005.gif}{here}) shows the successive image transformations taking place from one loop to the next one. By watching it, it is very easy to understand how the method works, making it a classic example of 
\gls{gls:explainableai}\index{explainable AI} [\href{https://en.wikipedia.org/wiki/Explainable_artificial_intelligence}{Wiki}].

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{fc005a2x.PNG}
\caption{Fractal classification, $s=0.05$.Loop: $6$ and $60$.}
\label{fc005a}
\end{figure}

Once the whole state is classified  (when no white area is left on the image), each subsequent loop randomly re-assigns the pixel labels (the cluster they belong to), around cluster borders. It allows you to compute, for a pixel on the border between two or more clusters, the a-posteriori probability that it belongs to any of these clusters. This makes the methodology similar to a \textcolor{index}{Bayesian classifier}\index{Bayesian classification} [\href{https://en.wikipedia.org/wiki/Bayes_classifier}{Wiki}]. The borders between clusters are statistically stable over time: the algorithm converges, at least from a stochastic point of view.

\subsection{Generalization to higher dimensions}

All the examples featured in this article are in two dimensions. This makes it easy to use image processing techniques for classification or clustering. In three dimensions, images can be replaced by videos, and one can still use standard filtering techniques. It becomes more challenging in four dimensions. One way to handle the problem in higher dimensions (or even in two dimensions) is to use \glspl{gls:tensor}\index{tensor} [\href{https://en.wikipedia.org/wiki/Tensor}{Wiki}]. A 2D tensor is a standard rectangular matrix. A 3D tensor is a $p\times q\times r$ matrix, or in other words, a cubic matrix. Each ``slice" of a 3D tensor can be treated as an image. The filters can be adapted to this type of data.

In higher dimensions, the training set occupies a tiny portion of the whole space.  It does not make sense to try to classify the whole space: this becomes time-prohibitive in dimension 5 and above. Instead, the solution consists of working with \href{https://www.tensorflow.org/guide/sparse_tensor}{sparse tensors}. They can be represented as \gls{gls:graphmodel} structures, with each point connected to its neighbors, then the neighbors of the neighbors and so on, with a depth or 4 or 5 levels. This is still a work in progress.

\subsection{Towards a very fast implementation}\label{fctav}

 The size $w$ of the local filter window is the bottleneck. 
When filtering the image using the algorithm in section~\ref{fc12323}, the window used at $(x,y)$, and the next one at $(x+1,y)$, both have $(2w+1)^2$ pixels, but these two windows have $(2w+1)^2 - 2\times (2w+1)$ pixels in common. So rather than visiting $(2w+1)^2$ pixels each time, the overlapping pixels can be kept in a $(2w+1)^2$ buffer. To update the buffer after visiting a pixel and moving to the next one to the right, one only has to update $2w+1$ values in the buffer: overwrite the column corresponding to the old $2w+1$ leftmost pixels, by the values derived from the new $2w+1$ rightmost pixels.  

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{fc4picb.PNG}
\caption{Fast (left) vs standard method (right), $3$ loops, $s=0.15, w=10$.}
\label{fc4picb}
\end{figure}

This leads to a particularly efficient implementation when using \texttt{method = 0} (the fast filter). Then, the \texttt{GD\_Maps} function in \texttt{GD\_util.py} can be further 
optimized, since it only 
counts pixels (based on their color) in the local filter window, without computing distances to the center of the window. It will speed up the procedure
by a factor proportional to $w$, both for supervised classification and clustering. Since I use $w=20$ (the parameter \texttt{window} in the code), the improvement is significant.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{fc005c2.PNG}
\caption{Fast method, $s=0.05,w=20$. Three loops (middle), one loop (right).}
\label{fc005c2}
\end{figure}


\renewcommand{\arraystretch}{1.0} %%%
\renewcommand{\arraystretch}{1.4} %%%

\section{Python code}\label{pythonviz3}

The Python code uses the \texttt{pillow} and \texttt{moviepy} libraries. To install Pillow, type 
\texttt{pip install pillow} on the Windows command prompt. The program \texttt{PB\_clustering\_video.py} in section~\ref{pyfc1} is a self-contained and short, separate piece of code. It is also the only one that produces videos (MP4 files). 
You might want to look at it first. The parameter $s$, represented by the
global variable \texttt{s}, is called the scaling or diffusion factor. It  determines the amount of overlap between clusters. If $s=0$,
all the points are located on a lattice. If $s$ is large (say $s>5$) the points are almost uniformly distributed; clustering becomes
meaningless, no matter what algorithm you use.

The main program \texttt{PB\_NN.py} in section~\ref{fc222} does not include any video / image processing. However, it requires
\texttt{GD\_util.py} (see section~\ref{fc223}), my home-made small library consisting of one function \texttt{GD\_Maps}. All the image processing is performed in that function. The \texttt{GD\_util.py} file is assumed to be in the same folder as \texttt{PB\_NN.py}. Part 3 and 4 in \texttt{PB\_NN.py} deal with the time-intensive computation of all intra-distances. You don't need it, and it is turned off by the global
variable \texttt{NNflag}, set to \texttt{False}. It is provided only for compatibility with an older Perl version in the first edition of my book on stochastic processes. This Python version replaces the Perl code, in the new edition. 

The output of  \texttt{PB\_NN.py} consists of PNG images, one for classification and one for clustering, per iteration. The input is the \gls{gls:syntheticdata} created in part 2, and transformed into a bitmap (array of colored pixels) also in part 2. Image filtering, and thus classification and clustering, takes place in part 5: it consists of a call to the \texttt{GD\_maps} function. As discussed earlier, parts 3 and 4 are skipped. The \gls{gls:hyperparam}\index{neural network!hyperparameter} \texttt{window}, referred to as $w$ in the figure captions, is the size of the local filter. To assign a cluster to a point (that is, a color to a pixel), the local filter consists of a $(2w+1)\times(2w+1)$ window centered at the point in question.
Iterations (or layers, if it was a \gls{gls:neuralnet}) are referred to as ``loops" in figure captions. The number of iterations
is determined by the variable \texttt{nloop} in the code. Finally, \texttt{Nprocess} is the number of clusters, and \texttt{method} determines the type of filter. The methodology has been extensively tested with \texttt{method = 1}, which is time consuming if
$w$ is large. Note that \texttt{method = 0} still provides satisfactory results, and can be implemented in a very efficient way,
as discussed in section~\ref{fctav}.

For colors, I use the \textcolor{index}{RGBA}\index{color model!RGBA} model [\href{https://en.wikipedia.org/wiki/RGBA_color_model}{Wiki}]. However, for the time being, the A component or fourth element of the color vector, known as transparency level, is not used. A future version of the code may use it, in a way similar to the supervised classification technique described in my article on visualizing 
high dimensional data in chapter~\ref{chapvisu}. 

\subsection{Fractal classification}\label{pyfc1}
 
On GitHub: \href{https://github.com/VincentGranville/Point-Processes/blob/main/Videos/PB_clustering_video.py}{\texttt{PB\_clustering\_video.py}}. Produces the fractal classification video with $400$ frames or layers, using the smallest possible filter window. Short, self-sufficient code, using the \texttt{Pillow} and \texttt{Moviepy} libraries. The input data is synthetic and created in the code: it shares some features with Bayesian Gaussian mixtures. However the classification itself is model-free. Description in section~\ref{fcfract}.  

\begin{lstlisting}
# PB_clustering_video.py

import math
import random
from PIL import Image, ImageDraw    # ImageDraw to draw rectangles etc.
import moviepy.video.io.ImageSequenceClip  # to produce mp4 video

Nprocess=4       # number of processes in the process superimposition
seed=82431       # arbitrary number
random.seed(seed) # initialize random generator 
s=0.05  # scaling factor
shiftX=[]
shiftY=[]

for i in range(Nprocess) :
  shiftX.append(random.random())
  shiftY.append(random.random())
processID=0
height,width = (800, 800)
bitmap = [[255 for k in range(height)] for h in range(width)]

for h in range(-25,26):   
  for k in range(-25,26):  
    for processID in range(Nprocess): 
      ranx=random.random()
      rany=random.random()
      ranID=random.random()
      if ranID < 0.20:
        processID=0
      elif ranID < 0.60:
        processID=1
      elif ranID < 0.90:
        processID=2 
      else:
        processID=3
      x=shiftX[processID]+h+s*math.log(ranx/(1-ranx)) 
      y=shiftY[processID]+k+s*math.log(rany/(1-rany))
      if x>-3 and x<3 and x>-3 and x<3:
        xmod=1+x-int(x)   # x modulo 2/lambda
        ymod=1+y-int(y)   # y modulo 2/lambda
        pixelX=int(width*xmod/2)   
        pixelY=int(height*(2-ymod)/2) # pixel (0,0) at top left corner
        bitmap[pixelX][pixelY]=processID

#---
img1  = Image.new( mode = "RGBA", size = (width, height), color = (0, 0, 0) )
pix1  = img1.load()   # pix[x,y]=col[n] to modify the RGB color of a pixel
draw1 = ImageDraw.Draw(img1,"RGBA")

col1=[] 
col1.append((255,0,0,255))
col1.append((0,0,255,255))
col1.append((255,179,0,255))
col1.append((0,179,0,255))
col1.append((0,0,0,255))
for i in range(Nprocess,256):
  col1.append((0,0,0,255))
 
for pixelX in range(0,width): 
  for pixelY in range(0,height): 
        topProcessID=bitmap[pixelX][pixelY]
        pix1[pixelX,pixelY]=col1[topProcessID]

draw1.rectangle((0,0,width-1,height-1), outline ="black",width=1)
fname="img_0.png"
img1.save(fname)

#---
nloop=400       # number of times the image is filtered 

oldBitmap = [[255 for k in range(height)] for h in range(width)]
flist=[]

for loop in range(1,nloop+1): 
  print("loop",loop,"out of",nloop+1) 
  for pixelX in range(0,width): 
    for pixelY in range(0,height): 
      oldBitmap[pixelX][pixelY]=bitmap[pixelX][pixelY]
  for pixelX in range(1,width-1): 
    for pixelY in range(1,height-1):   
      x=pixelX
      y=pixelY
      topProcessID=oldBitmap[x][y]
      if topProcessID==255 or loop>50: 
        r=random.random()
        if r<0.25: 
          x=x+1 
          if x>width-2: 
            x=x-(width-2)
        elif r<0.5:
          x=x-1 
          if x<1: 
            x=x+width-2
        elif r<0.75:
          y=y+1 
          if y>height-2: 
            y=y-(height-2)
        else:
          y=y-1 
          if y<1: 
            y=y+height-2         
        if loop>=50 and oldBitmap[x][y]==255:
          x=pixelX
          y=pixelY
      topProcessID=oldBitmap[x][y]  
      bitmap[pixelX][pixelY]=topProcessID
      pix1[pixelX,pixelY]=col1[topProcessID]
  draw1.rectangle((0,0,width-1,height-1), outline ="black",width=1)
  fname="img_"+str(loop+1)+'.png'
  flist.append(fname)   
  img1.save(fname)

clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(flist, fps=20) 
clip.write_videofile('img.mp4')
\end{lstlisting}

\subsection{GPU classification and clustering}\label{fc222}

On GitHub: \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PB_NN.py}{\texttt{PB\_NN.py}}. Produces the supervised classification and clustering using a large filter window and only $3$ layers. Requires
 the small, home-made graphic library \texttt{GD\_util.py} featured in section~\ref{fc223}. All image manipulations are performed in that library. The input data is synthetic and created in the code: it shares some features with Bayesian Gaussian mixtures. However the classification itself is model-free. Description in sections~\ref{fc12323} and~\ref{fc12324}.  

\begin{lstlisting}
# PB_NN.py  
# lambda = 1

import numpy as np
import math
import random

#---
# PART 1: Initialization

Nprocess=5                  # number of processes in the process superimposition
s=0.15                      # scaling factor
method=1                    # method=0 is fastest
NNflag=False                # set to True if you need to compute NN distances
window=20                   # determines size of local filter [the bigger, the smoother]
nloop=3                     # number of times the image is filtered [the bigger, the smoother]

epsilon=0.0000000001 # for numerical stability
seed=82431                  # arbitrary number
random.seed(seed)           # initialize random generator 

sep="\t"      # TAB character 
shiftX=[]
shiftY=[]
stretchX=[]
stretchY=[]
a=[]
b=[]
process=[]
sstring=[]   # string in Perl version

for i in range(Nprocess) :
  shiftX.append(random.random())
  shiftY.append(random.random())
  stretchX.append(1.0)
  stretchY.append(1.0)
  sstring.append(sep)   
  # i TABs separating x and y coordinates in output file for points
  # originating from process i; Used to easily create a scatterplot in Excel 
  # with a different color for each process.
  sep=sep + "\t"

processID=0
m=0  # number of points generated
height,width = (400, 400)

bitmap = [[255 for k in range(height)] for h in range(width)]

#---
# PART 2: Generate point process, its modulo 2 version; save to bitmap and output files.

OUT  = open("PB_NN.txt", "w")                # the points of the process 
OUT2 = open("PB_NN_mod.txt", "w") # the same points modulo 2/lambda both in x and y directions

for h in range(-25,26):   
    for k in range(-25,26):  
        for processID in range(Nprocess): 
            ranx=random.random()
            rany=random.random()
            x=shiftX[processID]+stretchX[processID]*h+s*math.log(ranx/(1-ranx)) 
            y=shiftY[processID]+stretchY[processID]*k+s*math.log(rany/(1-rany))
            a.append(x)  # x coordinate attached to point m
            b.append(y)  # y coordinate attached to point m
            process.append(processID) # processID attached to point m
            m=m+1
            line=str(processID)+"\t"+str(h)+"\t"+str(k)+"\t"+str(x)+sstring[processID]+str(y)+"\n"
            OUT.write(line)
            # replace sstring[processID] by \t if you don't care about Excel

            if x>-20 and x<20 and x>-20 and x<20:
                xmod=1+x-int(x)   # x modulo 2/lambda
                ymod=1+y-int(y)   # y modulo 2/lambda
                pixelX=int(width*xmod/2)   
                pixelY=int(height*(2-ymod)/2) # pixel (0,0) at top left corner
                bitmap[pixelX][pixelY]=processID
                line=str(xmod)+sstring[processID]+str(ymod)+"\n"
                OUT2.write(line)  
                # replace sstring[processID] by \t if you don't care about Excel
OUT2.close()
OUT.close()

#---
# PART 3: Find nearest neighbor points, and compute nearest neighbor distances.

if NNflag:

  OUT  = open("PB_NN_dist_small.txt", "w")     # the points of the process 
  OUTf = open("PB_NN_dist_full.txt", "w") # the same points modulo 2/lambda both in x and y directions

  NNx=[]
  NNy=[]
  NNidx=[]
  NNidxHash={}

  for i in range(m):
    NNx.append(0.0)
    NNy.append(0.0)
    NNidx.append(-1)
    mindist=99999999
    flag=-1
    if a[i]>-20 and a[i]<20 and b[i]>-20 and b[i]<20: 
      flag=0;
      for j in range(m):
        dist=math.sqrt((a[i]-a[j])**2 + (b[i]-b[j])**2)  # taxicab distance faster to compute
        if dist<=mindist+epsilon and i!=j: 
          NNx[i]=a[j]  # x-coordinate of nearest neighbor of point i
          NNy[i]=b[j]  # y-coordinate of nearest neighbor of point i
          NNidx[i]=j      # indicates that point j is nearest neighbor to point i
          #  NNidxHash[i] is the list of points having point i as nearest neighbor;
          #  these points are separated by "~" (usually only one point in NNidxHash[i]
          #  unless the simulated points are exactly on a lattice, e.g. if s = 0)
          if abs(dist-mindist) < epsilon: 
            NNidxHash[i]=NNidxHash[i]+"~"+str(j) 
          else:    
            NNidxHash[i]=str(j) 
          mindist=dist 
      if i % 100 == 0: 
        print("Finding Nearest neighbors of point",i)
      line=str(i)+"\t"+str(mindist)+"\n"
      OUT.write(line) 
      line=str(i)+"\t"+str(NNidx[i])+"\t"+str(NNidxHash[i])+"\t"+str(a[i])+"\t" 
      line=line+str(b[i])+"\t"+str(NNx[i])+"\t"+str(NNy[i])+"\t"+str(mindist)+"\n"
      OUTf.write(line) 

  OUTf.close()
  OUT.close()

#---
# PART 4: Produce data to use in R code that generates the nearest neighbors picture.

if NNflag:

  OUT  = open("PB_r.txt","w")     
  OUT.write("idx\tnNN\tNNindex\ta\tb\taNN\tbNN\tprocessID\tNNprocessID\n")

  for idx in NNidxHash:
    NNlist=NNidxHash[idx]
    list=NNlist.split("~")
    nelts=len(list)
    for n in range(nelts): 
      NNindex=int(list[n])
      line=str(idx)+"\t"+str(n)+"\t"+str(NNindex)+"\t"+str(a[idx])+"\t"+str(b[idx])
      line=line+"\t"+str(a[NNindex])+"\t"+str(b[NNindex])+"\t"+str(process[idx])
      line=line+"\t"+str(process[NNindex])+"\n"
      OUT.write(line)  
                
  OUT.close()

#---
# PART 5: Creates density and cluster images.

img_cluster="PB-cluster"  # use for output image filenames
img_density="PB-density"  # use for output image filenames

from GD_util import *
GD_Maps(method,bitmap,Nprocess,window,nloop,height,width,img_cluster,img_density)
\end{lstlisting}

\subsection{Home-made graphic library}\label{fc223}

On GitHub: \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/GD_util.py}{\texttt{GD\_util.py}}. Relies
on the \texttt{Pillow} library, to perform various filtering processes directly in image bitmaps rather than on the initial data. The unsupervised clustering technique uses a color equalizer, as the main machine learning algorithm. The library has only
one function \texttt{GD\_Maps}  that does both supervised classification at once. 

\begin{lstlisting}
import math
from PIL import Image, ImageDraw           # ImageDraw to draw rectangles etc.

def GD_Maps(method,bitmap,Nprocess,window,nloop,height,width,img_cluster,img_density):

  #---
  # PART 1: Allocate first image (clustering), including colors (palette)

  img1  = Image.new( mode = "RGBA", size = (width, height), color = (0, 0, 0) )
  pix1  = img1.load()   # pix[x,y]=col[n] to modify the RGB color of a pixel
  draw1 = ImageDraw.Draw(img1,"RGBA")

  col1=[] 
  col1.append((255,0,0,255))
  col1.append((0,0,255,255))
  col1.append((255,179,0,255))
  col1.append((0,0,0,255))
  col1.append((0,179,0,255))
  for i in range(Nprocess,256):
    col1.append((255,255,255,255))
  oldBitmap = [[255 for k in range(height)] for h in range(width)]
  densityMap= [[0.0 for k in range(height)] for h in range(width)]
  for pixelX in range(0,width): 
    for pixelY in range(0,height): 
      processID=bitmap[pixelX][pixelY]
      pix1[pixelX,pixelY]=col1[processID] 
  draw1.rectangle((0,0,width-1,height-1), outline ="black",width=1)
  fname=img_cluster+'.png'
  img1.save(fname)

  #---
  # PART 2: Filter bitmap and densityMap 

  for loop in range(nloop): #  

    print("loop",loop,"out of",nloop)
    for pixelX in range(0,width): 
      for pixelY in range(0,height): 
        oldBitmap[pixelX][pixelY]=bitmap[pixelX][pixelY]

    for pixelX in range(0,width): 
      for pixelY in range(0,height):   
        count=[0] * Nprocess
        density=0
        maxcount=0
        topProcessID=255 # dominant processID near (pixelX, pixelY)
        for u in range(-window,window+1): 
          for v in range(-window,window+1):
            x=pixelX+u
            y=pixelY+v
            if x<0: 
              x+=width      # boundary effect correction
            if y<0: 
              y+=height     # boundary effect correction
            if x>=width: 
              x-=width      # boundary effect correction
            if y>=height: 
              y-=height     # boundary effect correction
            if method == 0:
              dist2=1
            else: 
              dist2=1/math.sqrt(1+u*u + v*v)
            processID=oldBitmap[x][y]
            if processID < 255: 
              count[processID]=count[processID]+dist2
              if count[processID]>maxcount: 
                maxcount=count[processID]
                topProcessID=processID
              density=density+dist2 
        density=density/(10**loop)   # 10 at power loop (dampening)
        densityMap[pixelX][pixelY]=densityMap[pixelX][pixelY]+density
        bitmap[pixelX][pixelY]=topProcessID

    #---
    # PART 3:  Some pre-processing; output cluster image

    densityCountHash={}  # use to rebalance gray levels
    for pixelX in range(0,width): 
      for pixelY in range(0,height):   
        topProcessID=bitmap[pixelX][pixelY]
        density=densityMap[pixelX][pixelY]
        if density in densityCountHash:
          densityCountHash[density]=densityCountHash[density]+1
        else:
          densityCountHash[density]=1
        pix1[pixelX,pixelY]=col1[topProcessID]

    draw1.rectangle((0,0,width-1,height-1), outline ="black",width=1)
    fname=img_cluster+str(loop)+'.png'
    img1.save(fname)

    #---
    # PART 4: Equalize gray levels in the density image; output image as a PNG file 
    # Also try https://www.geeksforgeeks.org/python-pil-imageops-equalize-method/

    densityColorHash={} 
    col2=[]
    size=len(densityCountHash)  # number of elements in hash
    counter=0

    for density in sorted(densityCountHash):
      counter=counter+1
      quant=counter/size   # always between zero and one
      if quant < 0.08: 
        densityColorHash[density]=0
      elif quant < 0.18:
        densityColorHash[density]=30 
      elif quant < 0.28:
        densityColorHash[density]=55
      elif quant < 0.42:
        densityColorHash[density]=90
      elif quant < 0.62:
        densityColorHash[density]=120
      elif quant < 0.80:
        densityColorHash[density]=140
      elif quant < 0.95:
        densityColorHash[density]=170
      else:
        densityColorHash[density]=254

    # allocate second image (density image)

    img2  = Image.new( mode = "RGBA", size = (width, height), color = (0, 0, 0) )
    pix2  = img2.load()   # pix[x,y]=col[n] to modify the RGB color of a pixel
    draw2 = ImageDraw.Draw(img2,"RGBA")

    # allocate gray levels (palette)
    for i in range(0,256):
        col2.append((255-i,255-i,255-i,255))

    # create density image pixel by pixel
    for pixelX in range(0,width): 
      for pixelY in range(0,height):   
        density=densityMap[pixelX][pixelY] 
        color=densityColorHash[density]
        pix2[pixelX,pixelY]=col2[color]  

    # output density image
    draw2.rectangle((0,0,width-1,height-1), outline ="black",width=1)
    fname=img_density+str(loop)+'.png'
    img2.save(fname)

  return()
\end{lstlisting}

%----------------------------------------------------------------------------------------------------------------
\Chapter{Shape Classification via Explainable AI}{}\label{chaptershapes}

Here I define the mathematical concept of shape and shape signature in two dimensions, using parametric polar equations. The signature uniquely characterizes the shape, up to a translation or scale factor. In practical applications, the data set consists of points or pixels located on the shape, rather than the curve itself. If these points are not properly sampled - if they are not uniformly distributed on the curve - they need to be re-weighted to compute a meaningful centroid of the shape, and to perform shape comparisons. I discuss the weights, and then introduce metrics to compare shapes (observed as sets of points or pixels in an image). These metrics are related to the Hausdorff distance. I also introduce a correlation distance between two shapes. Equipped with these metrics, one can perform shape recognition or classification using training sets of arbitrary sizes. I use synthetic data in the applications. It allows you to see how the classifier performs, to discriminate between two very similar shapes, or in the presence of noise. Rotation-invariant metrics are also discussed.


\hypersetup{linkcolor=red}

\section{Introduction}

A central problem in \textcolor{index}{computer vision}\index{computer vision} is to compare shapes and assess how similar they are. This is used for instance in text recognition. Modern techniques involve \glspl{gls:neuralnet}. Here, I revisit a methodology developed before computer even existed. With modern technology, it leads to an efficient, automated AI algorithm. The benefit is that the decision process made by this black-box system, can be easily explained, and thus easily controlled.

To the contrary, neural networks use millions of weights that are impossible to interpret, potentially leading to over-fitting. Why they work very well on some data and no so well on other data is a mystery. My “old-fashioned” classifier, adapted to modern data and computer architectures, lead to full control of the parameters. In other words, you know beforehand how fine-tuning the parameters will impact the output. Thus the word \gls{gls:explainableai}\index{explainable AI} [\href{https://en.wikipedia.org/wiki/Explainable_artificial_intelligence}{Wiki}].

In an ideal world, one would want to blend both methods, to benefit from their respective strengths, and minimize their respective drawbacks. Such blending is referred to as \glspl{gls:ensembles}\index{ensemble methods} [\href{https://en.wikipedia.org/wiki/Ensemble_learning}{Wiki}]. Also, since we are dealing with sampled points located on a curve (the “shape”), the same methodology also applies to sound recognition.

\section{Mathematical foundations}

In this section, we are concerned with the mathematical concept of shape. Later on, I apply the methodology to shapes represented by sets of points or pixels, observed through a rectangular window -- a digital image. The center of the window is the origin of the coordinate system. Shapes can be anything: they may represent a letter, an hieroglyph, or a combination of symbols. They may consist of multiple, non-connected curves. We are only interested in the contour that defines the shape. It may or may not correspond to the boundary of a domain; the contour may not be closed and could consist of disconnected segments.

Furthermore there is no color or gray scale involved. The mathematical shape model can be viewed as black on a white background, with no thickness. In practical applications, the rectangular image is centered around the shape, and the noise has been filtered out. See example in Figure~\ref{fig:dash}, comparing two shapes.

\begin{figure}%[H]
\centering
\includegraphics[width=0.5\textwidth]{shapeb4.png} % 0.75
%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Comparing two shapes}
\label{fig:dash}
\end{figure}


\noindent It is convenient, for illustrations purposes, to define a 2-D mathematical shape using a parametric polar equation, as follows:

$$r_t =g(t), \quad\theta_t=h(t),  \quad \text{with}\quad  t\in T, \quad r_t\geq 0, \quad 0\leq \theta_t\leq 2\pi.$$

\noindent Here $g, h$ are real-valued functions, and $T$ is the index domain. An example with $n= 20$ points is as follows: 

\begin{equation}
\theta_t=(t+\eta)\bmod{2\pi},\quad r_t=c+d \sin(at) \sin(2\pi b- bt), \quad t = 2\pi k/n \text{ with } k=0,\dots, n-1. \label{eq2}
\end{equation}

\noindent This example is pictured in Figure~\ref{fig:dash}. The parameter $\eta$ controls the rotation angle or orientation of the shape. By definition, $\alpha \bmod \beta =\alpha - \beta\lfloor \alpha/\beta\rfloor$ where the brackets represent the integer part function. A more simple example, corresponding to an elliptic arc, is 
$$r_t=\frac{p}{1-\epsilon\cos t}, \quad \theta_t=t, \quad 0\leq t \leq t_0$$
where $0<\epsilon<1$, $p>0$ and $0<t_0<2\pi$ are the parameters. The parameter $\epsilon$ is the eccentricity. Since the functions $g(t)$ and $f(t)$ are arbitrary, can be discontinuous, and may contain infinitely many parameters (for instance, the coefficient of a Taylor or Fourier series), it covers all the possible shapes that exist in the universe. 

\section{Shape signature}

The concept of shape signature is not new, see \cite{stama2007,fpark2001}. Each shape (or set of points) is uniquely described by a normalized set called \textcolor{index}{signature}\index{shape signature}. In our context, this set can be a curve, a set of points, multiple broken curves, or a combination of these elements. The signature does not depend on the location or center of gravity of the shape. It depends on the orientation, though it is easy to generalize the definition to make it rotation-invariant, or to handle 3D shapes. The first step is to use the center of gravity (centroid) for the origin, and then rescale the shape by standardizing the variance of the radius $r_t$.

The centroid is the weighted average of the points located on the shape. Typically, the weight is constant. However, if the points are not uniformly distributed on the shape, you may use appropriate weights to correct for this artifact. This is illustrated in Figure~\ref{fig:ctr}. I now dive into the details of the reweighting procedure.

\subsection{Weighted centroid}\label{centr1}

Let $(x_t, y_t)$ be the standard coordinates of the observed points on the shape. In other words, $x_t=r_t \cos\theta_t$ and $y_t=r_t\sin\theta_t$. The centroid is defined as $(G_x,G_y)$ with
\begin{equation}
G_x=\frac{1}{\mu}\int_T w_t x_t dt, \quad G_y= \frac{1}{\mu}\int_T w_t y_t dt, \quad \text{with } \mu=\int_T w_t dt. \label{eq1}
\end{equation}
Here $w_t>0$ is the weight function, with $t\in T$. If $t$ is discrete (for instance, the shape consists of observed data points), then the integrals are replaced by sums.

In most cases, the points are not evenly distributed on the curve. On a real data set, it translates by a curve that appears darker or thicker in locations with high point density: see Figure~\ref{fig:ctr}. If this is not a desirable feature, it can be eliminated by proper reweighting. To get the points evenly distributed on the curve, when computing the centroid, proceed as follows. Using notations from infinitesimal calculus, you want $\Delta s_t$, the length of an infinitesimal curve segment encompassing $(x_t,y_t)$, to be proportional to $w_t$. Since the proportion factor does not matter, we must have 
$$\Delta s_t = \sqrt{(\Delta x_t)^2 +(\Delta y_t)^2}= w_t \Delta t.$$
This leads to
$$w_t = \sqrt{\Big(\frac{dx_t}{dt}\Big)^2 + \Big(\frac{dy_t}{dt}\Big)^2}.$$
It can be re-written using polar coordinates as
$$w_t=\sqrt{\Big(\frac{dr_t}{dt}\Big)^2 + r_t^2\Big(\frac{d\theta_t}{dt}\Big)^2}.$$
The formula assumes differentiability of the functions involved. In many cases, there are points (values of $t$) where the functions are either left- or right-differentiable [\href{https://en.wikipedia.org/wiki/Semi-differentiability}{Wiki}], but not both. Use the left or right derivative for these points. 


\subsection{Computing the signature}

We want a mathematical object, easy to compute, that uniquely characterizes a shape, up to a translation vector and scaling factor. The set of all polar coordinates $(r_t,\theta_t)$ with $t\in T$, uniquely characterizes the shape. But it is not scale or translation invariant. To fix this problem, you first need to change the coordinate system to make the centroid $(G_x, G_y)$ defined by formula~(\ref{eq1}), the origin. You may use the weight function $w_t$ discussed in section~\ref{centr1}.  Then, you need to rescale by a factor $\sigma$. Eventually, the new coordinates are 
\begin{align}
u_t & = \sigma^{-1} \cdot (x_t-G_x) = \rho_t \cos\varphi_t, \nonumber \\
v_t & = \sigma^{-1} \cdot (y_y-G_y) = \rho_t \sin\varphi_t. \nonumber
\end{align}
Here $u_t,v_t$ are the new Cartesian coordinates replacing $x_t, y_t$, and $\rho_t,\varphi_t$ the new polar coordinates replacing $r_t,\theta_t$. For reasons that will become obvious 
when comparing two shapes in section~\ref{s4}, the scaling factor is chosen as follows:
$$\sigma = \sqrt{\int_T (x_t-G_x)^2 + (y_t-G_y)^2 dt}.$$
It follows immediately that
\begin{equation}
\rho_t =\sigma^{-1}\cdot\sqrt{(x_t-G_x)^2 + (y_t-G_y)^2}, \quad \text{and }\int_T \rho^2_t dt = 1. \label{eq3}
\end{equation}
Now the signature is defined as the set of all $(\rho_t,\varphi_t)$ with $t\in T$. By construction, $0\leq \varphi_t\leq 2\pi$. When plotting the signature, to keep it  bounded on $[0, 2\pi] \times [0, 1]$ regardless of the shape, one can use $\rho_t/(1+\rho_t)$ instead of $\rho_t$ on the vertical axis. An example of signature is shown in Figure~\ref{fig:ctr} (right plot). 

\begin{figure}%[H]
\centering
\includegraphics[width=0.67\textwidth]{shapectr.png} % 0.67
%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Weighted centroid, shape signature}
\label{fig:ctr}
\end{figure}


\subsection{Example}

The shape illustrated in Figures~\ref{fig:ctr} and \ref{fig:3b} is different from that defined by (\ref{eq2}). This time, it is defined by the parametric polar equation
\begin{equation}
\theta_t=(2\pi + 2\pi \sin(ct)+\eta)\bmod{2\pi},\quad r_t=t^a(1-t)^b, \quad t \in T=[0, 1]. \label{eq2b}
\end{equation}
Again, $\eta$ is the angle determining the orientation of the shape. The point density, visible to the naked eye, is much higher on the right side of the shape on the left plot in Figure~\ref{fig:ctr}. This is even more pronounced on the lower part. As a result, the centroid (orange dot) gets attracted to the dense area of the curve. Once this effect is corrected by the weight function, the new centroid (gray dot) now appears well ``centered". Note that the weight function $w_t$, pictured in Figure~\ref{fig:3b}, is bimodal. It was chosen to integrate to one, thus it represents a probability distribution on
$T=[0, 1]$. 

\begin{figure}%[H]
\centering
\includegraphics[width=0.6\textwidth]{shapew.png}  %0.6
%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Weight function used in Figure~\ref{fig:ctr}}
\label{fig:3b}
\end{figure}


\section{Shape Comparison}\label{s4}

I start with a correlation metric based on the mathematical theory developed so far. I then discuss its strengths and weaknesses, and how to improve it. A full implementation on a real data set is investigated in section~\ref{s5}. I use the notation $\rho_t, \varphi_t$ for the first shape, 
and $\rho'_t, \varphi'_t$ for the second one. Assuming the parametric polar equations are such that (possibly after an appropriate transformation) $\varphi_t = \varphi'_t$ for $t\in T$, then define
$$\gamma = \int_T (\rho_t - \rho'_t)^2 dt =  \int_T \rho_t^2 dt + \int {\rho'_t}^2 dt - 2\int_T \rho_t\rho'_t dt =2 - 2\lambda, $$
where 
$$\lambda = \int_T \rho_t\rho'_t dt. $$ 
It follows from (\ref{eq3}) that $0\leq \lambda \leq 1$. Furthermore, the two shapes are identical (up to a scaling factor and translation vector) if and only if $\lambda = 1$. The correlation $\lambda$ measures how close the two shapes are from each other. It relies on the fact that $\varphi_t=\varphi'_t$. It this assumption is mildly violated, the classifier may still work on simple data sets, for instance to recognize the letters of the alphabet. But it may fail is the discrepancy between $\varphi_t$ and $\varphi'_t$ is significant. 

It is not always possible to satisfy $\varphi_t=\varphi'_t$ for complicated shapes consisting of multiple arcs. But it can always be done for closed, convex shapes. Also, if the shapes are identical but rotated, usually $\lambda \neq 1$. The coefficient $\lambda$ depends  on the orientation angles $\eta,\eta'$ 
of each shape, illustrated in formula~(\ref{eq2}). For this reason, $\lambda$ is also denoted as  $\lambda(\eta,\eta')$. 

\noindent To circumvent this problem, one can use
$$\lambda^* = \min_{\eta,\eta'} \lambda(\eta,\eta').$$
Then the two shapes are identical, up to the scaling factors, translation vectors, and orientations, if and only if $\lambda^*=1$. Due to symmetry, one can set $\eta=0$. In practice, the metric  $-\log(1-\lambda)$ or $-\log(1-\lambda^*)$ is used instead. See \cite{yuviz2012} for a general reference on shape correlation.

\noindent Of course, it is always possible to compare two shapes by comparing their signatures, see \cite{grauman2008}. One way to do it is as follows. For each point $P_t$ on the first shape signature, find its closest neighbor $Q_t$ on the second shape signature, and compute the distance $D_t$ between these two points. Then compute
$$D = \int_T D_t dt.$$
Repeat the operation by swapping the roles of the first and second shape: For each point $Q'_t$ on the second shape signature, find its closest neighbor $P'_t$ on the first shape signature, and compute the distance $D'_t$ between these two points. Then compute
$$D' = \int_T D'_t dt.$$
If $D=0$, the first shape is a subset of the second shape. If $D'=0$, the second shape is a subset of the first shape. If $D=D'=0$, the shapes are identical. Thus $\delta = \min(D,D')$ is a metric measuring shape similarity. It is closely related to the 
\textcolor{index}{Hausdorff distance}\index{Hausdorff distance} [\href{https://en.wikipedia.org/wiki/Hausdorff_distance}{Wiki}], albeit less
sensitive to outliers.

\subsection{Shape classification}

Now that we have a metric to compare two shapes, we can use it as a similarity measure to perform shape classification. If shapes $S_1$ and $S_2$ have
$-\log(1-\lambda)> \alpha$, we may write $S_1 \sim S_2$ to mean that they are equivalent (very similar). In this case, $\alpha=8$ is a good threshold. In character recognition, if you have a training set with thousands of hieroglyphs, you can use this technique to classify any new hieroglyph as equivalent to one or more in your training set, or as yet uncategorized (a brand new one, or one so poorly written that it is not recognizable). 

\begin{figure}%[H]
\centering
\includegraphics[width=0.6\textwidth]{shapeheart.png} %0.65
\caption{Another interesting shape}
\label{fig:34b}
\end{figure}

\section{Application}\label{s5}

In all the cases investigated, including the mathematical ones, the computations were performed using sample points on the shape, corresponding to evenly spaced values of $t$.  I used sums rather than integrals, and derivatives such as $dx_t/dt$ were replaced by differences between successive values of $x_t$. You can find the computations in my spreadsheet \texttt{Shapes4.xlsx}, located on my GitHub repository, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Spreadsheets/README.md}{here}.

\noindent {\bf Spreadsheet and data}\vspace{1ex}\\
The two main examples are:
\begin{itemize}
\item 20 points for the case pictured in Figure~\ref{fig:dash}. Here I tested 8 pairs of shapes; you can find the summary in the animated Gif, posted 
\href{https://mltechniques.com/2022/04/20/computer-vision-shape-classification-via-explainable-ai/}{here}. In addition, I introduced various levels of noise to test the discriminating power of the classifier. The amount of noise is controlled by the parameter \texttt{Precision}.
\item 1,000 points for the case pictured in Figure~\ref{fig:ctr}. Details are in the \texttt{Shape\_Signature} tab in the spreadsheet. In the same tab, you will find the computation of the weight function, the weighted centroid, and the computations related to the new coordinate system $\rho_t,\varphi_t$.
\end{itemize}
My simulations rely on \gls{gls:syntheticdata}\index{synthetic data} [\href{https://en.wikipedia.org/wiki/Synthetic_data}{Wiki}]. In other words, I use mathematically-generated shapes. The benefit is that you can generate a large class of shapes (actually, infinitely many), mimicking any existing shape, and compare the performance of various shape classifiers. 
In particular, you can assess how well a specific metric can detect different yet very similar shapes, or how it performs when various levels of noise are introduced. Modern methods combine real observations with synthetic data to further enrich \glspl{gls:trainingset}. This is known as \textcolor{index}{augmented data}\index{augmented data} [\href{https://en.wikipedia.org/wiki/Data_augmentation}{Wiki}].

Several other machine learning techniques, tested on synthetic data and accompanied by professional summary spreadsheets, are available (along with the data sets and source code), in my book ``Stochastic Processes and Simulations" \cite{vgsimulnew}. 

\section{Exercises}

The first exercise has an elegant solution. The second one is an application of the principles discussed. 

\begin{Exercise}Find the weight function satisfying $\Delta(w_t x_t)=\Delta(w_t y_t)$. This is related to the material presented in section~\ref{centr1}. It uses the same notations. \vspace{1ex} \\
\noindent{\bf Solution} \vspace{1ex} \\
You need to find $w_t$ satisfying $x_t\Delta w_t + w_t\Delta x_t = y_t\Delta w_t + w_t\Delta y_t$. Dividing by $\Delta t$, and letting $\Delta t\rightarrow 0$, we get
$$x_t \frac{dw_t}{dt}+w_t  \frac{dx_t}{dt} = y_t \frac{dw_t}{dt}+w_t  \frac{dy_t}{dt},$$
that is,
$$(x_t - y_t) \frac{dw_t}{dt} = \Big(\frac{dy_t}{dt} - \frac{dx_t}{dt}\Big) w_t.$$
This is successively equivalent to
\begin{align}
\frac{d}{dt}(\log w_t) & = \frac{1}{w_t}\frac{dw_t}{dt}=\frac{1}{x_t - y_t} \Big(\frac{dy_t}{dt} - \frac{dx_t}{dt}\Big), \nonumber \\
\log w_t & = \int \frac{1}{x_t - y_t} \Big(\frac{dy_t}{dt} - \frac{dx_t}{dt}\Big)dt +C,  \nonumber\\
w_t & = C' \exp\Big[-\int_0^t \frac{1}{y_\tau - x_\tau} \Big(\frac{dy_\tau}{d\tau} - \frac{dx_\tau}{d\tau}\Big)d\tau\Big], \nonumber
\end{align}
where $C, C'$ are constants, with $C'>0$. The value of $C'$ is unimportant when using formula~(\ref{eq1}). However, you can choose it so that the weight function integrates to one. Or you can use $C'=1$. I assumed, without loss of generality, that the integration domain $T$ is an interval containing the origin $\tau=0$.  

\noindent You can test this formula on a line segment, defined by $x_t=t, y_t=a+bt$, with $t\in[0, 1]$. This is the most basic shape other than a finite set of points! A slightly more difficult exercise is to find the weight function satisfying $|\Delta(w_t x_t)|=|\Delta(w_t y_t)|$. Exercise 1 is a starting point to solve this problem.
\end{Exercise}



\begin{Exercise}Compare the shape in Figure~\ref{fig:ctr} with that of Figure~\ref{fig:34b}, using the metrics presented in this article ($D$ and $\lambda$). These shapes correspond to equation~(\ref{eq2b}). The first one has parameters $a=7,b=6,c=8$. The second one has parameters $a=b=1,c=2\pi$. In both cases, $\eta=0$. Use 1,000 sample points on each shape for comparison purposes. Order the points according to $t$, then according to $\varphi_t$, to see the impact on $\lambda$. Set $\eta=0$, and choose $\eta'$ (the orientation of the second shape) to maximize the similarity between the two shapes. \vspace{1ex} \\
{\bf Solution} \vspace{1ex} \\
A starting point is my spreadsheet \texttt{Shapes4.xlsx}, available on my GitHub repository, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Spreadsheets/README.md}{here}. This type of shape is analyzed in the \texttt{Shape\_Signature} tab. 
\end{Exercise}

%-----------------------------------------------------------------------------------------------------------------
\Chapter{Synthetic Data, Interpretable Regression, and Submodels}{Little Known Secrets About Linear Regression}\label{chapterregression}

The technique discussed here handles a large class of problems. In this article, I focus on a simple one: linear \gls{gls:regression}. I solve it with an iterative algorithm (fixed point) that shares some resemblance to \gls{gls:gradient} boosting, using machine learning methods and explainable AI, as opposed to traditional statistics. In particular, the algorithm does not use matrix inversion. It is easy to implement in Excel (I provide my spreadsheet) or to automate as a black-box system. Also, it is numerically stable, can generalize to non-linear problems. Unlike the traditional statistical solution leading to meaningless regression coefficients, here the output coefficients are easier to understand, leading to better interpretation. I tested it on a rich collection of synthetic data sets: it performs just as well as the standard technique, even after adding noise to the data. I then show how to measure the impact of individual features, or groups of features (and feature interaction), on the solution. A model with $m$ features has $2^m$ sub-models. I show how to draw more insights by analyzing the performance of each sub-model. Finally, I introduce a new metric called {\em score} to measure model performance. Based on comparison with the base model, it is more meaningful than R-squared or mean squared error.

\hypersetup{linkcolor=red}

\section{Introduction}\label{regi1}

Here, $X$ denotes the input. It is represented as a matrix with $n$ rows and $m$ columns; $n$ is the number of observations, and $m$ the number of 
features, also called dependent variables. The response (also called independent variable or output) is a column vector with $n$ entries, and denoted as $Y$. The $m$ regression coefficients (unknown, to be estimated) are stored in a column vector denoted as $\beta$. Thus we have
\begin{equation}
Y=X\beta +\epsilon, \label{eqr1}
\end{equation} 
where $\epsilon$ -- also a column vector with $n$ entries -- is the error. The problem consists of finding a suitable $\beta$ that in some way, minimizes the error. If there was no error term, equation~(\ref{eqr1}) could be rewritten as $X^TY=X^T X\beta + \Lambda\beta - \Lambda\beta$, that is, 
$$\beta=\Lambda^{-1}X^TY +(I-\Lambda^{-1}X^TX)\beta.$$
Here $\Lambda$ is any non-singular (invertible) $m\times m$ matrix, and $ ^T$ denotes the matrix or vector transposition operator [\href{https://en.wikipedia.org/wiki/Transpose}{Wiki}]. This gives rise to the following iterative algorithm:
\begin{equation}
\beta_{k+1}=\Lambda^{-1}X^TY +(I-\Lambda^{-1}X^TX)\beta_k,\label{eqr2}
\end{equation}
starting with some initial configuration $\beta_0$ for the parameter vector (the regression coefficients). I use a diagonal matrix for $\Lambda$, so the methodology does not involve complicated matrix inversions. 

I also use the following notations: $M=X^TX$ and $S=I-\Lambda^{-1}M$. The convergence of this iterative algorithm, and how fast it converges, is entirely governed by
how fast $S^k\rightarrow 0$ as $k\rightarrow\infty$. It requires a careful choice of $\Lambda$. I discuss later how to update $\Lambda$ at each iteration $k$: in an adaptive version of this algorithm, $\Lambda$ is replaced by $\Lambda_k$ in (\ref{eqr2}), with the hope that it boosts convergence. The general term for this type of iteration, which also encompasses Newton optimization and gradient descent, is \textcolor{index}{fixed point algorithm}\index{fixed-point algorithm} [\href{https://en.wikipedia.org/wiki/Fixed-point_iteration}{Wiki}].

The remaining of this discussion focuses on the choice of $\Lambda$ and $\beta_0$, with convergence implications and computational complexity, 
tested on \textcolor{index}{synthetic data}\index{synthetic data} [\href{https://en.wikipedia.org/wiki/Synthetic_data}{Wiki}]. I show that with very few iterations, one generally gets a very good predictor, even though the estimated parameter vector is quite different from the target one used in the simulations. In short, the \gls{gls:rsquared} arising from rough approximations based on few iterations of the fixed point algorithm, is very similar to that obtained using the full standard statistical apparatus. Despite the non-statistical perspective and the absence of statistical model, I explain how to compute confidence intervals for the estimated regression coefficients and for the predicted values. The whole framework is designed to facilitate interpretation, 
and thus it falls in the category of \gls{gls:explainableai}\index{explainable AI} [\href{https://en.wikipedia.org/wiki/Explainable_artificial_intelligence}{Wiki}].

Finally, I want to offer a simple version of this method, simple enough to easily be implemented in Excel. The choice $\beta_0=0$ and $\Lambda$ minimizing the 
\textcolor{index}{Frobenius norm}\index{Frobenius norm} of $S$  [\href{https://en.wikipedia.org/wiki/Matrix_norm}{Wiki}] (see also \cite{numregxxx}), not only works well but it leads to simple formulas, and an interesting connection to \textcolor{index}{eigenvalues}\index{eigenvalue} [\href{https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors}{Wiki}] 
(see also \href{https://mathoverflow.net/questions/421309/power-of-a-matrix-largest-eigenvalue-in-absolute-value-and-convergence-acceler}{here}). The last part of this article focuses on assessing the influence of each feature, and the impact of feature interaction. The synthetic training set data discussed in section~\ref{sr001} allows you to simulate and test a large number of varied situations. Eventually, model performance is measured on a validation set, not on a training set.


\section{Synthetic data sets and the spreadsheet}\label{sr001}

Rather than testing the methodology on a few real-life data sets, I tested it on a large number of very different synthetic data sets, each with its unique correlation structure. These data sets are generated via simulations, as follows. First, generate $m$ column vectors $Z_1,\dots,Z_m$, with $Z_i$ consisting of $n$ deviates $Z_{ij}$ ($1\leq i \leq m, 1\leq j \leq n$). One of the simplest distributions to sample from is the \textcolor{index}{generalized logistic}\index{probability distribution!generalized logistic}\index{generalized logistic distribution}. See section 2.1.1 in my book on 
stochastic simulations \cite{vgsimulnew}. In this case I used
$$Z_{ij}= -\log \frac{U_{ij}^\gamma}{1-U_{ij}^\gamma},$$
where $\gamma>0$ is a parameter, and $U_{ij}$'s are independently and identically distributed uniform deviates on $[0,1]$. 
Then, I generated $m$ column vectors $X_1,\dots,X_m$ as random linear combinations of the $Z_i$'s: 
\begin{equation}
X_i=\sum_{j=1}^m w_{ij} Z_i,\quad i=1,\dots,m.\label{regwij}
\end{equation}
The $m \times m$ matrix $W=(w_{ij})$ is called the weight matrix. Here again, the $w_{ij}$ are deviates from the same family of generalized logistic distributions. Finally, the simulated response is 
$$Y=\sum_{i=1}^m \alpha_i X_i + \tau \epsilon, \quad i=1,\dots,m,$$
where $\epsilon$ (a column vector with $n$ independent entries) is an artificially generated white noise, and $\tau\geq 0$ controls the amount of noise. 
The $\alpha_i$'s can be pre-specified or randomly generated. In any case, the exact, 
pre-specified set of regression coefficients is the column vector $\alpha=(\alpha_1,\dots,\alpha_m)^T$. The estimated coefficients, at the $k$-th iteration of the fixed point algorithm, using formula~\ref{eqr2}, is the column vector $\beta_k=(\beta_{k1},\dots,\beta_{km})^T$. Thus in this setting, we are able to measure how close the estimate $\beta_k$ is to the exact value $\alpha$. In real-life applications, the exact value is never known. If it was, there would be no need to perform statistical inference.

\subsection{Correlation structure}\label{c6correlstr}

Let $\Omega_X$ (respectively $\Omega_Z$) be the $m\times m$ \textcolor{index}{covariance matrix}\index{covariance matrix} [\href{https://en.wikipedia.org/wiki/Covariance_matrix}{Wiki}] attached to $X_1,\dots,X_m$
 (respectively to $Z_1,\dots,Z_m)$. Likewise, define the correlation matrices as $R_X, R_Z$, with 
$$R_X=[D(\Omega_X)]^{-1/2}\Omega_X [D(\Omega_X)]^{-1/2}\quad \text{and } R_Z=[D(\Omega_Z)]^{-1/2}\Omega_Z [D(\Omega_Z)]^{-1/2}.$$
Here $D[A]$ is the matrix consisting of the diagonal elements of the matrix $A$. 
We have 
$$\Omega_X=W\Omega_Z W^T=(W\Omega_X^{1/2})(W\Omega_X^{1/2})^T,\quad \text{thus } W=\Omega_X^{1/2}\Omega_Z^{-1/2}.$$
These formulas allow you to easily compute $R_X$ based on the weight matrix $W$ and $\Omega_W$. Though more difficult, it is possible to solve the inverse problem: pre-specify the correlation structure $R_X$ of the data set, and then 
find $W$ that yields the desired, target $R_X$. This is best accomplished using an iterative algorithm similar to the fixed point discussed in section~\ref{regi1}, using the above formulas. 

The formulas to solve the inverse problem involve  the \textcolor{index}{square root}\index{square root (matrix)} of \textcolor{index}{positive semidefinite matrices}\index{positive semidefinite (matrix)} [\href{https://en.wikipedia.org/wiki/Square_root_of_a_matrix}{Wiki}]. The solution is not unique. See how it is done, in chapter~\ref{chapterlinear} entitled ``gentle introduction to linear algebra". Without loss of generality, a simplification consists of simulating standardized $Z_1,\dots,Z_m$ (from a distribution with zero mean and unit variance) so that $R_Z=\Omega_Z$  is the identity matrix. If in addition, the observed $X_1,\dots,X_m$ are also standardized, then $R_X=\Omega_X$, and thus, 
$W=R_X^{1/2}$. Multiple square roots exist, in the same way that $2$ and $-2$ are two ``square roots" of $4$.

\subsection{Standardized regression}

Under stable conditions,  the predicted values for $Y$ are very close to those obtained via standard statistical regression, 
even though the estimated regression coefficients may be quite different. The accompanying spreadsheet and computations are now stable. However,
in previous tests, with a different damping schedule (the matrix $\Lambda$),  sometimes the $\beta_k$ diverged as $k\rightarrow\infty$.  Yet after normalizing $\beta_k$, the instability was essentially removed and again, the predicted $Y$ was sound. I provide here the normalizing formula, to guarantee that the standard deviation of the response $Y$, denoted as $\sigma_Y$, is identical to that measured on the predicted $Y$. The new $\beta_k$, denoted as $\beta_k^*$, is computed as follows:

$$
\beta_k^* = \frac{\sigma_Y}{\sqrt{\beta_k^T\Omega_X\beta_k}} \cdot \Omega_W\beta_k.
$$
This may be useful if you modify $\Lambda$ when doing some research, as a technique to stabilize the predictions. I also included the computation of $\beta^*_k$ in the spreadsheet. However, it is best to avoid standardizing the regression coefficients when the algorithm is numerically stable. It results in more realistic variance in the predicted values as the non-corrected regression acts as a smoother, but it also comes with a price: a larger mean squared error.

To the contrary, shifting the predicted values so that their mean matches that of the observed values on the training set, is always useful. It is included in my computations (and in the spreadsheet) as a final, post-processing step. It does not impact  the R-squared. Also, it allows you to ignore the intercept parameter in the regression model. Indeed, this is an easy workaround to using an actual intercept parameter.

\subsection{Initial conditions}\label{reg3b}

The neutral choice $\beta_0=0$ as the starting vector of regression coefficients, for the iterative fixed point algorithm, works well. Another option consists of choosing regression coefficients that preserve the correlation sign between the response $Y$, and each feature $X_1,\dots,X_m$. Here, $X_i$ is the $i$-th column of the matrix $X$. Let
$$c_i= \frac{\text{Cov}[Y,X_i]}{\text{Var}[X_i]}, \quad c=(c_1,\dots,c_m)^T, \quad Q=\sum_{i=1}^m c_i X_i = Xc,$$
with $\omega$ a real number chosen to minimize the error $\epsilon^T\epsilon = (Y-\omega Q)^T(Y-\omega Q) = Y^TY-2Q^TY\omega+Q^T Q\omega^2$. We have
$$w=\frac{Y^TQ}{Q^TQ}=\frac{Y^TXc}{c^T X^T X c} \quad \text{and } \text{Var}[c_i X_i]=\text{Var}[Y]\cdot\rho^2[X_i,Y]=\sigma^2_Y\cdot\rho^2[X_i,Y],$$
where $\rho$ denotes the correlation function. Now,  $\beta_0=\omega c$ is a starting point that makes sense, easy to interpret, and better then $\beta_0=0$. It significantly reduces the residual error, over the base model $\beta=0$. In many cases, it yields a residual error almost comparable to that of the best predictors.

As an illustration, let's say that $X_2=X_3$. You should avoid highly correlated features in your data set, but in some cases the inter-dependencies among several features are strong but much harder to detect, and results in the same problem. In my example, assuming both $X_2$ and $X_3$ are positively correlated to the response $Y$, a model with $+4$ and $-2$ for the regression coefficients attached to $X_2$ and $X_3$, performs just as well as $-2, +4$ or $+1, +1$. The $\beta_0$ proposed here addresses this issue: it guarantees that the regression coefficients attached to $X_2$ and $X_3$ are both positive in this example, and identical if $X_2=X_3$. It makes the regression coefficients much easier to interpret. In addition, this technique is numerically stable and more robust.


\subsection{Simulations and Excel spreadsheet}\label{rbbb}

Formulas and computations described in section~\ref{sr001} are implemented in my spreadsheet \texttt{Regression5.xlsx}, available \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Spreadsheets/README.md}{here} on my GitHub repository. This material covers a large chunk of the spreadsheet, with the remaining explained in the next sections.

\noindent The \texttt{Test} and \texttt{Results} tabs in the spreadsheet contain the following:
\begin{itemize}
\item The random deviates $Z_1,\dots,Z_m$ are in the \texttt{Test} tab in columns \texttt{A:F}. The the zero-mean noise is in column \texttt{H}. The amount of noise in the response $Y$ is controlled by the parameter \texttt{Noise} in cell \texttt{E:5} in the \texttt{Results} tab. As a general rule, cells highlighted in light yellow 
in the \texttt{Results} tab correspond to parameters or hyper-parameters that you can modify. The parameter $\gamma$ just above in cell \texttt{E:4} is the core parameter of the generalized logistic distribution used to simulate the column vectors $Z_1,\dots,Z_m$.
\item The flag in cell \texttt{E:7} in the \texttt{Results} tab allows you to choose either $\beta_0=0$, or the special $\beta_0$ discussed in section~\ref{reg3b}.
\item  By default, the regression uses the traditional $\beta_k$. If you want to use the normalized $\beta^*_k$ instead, they are iteratively computed in cells \texttt{AX2:ES7} in the
\texttt{Test} tab. For instance, cells \texttt{BL2:BL7} represent $\beta_{15}^*$, that is, the $15$th iterate ($k=15$) in the fixed point algorithm, stored as a column vector. The standard $\beta_k$ are computed in cells \texttt{AX20:ES25} in the same tab.
\item Column \texttt{I} in the \texttt{Test} tab is the response $Y$. The features $X_1,\dots,X_m$, also called independent variables, are in columns \texttt{J:O} in the same tab. They are generated as linear combinations of the random vectors $Z_1,\dots,Z_m$. More precisely, $X=WZ$ as per equation~(\ref{regwij}), where
$W$ is the $m\times m$ weight matrix. The random weight matrix $W$ is stored in cells \texttt{B15:G20} in the \texttt{Results} tab. The actual, true (random) regression
parameters are just above in the same tab, in cells \texttt{B12:G12}.
\item Intermediary computations are in the \texttt{Test} tab. For instance, the $m\times m$ matrix $X^TX$ is stored in cells \texttt{AP2:AU7}, 
the vector $X^TY$ in cells  \texttt{AN2:AN7}, the correlation matrix $R_X$ in cells  \texttt{AE3:AJ8}, and the covariance matrix $\Omega_X$ in cells  \texttt{AE12:AJ17}.
\end{itemize} 
The interactive spreadsheet extensively uses the \texttt{SumProduct} and \texttt{Transpose} Excel functions, to easily multiply a row vector by a column vector with just one simple operation. It also makes matrix multiplications easier.

\section{Damping schedule and convergence acceleration}

The $m\times m$ diagonal matrix $\Lambda^{-1}$ in equation~(\ref{eqr2}) is called the damping parameter, or \textcolor{index}{preconditioner}\index{preconditioning} 
[\href{https://en.wikipedia.org/wiki/Preconditioner}{Wiki}] of the fixed point iteration. It governs the rate of convergence.
The fixed point algorithm converges if $|\varphi|<1$, where $\varphi=\varphi(S)$ is the largest eigenvalue of $S=I-\Lambda^{-1} X^T X$, in absolute value. 
The smaller $|\varphi|$, the faster the convergence. The convergence speed is eventually determined by how fast $S^k\rightarrow 0$ as $k\rightarrow\infty$, itself being a function of $|\varphi|$. Thus, it makes sense to choose $\Lambda$ so that $S$ is close to zero. One way to do it is to choose $\Lambda$ that minimizes the Frobenius norm of $S$, or in other words, $\Lambda$ that minimizes the sum of the square of the coefficients of $S$. This leads to 
\begin{equation}
\lambda_i^{-1}=\frac{1}{m_{ii}}\sum_{j=1}^m m_{ij}^2, \quad i=1,\dots,m \label{zizi}
\end{equation}
where $\lambda_1,\dots,\lambda_m$ are the diagonal elements of the diagonal matrix $\Lambda$, and $m_{ij}$ is the $j$-th element in column $i$, of the matrix $M=X^T X$. In practice, with this choice of $\Lambda$, assuming $M$ is not singular, the fixed point iteration always converges, and $m_{ii}>0$. See discussion on this topic, \href{https://mathoverflow.net/questions/421309/power-of-a-matrix-largest-eigenvalue-in-absolute-value-and-convergence-acceler}{here}.

\subsection{Spreadsheet implementation}\label{rbbb2}

The $\lambda_i$'s are computed in cells \texttt{AL2:AL7} in the \texttt{Test} tab. Also, $|\varphi|$ is iteratively computed in cells AY59:ES59 in the same tab. The method used in the spreadsheet to compute $|\varphi|$ is known as \textcolor{index}{power iteration}\index{power iteration}\index{eigenvalue!power iteration} [\href{https://en.wikipedia.org/wiki/Power_iteration}{Wiki}]. 
See also \href{https://math.stackexchange.com/questions/2554808/finding-approximation-of-largest-eigenvalue}{here}. 

Now we have everything in place to compute the regression coefficients. They are found in cells \texttt{K5:P5} (computation based on $k=15$ iterations) and  \texttt{K6:P6} (based on $k=100$ iterations) in the \texttt{Results} tab. The final value of $|\varphi|$ is in cell \texttt{K21}, also in the \texttt{Results} tab. Note that values of $|\varphi|$ above $0.95$ means that the system is \textcolor{index}{ill-conditioned}\index{ill-conditioned problem} [\href{https://en.wikipedia.org/wiki/Condition_number}{Wiki}]. For instance, some features are almost linear combinations of other features. It typically results in poor performance. In this case, using few iterations ($k=15$) together with the initial $\beta_0$ suggested in section~\ref{reg3b}, works best. It also avoids \gls{gls:overfitting} issues.

The predicted values of $Y$, obtained after $k=15$ and $k=100$ iterations, are in columns \texttt{P} and \texttt{Q} respectively, in the \texttt{Test} tab. The predicted values obtained with $\beta_0$ alone (as computed in section ~\ref{reg3b}) are in column \texttt{S}, while those obtained with traditional regression are in column \texttt{U} and produced with the \texttt{Linest} Excel function. The filtered response, obtained by removing the artificial noise introduced in the observed (synthetic) data, is in column \texttt{R}. The computational complexity of the fixed point regression is the same as multiplying two $m\times m$ matrices, multiplied by the number of iterations in the fixed point algorithm. 


\subsection{Interpretable regression with no overfitting}

The regression coefficient vector $\beta_0$ introduced in section~\ref{reg3b} is intuitive, robust and preserves the correlation signs: by design, a feature positively correlated with the
response $Y$ gets assigned a positive correlation coefficient, as previously discussed.  In addition, its performance is nearly as good as optimum regression
 coefficients,  in many situations. It definitely performs well above the base model, unless the amount of noise is substantial. But in that case, all models perform badly, unable to
improve over the base model. See section~\ref{rres} for performance comparison. If your problem is ill-conditioned (for instance some features are nearly identical or linear combinations of other features), traditional techniques will lead to meaningless regression coefficients, or fail entirely. To the contrary, the $\beta_0$ in question handles this situation very well.

All of this makes the $\beta_0$ in question a good starting point for the fixed point iteration, if the goal is to obtain a robust solution, easy to interpret, and not prone to overfitting. It can be improved by using a few iterations of the fixed point algorithm. The more iterations, the closer you get to the standard ``exact" solution, but you eventually lose interpretability. With fewer iterations, you may still get a good performance, yet avoid many of the aforementioned problems. One way to decide when to stop is to run many iterations, and compare your \gls{gls:rsquared} obtained after (say) $10$ versus $100$ iterations. If the difference is negligible, use the regression coefficients obtained at iteration $k=10$. Instead of using R-squared, I suggest to use the metric $s$  defined in section~\ref{pasr}.  




\subsection{Adaptive damping}

A possible way to boost convergence is to use a matrix $\Lambda$ that depends on $k$, and denoted as $\Lambda_k$. The most simple example is when
$\Lambda_k=\lambda(k)\cdot I$ where $\lambda(k)$ is a real number and $I$ is the $m\times m$ identity matrix. 
Then, choose $\lambda(k)$ that minimizes $||(I-\lambda^{-1})\beta_k ||_2$, that is
\begin{align}
\lambda(k) & =\underset{\lambda}{\arg\min} \text{ } ||(I-\lambda^{-1})\beta_k ||_2 \nonumber\\
 &= \underset{\lambda}{\arg\min} \text{ } [(I-\lambda^{-1}M)\beta_k]^T (I-\lambda^{-1}M)\beta_k \nonumber\\
&= \underset{\lambda}{\arg\min} \text{ }  \beta_k^T\beta_k -2\lambda^{-1}\beta_k^T M \beta_k+ \lambda^{-2}(M\beta_k)^T M\beta_k \nonumber \\
& =  \frac{(M\beta_k )^T M \beta_k}{\beta_k^T M \beta_k} .\nonumber
\end{align}
Again, $M=X^TX$. Preliminary tests did not show large improvements over using a fixed $\Lambda$. Indeed, about 20\% of the time, instead of converging, the successive iterates of the regression coefficients oscillate. Yet this issue is easy to address, and adaptive damping -- where $\Lambda$ depends on $k$ -- has potential to handle data sets with a larger $m$ (the number of features). The worst case by far in my 100 tests or so, is pictured in Figure~\ref{fig:regdash}. The drawback, compared to using preconditioning only (a fixed $\Lambda$) is that it requires more computations. For a recent machine learning application of preconditioners in a similar context, see
\cite{oxford2020}.

\begin{figure}%[H]
\centering
\includegraphics[width=0.8\textwidth]{regbad4.png}
%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Regression coefficients oscillating when using adaptive damping}
\label{fig:regdash}
\end{figure}


\section{Performance assessment on synthetic data}\label{pasr}


I use three metrics to measure the  \gls{gls:goodnessoffit} between the observed (synthetic) response $Y$ and the predicted response denoted here as $P$. These metrics are computed on a \gls{gls:validset}, 
not on the training set: this is a standard \gls{gls:crossvalid} procedure to avoid overfitting and performance inflation. Thus, what I call \gls{gls:rsquared} is actually closer, but not identical,  to 
``predictive R-squared" and PRESS statistics [\href{https://en.wikipedia.org/wiki/PRESS_statistic}{Wiki}]. The same applies to MSE.
The synthetic control set is in the \texttt{Control} tab in the spreadsheet. 

\noindent The three metrics are:
\begin{itemize}
\item The R-squared $r$, with $0\leq r\leq 1$, is the square of the correlation between $Y$ and $P$. It is also equal to $[\sigma^2_Y - (Y-P)^T (Y-P)]/\sigma_Y^2.$
\item The root mean squared error or RMSE [\href{https://en.wikipedia.org/wiki/Root-mean-square_deviation}{Wiki}], defined as $e = \sqrt{(Y-P)^T (Y-P)}/n$, where $n$ is the number of observations. It is a much better measure of the actual error, compared to the mean squared error (MSE). In particular, if $Y$ is measured in (say) miles,
then RMSE is measured in miles, while MSE is measured in square miles. Likewise, if $Y$ is measured in years, RMSE is measured in years  while MSE is measured in square years -- a meaningless metric.
\item The score $s$, with $0\leq s \leq 1$, defined as $s=1-e(P)/e(P_0)$. Here, $P_0$ is the base model, corresponding to $\beta=0$ (thus, the predicted value is constant after adjustment, equal to the mean value of $Y$ computed on the training set). In particular, $s$ measures the improvement over the base model, using RMSE ratios, while $r$ does the same using MSE ratios instead. In my opinion, $s$ is more intuitive and more realistic than $r$. It is related to $r$ via the formula $r=2s-s^2$, or equivalently, $s=1-\sqrt{1-r}$. We always have $r\geq s$, so $r$ is an inflated measure of  \gls{gls:goodnessoffit}.
\end{itemize}
Since the model is trained on the test data set, but model performance is measured on the validation set, the above performance metrics are hybrid. 
In particular, as a result, the relationship $r=2s-s^2$ is almost satisfied, but not exactly. Also, on rare occasions (with very noisy data), $r$ can be slightly negative. The performance results are shown in the \texttt{Results} tab, in cells \texttt{J13:R19}. I also included the performance metrics for the simple model consisting in using the regression coefficient vector $\beta=\beta_0$ 
defined in section~\ref{reg3b}. In the spreadsheet, this model is referred to as ``Predicted $P_b$".

In addition, the spreadsheet allows you to choose which features to include or not in the model, in the fixed point iteration. The \gls{gls:featureselection} flags are stored in cells \texttt{B4:B9} in the \texttt{Results} tab. The default value is $1$, corresponding to inclusion. This is particularly useful to test how much model improvement is gained by using all features, versus a subset of them. On real data, some features are usually missing because the model developer was not aware of them, and they are not captured in the data set: in other words, they are unobserved. The feature selection flags allow you to compare the performance on observed data, versus the performance obtained with a dream data set that would include all the relevant features.

Finally, the use of \gls{gls:syntheticdata} offers a big benefit: you can test and benchmark algorithms on millions of very different data sets, all at once. This assume that the synthetic data is properly generated. It is both an art and a science to design such data sets. It typically involves a decent amount of mathematics, sometimes quite advanced. It is my hope that this article will help the reader build good quality, rich synthetic data that covers a large spectrum of potential real life situations. Other synthetic data sets are found in chapter~\ref{chapterfastclassif}
 featuring synthetic clusters, and in chapter~\ref{chaptershapes} entitled ``Computer Vision: Shape Classification via Explainable AI", featuring synthetic shapes.


\subsection{Results}\label{rres}

I tested the methodology on a synthetic dataset with $n=1000$ observations and $m=6$ features. The validation set also has $1000$ observations. Because the data is artificially generated, the exact regression coefficients are known. They are listed in the row labeled ``Exact" in table~\ref{tabcoeff}. The observed data $Y$ is a mixture of the exact data  and artificial noise. The amount of noise is controlled by the parameter \texttt{Noise} in the spreadsheet. 
The exact data is the vector $X\beta$, with $\beta$ being the true, pre-specified regression coefficients (also artificially generated). Note that $X$ is also artificially generated, using the method described in section~\ref{rbbb}.


\begin{figure}%[H]
\centering
\includegraphics[width=0.95\textwidth]{regressb1.png} 
%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Convergence of regression coefficients (left) and distribution of residual error (right)}
\label{fig:regb}
\end{figure}

The metric $r$ in the \texttt{Exact} row in table~\ref{tabcoeff} measures the R-squared between the observed data $Y$, and the exact data. In practice, the exact data is not known. But one of the benefits of using synthetic data, is that we can simulate both the unobserved, exact data, and the observed, noisy, unfiltered version, denoted as $Y$.

The metric $r$ in table~\ref{tabcoeff} is the R-squared, but measured on the validation set, not on the training set. By contrast, 
in figure~\ref{fig:rega}, the R-squared on the right plot, attached to the blue dots and blue \gls{gls:regression} line,  is computed on the training set. On the left plot, it is computed on the validation set, and it matches the $r$ value attached to  $\beta_{15}$ in table~\ref{tabcoeff}. The blue dots in figure~\ref{fig:rega} is the scatterplot of $Y$ versus the predicted values obtained with $\beta_{15}$. The orange dots is the scatterplot of $Y$ versus the exact (unobserved) data.

\begin{figure}%[H]
\centering
\includegraphics[width=0.97\textwidth]{regressa1.png} %0.97
%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Goodness-of-fit: training set (right) versus validation set (left)}
\label{fig:rega}
\end{figure}

Figure~\ref{fig:regb} (left plot) shows how the six regression coefficients converge in the fixed point iterations, starting with the vector $\beta_0=0$. The same matrix $\Lambda$ is used at all times, unlike in figure~\ref{fig:regdash}. The oscillating behavior seen in figure~\ref{fig:regdash} never occurs with the fixed $\Lambda$ 
defined by equation~\ref{zizi}. The right plot in figure~\ref{fig:regb} is a scatterplot of $Y$ versus the residual error $\epsilon=Y-X\beta$, using $\beta=\beta_{15}$ measured on the \gls{gls:trainingset}. In idealized statistical models, $\epsilon$ is assumed to be a white noise. Clearly, this is not the case here, with larger observed values (on the horizontal axis) having a tendency to have a larger error. However, this data is very useful to simulate $\epsilon$, to use in 
model-free confidence and prediction intervals, as
discussed in section~\ref{rdfci}.



\begin{table}%[H]
\small
\[
\begin{array}{l|cc|rrrrrr}
\hline
\text{Method}  &  r & s &  \multicolumn{6}{c}{\text{Regression coefficients}}\\
\hline
\text{Exact}	&	0.891	&	67.0\%	&	-1.513	&	-0.202	&	1.514	&	-1.647	&	1.079	 &	-0.799	\\
\beta_{15}	&	0.828	&	58.5\%	&	-1.726	&	0.025	&	1.544	&	-1.208	&	1.045	&	-0.758	\\
\beta_{100}	&	0.830	&	58.8\%	&	-1.460	&	-0.204	&	1.517	&	-1.631	&	1.103	&	-0.746	\\
\beta_0	&	0.596	&	36.2\%	&	-1.630	&	0.116	&	0.072	&	-0.947	&	0.627	&	-0.689	\\
\text{Excel}	&	0.823	&	58.7\%	&	-1.426	&	-0.236	&	1.542	&	-1.666	&	1.153	&	-0.669	\\
\hline
\end{array}
\]
\caption{\label{tabcoeff}Regression coefficients and performance metrics $r, s$ based on methodology}
\end{table}

The synthetic data set used in tables~\ref{tabcoeff} and~\ref{tabcorrel} is different from that used in section~\ref{rdfci}.  In particular, I increased the amount of noise in the validation set, compared to the training set. I did it to better emulate real data, where performance is usually lower outside the training set. It explains why the R-squared is sensibly better when measured on the training set, as opposed to the validation set. 

The performance metric $s$ in table~\ref{tabcoeff} is discussed in section~\ref{pasr}. It is much more meaningful than the \gls{gls:rsquared} $r$. If measured on the training set, we would have $s=1-\sqrt{1-r}$. Here, it is measured on the validation set. Yet the equality is still almost satisfied. Also in table~\ref{tabcoeff}, the row labeled $\beta_0$ corresponds to the special, basic regression method discussed in section~\ref{reg3b}. The \texttt{Excel} row corresponds to the standard regression coefficients computed by any statistical package, in this case with the Excel \texttt{Linest} function. Note that $\beta_{15},\beta_{100}$ and the Excel regression yield different sets of regression coefficients. Yet the three of them have the same performance. 

The easiest to interpret is $\beta_0$, followed by $\beta_{15}$: in particular, they yield regression coefficients with the same sign as the correlations (the row labeled $Y$ in table ~\ref{tabcorrel}). If you run the fixed point iteration long enough, eventually $\beta_k$ will tend to the Excel solution, as $k\rightarrow\infty$. Because of the noise in the dataset, even with infinitely many iterations, it is impossible to retrieve the exact, pre-specified regression coefficients featured in the \texttt{Exact} row.
However, $\beta_{100}$ and \texttt{Excel} provide good approximations.

%xxxx discuss correl table
%xxx	Add link to MLT, put on GitHub w link back on ML/spreadsheets/readme	

\begin{table}%[H]
\small
\[
\begin{array}{lrrrrrrr}
\hline
  &  Y & X_1 & X_2  & X_3 & X_4 & X_5 & X_6\\
\hline
Y	&	1.000	&	-0.642	&	0.077	&	0.033	&	-0.481	&	0.404	&	-0.271	\\
X_1	&	-0.642	&	1.000	&	-0.216	&	0.458	&	0.559	&	-0.266	&	-0.162	\\
X_2	&	0.077	&	-0.216	&	1.000	&	-0.061	&	-0.840	&	-0.447	&	0.762	\\
X_3	&	0.033	&	0.458	&	-0.061	&	1.000	&	0.048	&	-0.543	&	-0.088	\\
X_4	&	-0.481	&	0.559	&	-0.840	&	0.048	&	1.000	&	0.281	&	-0.620	\\
X_5	&	0.404	&	-0.266	&	-0.447	&	-0.543	&	0.281	&	1.000	&	-0.700	\\
X_6	&	-0.271	&	-0.162	&	0.762	&	-0.088	&	-0.620	&	-0.700	&	1.000	\\
\hline
\end{array}
\]
\caption{\label{tabcorrel}Correlation matrix}
\end{table}

\subsection{Distribution-free confidence intervals}\label{rdfci}

This will be the topic of a future article. Here I provide a quick overview, using \textcolor{index}{resampling}\index{resampling} [\href{https://en.wikipedia.org/wiki/Resampling_(statistics)}{Wiki}] and related data-driven techniques. Let $Y=P+\epsilon, P=X\beta$ be the regression model, with $Y$ the observed (synthetic) response, $P$ the predicted values, $X$ the $n\times m$ matrix
representing the features (with values synthetically generated; $n$ is the sample size), $\beta$ the regression coefficient vector, and $\epsilon$ the residual error vector. In other words, $\beta$ is the vector minimizing $(Y-X\beta)^T(Y-X\beta)$, or its approximation using the fixed point algorithm.

The starting point is to randomly shuffle the $n$ entries of $\epsilon$, to create a new error vector $\epsilon'$. Let $Y'=Y-\epsilon+\epsilon'$, that is  $Y'=X\beta+\epsilon'$. Now find $\beta'$ that minimizes 
$(Y'-X\beta')^T(Y'-X\beta')$. Now you get a new set of regression coefficients, $\beta'$. Repeat this operation $100$ times, and you get $100$ sets of regression coefficients: $\beta, \beta', \beta''$ and so on. 

This model-free procedure immediately provides confidence intervals for the regression coefficients, by looking at the 
%\textcolor{index}{empirical distribution}
\gls{gls:empdistr}
\index{empirical distribution} [\href{https://en.wikipedia.org/wiki/Empirical_distribution_function}{Wiki}] of each regression coefficient across the $100$ data sets. Now for a particular feature vector $(x_1,\dots,x_m)$ -- whether part of the training set or not -- the predicted value can be obtained in $100$ different ways: $p=x\beta, p'=x\beta', p''=x\beta''$ and so on. The empirical distribution of $p,p',p''$ and so on, provides a \textcolor{index}{prediction interval}\index{prediction interval} for the predicted value of the response, at the arbitrary location $x$ in the feature space. Another way to do it to resample $\epsilon$ with replacements (rather than by random permutations). This is then a \gls{gls:bootstrap}\index{bootstrapping} technique [\href{https://en.wikipedia.org/wiki/Bootstrapping_(statistics)}{Wiki}].

This methodology assumes that the individual residual errors are independently and identically distributed. I discuss how to address this issue in 
section~\ref{rpb5}. Also it assumes that the error is additive, not multiplicative. In the latter case, one might want to work with a transformed version of $Y$ rather than $Y$ itself. For an Excel implementation, see the \texttt{Regression5\_Static} spreadsheet on my GitHub repository, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Spreadsheets/README.md}{here}. In the \texttt{Test} tab, column \texttt{AD} corresponds to $\epsilon'=\epsilon$, and column \texttt{AE} to $\epsilon''$. Sort columns \texttt{AE:AF} by \texttt{AF} (uniform deviates) to reshuffle the residual error.  Then $Y$ is automatically replaced by $Y''$ in 
column  \texttt{I}, and the new regression coefficient vector $\beta''$ is in cells \texttt{K6:P6} in the \texttt{Results} tab. 

\subsubsection{Parametric bootstrap}\label{rpb5}

Another option is to use Gaussian deviates for $\epsilon', \epsilon''$ and so on. They need to have the same variance as the one computed on the 
observed residual error $\epsilon$. This approach is known as \textcolor{index}{parametric bootstrap}\index{parametric bootstrap}. 

If $\epsilon$ is auto-correlated, for example if you are dealing with time series and one of the features is the time, it is possible to use an \gls{gls:armodels} process that has the samee auto-correlation structure as $\epsilon$, to simulate $\epsilon', \epsilon''$ and so on. Likewise, if $\epsilon$ is correlated to $Y$, this can be handled with some
appropriate parametric model. Parametric bootstrap for linear regression is discussed in \cite{parambr}. Distribution-free predictive inference for regression is discussed in \cite{predictrr}.


\section{Feature selection}

In section~\ref{combor1}, I compare the performance of the regression coefficients obtained for each of the potential feature combinations. The performance metric is a significantly improved version of  the R-squared; also, it is applied to the validation set, not to the training set. Then, in section~\ref{stepw}, I discuss stepwise \gls{gls:featureselection}\index{feature selection} techniques (forward, backward), 
adding or removing one or two features at a time, based on the feature table built in section~\ref{combor1}. The comprehensive feature summary table allows 
you to quickly perform stepwise regression (which is more interpretable than a full fledged regression), and to assess whether or not this technique fails to catch good enough configurations, at least for the data set investigated here.

\subsection{Combinatorial approach} \label{combor1}

Here I look at all the  $2^m - 1$ possible configurations of features $X_1,\dots,X_m$, to asses the importance of individual features and feature interaction, in a way that is more insightful than looking at cross-correlations between features. For each configuration, I computed the regression coefficient vector using three methods: fixed point with $15$ iterations, fixed point with $100$ iterations (starting with $\beta_0=0$ in both cases), and then the special $\beta_0$ alone (no iteration) defined 
in section~\ref{reg3b}. All rankings, unless otherwise specified, are based on  $100$ iterations of the fixed point algorithm. 

I use the score metric $s$ defined in section~\ref{pasr} -- a meaningful, monotonic function of the \gls{gls:rsquared} -- to measure performance. If you use the R-squared instead, the rankings would still be the same. The full model has $m=6$ features. All other models are sub-models, referred to as configurations: they miss one or more features. The metric $s$ is computed on the validation set, not the training set. 

The summary table in in the \texttt{Regression5\_Static.xlsx} spreadsheet, in the \texttt{Results} tab: see columns \texttt{U:AS}. 
The spreadsheet is available \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Spreadsheets/README.md}{here}. It has the same structure as the spreadsheet described in sections~\ref{sr001}, \ref{rbbb}, and~\ref{rbbb2}.The difference is that the data set is static in this case, so you can't generate different data sets. The methodology is inspired by the book ``Interpretable Machine Learning" \cite{cmol}, particularly chapter 7 focusing on permutation feature importance and feature interactions. The higher $s$, the better the performance. Note that if the performance was computed on the training set rather than the validation set, then we would have $0\leq s\leq 1$. 



\begin{table}%[H]
%\small
\[
\begin{array}{lccc}
\hline
 m &  s(\beta_{15}) & s(\beta_{100}) & s(\beta_0)  \\
\hline
1 & 0.2892 &	0.2892 &	0.2892\\
2 &	0.4080 &	0.4080 &	0.4075\\
3 &	0.4593 &	0.5019 &	0.4357\\
4 &	0.5175 &	0.5541 &	0.4382\\
5 &	0.5332 &	0.5865 &	0.4274\\
6 &	0.5243 &	0.5889 &	0.3597\\
\hline
\end{array}
\]
\caption{\label{tabrr123}Best performance given $m$ (number of features)}
\end{table}

Table~\ref{tabrr123} shows the top achievable performance given $m$ (the number of features used in the computation) on a same (synthetic) data set. The three performance columns correspond respectively to $15$ iterations, $100$ iterations, and the special $\beta_0$.  The table is also in the spreadsheet in cells \texttt{AP12:AS19}, where you can find the detailed computations. For the performance of each individual configuration, see tables~\ref{tabrr00} and ~\ref{tabrr01}: a configuration denoted as $3,4,6$ means that only $X_3, X_4$ and $X_6$ are used for the computation of the regression coefficients.


\noindent Below are some highlights, after ranking the feature configurations according to performance.  
\begin{itemize}
\item The top $8$ configurations include the simultaneous presence of features $X_1,X_2$ and $X_6$. While $X_1$ is strongly correlated to $Y$, the features $X_2$ and $X_6$ are not. Also, $X_2$ is negatively correlated to $X_1$.
\item The worst configurations are $X_2$ alone, $X_6$ alone, and $X_2, X_6$ combined together. This is surprising, since $X_2$ and $X_6$ are both required in all top configurations. It shows that this type of feature interaction analysis is more powerful than looking at the feature cross-correlation structure. 
\item The worst configuration out of $63$, consisting of $X_2$ alone, is so bad that it is the only one with a negative $s$ when computed on the validation set. It is worse than using no feature at all (that is, using the mean value of $Y$ for your predictions).
\item The $4$th configuration has only $m=4$ features and does quite well. It is also the best possible configuration, among all configurations 
based on $\beta_0$ alone (defined in section~\ref{reg3b}). It is thus the best configuration if you do not use a single iteration of the fixed point algorithm. With the full fixed point algorithm, only one configuration with fewer than $4$ features, beats that performance (it needs $3$ features only).  
\item The top configuration performs just as well as the classic statistical solution with $m=6$, but it also requires all $6$ features. 
\item The biggest improvement is from using two features, over one feature. Beyond two features, gains are smaller. 
\item The regression coefficient attached to $X_5$ is positive in the top configuration, but absent (zero) or negative in all the other top 8 configurations except the very top one. A negative value makes more sense, since the correlation between $Y$ and $X_5$ is strongly negative.
\item If for whatever reason, the feature $X_6$ is not in your data set, you miss all the top $14$ configurations, out of $63$. This is really the feature that you can't afford not to have. Surprisingly though, it is not highly correlated to $Y$, much less than $X_1, X_3$ or $X_5$. It shows its power not when left alone, but when combined
with other features. A bit like charcoal that sounds inoffensive, but when combined with sulfur and saltpeter, makes gun powder.
\end{itemize}
Of course identifying the ideal configuration is like cherry-picking. However, the goal is to minimize over-fitting and favor simplicity and interpretability. In that regard, the $4$th configuration is my favorite, as it uses only $m=4$ features out of $6$, and it is also the winner if you use the special $\beta_0$ alone with that same feature configuration. My second pick is the 15th configuration if you use the special $\beta_0$ alone. It is the second best configuration if using $\beta_0$ alone, it uses only $3$ features, and it performs just as well (at that level) as using $100$ iterations of the fixed point algorithm. It is also the best configuration without $X_6$. 

\subsection{Stepwise approach}\label{stepw}

Based on the tables~\ref{tabrr00} and~\ref{tabrr01}, it is easy to reconstruct how \textcolor{index}{stepwise regression}\index{stepwise regression} progresses [\href{https://en.wikipedia.org/wiki/Stepwise_regression}{Wiki}].  This method is a stepwise feature
selection procedure. Here (say) $\{2,3\}$ denotes the configuration consisting of the two features $X_2, X_3$.

\begin{itemize}
\item Forward regression, adding one feature at a time: 
$$1 \rightarrow \{1, 3\} \rightarrow \{1, 3, 6\} \rightarrow \{1, 2, 3, 6\}
 \rightarrow \{1, 2, 3, 4, 6\}  \rightarrow \text{Full}.$$   
The scores $s(\beta_{100})$ are respectively $0.289,0.408, 0.437, 0.554, 0.587, 0.589$ and the ranks 
are respectively $40, 23, 15, 4, 2, 1$. 

\item Backward regression, removing one feature at a time: 
$$\text{Full} \rightarrow \{1, 2, 3, 4, 6\} 
\rightarrow \{1, 2, 3, 6\} \rightarrow \{1, 2, 6\} \rightarrow \{1, 2\} \rightarrow 1.$$ 
The scores $s(\beta_{100})$ are respectively $0.589, 0.587, 0.554, 0.502, 0.318, 0.289$, and the ranks 
are respectively $1, 2, 4, 8, 33, 40$. 

\item Pairwise forward regression, adding two features at a time:   $\{1,3\} \rightarrow \{1,2,3,6\} \rightarrow \text{Full}.$

\item Pairwise backward regression, removing two features at a time: $\text{Full} \rightarrow \{1,2,3,6\} \rightarrow \{1,3\}.$
\end{itemize}
Note that the best configurations respectively with $m=5, 4, 3, 2, 1$ features, are $\{1, 2, 3, 4, 6\}, \{1, 2, 3, 6\}, \{1, 2, 6\},$ $\{1, 3\}, \{1\}$ scored respectively $0.587, 0.554, 0.502, 0.408, 0.289$, and ranked respectively $2, 4, 8, 23, 40$. So the stepwise procedures,
while not fully optimum when involving only one configuration at a time, are nevertheless doing a rather decent job on this data set. The forward regression is easily  interpretable if you stop at $4$ features.

\section{Conclusion}

Using linear regression as an example, I illustrate how to turn the obscure output of a machine learning technique, into an interpretable solution.  The method described here also shows the power of synthetic data, when properly generated. The use of synthetic data offers a big benefit: you can test and benchmark algorithms on millions of very different data sets, all at once. 

I also introduce a new model performance metric, superior to R-squared in many respects, and based on \gls{gls:crossvalid}. 
The methodology leads to a very good approximation, almost as good as the exact solution on noisy data, with few iterations, natural regression coefficients easy to interpret, while avoiding over-fitting. In fact, given a specific data set, many very different sets of regression coefficients lead to almost identical predictions. It makes sense to choose the ones that offer the best compromise between exactness and interpretability. 

My solution, which does not require matrix inversion, is also simple, compared to traditional methods. Indeed, it can easily be implemented in Excel, without requiring any coding. Despite the absence of statistical model, I also show how to compute confidence intervals, using parametric and non-parametric bootstrap techniques.

%--

%\pagebreak


\begin{table}%[H]
\footnotesize
\[
\begin{array}{lcc|rrr|rrrrrr}
\hline
\text{Rank}	&	\text{Configuration}	&	m	&	s(\beta_{15}) & s(\beta_{100}) & s(\beta_0)	&	\multicolumn{6}{c}{\text{Regression coefficients attached to } \beta_{100}}		\\
\hline
1	&	1,2,3,4,5,6	&	6	&	0.524	&	0.589	&	0.360	&	1.421	&	2.323	&	1.158	&	-1.296	&	0.211	&	2.221	\\
2	&	1,2,3,4,6	&	5	&	0.533	&	0.587	&	0.427	&	1.419	&	2.411	&	1.013	&	-1.119	&		&	2.230	\\
3	&	1,2,3,5,6	&	5	&	0.515	&	0.564	&	0.381	&	1.703	&	2.366	&	0.698	&		&	-0.405	&	1.873	\\
4	&	1,2,3,6	&	4	&	0.517	&	0.554	&	0.438	&	1.838	&	2.163	&	0.957	&		&		&	1.742	\\
5	&	1,2,5,6	&	4	&	0.488	&	0.549	&	0.323	&	1.672	&	2.909	&		&		&	-0.914	&	2.246	\\
6	&	1,2,4,5,6	&	5	&	0.489	&	0.548	&	0.287	&	1.718	&	2.849	&		&	0.205	&	-0.961	&	2.141	\\
7	&	1,2,4,6	&	4	&	0.463	&	0.516	&	0.285	&	1.853	&	2.962	&		&	-0.872	&		&	2.566	\\
8	&	1,2,6	&	3	&	0.459	&	0.502	&	0.308	&	2.165	&	2.747	&		&		&		&	2.170	\\
9	&	2,3,4,5,6	&	5	&	0.407	&	0.501	&	0.273	&		&	1.751	&	2.177	&	-4.243	&	1.080	&	2.995	\\
10	&	1,3,4,5,6	&	5	&	0.444	&	0.474	&	0.359	&	1.077	&		&	2.052	&	-1.527	&	1.011	&	0.898	\\
11	&	2,3,4,6	&	4	&	0.439	&	0.472	&	0.357	&		&	2.028	&	1.453	&	-3.138	&		&	2.886	\\
12	&	1,3,4,6	&	4	&	0.443	&	0.442	&	0.426	&	1.128	&		&	1.391	&	-0.306	&		&	0.565	\\
13	&	1,3,5,6	&	4	&	0.436	&	0.439	&	0.380	&	1.405	&		&	1.533	&		&	0.309	&	0.459	\\
14	&	3,4,5,6	&	4	&	0.366	&	0.437	&	0.272	&		&		&	2.784	&	-4.031	&	1.688	&	1.826	\\
15	&	1,3,6	&	3	&	0.437	&	0.437	&	0.436	&	1.261	&		&	1.363	&		&		&	0.471	\\
16	&	1,2,3,4,5	&	5	&	0.413	&	0.417	&	0.315	&	1.884	&	0.641	&	1.098	&	0.877	&	-0.246	&		\\
17	&	1,2,3,4	&	4	&	0.414	&	0.416	&	0.367	&	1.879	&	0.550	&	1.268	&	0.646	&		&		\\
18	&	1,2,3,5	&	4	&	0.412	&	0.415	&	0.345	&	1.692	&	0.360	&	1.512	&		&	0.254	&		\\
19	&	1,2,3	&	3	&	0.413	&	0.414	&	0.408	&	1.599	&	0.411	&	1.367	&		&		&		\\
20	&	1,3,5	&	3	&	0.408	&	0.411	&	0.344	&	1.570	&		&	1.607	&		&	0.347	&		\\
21	&	1,3,4,5	&	4	&	0.408	&	0.411	&	0.314	&	1.582	&		&	1.549	&	0.135	&	0.274	&		\\
22	&	1,3,4	&	3	&	0.409	&	0.409	&	0.367	&	1.538	&		&	1.369	&	0.376	&		&		\\
23	&	1,3	&	2	&	0.408	&	0.408	&	0.408	&	1.412	&		&	1.417	&		&		&		\\
24	&	1,2,4,5	&	4	&	0.379	&	0.397	&	0.245	&	2.168	&	1.226	&		&	2.320	&	-1.384	&		\\
25	&	3,4,6	&	3	&	0.374	&	0.374	&	0.356	&		&		&	1.704	&	-2.066	&		&	1.319	\\
26	&	2,4,5,6	&	4	&	0.319	&	0.372	&	0.200	&		&	2.604	&		&	-2.077	&	-1.247	&	3.095	\\
27	&	1,4,5,6	&	4	&	0.337	&	0.343	&	0.286	&	1.516	&		&		&	1.448	&	-1.071	&	0.128	\\
28	&	1,2,4	&	3	&	0.337	&	0.341	&	0.235	&	2.532	&	0.901	&		&	1.312	&		&		\\
29	&	1,4,5	&	3	&	0.336	&	0.341	&	0.244	&	1.593	&		&		&	1.649	&	-1.123	&		\\
30	&	1,2,5	&	3	&	0.332	&	0.335	&	0.292	&	1.597	&	0.731	&		&		&	-0.788	&		\\
31	&	1,5,6	&	3	&	0.330	&	0.330	&	0.321	&	1.122	&		&		&		&	-0.714	&	0.593	\\
32	&	2,4,6	&	3	&	0.291	&	0.328	&	0.175	&		&	2.727	&		&	-3.738	&		&	3.756	\\
\end{array}
\]
\caption{\label{tabrr00}Feature comparison table (top 32 feature combinations)}
\end{table}


\begin{table}%[H]
\footnotesize
\[
\begin{array}{lcc|rrr|rrrrrr}
\hline
\text{Rank}	&	\text{Configuration}	&	m	&	s(\beta_{15}) & s(\beta_{100}) & s(\beta_0)	&	\multicolumn{6}{c}{\text{Regression coefficients attached to } \beta_{100}}		\\
\hline
33	&	1,2	&	2	&	0.318	&	0.318	&	0.291	&	2.025	&	0.653	&		&		&		&		\\
34	&	1,4,6	&	3	&	0.309	&	0.311	&	0.283	&	1.681	&		&		&	0.350	&		&	0.494	\\
35	&	1,6	&	2	&	0.310	&	0.310	&	0.304	&	1.536	&		&		&		&		&	0.605	\\
36	&	1,5	&	2	&	0.304	&	0.305	&	0.291	&	1.320	&		&		&		&	-0.728	&		\\
37	&	1,4	&	2	&	0.300	&	0.300	&	0.234	&	2.033	&		&		&	0.938	&		&		\\
38	&	2,3,5,6	&	4	&	0.276	&	0.298	&	0.242	&		&	1.454	&	0.501	&		&	-1.711	&	1.878	\\
39	&	2,5,6	&	3	&	0.276	&	0.293	&	0.205	&		&	1.845	&		&		&	-2.057	&	2.138	\\
40	&	1	&	1	&	0.289	&	0.289	&	0.289	&	1.746	&		&		&		&		&		\\
41	&	3,5,6	&	3	&	0.252	&	0.252	&	0.242	&		&		&	1.076	&		&	-1.089	&	0.945	\\
42	&	2,3,4,5	&	4	&	0.227	&	0.249	&	0.200	&		&	-0.989	&	2.350	&	-1.997	&	0.477	&		\\
43	&	2,3,4	&	3	&	0.241	&	0.241	&	0.214	&		&	-0.820	&	2.022	&	-1.535	&		&		\\
44	&	4,5,6	&	3	&	0.216	&	0.216	&	0.199	&		&		&		&	-0.638	&	-1.340	&	1.119	\\
45	&	3,4	&	2	&	0.215	&	0.215	&	0.216	&		&		&	2.120	&	-1.879	&		&		\\
46	&	3,4,5	&	3	&	0.204	&	0.209	&	0.200	&		&		&	1.592	&	-1.070	&	-0.716	&		\\
47	&	5,6	&	2	&	0.203	&	0.203	&	0.203	&		&		&		&		&	-1.642	&	0.972	\\
48	&	2,3,5	&	3	&	0.188	&	0.188	&	0.178	&		&	-0.564	&	1.329	&		&	-1.029	&		\\
49	&	3,6	&	2	&	0.180	&	0.180	&	0.179	&		&		&	1.891	&		&		&	1.198	\\
50	&	3,5	&	2	&	0.180	&	0.180	&	0.179	&		&		&	1.123	&		&	-1.384	&		\\
51	&	2,3,6	&	3	&	0.177	&	0.178	&	0.179	&		&	-0.242	&	1.910	&		&		&	1.019	\\
52	&	4,6	&	2	&	0.169	&	0.169	&	0.171	&		&		&		&	-2.353	&		&	1.730	\\
53	&	5	&	1	&	0.139	&	0.139	&	0.139	&		&		&		&		&	-1.970	&		\\
54	&	2,3	&	2	&	0.138	&	0.138	&	0.094	&		&	-1.132	&	2.086	&		&		&		\\
55	&	2,5	&	2	&	0.137	&	0.137	&	0.139	&		&	-0.190	&		&		&	-1.887	&		\\
56	&	4,5	&	2	&	0.135	&	0.134	&	0.126	&		&		&		&	0.487	&	-2.163	&		\\
57	&	2,4,5	&	3	&	0.135	&	0.133	&	0.126	&		&	-0.158	&		&	0.454	&	-2.081	&		\\
58	&	3	&	1	&	0.096	&	0.096	&	0.096	&		&		&	2.257	&		&		&		\\
59	&	4	&	1	&	0.071	&	0.071	&	0.071	&		&		&		&	-2.187	&		&		\\
60	&	2,4	&	2	&	0.033	&	0.033	&	0.072	&		&	-1.073	&		&	-1.719	&		&		\\
61	&	2,6	&	2	&	0.021	&	0.025	&	0.022	&		&	0.127	&		&		&		&	1.735	\\
62	&	6	&	1	&	0.020	&	0.020	&	0.020	&		&		&		&		&		&	1.643	\\
63	&	2	&	1	&	-0.057	&	-0.057	&	-0.057	&		&	-1.433	&		&		&		&		\\

\end{array}
\]
\caption{\label{tabrr01}Feature comparison table (bottom 31 feature combinations)}
\end{table}

%---------------------------------------------------------------------------------------------------------------------
\Chapter{From Interpolation to Fuzzy Regression}{}\label{chapterfuzzy}

The innovative technique discussed here does much more than regression. It is useful in signal processing, in particular spatial filtering and smoothing. Initially designed
using hyperplanes, the original version can be confused with support vector machines or support vector regression. However, the closest analogy is fuzzy regression.
A weighted version based on splines makes it somewhat related to nearest neighbor or \textcolor{index}{inverse distance interpolation}\index{nearest neighbor interpolation}, and highly non-linear.  In the end, it is a kriging-like spatial \gls{gls:regression},
with many potential applications ranging from compression to signal enhancement and prediction. It comes with confidence intervals for the predicted values,
despite the absence of statistical model. A predicted value is determined by hundreds or thousands of splines. The splines play the role of nodes 
in \glspl{gls:neuralnet}. Unlike neural networks, all the parameters -- the distances to the splines -- have a natural interpretation. 

The methodology was tested on synthetic data. The performance, depending on \glspl{gls:hyperparam} and the number of splines, is measured on the \gls{gls:validset}\index{validation set}, not on the \gls{gls:trainingset}\index{training set}. Despite (by design) nearly perfect predictions for training set points, it is robust against outliers, numerically stable, and does not lead to \gls{gls:overfitting}. There is no regression coefficients, no intercept, no matrix algebra involved, no calculus, no statistics beyond empirical percentiles, and not even square roots. It is accessible to high school students. Despite the apparent simplicity, the technique is far from trivial. In its simplest form, the splines are similar to multivariate Lagrange interpolation polynomials. Python code is included in this document.


\hypersetup{linkcolor=red}

\section{Introduction}\label{fregi1}


The original problem consisted of fitting a line to a set of points -- a classic linear regression problem. I explored alternatives to the traditional \textcolor{index}{ordinary least squares}\index{ordinary least squares} (OLS) 
solution [\href{https://en.wikipedia.org/wiki/Ordinary_least_squares}{Wiki}]. The line that yields the \textcolor{index}{least absolute residuals}\index{least absolute residuals} (LAR) [\href{https://en.wikipedia.org/wiki/Least_absolute_deviations}{Wiki}] is such an example. It has the benefit of being more robust. The next step was to look at all potential line combinations. For a set of $n$ points, there are $M=n(n-1)/2$ potential lines, as each pair of points determines a line. The LAR line is just one of them and in some sense, the best one.  

The idea is that for any local location $x$ on the real axis, one can choose between multiple lines $z=L_k(x)$ to compute the predicted response $z$, with 
$k=1,\dots,M$. Some lines provide a better fit than others, at the local level. Or in other words, each of the $M$ lines has some unique, location-dependent  
probability to contribute to the predicted response computed at $x$. This perspective is very similar to the Bayesian approach.  In the literature, this is known as the
\textcolor{index}{Theil-Sen estimator}\index{Theil-Sen estimator} [\href{https://mltblog.com/3LomHbJ}{Wiki}]. In the simplest version, the median value of $L_k(x)$ computed across the $M$ lines, is the final
point estimate of the response, at location $x$. An improved version uses different weights for each line, involving weighted averages or 
\textcolor{index}{weighted quantiles}\index{weighted quantiles}\index{quantiles!weighted} \cite{wpnumpy}. 
Since there are $M$ potential predicted values attached to each $x$ -- one for each line -- you can define
a 80\% confidence interval for the response, as follows: the lower (resp. upper) bound is the 10\% (resp. 90\%) \textcolor{index}{empirical quantile}\index{empirical quantiles}\index{quantiles!empirical}
[\href{https://en.wikipedia.org/wiki/Quantile}{Wiki}] (also called percentile) of the $M$ predicted values computed at $x$. 

The term 
\textcolor{index}{prediction interval}\index{prediction interval} [\href{https://en.wikipedia.org/wiki/Prediction_interval}{Wiki}], rather than confidence interval, is used in the literature. 
Note that the methodology to build these confidence intervals should not be confused with the \textcolor{index}{percentile bootstrap method}\index{percentile bootstrap}\index{bootstrapping!percentile method} [\href{https://stats.stackexchange.com/questions/355781/is-it-true-that-the-percentile-bootstrap-should-never-be-used}{Wiki}]. Here, there is no resampling involved. The $M$ lines are computed on the \gls{gls:trainingset}\index{training set}, while model performance is measured on the 
\gls{gls:validset}\index{validation set} [\href{https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets}{Wiki}]. Note that I use the word ``model" to represent the embodiment described in this article. There is no
statistical or probabilistic model involved.  

\section{Original version}\label{fuzzy1}

Before moving to the full, non-linear model in higher dimensions, let's focus on the original method: the first version of my fuzzy regression technique. This version is easier to understand, more traditional, and leads to
simple visualizations. It will help you better understand the new version, which is considerably more abstract and generic.

Let the $n$ observed points be labeled as $(x_1,z_1),\dots,(x_n,z_n)$. The line that contains $(x_i,z_i)$ and $(x_j,z_j)$ is denoted as
$L_k = L_{i,j}$. Its equation is

$$
L_{i,j}(x)=\frac{z_{i}-z_j}{x_i-x_j} \cdot x + \frac{ x_iz_j-x_jz_i}{x_i-x_j}.
$$
It immediately follows that
$$
L_{i,j}(x_i)=z_i, \quad L_{i,j}(x_j)=z_j.
$$
Note that if $x_i=x_j$, the equation does not make sense. I will address this issue in the general case. I also use the notation $z_{i,j}=L_{i,j}(x)$. It represents the predicted value of $z$ at location $x$, based on line $L_{i,j}$ solely. In its simplest form, the predicted value at $x$ is the median of the $z_{i,j}$.

\begin{figure}%[H]
\centering
\includegraphics[width=0.89\textwidth]{fuzzyr2.png}
%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Fuzzy regression with prediction intervals, original version, 1D}
\label{fig:fuzzyr}
\end{figure}


Figure~\ref{fig:fuzzyr} shows the result of this regression technique. To the naked eye, the regression curve is indistinguishable from the traditional regression line.  It is also indistinguishable from a straight line, but it is actually a curve. The data set used here and pictured in Figure~\ref{fig:fuzzyr} (the blue dots) comes from chapter~\ref{chapterregression} on interpretable  regression. It is a synthetic data set with $n=1,000$ observations.

The originality of the fuzzy regression procedure is that it allows you to compute prediction intervals without any underlying statistical model or bootstrap / resampling techniques.  However, it requires the computation of $L_k(x)$ for $k=1,\dots,M$, for each sampled value of $x$. In higher dimensions, it is natural to replace the lines $L_k$ by hyperplanes. However, this is not the path that I chose. Instead of hyperplanes, I used splines. The reason is that it leads to trivial computations and
more flexibility. In particular, it does not involve matrix products or inversions. It does not involve matrices at all, nor calculus, thus my claim that the methodology is accessible to high school students.

\section{Full, non-linear model in higher dimensions}\label{fuzq1}

I now discuss the general model. For the sake of simplicity, I focus on the 2-dimensional case: a response $z$, with two features $x, y$. It is an important case, with applications
in \textcolor{index}{geostatistics}\index{geostatistics}\index{spatial statistics} [\href{https://en.wikipedia.org/wiki/Geostatistics}{Wiki}]. The $d$-dimensional case is not more complicated, but the notations quickly become cumbersome.
The line $L_k$ is now replaced by a spline, also denoted as $L_k$. But this time,
$k=(k_1,\dots,k_r)$ is a vector, with $1\leq k_i \leq n$ ($i=1,\dots,r$). Just like a plane is uniquely determined by exactly 3 points, we want the spline $L_k$ to be uniquely determined by
exactly $r$ points, in this case $(x_{k_1},y_{k_1},z_{k_1}),\dots,(x_{k_r},y_{k_r},z_{k_r})$. That is, we want

$$
z_{k_i} = L_k(x_{k_i}, y_{k_i}), \quad i=1,\dots,r.
$$
There is very simple type of splines satisfying this property, namely
\begin{equation}
L_k(x,y)=\sum_{i=1}^r \frac{z_{k_i}}{2} \Bigg[\prod_{ j\neq i} \frac{\psi(x-x_{k_j})}{\psi(x_{k_i}-x_{k_j})} + \prod_{ j\neq i} \frac{\psi(y-y_{k_j})}{\psi(y_{k_i}-y_{k_j})}\Bigg],
\label{feqzz}
\end{equation}
where $\psi$ is any real-valued function satisfying $\psi(0)=0$. In practice, one can choose the identity function for $\psi$. The resulting splines are then similar to
multivariate Lagrange polynomials of degree $r-1$, used for interpolation \cite{siam1}. I now discuss the various features, issues, capabilities, and potential implementations of this type of regression.

\subsection{Geometric proximity, weights, and numerical stability}\label{fuzq2}

From now on, I assume that $\psi$ is the identity function. A key concept is the proximity between a location $(x,y)$ in the plane, and a spline. When predicting $z$, given $(x,y)$, one has to compute $L_k(x,y)$ for each spline $L_k$. The number of splines quickly increases with $r$. For a specific location $(x,y)$, some splines are more relevant than others. The following metric measures the proximity to $L_k$:
\begin{equation}
\delta_k(x,y)=\Big(\prod_{i=1}^r \max(|x-x_{k_i}|,|y-y_{k_i}|\Big)^{1/r} \label{feqab}.
\end{equation}
It is the geometric mean of $r$ Chebyshev distances [\href{https://en.wikipedia.org/wiki/Chebyshev_distance}{Wiki}] in $\mathbb{R}^2$. In particular, it is zero if any of these distances is zero. This essential property can not be satisfied with the arithmetic mean, but it is with the geometric mean. The relevance or importance of $L_k$, relative to $(x,y)$, is then defined as the weight 
\begin{equation}\label{fuzze1}
w_k(x,y)=\exp[-s\cdot\delta_k(x,y)],
\end{equation}
where $s>0$ is the \textcolor{index}{smoothing parameter}\index{smoothing parameter}. It is maximum when $\delta_k(x,y)=0$. On the other side, some splines are always problematic, or may not even exist. These splines are identified by the accuracy metric
\begin{equation}
\epsilon_k=\prod_{i=1}^r \prod_{j=i+1}^r|x_{k_i}-x_{k_j}|\cdot |y_{k_i}-y_{k_j}|.\label{eqfu7}
\end{equation}
In particular, if $\epsilon_k=0$, the spline $L_k$ is undefined. It is a good practice to reject or ignore splines with $\epsilon_k<10^{-5}$. It increases the numeral stability of the system.

In addition, some splines may produce outlier predictions, depending on the location $(x,y)$. Such abnormal predictions should be ignored to boost performance, when blending the $M$ 
predicted values $L_k(x,y)$ -- one per spline -- to compute the final predicted value and prediction interval at $(x,y)$. This final predicted value is the median or weighted average computed across all splines at $(x,y)$, after rejecting undesirable splines or predictions. Outlier predictions are detected and rejected in the Python code, via the 
\textcolor{index}{hyperparameter}\index{hyperparameter} \texttt{zzdevratio}.

There is no need to be overly aggressive when penalizing and rejecting undesirable individual splines or predicted values. The median does a good job at filtering out non-robust 
measurements. The more aggressive, the fewer splines used, resulting in lower statistical confidence. At the extreme, some locations may end up with no predicted value at all: for such locations, the variable \texttt{count} in the Python code is equal to zero, and the counter \texttt{missing} is incremented by one. This may be a good thing, or not.  

\subsection{Predicted values and prediction intervals}\label{fuzq3}

In section~\ref{fuzzy1}, I used the median predicted value computed across all $M=n(n-1)/2$ splines, as the final predicted value for the response $z$, at a specific location. Here $n$ is the number of observations in the training set. Then I used the quantiles of these $M$ values, computed at the same location, to build the prediction interval. The same applies to the general case, but now, $M= {n \choose r}$ is a binomial coefficient. Note that the actual number of splines will depend on the location $(x,y)$, and is smaller than $M$ if some splines are rejected.
In the Python code, each call to the function \texttt{F} generates a new, random spline. The number $M$ is pre-specified and is chosen to be large ($> 500$) 
but much smaller than ${n \choose r}$. Also, I mostly used $r=2$.

An alternative to the median is to use a weighted average to compute a predicted value. For the weight attached to spline $L_k$, use formula~(\ref{fuzze1}). These weights and the whole system were designed to satisfy the following property. Let $(x,y)$ be the location of a training set point.  Then the predicted value at $(x,y)$, using the weights in question with $s\rightarrow\infty$ and $M=n$ carefully chosen splines, is identical to the observed value. This is true for instance if the index $k_1$ in $k=(k_1,\dots,k_r)$ covers all integer values between $1$ and $n$, that is, all training set points. In that case, there is always at least one index vector $k$ such that $\delta_k(x,y)=0$, corresponding to a spline containing $(x,y,z)$, with a weight equal to one, dwarfing all other weights. For a formal proof, see exercise~\ref{fex1}.

Indeed, when $s$ is large, the weighted methodology is similar to 
\textcolor{index}{inverse distance weighting}\index{inverse distance weighting}\index{Shepard's method} (Shepard's method) [\href{https://en.wikipedia.org/wiki/Inverse_distance_weighting}{Wiki}], or 
\textcolor{index}{nearest neighbor interpolation}\index{nearest neighbor interpolation} [\href{https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation}{Wiki}]. The weighted version is implemented in the Python code.
Another way to include neighboring data in the predictions is to only use local splines determined by training set points close to the target location $(x,y)$. Finally, an efficient implementation still 
needs to be developed. The methodology can easily be implemented using a parallel computer architecture. 

\subsection{Illustration, with spreadsheet}

See figure~\ref{fig:fuzzyr} for an illustration of the original method. Here I focus on the general method discussed in section~\ref{fuzq1}, in two dimensions, and with non-linear splines. Figure~\ref{fig:fuzzybig} illustrates several aspects of this technique. Unlike in figure~\ref{fig:fuzzyr} (the one-dimensional case), it is difficult to show residual errors or prediction intervals for specific locations, because the locations $(x,y)$ are now 2-dimensional. A workaround is to show a scatter plot of observed values $z$ versus the predicted values $z_{\text{m}}$. These are the blue dots in
figure~\ref{fig:fuzzybig}. The notation $z_{\text{m}}$ stands for the predicted value based on the median. The predicted value based on the weighted average is denoted as $z_{\text{w}}$, and not shown in the picture. The observed value $z$ is also denoted as $z_{\text{obs}}$. The dashed blue line shows the quality of the fit between predicted and observed values. The \gls{gls:rsquared} is $0.8096$. 

However, the slope of the dashed line is only $0.4640$. Maybe you expected it to be close to $1$: after all, a perfect fit means all the blue dots are on the main diagonal. In practice, the slope will always be between $0$ and $1$. The explanation is as follows. The regression technique (spatial regression, to be precise), acts as a smoother or noise filtering technique, damping amplitudes. To eliminate the damping effect, you need to restore the amplitude. This is easily done by standardizing the predictions, so that their mean and variance corresponds to that of the original signal (observed response) $z$ measured on the training set. Doing so won't change the R-squared, as it is invariant under translation and multiplication. Indeed, the R-squared is the square of the correlation between $z$ and $z_{\text{m}}$.

\begin{figure}%[H]
\centering
\includegraphics[width=0.81\textwidth]{fuzzybig.png}
%  \includegraphics[width=\linewidth]{PB-hexa.PNG}
\caption{Fuzzy regression with prediction intervals, full model, 2D}
\label{fig:fuzzybig}
\end{figure}

The prediction levels are based on the 20\% (lower bound) and 80\% (upper bound) empirical quantiles. Thus, the confidence level is 60\%. It is not possible to directly show prediction bands on a scatterplot in any meaningful way. Instead, to each blue dot in figure~\ref{fig:fuzzybig}, corresponds one green and one red dot: the upper and lower bounds of the prediction interval. For each blue dot, the associated red and green dots are all on a same vertical line (not shown), parallel to the vertical axis. The details are unimportant; in the end, figure~\ref{fig:fuzzybig} still gives a good sense of how the methodology performs, and how the prediction intervals look like.

The detailed implementation is in the \href{https://ln5.sync.com/dl/0caeb8e10/mztnibg9-xrkdks7g-r8bsgabw-3fsizwif}{Fuzzy4.xlsx} spreadsheet. Most of the heavy computations are done in Python. The spreadsheet provides the final steps: prediction intervals and visualizations. It also includes the output file \texttt{fuzzy\_big.txt} produced by Python. Now, I am going to discuss 
the various fields in that spreadsheeet.

\subsubsection{Output fields}

I focus on the \texttt{2-D} tab in the spreadsheet. It contains three separate sets of columns, organized as follows:

\begin{itemize}
\item Columns \texttt{A}, \texttt{B}, \texttt{C}, \texttt{D} correspond respectively to $x, y, z$ and the traditional predicted $z$ using standard regression. It has $1000$ rows, corresponding to the $n=1000$ training set points. Only the first $800$ points are used for training, the remaining $200$ are used for validation.
\item Columns \texttt{F} to \texttt{N} correspond to the output fields of  \texttt{fuzzy\_big.txt} produced by the Python code. It features the $200$ points of the validation set, with for each point, up to $M=800$ entries, one per spline. These are used to build the prediction intervals. Some splines are rejected as discussed in section~\ref{fuzq3}, thus the actual number of rows is less than $800$ per validation point. Columns \texttt{F} - \texttt{I} are trivial. Column \texttt{J} is the predicted value for the associated validation point in column \texttt{I}, arising from one single spline. The final predicted value for that point is the median of these values computed across all splines. It is stored in column \texttt{R}. Columns \texttt{K} and \texttt{L} correspond respectively 
to $\delta_k(x,y)$ and $w_k(x,y)$. 
\item Columns \texttt{P} to \texttt{U} correspond to summary statistics for each point of the validation set. Thus it has $200$ rows, one per validation point. The median-based predicted
 value $z_{\text{m}}$ is in column \texttt{R}, the weight-based predicted value $z_{\text{w}}$ is in column \texttt{U}, the observed $z$ is in column \texttt{Q}, and the lower and upper bounds of the prediction intervals are in columns \texttt{S} and \texttt{T}. 
\end{itemize} 

\noindent The cells \texttt{X2} and \texttt{Y2} in the \texttt{2-D} tab are the percentile levels for the prediction intervals. You can change these parameters, and it will automatically update figure~\ref{fig:fuzzybig} in the spreadsheet. The predicted $z_{\text{w}}$'s could be computed using the \texttt{AverageIf} Excel function. However there is no 
\texttt{MedianIf} or \texttt{PercentileIf} function in Excel.  There is an easy workaround: for instance, instead of using the non-existent 
call \texttt{MedianIf(F:F,P2,J:J)}, use \texttt{Median(If(F:F=P2,J:J))}. This instruction means ``compute the median value of column \texttt{J}, but only for rows that have the element in column \texttt{F} equal to cell \texttt{P2}". The same applies to the \texttt{Percentile} function.

\section{Results}

In this section, I present the main results. I tested the methodology on the \gls{gls:syntheticdata} described in chapter~\ref{chapterregression}. It consists of $m=1000$ observations with known response. The first 800 points are used to train the ``model", and the remaining 200 -- the validation data -- for testing and assessing performance. The dataset is available in the
spreadsheet \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Spreadsheets/README.md}{fuzzyf2.xlsx}, available on my GitHub repository. It corresponds to the small output file \texttt{fuzzy\_small.txt} produced by the Python code in section~\ref{pythonfu}. Predictions intervals are discussed in section~\ref{fuzq3} and illustrated in figure~\ref{fig:fuzzybig}.

The main performance metric is the R-squared. It is certainly not the best metric for reasons discussed in chapter~\ref{chapterregression}, where I suggest alternatives. However, the dataset is large enough, and relatively well behaved. Thus the R-squared is adequate enough in this case. Note that it is mostly measured on the validation set rather than the training set, so technically it is not the true R-squared in the typical sense. Also, it is defined here as the square of the correlation coefficient. This is discussed in section~\ref{fpe1stt}.


\begin{figure}%[H]
\centering
\includegraphics[width=0.85\textwidth]{fuzzyperf.png}
\caption{Scatterplots: median vs weighted method, on validation (left) vs training set (right)}
\label{fig:frfu}
\end{figure}

\subsection{Performance assessment}\label{fpe1s}

Table~\ref{futabcorrel} summarizes my main experiment. The subscripts v, t, m, w stand respectively for the validation set, the training set, the median and the weighted predicted value. Regardless of the number $M$ of random splines used per location, the weighted predicted value does best with splines defined by $r=1$ point. Such splines have a constant value everywhere. This is not surprising, since this method, with $r=1$, is similar to kriging or inverse distance interpolation. Note that with $r=1$, the maximum number of distinct splines is $M=n$. Thus, if $n=800$ and $M=5000$, some splines are used multiple times. 

The median predicted value is less sensitive to outliers, but it tends to reduce the amplitude, resulting in $\beta$ values well below one. This is not an issue, as predicted values can easily be scaled back without impacting the R-squared. The median method, for this particular 2-D dataset, works best with $r=2$. Also, it performs equally well inside and outside the training set. To the contrary, the weighted method experiences a sharp drop of performance, outside the \gls{gls:trainingset}.
Larger values of $r$ do not lead to further improvement. This is encouraging, as you want to work with small values of $r$ and $M$ to speed up of the algorithm.  The larger $r$, the
more potential splines to choose from, which leads to more accurate prediction intervals.


\begin{table}[H]
\small
\[
\begin{array}{lccccccccc}
\hline
  r & M & \rho^2_{\text{vm}}  & \rho^2_{\text{vw}} & \rho^2_{\text{tm}}  & \rho^2_{\text{tw}} & \beta_{\text{vm}} & \beta_{\text{vw}} & \beta_{\text{tm}} & \beta_{\text{tw}}\\
\hline
1	&	150	&	0.1922	&	0.6946	&	0.2490	&	0.7495	 & 0.0916 & 0.7635 & 0.1066 & 0.7563\\
2	&	150	&	0.7688	&	0.4756	&	0.7525	&	0.7301	 & 0.4128 & 0.6650 & 0.4210 & 0.7853\\
3	&	150	&	0.4272	&	0.3930	&	0.5601	&	0.5865	 & 0.3032 & 0.6695 & 0.3306 & 0.7834\\
1	&	800	&	0.2600	&	0.7734	&	0.2936	&	0.8682	 & 0.1025 & 0.7367 & 0.1039 & 0.8375\\
2	&	800	&	0.7941	&	0.6849	&	0.7913	&	0.9331	 & 0.4058 & 0.7368 & 0.4154 & 0.9291\\
3	&	800	&	0.6838	&	0.5168	&	0.7204	&	0.9770	 & 0.2986 & 0.6649 & 0.3392 & 0.9680\\
1	&	5000	&	0.2795	&	0.7876	&	0.4276	&	0.9294	 & 0.1071 & 0.7421 & 0.1980 & 0.8945\\
2	&	5000	&	0.8167	&	0.7423	&	0.8080	&	0.9869	 & 0.4038 & 0.7359 & 0.4157 & 0.9617\\
3	&	5000	&	0.7605	&	0.7203	&	0.7740	&	0.9988	 & 0.3114 & 0.7063 & 0.3308 & 0.9892\\
\hline
\end{array}
\]
\caption{\label{futabcorrel}R-squared $\rho^2$ and slope $\beta$, on training and validation sets, median vs weighted}
\end{table}

\subsection{Visualization}

Figures~\ref{fig:fuzzybig} and~\ref{fig:frfu} further illustrate the methodology. The blue dots in the scatterplots represent the observed value (horizontal axis) versus the predicted value (vertical axis), computed using the median method. It provides a much better picture about the distribution of residual errors, than the R-squared alone. The orange dots show the same distribution of points, but computed using the weighted method instead. The fact that the slopes are different is not an issue: the predicted values need to go though a 
final re-scaling step described in section~\ref{fpe1stt}, to correct the damping effect caused by the fuzzy regression, acting as a smoothing, low pass filter. Once corrected, the slopes will be nearly identical, and the R-squared unchanged.

Figure~\ref{fig:fuzzybig} is an original visualization, rarely seen. It allows you to look at individual residual errors and prediction intervals, regardless of the dimension of the problem.


\subsection{Amplitude restoration}\label{fpe1stt}

As mentioned a few times earlier, the fuzzy regression, especially the methodology based on the median, acts as a low-pass filter in signal processing. This is not surprising: after all it removes the noise. Indeed, it can be used as a data compression technique. As a result, predicted values have a lower variance than the observed ones, and the slope $\beta$ in table~\ref{futabcorrel} or figure~\ref{fig:frfu} is well below one. To correct this ``issue", one has to standardize the predicted values, so that the mean and variance match that of the observed response in the training set. In short, the predicted values must be re-calibrated. Because of this, the mean squared error is not a good metric to assess performance. Also, here the  R-squared is 
the square of the correlation, but it is not equal to $1- SS_{\text{res}}/SS_{\text{tot}}$, unlike in traditional regression where both agree.

The same phenomenon takes place when smoothing time series. A moving average can be used to predict or interpolate values. It also removes some noise, and reduces the amplitude of the signal. A scatterplot of exact values versus moving average will exhibit the same sharp drop in the slope. And it can be corrected using the same strategy, with no impact on the R-squared measured as the square of the correlation between observed and predicted (smoothed) values. 

%fuzzy regression PDF: https://mltblog.com/3MOMewc
%xxx explain orange text


\section{Exercises}

The first exercise consists of proving a fundamental result: the fact that, under certain circumstances,  the fuzzy regression technique described in this article is an exact interpolation
technique. The proof does not involve math beyond elementary arithmetic, but rather, out-of-the-box thinking. The second exercise is about another simple, numerically stable interpolation technique, this time based on partial fractions. The prerequisite is a first course in calculus, to understand and solve this problem. The third exercise explores an alternative to validation sets. The fourth exercise is a generalization to higher dimensions.

\begin{Exercise}\label{fex1}{\bf Fuzzy regression for interpolation}. Let $(x_1,y_1,z_1),\dots, (x_n,y_n,z_n)$ be the training set points. We use a spline system with $M=n$ splines. Each spline is uniquely defined by $r$ training set points. The $k$-th spline ($k=1,\dots,n$) always contains $(x_k, y_k, z_k)$. In other words, $L_k(x_k,y_k)=z_k$. Prove that as $s\rightarrow\infty$, the weight-based predicted value $z_{\text{w}}$ evaluated at any training set location $(x, y)$, is equal to the observed value $z$ at that location.  \vspace{1ex} \\
{\bf Solution} \vspace{1ex} \\
When $s=\infty$, $w_k(x,y)=1$ if $(x,y)=(x_k,y_k)$ is the location of a training set point, and $0$ otherwise. If multiple training set points have the same $(x,y)$ but different $z$'s, then the predicted $z_{\text{w}}$ at $(x,y)$ will be the average of those $z$'s. This is because at least one factor in the product formula~(\ref{feqab}) is equal to zero, and thus the product is zero.

\noindent To complete the proof, one has to carefully look at formula~(\ref{feqzz}). Assume that $(x,y)=(x_k,y_k)$. If $i\neq k$, the $i$-th term in formula~(\ref{feqzz}) is zero, because at least one factor in each inner product if zero. But if $i=k$, both products are equal to one, and thus $L_k(x,y)=z_k$.
\end{Exercise}

\begin{Exercise}{\bf Partial fractions for interpolation}. This may be particularly useful for time series interpolation. Assume $f$ is a smooth, slow growing even function, and $f(t)$ is known if $t$ is a positive integer. Then $f(t)$ is uniquely determined everywhere on the real axis. In short, there is an exact interpolation formula for the whole function, if we know $f(t)$ for $t=0,1,2$ and so on. The formula is
\begin{equation}
f(t) = \frac{\sin\pi t}{\pi}\cdot \Big[\frac{f(0)}{t} +\phi'(t)\sum_{k=1}^\infty (-1)^k \frac{f(k)}{\phi(t)-\phi(k)}
\Big],\label{fct1}
\end{equation}
and it works in particular if $\phi(t)=t^2$, $\phi'(t)=2t$ is the derivative with respect to $t$, and
\begin{equation}
f(t)=\sum_{k=0}^\infty \alpha_k \cos \beta_k t, \mbox{ with } |\beta_k|<\pi. \label{feqpp}
\end{equation}
The purpose of this exercise is to prove the validity of formula~(\ref{fct1}) under the right conditions, and to apply it to the real part of the Dirichlet eta function $\eta(\sigma+it)$
 [\href{https://en.wikipedia.org/wiki/Dirichlet_eta_function}{Wiki}], for (say)
$\sigma=0.8$ and $0<t<30$. Unlike the interpolation technique in exercise~\ref{fex1}, formula~(\ref{fct1}) provides only an approximation, albeit an excellent one. The approximation is exact
 if you include all the infinitely many terms. It can be used for \textcolor{index}{time series disaggregation}\index{time series!disaggregation} \cite{vgsmith}. 
A potential application is to break down hourly temperature predictions into 5 minute increments.\vspace{1ex} \\
{\bf Solution} \vspace{1ex} \\
A detailed discussion about this interpolation formula and its generalization, can be found \href{https://mathoverflow.net/questions/376081/infinite-partial-fraction-expansions-to-compute-fractional-iterations-and-recurr}{here}. Note that the real part of the Dirichlet eta function (closely linked to the \textcolor{index}{Riemann Hypothesis}\index{Riemann Hypothesis}) is
$$
\Re[\zeta(\sigma+it)]=\sum_{k=1}^\infty (-1)^{k+1}\frac{\cos(t\log k)}{k^\sigma}, \quad \sigma>0.
$$
Figure~\ref{fig:friemann} shows how accurate the interpolation formula is, for this particular example. The full function was reconstructed, based on $f(k)$ computed at $k=0,1,\dots,249$. The horizontal axis represents $t$. Note that to estimate $f(t)$ beyond $t=30$, more than 250 terms are needed in formula~(\ref{fct1}),
to keep the error smaller than $3\times 10^{-4}$. Interestingly, the interpolation formula seems to be working even though condition (\ref{feqpp}) is not satisfied. At integer arguments, the error is minimum in absolute value, and smaller than $10^{-6}$. 

For the imaginary part -- an odd function -- you can multiply it by $\sin \lambda t$ to turn it into an even function, then apply the same methodology to the transformed function to interpolate it, then divide back by $\sin \lambda t$. Here $\lambda\neq 0$ is an arbitrary constant.
\end{Exercise}

\begin{figure}%[H]
\centering
\includegraphics[width=0.65\textwidth]{riemanninterpol2.png}
\caption{Dirichlet eta function (real part, bottom) and interpolation error (top)}
\label{fig:friemann}
\end{figure}

\begin{Exercise}\label{fex3}{\bf A new type of validation set}. Since we are dealing with a \gls{gls:regression} problem, it is natural to see how the methodology performs on a linear combination
of training set points. In other words, a validation point could be a \textcolor{index}{convex linear combination}\index{convex linear combination} [\href{https://en.wikipedia.org/wiki/Convex_combination}{Wiki}] of two or more training set points. A convex combination guarantees that the validation point is inside the convex hull of the training set points, and is good for interpolation.  Also try with non-convex combinations, with validation points outside the convex hull. One would expect the performance to be lower for these points. It allows you to see how the method performs for \textcolor{index}{extrapolation}\index{extrapolation}. Note that in $d$ dimensions, the convex hull is obtained by computing all convex combinations of $d+1$ points in the training set.
\end{Exercise}

\begin{Exercise}\label{fex4}{\bf Fuzzy regression in higher dimensions}. In three dimensions, $(x,y)$ becomes $(x,x',x'')$ and formula~(\ref{feqzz}) has three inner products. The weighted method will continue to work best with $r=1$, but my guess is that the median method will work best with $r=3$. The response is still denoted as $z$.
In formula~(\ref{feqab}), $\max(|x-x_{k_i}|,|y-y_{k_i}|)$ becomes $\max(|x-x_{k_i}|,|x'-x'_{k_i}|,|x''-x''_{k_i}|)$. Formula~(\ref{eqfu7}) is updated accordingly.
\end{Exercise}

\section{Python source code and datasets}\label{pythonfu}

I described the input/output data of the Python code in the previous sections. The Python source code is available on my GitHub repository, \href{https://github.com/VincentGranville/Machine-Learning}{here}. Below, it is broken down into four parts: commented introduction and setting the hyperparameter values,
 reading the input file, the core function, and the main part. 

\quad \\
%\pagebreak
\noindent {\bf Part 1: the hyperparameters}

\begin{lstlisting}
# Kriging-style spatial regression / inverse distance interpolation

import numpy as np
import random
random.seed(100)

# --- Highlights of this "fuzzy regression" code:

# Model-free; produces big output file to compute prediction intervals; Bivariate case, featuring nearest-neighbor approach (the weights); Exact predictions for training set, yet robust (no \gls{gls:overfitting}); Increasing M is "lazy way" to boost performance, but it slows speed
# Math-free (no matrix algrebra, square root or calculus); Statistics-free (no statistical science involved at all); Requires no technical knowledge beyond high school, but far from trivial!
# Acts as low-pass, amplitude reduction, or signal compression filter; Also acts as noise filtering, signal enhancement. Amplitude restoration step not included, but easy to do.

# By Vincent Granville, www.MLTechniques.com

# --- Hyperparameters

# n (number of obs, called points) set after reading input file [n=1000 here]

P=0.8           # proportion of data allocated to training the remaining is for validation
M=5000          # max number of splines used per point; M=5000 offers modest gain over M=800
r=2             # number of points defining a spline; also works with r=1 or larger r
smoother=1.5    # smoothing param used in weighted predictions; try 0.5 for more smoothing (0 = max smoothing)
thresh1=25.0  # max distance allowed to nearby spline; increase to eliminate points with no predictions; decrease to narrow (improve) confidence intervals
thresh2=1.5   # max outlier level allowed for predicted values ; if < 1, predicted can't be more extreme than observed; if too low, may increase number of points with no prediction; if too large, may produce a few strong outlier predictions;
thresh3=0.001 # control numerical stability (keep 0.001)

# --- Output var (defined later) 

# missing       : number of points not assigned a prediction
#
# count       : actual number of splines used for a specific point
# error       : code telling why a point is not assigned a prediction
# weight      : weight assigned to a spline, for a given point
# zpred       : predicted value for a point zz = (xx, yy)
# zpredw      : weighted predicted value

# Input var (defined later)
#
# xx, yy, zz: coordinates of a point 
\end{lstlisting}

\quad \\
\noindent {\bf Part 2: reading the input file}

\begin{lstlisting}
# --- Reading input file 

x=[]
y=[]
z=[]

file=open('fuzzy2b.txt',"r")
lines=file.readlines()
for aux in lines:
    x.append(aux.split('\t')[0])
    y.append(aux.split('\t')[1])
    z.append(aux.split('\t')[2])
file.close()

x = list(map(float, x))
y = list(map(float, y))
z = list(map(float, z))

zmin=np.min(z)
zmax=np.max(z)
zavg=np.mean(z)
zdev=max(abs(zmin-zavg),abs(zmax-zavg))

n=len(x)

\end{lstlisting}

\quad \\
\noindent {\bf Part 3: the core function}

\begin{lstlisting}
# --- Core function: spline-based interpolator

def F(xx,yy,r):

  zz=0 
  distmin=1
  error=0

  idx=[]
  A=[]
  B=[]

  for i in range(0,r):
    idx.insert(i,int(n*P*random.random()))

  prod=1.0;
  for i in range(0,r): 
    for j in range(i+1,r): 
      prod*=(x[idx[i]]-x[idx[j]])*(y[idx[i]]-y[idx[j]])
  if abs(prod)>thresh3:
    for i in range(0,r): 
      A.insert(i,1.0)
      B.insert(i,1.0)
      for j in range(0,r): 
        if j != i:
          A[i]*=(xx-x[idx[j]])/(x[idx[i]]-x[idx[j]])
          B[i]*=(yy-y[idx[j]])/(y[idx[i]]-y[idx[j]])
      zz+=z[idx[i]]*(A[i]+B[i])/2
      distmin*=max(abs(xx-x[idx[i]]),abs(yy-y[idx[i]]))
    distmin=pow(distmin,1/r)
  else:
    error=1; 
 
  return [zz,distmin,error]
\end{lstlisting}

\quad \\
\noindent {\bf Part 4: main step}

\begin{lstlisting}
# --- Main step: predictions for points in validation set

# For training set predictions, change range(int(P*n),n) to range(0,int(P*n))

file_small=open("fuzzy_small.txt","w")
file_big=open("fuzzy_big.txt","w")

for j in range(int(P*n),n): # loop over all validation points 

  xx=x[j]
  yy=y[j]
  zobs=z[j]
  count=0
  missing=0
  sweight=0.0
  zpredw=0.0
  zpred=0.0

  for k in range(0,M): # inner loop over all splines

    list=F(xx,yy,r)
    zz=list[0]
    distmin=list[1]
    error=list[2]
    weight=np.exp(-smoother*distmin)  
    zzdevratio=abs(zz-zavg)/zdev

    if distmin<thresh1 and zzdevratio<thresh2 and error==0: 
      count+=1
      sweight+=weight
      zpredw+=zz*weight
      zpred+=zz
      row=[j,xx,yy,zobs,zz,distmin,weight,zzdevratio]
      for field in row:
        file_big.write(str(field)+"\t")
      file_big.write("\n")

  if count>0:
    zpredw=zpredw/sweight
    zpred=zpred/count
  else:
    missing+=1
    zpredw=""
    zpred=""

  row=[j,count,xx,yy,zobs,zpred,zpredw] 
  for field in row:
    file_small.write(str(field)+"\t")
  file_small.write("\n")

file_big.close()
file_small.close()
print(missing,"ignored points\n")
\end{lstlisting}

%----------------------------------------------------------------------------------------------------------------
\Chapter{Detecting Subtle Departures from Randomness}{}\label{chapterPRNG}

I discuss a new test of randomness for pseudo random number generators (PRNG), to detect subtle patterns in binary sequences. The test shows that congruential PRNGs, even the best ones, have flaws that can be exacerbated by the choice of the seed. This includes the Mersenne twister used in many programming languages including Python. I also show that the digits of some numbers such as $\sqrt{2205}$, conjectured to be
perfectly random, fail this new test, despite the fact that they pass all the standard tests. I propose a methodology to avoid these flaws, implemented in Python. The test is particularly useful when high quality randomness is needed. This includes cryptographic and military-grade security applications, as well as
synthetic data generation and simulation-intensive \textcolor{index}{Markov chain Monte Carlo}\index{Monte Carlo simulations}\index{Markov chain!MCMC} methods.  The origin of this test is in number theory and connected to the Riemann 
Hypothesis. In particular, it is based on Rademacher stochastic processes. These random multiplicative functions are a number-theoretic version of Bernoulli trials.  My article features state-of-the-art research on this topic, as well as an original, simple, integer-based formula to compute square roots to generate random digits. It is offered with a Python implementation that handles integers with millions of digits. 

\hypersetup{linkcolor=red} 

\section{Introduction}\label{pivizintro}

Let $\chi(\cdot)$ be a function defined for strictly positive integers, with $\chi(1)=1$ and $\chi(ab)=\chi(a)\chi(b)$ for 
any integers $a,b>0$. Such a function is said to be
\textcolor{index}{completely multiplicative}\index{multiplicative function!completely multiplicative} [\href{https://en.wikipedia.org/wiki/Completely_multiplicative_function}{Wiki}]. 
Here we are interested in the case where $\chi$ takes on two possible values: $+1$ and $-1$. The core of my methodology is based on the following, well-known identity:
\begin{equation}
\sum_{k=1}^\infty \chi(k) k^{-z} = \prod_{p\in P} \frac{1}{1-\chi(p) p^{-z}}.\label{bore}
\end{equation}
The product is over all prime integers ordered by increasing values: $P=\{2,3,5,7,11,\dots\}$ is the set of all primes. Such a product is called an \textcolor{index}{Euler product}\index{Euler product} [\href{https://en.wikipedia.org/wiki/Euler_product}{Wiki}]. The series on the left-hand side is called
a \textcolor{index}{Dirichlet series}\index{Dirichlet series} [\href{https://en.wikipedia.org/wiki/Dirichlet_series}{Wiki}]. The argument $z=\sigma+it$ is a complex number. You don't need to know anything about complex numbers to understand this article. The only important fact is that the series or product converges only if $\sigma$ -- the real part of $z$ -- is large enough, typically larger than $0$, $\frac{1}{2}$ or $1$, depending on $\chi$. If $\chi$ is a constant function, thus equal to $1$, then the product and series converge to the
 \textcolor{index}{Riemann zeta function}\index{Riemann zeta function} $\zeta(z)$ [\href{https://en.wikipedia.org/wiki/Riemann_zeta_function}{Wiki}] if $\sigma>1$.

For primes $p$,  let the $\chi(p)$'s be independent random variables, with $\text{P}[\chi(p)=1] =  \text{P}[\chi(p)=-1] =\frac{1}{2}$.
The product, denoted as $L_P(z,\chi)$, is known as a \textcolor{index}{Rademacher random multiplicative function}\index{probability distribution!Rademacher}\index{multiplicative function!Rademacher}\index{random multiplicative function}\index{Rademacher function}, see \cite{harper2020bb}, \cite{harper2020} and \cite{yukkam2013}. If $z$ is a complex number, we are dealing with
 \textcolor{index}{complex random variables}\index{random variable!complex}\index{complex random variable} [\href{https://en.wikipedia.org/wiki/Complex_random_variable}{Wiki}]. From the product formula and the independence assumption, it is easy to obtain
\begin{equation}
\text{E} [L_P(z,\chi)]=\prod_{p\in P} \text{E}\bigg[\frac{1}{1-\chi(p)p^{-z}}\bigg]=\prod_{p\in P }\frac{1}{1-p^{-2z}}=\zeta(2z).\label{proofrn}
\end{equation}
Thus the expectation is finite if $\sigma=\Re(z)>\frac{1}{2}$. A similar argument can be used for the variances. 

Now let us replace the random variables $\chi(p)$ by \glspl{gls:prng}, taking the values $+1$
 or $-1$ with probability $\frac{1}{2}$. If these generated numbers are ``random enough'', free of dependencies, then one would expect them to 
 satisfy the laws of Rademacher random multiplicative functions. The remaining of this article explores this idea in
 details, with a focus on applications.  

\section{Pseudo-random numbers}

There is no formal definition of pseudo-random numbers. Intuitively, a good set of pseudo-random numbers is a  
deterministic binary sequence of digits that satisfies all statistical tests of randomness. Of course, it makes no sense to talk about 
randomness if the sequence contains very few digits, say one or two. So pseudo-random numbers (PRN) are associated with
 infinite sequences, even though in practice one only uses finite sub-sequences.


A rigorous definition of PRN sequences requires the convergence of the 
%\textcolor{index}{empirical joint distribution}
multivariate \gls{gls:empdistr}
\index{empirical distribution!multivariate}
 [\href{https://en.wikipedia.org/wiki/Empirical_distribution_function}{Wiki}]
 of any finite sub-sequence of $m$ digits, to the known theoretical value under the assumption of full randomness.  Let the PRN 
sequence be denoted as $\{d(k)\}$ with $k=1,2$ and so on. A sub-sequence of $m$ digits is defined by its indices, denoted as $i_1,i_2,\dots i_m$.  The convergence of the empirical distribution means that regardless 
of the indices $0\leq i_1 < i_2< \dots$ we have:
\begin{equation}
 \lim_{n\rightarrow\infty} \frac{1}{n}\sum_{k=1}^n I\Big[d(k+i_1)=k_1,d(k+i_2)=k_2,\dots,d(k+i_m)=k_m\Big] = 2^{-m} \label{eqrdv}
\end{equation}
for any $(k_1,k_2,\dots,k_m)$ in $\{-1,+1\}^m$. Here $I$ is the indicator function: $I[A]=1$ if $A$ is true, otherwise $I[A]=0$. The following number $\lambda$ is of particular interest:
\begin{equation}
\lambda=\sum_{k=1}^\infty d'(k) \cdot 2^{-k}, \quad \text{with } d'(k)=\frac{1+d(k)}{2} \in \{0, 1\}. \label{zzxdx}
\end{equation}
Thus the $d'(k)$'s are the binary digits of the number $\lambda$, with $0\leq \lambda\leq 1$. 

The connection between the multiplicative function $\chi(\cdot)$ in Formula~(\ref{bore}) and the $d(k)$'s is as follows. Let denote the $k$-th prime as $p_k$, with $p_1=2$. Then $d(k)=\chi(p_k)$. The traditional definition of PRN's is equivalent to requiring $\lambda$ to
be a \textcolor{index}{normal number}\index{normal number} in base $2$ [\href{https://en.wikipedia.org/wiki/Normal_number}{Wiki}]. I introduce a stronger
 criterion of randomness in section~\ref{sprng}


\subsection{Strong pseudo-random numbers}\label{sprng}

Convergence of the empirical joint distributions, as defined by Formula~(\ref{eqrdv}), has a few important implications. 
The \textcolor{index}{Kolmogorov-Smirnov test}\index{Kolmogorov-Smirnov test} [\href{https://en.wikipedia.org/wiki/Kolmogorov\%E2\%80\%93Smirnov_test}{Wiki}], the \textcolor{index}{Berry-Esseen inequality}\index{Berry-Esseen inequality} [\href{https://en.wikipedia.org/wiki/Berry\%E2\%80\%93Esseen_theorem}{Wiki}] 
and the \textcolor{index}{law of the iterated logarithm}\index{law of the iterated logarithm}\index{iterated logarithm} [\href{https://en.wikipedia.org/wiki/Law_of_the_iterated_logarithm}{Wiki}] can be applied to the PRN sequence $\{d(k)\}$. These three fundamental results provide strong limitations on the behavior of finite PRN 
 sequences. If a sequence $\{d(k)\}$ or its representation by the number $\lambda$ does no stay within these limits, 
  then it does not emulate pure randomness.  However, some quasi-random PRN sequences, with weak dependencies, meet these requirements yet are not truly ``random". For instance, a number can be normal in base $2$ yet have digits that exhibit some dependencies, 
 see \href{https://mathoverflow.net/questions/426815/normal-numbers-and-law-of-the-iterated-logarithm}{here}.  The purpose of this section is to introduce stronger requirements in order to catch some of these exceptions. This is where the multiplicative function $\chi(\cdot)$ comes into play. 

The function $\chi(\cdot)$, initially defined for primes $p$, is extended to all strictly positives integers via $\chi(ab)=\chi(a)\chi(b)$. Because the $\chi(p)$'s are independent among prime numbers (by construction), the full sequence $\{\chi(k)\}$ over all $k$ must behave in a certain way. Obviously, if $k$ is a square integer, $\chi(k)=1$. But if $k$ is not a square, we still have 
 $P[\chi(k)=1]=P[\chi(k)=-1]=\frac{1}{2}$. For instance, $\chi(4200)=\chi(4)\chi(25)\chi(6)\chi(7)=\chi(6)\chi(7)$.
 Since the product of two independent random variables with \textcolor{index}{Rademacher distribution}\index{Rademacher distribution}\index{probability distribution!Rademacher}
 [\href{https://en.wikipedia.org/wiki/Rademacher_distribution}{Wiki}] has a Rademacher distribution, it follows that 
 $\chi(6)=\chi(2)\chi(3)$ has a Rademacher distribution, and thus $\chi(4200)=\chi(6)\chi(7)$ also has a Rademacher distribution.
 So, the $\chi(k)$'s are identically distributed with zero mean, unless $k$ is a square integer. However, they are not independently distributed, even after removing square integers, or even if you only keep \textcolor{index}{square-free integers}\index{square-free integer} 
 [\href{https://en.wikipedia.org/wiki/Square-free_integer}{Wiki}].

I now define three fundamental functions, which are central to my new test of randomness. First, define the following sets:
\begin{itemize}
\item $S_1(n)$  contains all prime integers $\leq n$. 
\item $S_2(n)$ contains all positive square-free integers $\leq n$. 
\item $S_3(n)$ contains all positive non-square integers $\leq n$. 
\end{itemize}
So each of these sets contains at most $n$ elements. Then, define the three functions as
\begin{equation}
L_1(n)=\sum_{k\in S_1(n)} \chi(k),\quad L_2(n)=\sum_{k\in S_2(n)} \chi(k),\quad L_3(n)=\sum_{k\in S_3(n)} \chi(k).\label{oopi}
\end{equation}
 Now, I can introduce my new test of randomness.

\subsubsection{New test of randomness for PRNGs}

Let $d(1),d(2),\dots$ be a sequence of integer numbers, with $d(k)\in \{-1,1\}$ and
$P=\{p_1,p_2,\dots\}$ be the set of prime numbers. The goal is to test how random the sequence $\{d(k)\}$ is, based on the first $n$ elements $d(1),\dots,d(n)$. The algorithm is as follows.
\begin{itemize}
\item Step 1: Set $\chi(p_k)=d(k)$, where $p_k$ is the $k$-th prime number ($p_1=2$). 
\item Step 2: For $k\notin P$ with
 prime factorization $k=p_1^{a_1}p_2^{a_2}\cdots$, set
$\chi(k)=\chi^{a_1}(p_1)\chi^{a_2}(p_2)\cdots$, with $\chi(1)=1.$
\item Step 3: Using Formula~(\ref{oopi}), compute $L^*_3(n) = |L_3(n)|/\sqrt{n\log\log n}$. 
\item Step 4: If $L^*_3(n)<0.5$ or $L^*_3(n)>1.5$, the sequence $\{d(k)\}$ (the first $n$ elements) lacks true randomness.
\end{itemize}
This test is referred to as the ``\textcolor{index}{prime test}"\index{pseudo-random numbers!prime test}\index{prime test (of randomness)}.
Let's illustrate step 2 with $k=4200$: since $4200=2^3\cdot 3\cdot 5^2 \cdot 7$, we have $\chi(4200)=\chi^3(2)\chi(3)\chi^2(5)\chi(7)
 = \chi(2)\chi(3)\chi(7)$. 

Some non-random sequences may pass the prime test. So you should never use this test alone to decide whether a sequence is good enough.
 Also, the standardization of $L_3(n)$, using the $\sqrt{n\log\log n}$ denominator, is not perfect, but good enough for all practical purposes, assuming $10^4<n<10^{15}$.  This test can detect departure from randomness that no other test is able to uncover. 
I discuss practical examples and
 a Python implementation later in this article.

A PRN sequence that satisfies~(\ref{eqrdv}) and passes all the existing tests, including the prime test, is called \textcolor{index}{strongly pseudo-random}\index{pseudo-random numbers!strongly random}. The
corresponding real number $\lambda$ defined by Formula~(\ref{zzxdx}) is called \textcolor{index}{strongly normal}\index{normal number!strongly normal}. It should not be difficult to
prove that almost all numbers are strongly normal. Thus almost all PRN sequences are strongly pseudo-random. Yet creating
one that can be proved to be strongly pseudo-random is as difficult as proving that a given number is normal (and a fortiori, strongly normal). Interestingly, none of the sequences produced by \textcolor{index}{congruential random number generators} 
[\href{https://en.wikipedia.org/wiki/Linear_congruential_generator}{Wiki}] are strongly pseudo-random,
for the same reason that no rational number is normal: in both cases, $d(k)$ is periodic. 


Modern test batteries
 include the \textcolor{index}{Diehard tests}\index{Diehard tests of randomness}\index{pseudo-random numbers!Diehard tests} [\href{https://en.wikipedia.org/wiki/Diehard_tests}{Wiki}] published in 1995, and the
 \textcolor{index}{TestU01}\index{pseudo-random numbers!TestU01} framework [\href{https://en.wikipedia.org/wiki/TestU01}{Wiki}], introduced in 2007. 



\subsubsection{Theoretical background: the law of the iterated logarithm}

The prime test checks whether the multiplicative function $\chi(k)$ derived from $\{d(k)\}$, satisfies a particular version of the 
 \textcolor{index}{law of the iterated logarithm}\index{law of the iterated logarithm}\index{iterated logarithm} [\href{https://en.wikipedia.org/wiki/Law_of_the_iterated_logarithm}{Wiki}]. Truly random sequences $\{d(k)\}$ satisfy that law. Since $\{\chi(k)\}$ is multiplicative and thus non-random, the law of the iterative algorithm
 must be adapted to take care of the resulting dependencies. In particular, the $\sqrt{n\log\log n}$ weight used to standardize
  $L_3(n)$  provides only an approximation, good enough for all practical purposes. The exact weight is discussed in 
  \cite{harper2020} and \cite{yukkam2013}.  

Many sequences $\{d(k)\}$ satisfy the basic law of the iterated logarithm without the prime number / Euler product apparatus introduced 
 in this article. This is the case for most of the sequences studied here. However, by looking at $\chi(k)$ rather than the original 
 $d(k)$, we are able to magnify flaws that are otherwise undetectable by standard means. An example is the Mersenne twister
  implemented in Python, passing the standard test, but failing the prime test when the seed is set to $200$. 

\subsubsection{Connection to the Generalized Riemann Hypothesis}

The \textcolor{index}{Generalized Riemann Hypothesis}\index{Riemann Hypothesis!Generalized} (GRH) [\href{https://en.wikipedia.org/wiki/Generalized_Riemann_hypothesis}{Wiki}] is one of the most famous unsolved problems in
mathematics. It states that the function $L(z,\chi)$ defined by Formula~(\ref{bore}) has no root if $\frac{1}{2}<\sigma=\Re(z)<1$. Here $z=\sigma+it$ is a complex number, and $\sigma=\Re(z)$ is its real part.
It applies to a particular class of functions $\chi(\cdot)$, those that are ``well behaved". Of course, without any restriction on $\chi(\cdot)$,
 there are \textcolor{index}{completely multiplicative functions}\index{multiplicative function!completely multiplicative} [\href{https://en.wikipedia.org/wiki/Completely_multiplicative_function}{Wiki}]
 known to satisfy GRH. For instance, the function defined by $\chi(p_{2k})=-1, \chi(p_{2k+1})=1$ where $p_k$ is the $k$-th prime number. The
 corresponding $L(z,\chi)$ has no root if $\sigma>\frac{1}{2}$ because the product converges for $\sigma>0$, and of course, the product has no root. The sequence $\{\chi(p_k)\}$ is obviously non-random as it perfectly alternates, and thus I labeled it \texttt{CounterExample} in Table~\ref{tabuchi}. 

If the Euler product in  Formula~(\ref{bore}) converges for some $\sigma>\sigma_0$, it is equal to its series expansion when $\sigma>\sigma_0$, 
 it converges for all $\sigma>\sigma_0$, and $L(z,\chi)$ satisfies GRH 
when $\sigma>\sigma_0$. When $\sigma<1$, the convergence is \textcolor{index}{conditional}\index{convergence!conditional} [\href{https://en.wikipedia.org/wiki/Conditional_convergence}{Wiki}], making things more difficult. Another example that trivially satisfies GRH if $\sigma>\frac{1}{2}$ is when $\chi(\cdot)$ is a random variable with
 $P[\chi(p)=1]=P[\chi(p)=-1]=\frac{1}{2}$ for primes $p$. In this case convergence means that $L(z,\chi)$ has finite expectation as proved by 
 Formula~(\ref{proofrn}), and finite variances. This function $\chi(\cdot)$ is called a \textcolor{index}{random Rademacher multiplicative function}\index{random multiplicative function!Rademacher}, see \cite{RH1002}. Here the $\chi(p)$'s are identically and independently distributed over the set of all primes, and the definition is extended to all strictly positive  integers with the formula $\chi(ab)=\chi(a)\chi(b)$.

So, if we were able to find a ``nice enough" pseudo-random yet deterministic function $\chi(\cdot)$ with convergence of the 
 Euler product when $\sigma>\sigma_0$ for some $\sigma_0<1$, a function $\chi(\cdot)$ that is random enough over the primes (like its sister, the truly stochastic version) to guarantee the convergence of the product, then it would be a major milestone towards proving GRH. Convergence of the product would imply that:
\begin{itemize} 
\item $L(z,\chi)$ is \textcolor{index}{analytic}\index{analytic function} [\href{https://en.wikipedia.org/wiki/Analytic_function}{Wiki}], because  the product is equal to its series expansion, which trivially satisfies the \textcolor{index}{Cauchy-Riemann equations}\index{Cauchy-Riemann equations} [\href{https://en.wikipedia.org/wiki/Cauchy\%E2\%80\%93Riemann_equations}{Wiki}],
\item $L(z,\chi)$ has no root if $\sigma>\sigma_0$ since the product has no root.
\end{itemize}
As discussed, examples that are ``not so nice" exist; ``nice enough" means that $L(z,\chi)$ satisfies a
 \textcolor{index}{Dirichlet functional equation}\index{Dirichlet functional equation} [\href{https://en.wikipedia.org/wiki/Functional_equation_(L-function)}{Wiki}]. Typically, ``nice enough" 
 means that $L(z,\chi)$ is a \textcolor{index}{Dirichlet-$L$ function}\index{Dirichlet-$L$ function} [\href{https://en.wikipedia.org/wiki/Dirichlet_L-function}{Wiki}]. Besides the \textcolor{index}{Riemann zeta function}\index{Riemann zeta function} where $\chi(p)=1$ is the trivial Dirichlet character, the most 
 fundamental example is when $\chi=\chi_4$ is the non-trivial \textcolor{index}{Dirichlet character modulo $4$}\index{Dirichlet character} [\href{https://en.wikipedia.org/wiki/Dirichlet_character}{Wiki}]. This example is featured in Figure~\ref{fig:rn2x} where $\sigma=\frac{1}{2}$ (left plot) and contrasted with a not so nice example on the right plot, corresponding to a pseudo-random sequence $\{\chi(p_k)\}$. The $\chi_4$ example 
 is referred to as \texttt{Dirichlet4} in Table~\ref{tabuchi}. Note that for $\sigma=\frac{1}{2}$, $L(z,\chi_4)$ has infinitely many roots just like the Riemann zeta function, though its roots are different: this is evident when looking at the left plot in Figure~\ref{fig:rn2x}. The (conjectured) absence of root is when $\frac{1}{2}<\sigma<1$.

I went as far as to compute the Euler product for $L(z,\chi_4)$ when $z=\sigma=0.99$. It nicely converges to the correct value, without the typical 
 growing oscillations associated to lack of convergence, 
 agreeing with the value computed using the series expansion in Formula~(\ref{bore}). It means that the product converges at least for all $z$ with
 $\sigma=\Re(z)\geq 0.99$. Thus there is no root for $L(z,\chi_4)$ if $\sigma\geq 0.99$. This would be an immense milestone compared to the best known result (no root if $\sigma\geq 1$) if it was theoretically possible to prove the convergence in question, supported only by empirical evidence. 
 Convergence implies that the sequence $\{\chi_4(p_k)\}$ is random enough over the primes $p_1,p_2$ and so on. That is, the gap between $+1$ and $-1$ in the sequence never grows too large (that is, runs of same value can only grow so fast), and the proportion of $+1$ and $-1$ tends to $\frac{1}{2}$ fast enough, despite the known \textcolor{index}{Chebyshev's bias}\index{Chebyshev's bias (prime numbers)} [\href{https://en.wikipedia.org/wiki/Chebyshev\%27s_bias}{Wiki}]. The fact that the proportion eventually converges to $\frac{1}{2}$ when using more and more terms in the sequence, is a consequence 
 of \textcolor{index}{Dirichlet's theorem}\index{Dirichlet theorem} [\href{https://en.wikipedia.org/wiki/Dirichlet\%27s_theorem_on_arithmetic_progressions}{Wiki}]. This is how close we are -- or you may say how far -- to proving GRH.

There are other ``nice functions" $\chi(\cdot)$ that fit within the GRH framework. For instance, with primes that are not integers, such as Beurling primes~\cite{bzf2004} (discussed later in this article) or \textcolor{index}{Gaussian primes}\index{Gaussian primes} [\href{https://en.wikipedia.org/wiki/Gaussian_integer}{Wiki}]. For a general family of such functions, see the \textcolor{index}{Dedekind zeta function}\index{Dedekind zeta function} [\href{https://en.wikipedia.org/wiki/Dedekind_zeta_function}{Wiki}]. For a general introduction to the Riemann zeta function and related topics, see \cite{kconrad2018} and \cite{tdr1987}.

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{rn3.png}  
\caption{Orbit of $L(z,\chi)$ at $\sigma=\frac{1}{2}$, with $0<t<200$ and $\chi=\chi_4$ (left) versus pseudo-random $\chi$ (right)}
\label{fig:rn2x}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------


\subsection{Testing well-known sequences}\label{twist}

The binary sequences analyzed here are denoted as $\{d(k)\}$, with $d(k)\in\{-1,+1\}$ and $k=1,2$ and so on. 
The tests, restricted to $d(k)\leq n$, are based on 
$L^*_3(n)=L_3(n)/\sqrt{n\log\log n}$, with  $L_3(n)$ defined by~(\ref{oopi}). Again, $\chi(p_k)=d(k)$ for prime numbers,
and $\chi(\cdot)$ is extended to non-primes positive integers via $\chi(ab)=\chi(a)\chi(b)$. Finally,
 $p_k$ is the $k$-th prime with $p_1=2$. In my examples, $n=\num{20000}$ in Table~\ref{tabuchi}, and $n=\num{80000}$ 
 in Figures~\ref{fig:rn1} and~\ref{fig:rn2}.  

The fact that a sequence fails the test for a specific $n$ does not mean it fails for all $n$. The success or failure also depends on the seed (the initial conditions). Some seeds require many iterations -- that is, a rather large $n$ -- before randomness kicks in. The test should not be used for small $n$, say $n<1000$. Finally, passing the test does not mean that the sequence is random enough. I provide examples of poor PRNGs that pass the test. Typically, to assess the randomness character of a sequence, one uses a battery of tests, not just one test. However, the prime test can detect patterns that no other one can. In some sense, it is a last resort test.

\noindent The sequences investigated here fall into four types:

\begin{itemize}
\item Discrete chaotic \textcolor{index}{dynamical systems}\index{dynamical systems} [\href{https://en.wikipedia.org/wiki/Dynamical_system}{Wiki}]. 
 In one dimension, many of these systems are driven by a recursion $x_{k+1}=g(x_k)$, where $g$ is a continuous mapping from
$[0,1]$ onto $[0,1]$. The initial value $x_0$ is called the seed. Typically, $d(k)=1$ if $x_k<0.5$, otherwise $d(k)=-1$. The \textcolor{index}{logistic map}\index{logistic map}\index{dynamical systems!logistic map} [\href{https://en.wikipedia.org/wiki/Logistic_map}{Wiki}], with $g(x)=4x(1-x)$, is labeled \texttt{Logistic} in Table~\ref{tabuchi} as well as in the Pyhton code
 in section~\ref{prngpython}. The \textcolor{index}{shift map}\index{shift map}\index{dynamical systems!shift map} in base $b$, defined by $g(x)=bx-\lfloor bx\rfloor$ where the brackets represent the integer part function, here with $b=3$, is labeled  
\texttt{Base3}. The case $b=2$ is known as the 
 \textcolor{index}{dyadic map}\index{dyadic map}\index{dynamical systems!dyadic map} [\href{https://en.wikipedia.org/wiki/Dyadic_transformation}{Wiki}]. 
The number $\lfloor bx_k\rfloor$
  is the $k$-digit of the seed $x_0$, in base $b>1$. In particular, if $x_0$ is a rational number, then the sequence
$\{d(k)\}$ is periodic, and thus non random. Even in the best-case scenario (using a random seed), the sequence $\{d(k)\}$ is auto-correlated. These dynamical systems are studied in detail in my book on probabilistic properties of numeration systems \cite{vgdyn}.

\item \textcolor{index}{Mersenne twister}\index{Mersenne twister} [\href{https://en.wikipedia.org/wiki/Mersenne_Twister}{Wiki}] as
 implemented in the Python function \texttt{random.random()}. This congruential PRNG is also another type of dynamical system,
 though technically ``non-chaotic" because the sequence $\{x_k\}$ 
is periodic. It emulates randomness quite well as the period is very large.
 Likewise, the shift map with a large base $b$ and a bad seed $x_0$ (a rational number resulting in periodicity) will emulate
 randomness quite well if $x_0=q/p$, where $p,q$ are integers and $p$ is a very large prime. Both in the table and in the figures, I use the label \texttt{Python} for the Mersenne twister. It fails the prime test for various seeds, especially if the seed is set to $200$. See also section~\ref{fixp}.
\item Number theoretic sequences related to the distribution of prime numbers or
 \textcolor{index}{Beurling primes}\index{Beurling primes}. Sequences of this type are labeled \texttt{Dirichlet4}. The main one, with the seed set to $3$, corresponds to
 $\chi(p_k)=+1$ if $p_k \equiv 1 \bmod 4$ and $\chi(p_k)=-1$ if $p_k \equiv 3\bmod 4$. Here $\chi(2)=0$. This function, denoted as $\chi_4$, is the non-trivial \textcolor{index}{Dirichlet character modulo 4}\index{Dirichlet character} [\href{https://en.wikipedia.org/wiki/Dirichlet_character}{Wiki}]. The sequence barely fails the $L^*_3$ test; the full function $\chi_4$ defined over all positive integers (not just the non-square integers) is periodic. The sister sequence (with the seed set to $1$) has 
%the opposite sign of 
 $\chi(p)=-\chi_4(p)$ if $p$ is prime. Sequences based on Beurling primes (a generalization of prime numbers to non-integers) are not included yet, but discussed in my upcoming article on the Riemann Hypothesis, to be published \href{https://mltechniques.com/resources/}{here}. The Python code 
in section~\ref{prngpython} can easily be adapted to handle them. The \texttt{DirichletL.py} code posted 
 \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/dirichletL.py}{here} on my GitHub repository, includes Beurling primes.  These numbers are studied in 
 Diamond \cite{wen2016} and Hilberdink  \cite{bzf2004}.
\item Binary digits of \textcolor{index}{quadratic irrational}\index{quadratic irrational} numbers. I use a simple, original recursion to compute these 
digits: see code description in section~\ref{zw23}. The Python code painlessly handles the very large integers involved in these computations. Surprisingly, $\sqrt{2}$ passes the prime test as expected, but $\sqrt{2205}$ does not. Sequences based on these digits are labeled \texttt{SQRT} in this document.
\end{itemize}

\noindent The dyadic map is impossible to implement in Python due to the way computations are performed in the CPU: the iterates 
$\{x_k\}$ are (erroneously) all equal to zero after about $45$ iterations. This is why I chose the shift map with $b=3$. In this case, the iterates are also all wrong after $45$ iterations due to propagating errors caused by limited machine precision (limited to $32$ bits). Even with $64$ or any finite number of bits, the problem persists.  However, with $b=3$, the $x_k$'s keep oscillating properly and maintain their statistical properties forever (including randomness or lack of), due to the \textcolor{index}{ergodicity}\index{ergodicity (dynamical systems)}\index{dynamical systems!ergodicity} 
 [\href{https://en.wikipedia.org/wiki/Ergodicity}{Wiki}] of the system. The result is identical to using a new seed every $45$ iterations or so. 

The dyadic map with $b=2$, in principle, could be used to compute the binary digits of the seed  
$x_0=\sqrt{2}$, but because of the problem discussed, it does not work. Instead, I use a special recursion to compute these digits. If you replace $b=2$ by $b=2-2^{-31}$ (the closest you can get to avoid complete failure) the $x_k$'s produced by the Python code,  even though also completely wrong after $45$ iterations, behave as expected from a statistical point of view: this is a workaround to  using $b=2$. The same problem is present in other programming languages.

\subsubsection{Reverse-engineering a pseudo-random sequence}

Many of the sequences defined by a recursion $x_{k+1}=g(x_k)$, where $x_0$ is the seed, can be reverse-engineered, and are thus 
 unsafe to use when security is critical. This includes sequences produced by congruential PRNGs. By reverse engineering, I mean that if you observe $m$ consecutive digits, you can easily compute all the digits, and thus correctly ``guess" the whole sequence. In the case of the Mersenne twister, $m=624$ is conjectured to be the smallest possible value even though the period is $2^{\num{19937}}-1$, see \href{https://en.wikipedia.org/wiki/Mersenne_Twister}{here}. For the shift map in base $b$, while $x_k$ is asymptotically uniformly distributed on $[0, 1]$ if $x_0$ is random, the vectors $(x_k,x_{k+1})$ lie in a very specific configuration: $x_{k+1}-x_k$ is a small integer, making the sequence $\{x_k\}$ anything but random. As a result, for any positive integer $q$, the empirical \textcolor{index}{autocorrelation}\index{auto-correlation} 
 [\href{https://en.wikipedia.org/wiki/Autocorrelation}{Wiki}] between $(x_1, x_2,x_3,\dots)$ 
and $(x_{q+1}, x_{q+2},x_{q+3},\dots)$ computed on the infinite sequence, is equal to $1/b^q$ if $b$ is an integer $\geq 2$. 
 A good sequence should have zero autocorrelations for all $q$. 

It is possible to significantly improve the base sequence $\{x_k\}$, to make it impossible to reverse-engineer. In the case of the shift map, using $d(k)=\lfloor bx_k\rfloor$ instead of $x_k$, results in zero autocorrelations and perfect randomness if the seed $x_0$ is random. A seed such as $x_0=\sqrt{2}/2$ or $x_0=\log 2$ 
is conjectured to achieve this goal. The explanation is as follows: $d(k)$ is the $k$-th digit of $x_0$ in base $b$. Even if you were to observe $m=10^{\num{50000}}$ consecutive digits of $\sqrt{2}/2$, there is no way to predict what the next digit will be, if you don't know that $x_0=\sqrt{2}/2$. Actually even if you have that information, it is still impossible to predict the next digit. Any sequence of $m$ digits is conjectured to occur infinitely many times at arbitrary locations for a seed such as $\sqrt{2}/2$. So given any such string of $m$ digits (no matter how large $m$ is), it is impossible to tell where it takes place in the infinite sequence of digits, and thus impossible to correctly predict all the subsequent digits.    

However, because of machine precision (the problem discussed in section~\ref{twist}), the $x_k$'s generated by a computer for the shift map (or any map for that matter), eventually become periodic. Thus $\{d(k)\}$ becomes periodic too. A workaround is the use exact arithmetic to compute $d(k)$, as in my Python code in section~\ref{zw23}. Another solution is to use 
 \textcolor{index}{Bailey–Borwein–Plouffe formulas}\index{Bailey–Borwein–Plouffe formulas} [\href{https://en.wikipedia.org/wiki/Bailey\%E2\%80\%93Borwein\%E2\%80\%93Plouffe_formula}{Wiki}]
 to compute the digits. There are many BBP formulas for various good \textcolor{index}{transcendental}\index{transcendental number} seeds [\href{https://en.wikipedia.org/wiki/Transcendental_number}{Wiki}]  
  such as $x_0=\frac{\pi}{4}$, but as far as I know, none for
  the subset of \textcolor{index}{algebraic numbers}\index{algebraic number} [\href{https://en.wikipedia.org/wiki/Algebraic_number}{Wiki}] such as $x_0=\sqrt{2}/2$. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.69\textwidth]{rn1b.png}  
\caption{$L_3^*(n)$ test statistic for four sequences: Python[200] and SQRT[90,91] fail}
\label{fig:rn1}
\end{figure}
%-------------------------

%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.69\textwidth]{rn2b.png}  
\caption{$|L_3(n)|$  test statistic for four sequences: Python[200] and SQRT[90,91] fail}
\label{fig:rn2}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

\subsubsection{Illustrations}

Figures~\ref{fig:rn1} and~\ref{fig:rn2} show the core statistics of the prime test,
 defined by Formula~(\ref{oopi}): $L^*_3(n)$ and $|L_3(n)|$, for $n$ between
 $\num{1000}$ and $\num{80000}$.  If $L^*_3(n)<0.5$ or $L^*_3(n)>1.5$, the sequence $\{d(k)\}$ (the first $n$ elements) lacks true randomness; it is not 
 \textcolor{index}{strongly pseudo-random}\index{pseudo-random numbers!strongly random}. Table~\ref{tabuchi} summarizes these findings for a larger collection of sequences, 
 computed at $n=\num{20000}$.
  The notation \texttt{Python[200]} corresponds to the Python implementation 
 of the Mersenne twister, using the \texttt{random.random()} function and the seed $200$, that is, \texttt{random.seed(200)}. 
 Similarly, \texttt{SQRT[90,91]} is for the binary digits of $\sqrt{2205}$, obtained using the bivariate seed $y=90, z=91$ in the code in section~\ref{zw23}. Not surprisingly, the sequence \texttt{Base3[0.72]} fails, as $0.72=18/25$ is a rational number with a small denominator. 
 Thus $d(k)=\chi(p_k)$ is periodic with a rather small period. The column labeled \texttt{Status} in Table~\ref{tabuchi} indicates if the sequence in question fails or passes the prime test.



For convenience, I also included a type of sequences called \texttt{CounterExample}. For this type of sequences, $\chi(p_k)$ perfectly alternates between $-1$ and $+1$. One of the two resulting sequences $\{d(k)\}$ barely passes the test, the other one fails.
Now, the \texttt{Dirichlet4} sequence with seed set to $3$, has perfectly alternating $d(k)$'s and is thus non-random.  It fails the prime test, but barely.
This  means that passing this test is not a guarantee of randomness. Only failing the test is a guarantee of non-randomness. 
 

The prime test can be extended using the option \texttt{All} in the Python code. To do so, define the $L_4$ statistics as follows:
\begin{equation}
L_4(n)=\sum_{k=1}^n \chi(k), \quad L^*_4(n)=\frac{|L_4(n)|}{\sqrt{n\log\log n}}.\label{l4}
\end{equation}
Now with $L^*_4$ rather than $L^*_3$, the \texttt{Dirichlet4} sequence with the seed set to $3$ would dramatically fail the prime test,
 rather than just barely failing. It would reveal that despite the appearances, there is something definitely non random about this sequence. Indeed, it satisfies 
$\chi(4k+1)=1, \chi(4k+3)=-1$ and $\chi(2k)=0$.  The details of the $L^*_4$ version of the prime test still need to be worked out, thus I did not include it in this article. 

Finally, if you swap $-1 / +1$ in the $\{d(k)\}$ sequence, the new sequence may pass the test even if the original fails (or the other way around). This is the case for the sequence \texttt{SQRT[90,91]}. Also, the $L^*_3$ scale should be interpreted as an earthquake scale: an increase from $0.35$ to $0.45$, or from $1.3$ to $1.8$, represents a massive difference. A sequence with a low $L^*_3$ alternates too frequently compared to a random
sequence, resulting in a ratio $+1$ versus $-1$ too close to $50\%$ among the $d(k)$'s. The ratio in question corresponds to the 
 column labeled $P[d(k)=1]$ in Table~\ref{tabuchi}.



\begin{table}%[H]
\[
\begin{array}{lccccc}
\hline
\text{Sequence}	& \text{Seed}&	|L_3(n)|&	P[d(k)=1]&	L^*_3(n) & \text{Status}\\
\hline
\hline
\text{Base3} & 0.181517&239&49.49\%&1.1202 & \text{Pass}\\	
\text{Base3} & 0.72&81&49.93\%&0.3796 & \text{Fail}\\	
\hline
\text{CounterExample} & 1&137&49.69\%&0.6421 &  \text{Pass}\\	
\text{CounterExample} & 0&91&49.80\%&0.4265 &  \text{Fail}\\	
\hline
\text{Dirichlet4} & 1&113&50.11\%&0.7611 &  \text{Pass}\\	
\text{Dirichlet4} & 3&70&49.65\%&0.4715 &  \text{Fail}\\	
\hline
\text{Logistic} & 0.181517&115&49.82\%&0.539 &  \text{Pass}\\	
\text{Logistic} & 0.72&254&49.37\%&1.1905 &  \text{Pass}\\
\hline	
\text{Python} & 0&220&49.71\%&1.0311 &  \text{Pass}\\	
\text{Python} & 1&150&50.03\%&0.7031 &  \text{Pass}\\	
\text{Python} & 2&279&49.46\%&1.3077 &  \text{Pass}\\	
\text{Python} & 4&365&50.81\%&1.7108 & \text{Fail} \\	
\text{Python} & 100&386&49.10\%&1.8092 &  \text{Fail}\\	
\text{Python} & 200&922&52.29\%&4.3214 &  \text{Fail}\\	
\text{Python} & 500&258&49.67\%&1.2093 &  \text{Pass}\\	
\hline
\text{SQRT} & (2, 5)&146&49.63\%&0.6843 &  \text{Pass}\\	
\text{SQRT} & (90, 91)&1236&53.07\%&5.7932 & \text{Fail} \\
\hline
\end{array}
\]
\caption{\label{tabuchi} $L^*_3(n)$, for various sequences ($n=\num{20000}$); ``Fail" means failing the prime test}
\end{table}
%\vspace{1pt}
\begin{Exercise}\label{q23}{\bf -- Pseudo-random sequence generated by rational numbers}. Let $q_k=2^{-\{k\log_2 3\}}$ be a rational number ($k=1,2$ and so on), where the brackets represent 
the \textcolor{index}{fractional part function}\index{fractional part function} [\href{https://en.wikipedia.org/wiki/Fractional_part}{Wiki}]. 
For instance, $q_6=512/729$. Let $M_n$ be the median of
$\{q_1,\dots,q_n\}$. Thus if $n$ is odd, then $M_n$ is the middle term after rearranging the $q_k$'s in increasing order. Prove
 that (1) $M_n\rightarrow\sqrt{2}/2$ as $n\rightarrow\infty$, (2) the binary digit expansion of $q_k$ has period $2\cdot 3^{k-1}$ and (3) the proportion of  $0$ and $1$ among these digits, is exactly 50/50.
\vspace{1ex} \\
{\bf Solution} \vspace{1ex} \\
Solution to (2) and (3) is found \href{https://math.stackexchange.com/questions/3310862/what-is-the-period-of-the-fraction-1-3k-in-base-2-for-k-1-2-dots}{here}; (3) follows from the \textcolor{index}{equidistribution modulo $1$}\index{equidistribution modulo $1$} [\href{https://en.wikipedia.org/wiki/Equidistribution_theorem}{Wiki}] 
of the sequence $\{ k \log_2 3\}$.
%$\{k \og_2 3\}$. 
This implies that the $q_k$'s are distributed like $2^{-U}$ where $U$ is uniformly distributed on $[0, 1]$.
\end{Exercise}

\renewcommand{\arraystretch}{1.0} %%%
\renewcommand{\arraystretch}{1.4} %%%

\section{Python code}\label{pythonviz}

The code in section~\ref{zw23} focuses on big integers and computing the binary digits for a class of quadratic irrational numbers,
  using an original, not well-known recursion, possibly published here for the first time. This code is very short, and the description is accompanied 
 by pretty cool math. I recommend that you start  looking at it, before digging into the main program in section~\ref{prngpython}.

The main program deals with the prime test. Before that, section~\ref{fixp} discusses some generalities related to Python and other languages,
 pertaining to PRNG issues and fixes.  


 %--- 
\subsection{Fixes to the faulty random function in Python}\label{fixp}

The default Python function to generate \glspl{gls:prng} is \texttt{random.random()}, available in the \texttt{random} library.
It is based on the \textcolor{index}{Mersenne twister}\index{Mersenne twister}\index{pseudo-random numbers!Mersenne twister}\index{pseudo-random numbers!congruential generator} congruential generator [\href{https://en.wikipedia.org/wiki/Mersenne_Twister}{Wiki}], and documented \href{https://docs.python.org/3/library/random.html}{here}. As discussed in section~\ref{twist}, it is not suitable for cryptographic and other applications where pure randomness is critical. Indeed, the documentation comes with the following warning: ``The pseudo-random generators of this module should not be used for security purposes". 

One way to improve  \texttt{random.random()} is to avoid particularly bad seeds, such as $200$ or $4$, in the \texttt{random.seed()} call. You may also use binary digits of some 
\textcolor{index}{quadratic irrational numbers}\index{quadratic irrational} [\href{https://en.wikipedia.org/wiki/Quadratic_irrational_number}{Wiki}], using the Python code in section~\ref{prngpython}. Again, it is a good idea to check, using the prime test proposed in this article, which irrational numbers to avoid. Also this method may be slow, as it involves working with very big integers. A workaround is to store large tables of pre-computed digits in a secure location. The number of quadratic irrationals you can choose from is infinite. Also, your digit sequence
 should never start with the first binary digit of such numbers, but rather at a random position, to make hacking more difficult. 

For instance, to generate your sequence $\{d(k)\}$, set $d(3k)$ to $\delta(g_1(k),\alpha_1)$, set 
 $d(3k+1)$ to $\delta(g_2(k),\alpha_2)$, and set  $d(3k+2)$ to $\delta(g_3(k),\alpha_3)$ where 
\begin{itemize}
\item The numbers $\alpha_1, \alpha_1, \alpha_3$ are three quadratic irrationals, say $\sqrt{2},\sqrt{10},\sqrt{41}$,
\item The number $(\delta(k,\alpha)+1)/2$ is the $k$-th binary digit of the quadratic irrational $\alpha$, 
\item The functions $g_1,g_2,g_3$ are used for scrambling: for instance, 
$g_1(k)=5\cdot 10^5 +2k$, $g_2(k)=3\cdot 10^5 +3k$, and $g_3(k)=7\cdot 10^6 -k$.
\end{itemize}
Another solution is to use for your sequence $\{d(k)\}$ a \textcolor{index}{bitwise XOR}\index{XOR operator} [\href{https://en.wikipedia.org/wiki/Bitwise_operation}{Wiki}] on two pseudo-random sequences: the binary digits of (say) $\sqrt{2}$ and $\sqrt{41}$, starting at arbitrary positions.

There are also Python libraries that provide solutions suitable for cryptographic applications. For instance, 
\texttt{os.urandom()} 
uses the operating system to create random sequences that can not be seeded, and are thus not replicable.
See \href{https://docs.python.org/3/library/os.html#os.urandom}{here} and 
\href{https://docs.python.org/3/library/secrets.html\#module-secrets}{here}.

\subsection{Prime test implementation to detect subtle flaws in PRNG's}\label{prngpython}

The code presented here performs the prime test, computing $L_3(n)$. The variable \texttt{nterms} represents $n$, and it is set to
 $\num{10000}$. Rather than directly computing $L_3(n)$, the code iteratively computes more granular statistics, namely \texttt{minL} and \texttt{maxL}; $L_3(n)$ is the maximum between \texttt{-minL} and \texttt{maxL}, obtained at the last iteration. 

The seeds and the sequences are initialized in the main part, at the bottom. The default category \texttt{nonSquare} is used for $L_3$. The other categories, \texttt{Prime} and \texttt{All}, are respectively for $L_1$ defined in Formula~(\ref{oopi}) and $L_4$ defined in Formula~(\ref{l4}). If you use the function \texttt{createSignHash()} rather 
than the default \texttt{createSignHash2()}, you can easily compute $L_2$. The code is somewhat long only because it covers all the options discussed 
 in section~\ref{twist}, and more. It heavily relies on hash tables (dictionaries in Python) rather than arrays, because the corresponding arrays would be  rather sparse, consume a lot of memory, and slow down the computations. In addition, the code can easily handle Beurling primes (non-integer primes) thanks to the hash tables. A lengthier version named \texttt{dirichletL.py}, computing the orbit of $L_P(z,\chi)$ for $z$ in the complex plane
 when $\sigma=\Re(z)\geq \frac{1}{2}$ is fixed, for any set of primes $P$ (finite or infinite) including Beurling primes, is available on GitHub, \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/dirichletL.py}{here}. 

The Python code does not use any exotic library other than \texttt{primePy}.  To install this library,
 type in the command \texttt{pip install primePy} on the Windows Command Prompt or its Unix equivalent, as you would to install any library.
There is a possibility that some older versions of Python would require the \texttt{BigNumber} library. The code was tested under Python 3.10.
The source code, featured below, is also on GitHub: look for \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/randomNumbersTesting.py}{\texttt{randomNumbersTesting.py}}.  \\

\begin{lstlisting}
# Test randomness of binary sequences via the law of the iterated logarithm
# By Vincent Granville, www.MLTechniques.com

import math
import random
import numpy as np
from primePy import primes

#--
def createRandomDigits(method,seed): 
  primeSign={}
  idx=0
  if method=='SQRT':
    y=seed[0]
    z=seed[1]
  elif method=='Python':
    random.seed(seed)
  else:
    x=seed
  start=2
  if method=='Dirichlet4':
    start=3
  for k in range(start,nterms):
    if k%2500==0:
      print(k,"/",nterms)  
    if primes.check(k):
      primeSign[k]=1
      if method=='SQRT':
        if z<2*y:   
          y=4*y-2*z
          z=2*z+3
        else:     
          y=4*y
          z=2*z-1
          primeSign[k]=-1
      elif method=='Dirichlet4':
        if k%4==seed:
          primeSign[k]=-1
      elif method=='CounterExample':
        idx=idx+1
        if idx%2==seed:
          primeSign[k]=-1
      elif method=='Python':
        x=random.random()
      elif method=='Logistic':
        x=4*x*(1-x)
      elif method=='Base3':
        x=3*x-int(3*x)
      if method in ('Python','Logistic','Base3') and x>0.5:
        primeSign[k]=-1
  return(primeSign)

#--
def createSignHash2():
  signHash={}
  signHash[1]=1
  for p in primeSign:
    oldSignHash={}
    for k in signHash:
      oldSignHash[k]=signHash[k]
    for k in oldSignHash:
      pp=1
      power=0
      localProduct=oldSignHash[k]  
      while k*p*pp<nterms:
        pp=p*pp
        power=power+1
        new_k=k*pp
        localProduct=localProduct*primeSign[p]
        signHash[new_k]=localProduct  
  return(signHash)

#--
def createSignHash():
  # same as createSignHash() but for square-free integers only
  signHash={}
  signHash[1]=1
  for p in primeSign:
    oldSignHash={}
    for k in signHash:
      oldSignHash[k]=signHash[k]
    for k in oldSignHash:
      if k*p<nterms:
        new_k=k*p 
        signHash[new_k]=oldSignHash[k]*primeSign[p]  
  return(signHash)
 
#--
def testRandomness(category):
  signHash=createSignHash2()
  isSquare={}
  sqr=int(math.sqrt(nterms))
  for k in range(sqr):
    isSquare[k*k]=1
  count=0
  count1=0
  sumL=0
  minL= 2*nterms
  maxL=-2*nterms
  argMin=-1
  argMax=-1
  for k in sorted(signHash):
    selected=False
    if category=='Prime' and k in primeSign:
      selected=True
    elif category=='nonSquare' and k not in isSquare:
      selected=True
    elif category=='All':
      selected=True
    if selected==True:
      if signHash[k]==1:
        count1=count1+1  
      count=count+1
      sumL=sumL+signHash[k]
      if sumL<minL:
        minL=sumL
        argMin=count
      if sumL>maxL:
        maxL=sumL
        argMax=count
  return(minL,argMin,maxL,argMax,count,count1)

#--
# Main Part. Requirements:
#   0 < seed < 1 for 'Base3' and 'Logistic'; rational numbers not random
#   seed=(y,z) with z>y, z!=2y, y!=2x and x,y>0 are integers for 'SQRT'
#   swapping -1/+1 for seed=(90,91) in 'SQRT' does well, the original does not

seedMethod={}
seedMethod['Python']=(0,1,2,4,100,200,500)
seedMethod['Logistic']=(0.181517,0.72)
seedMethod['Base3']=(0.181517,0.72)
seedMethod['SQRT']=((2,5),(90,91))
seedMethod['Dirichlet4']=(1,3)
seedMethod['CounterExample']=(1,0)
categoryList=('Prime','nonSquare','All')

nterms=10000

OUT=open("prngTest.txt", "w")
for method in seedMethod:
  for seed in seedMethod[method]:
    for category in categoryList:

      primeSign=createRandomDigits(method,seed)
      [minL,argMin,maxL,argMax,count,count1]=testRandomness(category)

      string1=("%14s %9s|%5d %5d|%5d %5d|%5d %5d|" % (method,category,\
        minL,maxL,argMin,argMax,count1,count))+str(seed) 
      print(string1)
      string2=("%s\t%s\t%d\t%d\t%d\t%d\t%d\t%d\t" % (method,category,\
        minL,maxL,argMin,argMax,count1,count))+str(seed)+'\n'
      OUT.write(string2)
    
OUT.close()
\end{lstlisting}

\subsection{Special formula to compute 10 million digits of $\sqrt{2}$}\label{zw23}

The purpose of this code is twofold: to show you how to process integers with millions of digits in Python, and to offer a simple mechanism to compute the binary digits of some quadratic irrational numbers such as $\sqrt{2}/2$. The first problem is solved transparently with no special code or library  
 in Python 3.10. In short, this is a non-issue. With older versions of Python, you might have to install the \texttt{BigNumber} library. See
 documentation \href{https://pypi.org/project/BigNumber/}{here}. Nevertheless, it would be a good idea to track the size of the integers that you are working with (\texttt{y} and \texttt{z} in my code), as eventually their size will become the bottleneck, slowing down the computations. 

As for the actual computation of the digits, it is limited here to $\num{10000}$ digits, but I compare these digits with those obtained from an external source: Sagemath, see \href{https://mltblog.com/3uMZQ4s}{here}.  It shows, as it should, that both methods produce the same digits, for the number 
 $\sqrt{2}/2$ in particular.

\noindent The special recursion used for the digit computation is as follows:  \vspace{1ex} \\
\noindent  \textcolor{white}{0000}{\bf If}  $ z_k  <2y_k$   {\bf then}   \\
  \textcolor{white}{000000}  $y_{k+1}=4y_k-2z_k$\\
 \textcolor{white}{000000} $z_{k+1}=2z_k+3$\\
 \textcolor{white}{000000} $d(k)=1$ \\
\textcolor{white}{0000}{\bf else} \\  
\textcolor{white}{000000} $ y_{k+1}=4y_k$\\
\textcolor{white}{000000} $ z_{k+1}=2z_k-1$\\
\textcolor{white}{000000} $ d(k)=0$. 

\noindent The bivariate seed (the initial condition) is determined by the values of $y_0$ and $z_0$. You need $z_0>y_0$ and $z_0\neq 2y_0$. Then the binary digits $d(k)$ are those of the number 
 $$x_0 = \frac{-(z_0-1) + \sqrt{(z_0-1)^2+8y_0}}{4},$$ 
see \href{https://mltblog.com/3REtOB9}{here}. In particular, if $y_0=2, z_0=5$, then $x_0=-1+\sqrt{2}$. Using the change of variables
  $u_k=2y_k-z_k$ and $v_k = 2z_k+3$, the recurrence can be rewritten as: \vspace{1ex} \\

%\pagebreak %%

\noindent  \textcolor{white}{0000}{\bf If}  $ u_k>0$   {\bf then}   \\
  \textcolor{white}{000000} $u_{k+1}=4u_k -v_k$ \\
 \textcolor{white}{000000} $v_{k+1} = 2v_k + 3$\\
 \textcolor{white}{000000} $d(k)=1$\\
\textcolor{white}{0000}{\bf else} \\  
\textcolor{white}{000000} $u_{k+1}=4u_k + v_k-2$\\
\textcolor{white}{000000} $v_{k+1} = 2v_k-5$\\
\textcolor{white}{000000} $ d(k)=0$. 

\noindent Now $v_k-5$ is divisible by $8$. Let $w_k=(v_k-5)/8$. We have $d(k)=1$ if $w_{k+1}$ is odd, otherwise $d(k)=0$. We also have the 
 following one-dimensional backward recursion, allowing you to compute the digits backward all the way down to the first digit:\vspace{1ex} \\
\noindent  \textcolor{white}{0000}{\bf If}  $w_{k+1}$ is odd,  {\bf then}   \\
 \textcolor{white}{000000} $v_{k} = (v_{k+1} -3)/2$\\
\textcolor{white}{000000} $d(k)=1$\\
\textcolor{white}{0000}{\bf else} \\  
\textcolor{white}{000000} $v_{k} = (v_{k+1}+5)/2$\\
\textcolor{white}{000000} $d(k)=0$.

\noindent These recursions are reminiscent of the unsolved \textcolor{index}{Collatz conjecture}\index{Collatz conjecture} [\href{https://en.wikipedia.org/wiki/Collatz_conjecture}{Wiki}]. Below is the source code, also available on GitHub: look for \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/randomNumbers-sqrt2.py}{\texttt{randomNumbers-sqrt2.py}}.  \\


\begin{lstlisting}
# Comparing binary digits of SQRT(2) obtained with two different methods

# Method 1:
# 10,000 binary digits of SQRT(2) obtained via https://mltblog.com/3uMZQ4s
# Using sagemath.org. Sagemath commmand: N(sqrt(2),prec=10000).str(base=2)

sqrt2='011010100000100111100110011001111111001110111100110010010000100010110010111110110\
0010011011001101110101010010101011111010011111000111010110111101100000101110101000100100\
1110111010100001001100111011010001011110101100100001011000001100110011100110010001010101\
0010101111110010000011000001000011101010111000101000101100001110101000101100011111111001\
1011111101110010000011110110110011100100001111011101001010100001011110010000111001110001\
1110110100101001111000000001001000011100110110001111011111101000100111011010001101001000\
1000000010111010000111010000101010111100011111010011100101001100000101100111000110000000\
0100011011110000110011011110111100101010110001101111001001000100010110100010000100010110\
0010100100011000001010101111000111001000101111011111000100111000110011110001101101010110\
1010001010001110001011101101111110100111011100110010110010101001100011010000110011000111\
1100111100100001001101111101010010111100010010000011111000001101101110010110000010111011\
1010101010010010100000100010011001000001000000110010100100101010000001001110010100101010\
1101101101100011111101000011101111110111110100110100111010000000101100111010111100100100\
1111100000110001000010011001001101101010111100110101010010100010110110010100011011100011\
0011110011010000011011011011111000001000110110110001110000000100000100110111000000000111\
1111100011001000110101001011110011001100101010010111101001111101111011110110100001111010\
1111111111011010100001101111100011111111001010001000100001001100000111110111101010000001\
1000100100000111110111101010101000000111000010110000011111110010111101110111101010001011\
1101111101110000110011000110001000001110001010001011101010111111110101111100111011001011\
0100100100111101001010011101100011111110101100101110001000001011111101111111100001011100\
0011111101001110110001111101111000000011111001101011001100111001000001011110010111111100\
1010000000010111000010010001100111100001011110100100101001010101110000001000110101111111\
0011111000111101111011110010100011111010100011001110110100110101111100011000001010010111\
1001100011001111100011111000010100100001011111001110110100100101001101100000101011110001\
1000001000010110101100001001111101101001000011111001001001001001111010110110111000111111\
0000010110111001101001010010000001101100000100111011110111010001001000100101001100000111\
1010100111010101010110100001110111010100011001000011111011101001000100111110100011010100\
1111110100100000011000010111110001000001111101101110110100101001110111110110101100011001\
0011001100001001100110111010111000010100001110110101001000001000101110000111101000100110\
0111010000001101000010000100011110101101110001110000011000000111101100100000001101110101\
0011101101000110100011101100110100001000111100101101100010111001101010110010111001011011\
1000000111111110011010101000000110000110100000100101001010000101101011001000000011000010\
0000001111011110110111111000110110111101001000100010100101000101011010011110010010010001\
1000110100010011100011000000000101110110100000010101000101101010110101100000100000111111\
1010101110111100110111000001101110100001100011001101101010010000011000111111110011111111\
1111111010101110101011111100100011100010000100110000000110011011011110101011100001001101\
0000100100001011101101001011110100100011011001111100010111100100000010110110111001001110\
0100101101110010111000111010110000010100001111110001000100011110000100010100000101001101\
1100001000000001100110011101101110000101100111001011011110110011000101011101110011100011\
1100100001011100010011010001101001101111010011000000111001011110010001000000010001101000\
1100001001111111111110000100010010001010011010000111001111010101001011110101001100110011\
0110110110010011110001111001110011100011111100101001101011000001001001010101100111000010\
0100000101010111000111001010101000011100000110101010101000100010110100010000110001100010\
1110111100011001111011000000011010100001101000000101111111010000101111100101001111011001\
0001011111001011100011100101011100000000111101011110101011101110001101110000010010110110\
0110000101000001110011110000011011101101010100100011100000010001100011010100111111100001\
1111000111100110010001110110011011000101000000111010010001001010110100010011100000010110\
1011010100000100000010001111101101110011000100111110110001110101000101001100100111010000\
1010001100110111100000011010000110000011110001000100010100000000100100001001101100101001\
1110100111110011011101100111101010010110011000111010100100010011101011101010010001100011\
0110101110001100011001111000100100000001100100101101011111001010100100110111111010111011\
0010110011001111000101101001101001011000100110111011010100001100111101111011000111001001\
0000000001010011111111010101000110010000001011100110101011001111100001010111010001111010\
0111100111010011101111111100000101111010001101101001101110101110111000100010111100000000\
1010111101101101001010110110111111010101111000110110000010111000010001011001000110101011\
1111110111010111101000001110111111111101100100101101101101110001111011101111000111111000\
0011100001010111011101111001110000110110100100111101011111101011010111101000100011000001\
0001000001010010101100010101101110100000001110010100111111100011011011010111100000010110\
1111110110000011011110001101110000010100110111011011011110001100111111110000101101100100\
1110100111000001000011000011011001110100001001101110101011100011011000101100101010010011\
0000111001111010010000100011100011010101010111001101001110110000000000111100101011110010\
1000100010110111100011000001010001111100000101001001111110110001001011010001110101101111\
0010100101111011100011100000010110101101001010001011101101010010100101100000100101001000\
1000000110010101011110010010100001110000111110000111101001001101111110100001111001110111\
1101000101011110110001100000101101001010011101000010111011100101000100010101100101000010\
0111111101101000000011011001001100000101001000101011101100000111011101000001101001101010\
1011000110110000001110111010001000101011001111010010110010000111110101010100010011111101\
0110111011101010111101000100000010100101001011101011011011011110100101010001000100111100\
0111101000010010010101110110001110001101000010101001000000011011100001011101001100110010\
1000111101100110111001011011110110110100000010111100010000110010010111110010101101100011\
0111001001001100101000100101011010000000100110000110011000110000001110101100100101000110\
1011001101010001001101101101111100010000110010011110011110101010111010100110001100100110\
1011001101010100011100001100010010101111010100100111010010101111010010110010111011101010\
1000111111110100000111101100000011011001100001100101110111010101000010011101111011100010\
0000011110100101101001011010101011110110110110000010101001110100101111100110111110000101\
1000111011111111110001010010010100011010011001000011011001010010100011100001110101011100\
1000001010110001011001111001110110111000110010010101101111000011110010000110001101000010\
0111000101110110111101111100111001100000101101101000110111101111111111000001011110001110\
1010100111111110011111011000000010000101011110111001000010110110011001001011001100101100\
1010010001010101000011110111000011001000011111000111101000101111000011100001111100011000\
0100101101101011001011110000000011000001101010101011011100000101111110101001110110100111\
1110011111000010101100010010010101101011011010100100011011000011000001011001011001010000\
0110001111101000100110010100010011110101101000110100110011011111101010101101111110111000\
0101001101001111100011110101001001111100100001001111001101010100001001011010100011000001\
0100000110101001001010010010101101101101011111001001101110001110011110111011111100101101\
1101110100000001010100111011111110000100010110110001101001110000101111010100010111011001\
1001000000000011101011101000000110000011001100100001100011110111111001010000100010110000\
1001000000001010010011110111011100010111010100000010000100111010010011101110000110000111\
1111111110111100001001000011010011110111110010000110100111110000011110100001010101010111\
1111011110111100101001100011111011101001011101011111101101111000010110110101111101111101\
1001110000000101110011001011011101101110111101110001111101110111000110011011011010010101\
1011000111001001011100111101111000000011110001011001001000011111000000001101001110100100\
0111110110011111011000011001000111000101110001001111101011001000111110110101010010000110\
1110110001010100110010010111001110001111101011101000101001101000100111111011011010100010\
1101111110001010110111110100001111010101110011010101001010110000111010011111011010100000\
0011111011010011011000101111011011001101011100000011101111110111010000100011110110101010\
0100100000101011101011111101110001111100101001111111001010110000101100101000110101110110\
0100000100000010110000101111000000001011001100101101000000001011011101101001111100111011\
0100111100101101111011000110011110010011101000100111111011011000001011100101100001110101\
1111110001101000000000110100011001011101001100111011101011010100010010001111110001001111\
0010110000000001010001111110101110000001001100110010001101001100101010101001010001101010\
0000000000011111111111000001001000010101011111110111100001011011000101001001010100010000\
0010100010110011110110100010010011110100101010011001100111011001011011000111100111101100\
0110111001110100111000100011100100111001111100100010101011001101100001110010111101100101\
0000010011110001010011111110000101110111000101110111100010001010001011011011101110010101\
0110000111101101110100101010101011101000100100011111001001111101110101110000000111101101\
0011111001101001011010100100101110001110110000011110011001110000000111011111001111011011\
0111000101011001110111011111010100011011011001111000111110011100010011011100100001010101\
0100101100110000100110101001101101110010101011000010010111011000101001010001010001001000\
1011111110011110100001111011011001101110101011111110000101000001010000111111100001001001\
1111110001000011000111101001011011000110000011100100000100100000100000001000100101100100\
0110010111100111000110001000000001100101111000111000011101010111000100101110001110111010\
1001111011110010110011011011101100101110110000101001011101011101000110101111001011110100\
0001000010010001011000111010101110011111101101100010011011111010001'

size=len(sqrt2)

# Method2:
# 10,000 binary digits of SQRT(2) obtained via formula at https://mltblog.com/3REtOB9
# Implicitly uses the BigNumber Python library (https://pypi.org/project/BigNumber/)

y=2
z=5
for k in range(0,size-1): 
  if z<2*y:
    y=4*y-2*z
    z=2*z+3
    digit=1 
  else:
    y=4*y
    z=2*z-1
    digit=0 
  print(k,digit,sqrt2[k])
\end{lstlisting}


%------------------------------------------------------------------------------------------------------------------
\Chapter{Some Unusual Random Walks}{Testing and Leveraging Quasi-randomness}

This is a follow-up to chapter~\ref{chapterPRNG} about ``Detecting Subtle Departures from Randomness", where I introduced the prime test to identify very weak violations of
 various laws of large numbers. Pseudo-random sequences failing this test usually pass most test batteries, yet are unsuitable for a number of applications, such as security, strong cryptography, or intensive simulations. The purpose here is to build such sequences with very low, slow-building, long-range dependencies, but that otherwise appear as random as pure noise. They are useful not only for testing and benchmarking tests of randomness, but also in their own right to model almost random systems, such as stock market prices. I introduce new categories of random walks (or quasi-Brownian motions subject to constraints),  and discuss the peculiarities of each category. For completeness, I included related stochastic processes discussed in
 some of my previous articles, for instance integrated and 2D clustered Brownian motions. All the processes investigated here 
 are drift-free and symmetric, yet not perfectly random. They all start at zero.


\hypersetup{linkcolor=red}


\section{Symmetric unbiased constrained random walks}\label{pivizintrou}

The standard symmetric 1D \textcolor{index}{random walk}\index{random walk} [\href{https://en.wikipedia.org/wiki/Random_walk}{Wiki}] fundamental to this article is a sequence $\{S_n\}$ with $n\geq 0$, starting at $S_0=0$, and recursively defined by 
$S_{n}=X_{n}+S_{n-1}$, for $n>0$.  Here $X_1,X_2$ and so on are independent random variables with $P[X_n=1]=P[X_n=-1]=\frac{1}{2}$. 
 Thus $\{S_n\}$ is a time-discrete stochastic process, and indeed the most basic one. In sections~\ref{azxa} and~\ref{azxb}, I drop the assumption
 of independence, leading to modified random walks such as those described in~\cite{nkrn2018,lanwu2012}.
More general references include~\cite{gtm2021,peresbrown}.

With proper rescaling, a random walk becomes a time-continuous stochastic process $S_t$ called \textcolor{index}{Brownian motion}\index{Brownian motion} [\href{https://en.wikipedia.org/wiki/Brownian_motion}{Wiki}], with $t\in\mathbb{R}^+$. See the time series in gray in Figure~\ref{fig:walk}, displaying a particular instance: it shows the first $\num{50000}$ values of
 $S_n$ in a short window, giving the appearance of a Brownian motion. 
By contrast, each of the orange, red and gray time series represents one instance of a specific type of non-Brownian motion. Sections~\ref{azxa} and~\ref{azxb} focuses on these three types of processes,  which are quasi, but not fully random.





\subsection{Three fundamental properties of pure random walks}\label{poyt}

The standard random walk $\{S_n\}$ (illustrated in gray in Figure~\ref{fig:walk}) is the base or reference process, used to build more sophisticated models. It has too many properties to list in this short article. However, the following are the most relevant to our discussion.

\begin{itemize}
\item \textcolor{index}{Law of the iterated logarithm}\index{law of the iterated logarithm}\index{iterated logarithm} [\href{https://en.wikipedia.org/wiki/Law_of_the_iterated_logarithm}{Wiki}]. 
 In our context, it is stated as follows:
\begin{equation}
\lim \sup \frac{|S_n|}{\sqrt{2nv\log \log n}} = 1 \quad \text{as } n\rightarrow \infty.\label{lil12}
\end{equation}
Here, as per the 
 \textcolor{index}{Hartman–Wintner theorem}\index{Hartman–Wintner theorem} [\href{https://encyclopediaofmath.org/wiki/Law_of_the_iterated_logarithm}{Wiki}],  $v=\text{Var}[X_1]=1$. See~\cite{peresbrown} pages 118--123 for a version adapted to Brownian motions.
\item Expected number of \textcolor{index}{zero crossings}\index{random walk!zero crossing} in $S_1,\dots,S_n$, denoted as $N_n$. Here a zero-crossing is an index $0<k\leq n$
 such that $S_k=0$. For $n>0$, we have (see \href{https://math.stackexchange.com/questions/1684576/expected-of-returns-in-a-symmetric-simple-random-walk}{here}): 
$$
\text{E}[N_{2n}]=-1+\frac{2n+1}{4^n} \binom{2n}{n}  \sim  \frac{2}{\sqrt{\pi}}\cdot \sqrt{n}  \quad \text{as } n\rightarrow \infty.
$$
\item Distribution of \textcolor{index}{first hitting time}\index{random walk!first hitting time} to zero [\href{https://en.wikipedia.org/wiki/First-hitting-time_model}{Wiki}], or
 first zero crossing after $S_0=0$, also called time of first return. The random variable in question is denoted as $T$. It is defined as follows: 
$T=n$ (with $n>0$) if and only if $S_{n}=0$ and $S_k\neq 0$ if $0<k<n$. We have $P[T=n]=0$ if $n$ is odd, and $\text{E}[T]=\text{Var}[T]=\infty$. Yet, our random walks cross the X-axis infinitely many times.   We also have the following
 \textcolor{index}{probability generating function}\index{probability generating function} [\href{https://en.wikipedia.org/wiki/Probability-generating_function}{Wiki}] (see \href{https://math.stackexchange.com/questions/64919/biased-random-walk-and-pdf-of-time-of-first-return}{here}):
$$
\sum_{n=1}^\infty  (2x)^{2n} P[T=2n] =1-\sqrt{1-4x^2} \quad \text{if } x\leq \frac{1}{4}.
$$
 From there, one can obtain 
\begin{align}
P[T=2n] & =\frac{1}{(2n-1)4^n}\binom{2n}{n}\sim \frac{1}{\sqrt{4\pi}}\cdot n^{-3/2} \quad \text{as } n\rightarrow \infty,\nonumber \\
\text{E}[T^{-1}] & = \int_{0}^{1/2} \frac{1-\sqrt{1-4x^2}}{x}dx = 1-\log 2. \nonumber
\end{align}
\end{itemize}

\noindent Note that $\text{E}[T^{-1}]$ is finite, while $\text{E}[T]$ is infinite. The fact that
 $\text{E}[T]=\infty$ explains why the sequence $S_n$ can stay above or below the X-axis for incredibly long time periods, as shown
 in Figure~\ref{fig:walk} for the gray curve. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.8\textwidth]{walk.png}  
\caption{Typical path $S_n$ with $0\leq n\leq \num{50000}$ for four types of random walks}
\label{fig:walk}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

The above three statistics $|S_n|/\sqrt{2n\log\log n}, N_{2n}$ and $T^{-1}$can be used to design tests of randomness for pseudo-random number generators. Indeed, the \textcolor{index}{prime test}\index{pseudo-random numbers!prime test}\index{prime test (of randomness)} in chapter~\ref{chapterPRNG}  relies on a number theoretic version of the law of the iterated logarithm (LIL). The purpose is to detect very weak departures from randomness, even in sequences that are random enough to pass the classic LIL test, yet not fully random. In this article, the goal is to
 simulate quasi random sequences, rather than creating new tests of randomness.
 
I now describe special types of modified random walks that lack true independence in the sequence $\{X_n\}$. In particular, I discuss why
 they are special and of great interest, with a focus on applications.


\subsection{Random walks with more entropy than pure random signal}\label{azxa}

One way to introduce dependencies in the sequences is to increase the frequency of oscillations (and thus the entropy) in the gray curve in 
 Figure~\ref{fig:walk}. The gray curve represents a realization of a pure random walk. 
 To achieve this goal, you may want the sequence to violate the law of the iterated logarithm: you want to build a sequence that would satisfy a modified law of the iterated logarithm with $\sqrt{2n\log\log n}$ in Formula~(\ref{lil12}) replaced by (say) $n^{2/5}$. 

To accomplish this, you need to add constraints when simulating the sequence in question.  Yet you want to preserve quasi randomness: 
the absence of drifts and auto-correlations in 
 the sequence $\{X_n\}$, even though there is some modest lack of independence. So modest indeed that most statistical tests would fail to catch it, even though it can made highly visible to the naked eye: see the red, and especially the blue curve in Figure~\ref{fig:walk}. 

\subsubsection{Applications}

Such sequences can be used to generate \gls{gls:syntheticdata}\index{synthetic data} (see chapter~\ref{chapterregression}) or to model barely constrained stochastic processes, such as stock price fluctuations in an almost perfect market. See also~\cite{pac203}. Another application is  
 to introduce an undetectable backdoor in some encryption systems without third parties (government or hackers) being able to notice it, depending on the strength of the dependencies. This type of backdoor can help the encryption company decrypt a message when requested by a 
 legitimate user who lost his key, even though the encryption company has no standard mechanism to store or retrieve keys (precisely to avoid government interference). 

This assumes that there is a mapping between the  type of weak dependencies introduced in a specific sequence, and the 
 type of algorithm (or the key) used to decrypt the sequence in question. The mapping can be made too loose for full decryption even by the parent company, but helpful to retrieve partial data, such as where the sequence originates from: in this case, the type of dependencies is a proxy for a signature. All that is needed is to add some extra bits so that the sequence has the desired statistical behavior.

Ironically, you need a very good, industrial-grade \textcolor{index}{pseudo-random number generator}\index{pseudo-random numbers} (PRNG) to generate almost perfectly random sequences. PRNG's that are not good enough -- such as the \textcolor{index}{Mersenne twister}\index{Mersenne twister}\index{pseudo-random numbers!Mersenne twister} -- may introduce irregularities that can interfere with the ones you want to introduce. This is discussed 
 in detail in chapter~\ref{chapterPRNG} on PRNG's.

\subsubsection{Algorithm to generate quasi-random sequences}\label{qrrnd}

One way to generate such sequences is as follows: \vspace{1ex}

\noindent\textcolor{white}{00} $S=0$ \\
\textcolor{white}{00} {\bf For} $n = 1, 2,\dots$\\
\textcolor{white}{0000}  Generate random deviate $U$ on $[0,1]$\\
\textcolor{white}{0000}  $M=g(n)$\\
\textcolor{white}{0000}  {\bf If} ($S< -M$ and $U < \frac{1}{2}-\epsilon$) 
or ($S> M$ and $U < \frac{1}{2}+\epsilon$) or ($|S|\leq M$ and $U<\frac{1}{2}$) \\ 
\textcolor{white}{0000} {\bf Then} \\
\textcolor{white}{000000} $X_n=-1$ \\
\textcolor{white}{0000} {\bf Else} \\
\textcolor{white}{000000} $X_n=1$\\
\textcolor{white}{0000} $S=S+X_n$\\
\textcolor{white}{0000} $S_n=S$ \vspace{1ex}

\noindent Here $0<\epsilon<\frac{1}{2}$ and $\alpha>0$.  The function $g(n)$ is positive and 
 growing more slowly than $\sqrt{n}$. Typically, $g(n)=\alpha n^\beta$ with $0\leq \beta\leq\frac{1}{2}$, or $g(n)=\alpha (\log n)^\beta$
 with $\beta\geq 0$. 
The Python code in section~\ref{paths} performs this simulation: choose the option \texttt{deviations='Small'}. You can customize the function
 $g(n)$, denoted as \texttt{G} in the code. The option \texttt{mode='Power'} corresponds to $g(n)=\alpha n^\beta$,
 while \texttt{mode='Log'} corresponds to $g(n)=\alpha (\log n)^\beta$. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.9\textwidth]{iteratedlog1b.png}  
\caption{$\delta_n=1-\text{Var}[S_{n+1}]+\text{Var}[S_n]$ for four types of random walks, with $0\leq n\leq\num{5000}$}
\label{fig:lollog1b}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------



\noindent Results are displayed in 
 Figure~\ref{fig:walk}. The color scheme is as follows:
\begin{itemize}
\item Gray curve: $\epsilon=0$, corresponding to a pure random walk.
\item Blue curve: $g(n)=\log n$, $\epsilon=0.05$.
\item Red curve: $g(n)=n^\beta$ with $\beta=0.35$, $\epsilon=0.05$.
\end{itemize}
The yellow curve represents a very different type of process, discussed in section~\ref{azxb}.

\subsubsection{Variance of the modified random walk}

The symmetric nature of the modified random walk $\{S_n\}$ defined in section~\ref{qrrnd} results in several
identities.  Let $p_n(m)=P(S_n=m)$, with $-n\leq m \leq n$. Also, let $S_0=0$ and $p_0(0)=1$. Then $p_n(m)$ can be recursively computed using some modified version of the Pascal triangle recursion:
\begin{equation}
p_{n+1}(m)=\Big[\frac{1}{2}+\epsilon \cdot A_n(m-1)\Big]p_n(m-1)+\Big[\frac{1}{2}-\epsilon\cdot A_n(m+1)\Big]p_n(m+1),\label{zxzdc}
\end{equation}
where $A_n(m)=\chi[m<-g(n)]-\chi[m>g(n)]$. Here $\chi$ is the indicator function: $\chi(\omega)=1$ if $\omega$ is true, otherwise $\chi(\omega)=0$.
Some of the identities in question include:
$$
\sum_{m=-n}^n m \cdot p_n(m)=\text{E}[S_n]=0,\quad
\sum_{m=-n}^n A_n(m)p_n(m)=0,\quad
\sum_{m=-n}^n m^2 A_n(m)p_n(m)=0,
$$
$$
\sum_{m=-n}^n m^2 \Big[A_{n-1}(m-1)p_{n-1}(m-1)+A_{n-1}(m+1)p_{n-1}(m+1)\Big]=0.
$$
From these identities, it is easy to establish a recursion for the variance:
\begin{equation}
\text{Var}[S_{n+1}]=\text{Var}[S_n]+1-\delta_n, \quad \text{with }\delta_n=8\epsilon\cdot \sum_{m>g(n)} m \cdot p_n(m).\label{varzes}
\end{equation}
The sum for $\delta_n$ is finite since $p_n(m)=0$ if $m>n$.   Of course, 
$\text{Var}[S_0]=0$. Also, if $\epsilon=0$, the sequence is perfectly random: $\delta_n=0$, $\text{Var}[S_n]=n$ and
 $S_n/\sqrt{n}$ converges to a normal distribution. In turn, the law of the iterated logarithm is satisfied. Conversely, 
 this is violated if $\epsilon>0$. Formula~(\ref{varzes}) 
 combined with \textcolor{index}{Hoeffding's inequality}\index{Hoeffding inequality} [\href{https://en.wikipedia.org/wiki/Hoeffding\%27s_inequality}{Wiki}], may
 provide some bounds for $\text{Var}[S_n]$. 

\noindent Figure~\ref{fig:lollog1} shows $\delta_n$ for four types of modified random walks, using the following color scheme:
\begin{itemize}
\item Yellow: $g(n)=10,\epsilon=0.05$
\item Red: $g(n)=n^\beta,\beta=0.50, \epsilon=0.05$
\item Blue: $g(n)=n^\beta,\beta=0.45, \epsilon=0.05$
\item Purple: $g(n)=n^\beta,\beta=0.55, \epsilon=0.05$
\end{itemize}
The curve that coincides with the X-axis ($\delta_n = 0$) corresponds to $\epsilon=0$, that is, to  pure randomness regardless of $g(n)$. It is not
 colored in Figure~\ref{fig:lollog1}. Finally, the Python code in section~\ref{pypy1bv} computes $\text{Var}[S_n]$ exactly (not via simulations) using two different methods, proving that
 Formula~(\ref{varzes}) is correct. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.9\textwidth]{iteratedlog1.png}  
\caption{Same as Figure~\ref{fig:lollog1b}, using a more aesthetic but less meaningful chart type}
\label{fig:lollog1}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

\subsection{Random walks with less entropy than pure random signal}\label{azxb}

In section~\ref{azxa}, I focused on creating sequences with higher oscillation rates than dictated by randomness, resulting in lower amplitudes.
Doing the opposite -- decreasing the oscillation rate -- is more difficult. For instance, using $g(n)=n^\beta$ with $\beta>\frac{1}{2}$ won't work. 
 You can't do better than $\sqrt{n}$ because of the law of the iterated logarithm: boosting $\beta$ beyond the threshold $\frac{1}{2}$  is
 useless.

\noindent A workaround is to use the following algorithm: \vspace{1ex}

\noindent\textcolor{white}{00} $S=0$ \\
\textcolor{white}{00} {\bf For} $n = 1, 2,\dots$\\
\textcolor{white}{0000}  Generate random deviate $U$ on $[0,1]$\\
\textcolor{white}{0000}  $M=g(n)$\\
\textcolor{white}{0000}  {\bf If} ($-M< S< 0$ and $U < \frac{1}{2}+\epsilon$) 
or ($0<S< M$ and $U < \frac{1}{2}-\epsilon$) or ($S=0$ and $U<\frac{1}{2}$) \\ 
\textcolor{white}{0000} {\bf Then} \\
\textcolor{white}{000000} $X_n=-1$ \\
\textcolor{white}{0000} {\bf Else} \\
\textcolor{white}{000000} $X_n=1$\\
\textcolor{white}{0000} $S=S+X_n$\\
\textcolor{white}{0000} $S_n=S$ \vspace{1ex}

\noindent The Python code in section~\ref{paths}, with the option \texttt{deviations='Large'}, performs this simulation. The yellow time series in 
 Figure~\ref{fig:walk} is a realization of such a modified random walk, in this case with $g(n)=\alpha n^\beta$, with 
 $\alpha=0.30,\beta=0.54$ and $\epsilon=0.01$. It is unclear if the yellow curve will ever cross again the horizontal axis after
 $\num{50000}$ iterations, but it is expected to do so. To the contrary, the other three curves (gray, red, blue) are guaranteed to cross the
 horizontal axis infinitely many times, even though the random variable $T$ measuring the spacing between two crossings 
 (referred to as the \textcolor{index}{hitting time}\index{random walk!first hitting time} in section~\ref{poyt}) has infinite expectation.

For pure random walks (the gray curve in Figure~\ref{fig:walk}), the average number of times that $S_k=0$ when $0<k\leq 2n$
 is asymptotically equal to $\sqrt{4n/\pi}$ , as discussed in section~\ref{poyt}. One would expect this value to be about $178$
 when $2n=\num{50000}$. For the gray curve, the observed value is $243$. Keep in mind that huge variations are expected between different realizations of the same random walk, due to the fact that $\text{E}[T]=\infty$. Indeed, averaged over three realizations, the value $243$ was down to 
$185$. Also, a faulty pseudo-random number generator could easily lead to results that are off, in this case.

One would expect much larger 
 values for the ``non-random" red and blue curves. The observed 
values are respectively $747$ and $1783$, based on a single realization in each case. Likewise, the yellow curve is expected to have a much smaller value: in Figure~\ref{fig:walk}, 
that value is $105$.

 
\section{Related stochastic processes}  

There are countless types of random walks or quasi-Brownian motions that are -- on purpose and by design -- not perfectly random. One could write an encyclopedia on
 this topic. A good reference is the book by Mörters and Peres~\cite{peresbrown}, published in 2010.
My goal in this section is to present two examples (one in two dimensions) that are very recent, interesting, and related to the material 
 discussed in section~\ref{pivizintrou}. I built these stochastic processes in the last two years, to address modeling issues with fintech applications in mind.



\subsection{From Brownian motions to clustered Lévy flights}\label{lvfgf}

Here I discuss a 2B Brownian motion generated using some specific probability distributions. Depending on the parameters, these distributions may or may not have an infinite expectation or variance. Things start to get interesting when the expectation becomes infinite (and the Brownian motion is no
 longer Brownian), resulting in a system exhibiting a strong clustering structure. 

In some sense, it is similar to the examples studied earlier, where moving away from the law of the iterated logarithm resulted in unusual patterns: either very strong or very weak oscillations. Note that all the simulations performed here consist of discrete random walks rather than 
 time-continuous Brownian motions.
 They approach Brownian motions very well, but since modern computers (at least to this date) are ``digital" as opposed to ``analog", everything is broken down into bits, and is thus discrete, albeit with a huge granularity.
   
  In one dimension, we start with $S_0=0$ and $S_n=S_{n-1}+R_n\theta_n$, for $n=1,2$ and so on. If the $R_n$'s are independently and identically distributed (iid) with an exponential distribution of expectation $1/\lambda$ and $\theta_n=1$, then the resulting process is a stationary 
\textcolor{index}{Poisson point process}\index{Poisson point process} [\href{https://en.wikipedia.org/wiki/Poisson_point_process}{Wiki}] of 
intensity function $\lambda$ on $\mathbb{R}^{+}$; the $R_n$'s are the successive 
interarrival times \textcolor{index}{interarrival times}\index{interarrival times}. If the $\theta_n$'s are iid with $P(\theta_n=1)=P(\theta_n=-1)=\frac{1}{2}$, and independent from the $R_n$'s, then we get a totally different type of process, which, after proper re-scaling, represents a time-continuous 
 \textcolor{index}{Brownian motion}\index{Brownian motion} in one dimension. For general references, see \cite{daleyA2002,daleyB2008}.


%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.5\textwidth]{brownian.png}  
\caption{Clustered Brownian process}
\label{fig:lolbrown}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

I generalize it to two dimensions, as follows. Start with $(S_0,S'_0)=(0,0)$. Then generate the points $(S_n, S'_n)$, with $n=1,2$ and so on, using the recursion
\begin{align}
S_n &  =  S_{n-1}+R_n \cos(2\pi\theta_n) \label{brown10} \\
S'_n & = S'_{n-1}+ R_n\sin(2\pi\theta_n) \label{brown11}
\end{align}
where $\theta_n$ is uniform on $[0, 1]$, and the radius $R_n$ is generated using the formula
\begin{equation}
R_n=\frac{1}{\lambda}\Big(-\log(1-U_n)\Big)^\gamma, \label{gam11}
\end{equation}
where $U_n$ is uniform on $[0,1]$. Also, $\lambda>0$, and the random variables $U_n,\theta_n$ are all independently distributed. If $\gamma>-1$, then
$\mbox{E}[R_n]=\frac{1}{\lambda}\Gamma(1+\gamma)$ where $\Gamma$ is the \textcolor{index}{gamma function}\index{Gamma function} 
[\href{https://en.wikipedia.org/wiki/Gamma_function}{Wiki}]. In order to standardize the process, I use
$\lambda=\Gamma(1+\gamma)$. Thus, $\mbox{E}[R_n]=1$ and if $\gamma>-\frac{1}{2}$,
$$\mbox{Var}[R_n]=\frac{\Gamma(1+2\gamma)}{\Gamma^2(1+\gamma)}-1.$$
We have the following cases:


\begin{itemize}
\item If $\gamma=1$, then $R_n$ has an exponential distribution.
\item If $-1<\gamma<0$, then $R_n$ has a \textcolor{index}{Fréchet distribution}\index{Fréchet distribution}\index{probability distribution!Fréchet}. If in addition, $\gamma>-\frac{1}{2}$, then its variance is finite. 
\item If $\gamma>0$, then $R_n$ has a \textcolor{index}{Weibull distribution}\index{Weibull distribution}\index{probability distribution!Weibull}, with finite variance. 
\end{itemize}
Interestingly, the Fréchet and Weibull distributions are two of the three 
\textcolor{index}{attractor distributions}\index{attractor distribution} in \textcolor{index}{extreme value theory}\index{extreme value theory}.  

The two-dimensional process consisting of the points $(S_n,S'_n)$ is a particular type of random walk. The random variables $R_n$ represent the (variable) lengths of the successive increments. Under proper re-scaling, assuming the variance of $R_n$ is finite, it tends to a time-continuous
two-dimensional Brownian motion. However, if $\mbox{Var}[R_n]=\infty$, it may not converge to a Brownian motion. Instead, it is very similar to a 
\textcolor{index}{Lévy flight}\index{Lévy flight}\index{Brownian motion!Lévy flight} [\href{https://en.wikipedia.org/wiki/L\%C3\%A9vy_flight}{Wiki}], and produces a strong cluster structure, with well separated clusters. See Figure~\ref{fig:lolbrown}, based on $\gamma=-\frac{1}{2}$, $\lambda=8$, and featuring the first $\num{10000}$ points
 of the bivariate sequence $\{(S_n,S'_n)\}$. 

The 
Lévy flight uses a \textcolor{index}{Lévy distribution}\index{Lévy distribution}\index{probability distribution!Lévy}
[\href{https://bit.ly/3rV7mrq}{Wiki}] for $R_n$, which also has infinite expectation and variance. Along with 
the \textcolor{index}{Cauchy distribution}\index{Cauchy distribution}\index{probability distribution!Cauchy} (also with infinite expectation and variance), it is one of the few \textcolor{index}{stable distributions}\index{stable distribution} [\href{https://en.wikipedia.org/wiki/Stable_distribution}{Wiki}]. Such distributions are attractors 
for an adapted version of the \textcolor{index}{Central Limit Theorem}\index{central limit theorem} (CLT), just like the Gaussian distribution is the attractor
for the CLT. A well written, seminal book on the topic, is ``Limit Distributions for Sums of Independent Random Variables", by Gnedenko and Kolmogorov \cite{gk1954}.


For a simple introduction to Brownian and related processes, see the website RandomServices.org by Kyle Siegrist, especially the chapter on
standard Brownian motions, \href{https://www.randomservices.org/random/brown/Standard.html}{here}. The processes discussed in
 section~\ref{lvfgf} are further investigated in my book ``Stochastic Processes and Simulations: A Machine Learning Perspective"~\cite{vgsimulnew}.
 
%-------------
%xxxxx
%put code on github: both Py scripts


\subsection{Integrated Brownian motions and special auto-regressive processes}

The Brownian motions pictured in Figure~\ref{fig:linearbv2} are generated by simple time-discrete \textcolor{index}{auto-regressive time series}\index{auto-regressive process}\index{time series!auto-regressive} 
 [\href{https://en.wikipedia.org/wiki/Autoregressive_model}{Wiki}]. Thus, the base process is auto-correlated, but the limit (after rescaling) is still
 a standard Brownian motion and thus perfectly random, if the auto-correlation structure is weak enough.  

This \gls{gls:armodels} (AR) model is driven initial conditions $S_1,\dots,S_p$ and the recursion
$$
S_n=a_1 S_{n-1}+\dots + a_p S_{n-p}+e_n,
$$ 
where $a_1,\dots,a_p$ are real coefficients satisfying some conditions to guarantee \textcolor{index}{stationarity}\index{stationary process}. By choice, the sequence $\{e_n\}$ is  a \textcolor{index}{white noise}\index{white noise}: $\text{E}[e_n]=0$, $\text{Var}[e_n]=\sigma^2$ is fixed (it does not depend on $n$), and the 
 $e_n$'s are independently and identically distributed. 

%----------------------
\begin{figure}[H]
\centering
\includegraphics[width=0.94\textwidth]{linear.png} %0.94
\caption{AR models, classified based on the types of roots of the characteristic polynomial}
\label{fig:linearbv2}
\end{figure}
%------------------------

The behavior of this process, and thus of the resulting Brownian motions pictured in Figure~\ref{fig:linearbv2}, is determined by the roots of
 its  \textcolor{index}{characteristic polynomial}\index{characteristic polynomial} of degree $p$:
$$
x^p=a_1 x^{p-1}+a_2 x^{p-2}+\dots + a_{p-1}x+ a_p.
$$
These roots can be real or complex, and simple or multiple. A borderline case, as far as stationarity is concerned, is when the root with highest modulus    
 has  a \textcolor{index}{modulus}\index{modulus (complex number)} equal to $1$. If the modulus in question is above $1$, the process is no longer stationary. The modulus
 of a real number $a$ is its absolute value $|a|$, and for a complex number $a+bi$, it is defined as $\sqrt{a^2+b^2}$. If in addition the root 
 with largest modulus is 
 multiple, then something unusual happens: the resulting Brownian motion is very smooth and is not a Brownian motion anymore. 
  It becomes is an integrated Brownian motion; its derivarive is a Brownian motion.  See top left plot in Figure~\ref{fig:linearbv2}. The other plots in the same figure correspond to other akward situations, regarding the roots of the characteristic polynomials and the resulting behavior. This is discussed in detail in chapter~\ref{chapterlinear} on linear algebra.




\renewcommand{\arraystretch}{1.0} %%%
\renewcommand{\arraystretch}{1.4} %%%

\section{Python code}\label{pythonviz}

Section~\ref{pypy1bv} covers the exact computation of the variances, while section~\ref{paths} focuses on simulations: generating realizations of 
 the sequence $\{S_n\}$, for various types of quasi-random walks described in section~\ref{pivizintrou}.


\subsection{Computing probabilities and variances attached to $S_n$}\label{pypy1bv}

This Python code is related to section~\ref{qrrnd}, where you can find more details.
It computes the variance of $S_n$ for $n=1, 2$ and so on, using two different methods:
 one based on the standard definition of the variance (denoted as \texttt{var1} in the code), and one based 
 on Formula~\ref{varzes}. The latter is denoted as \texttt{var2} in the code.  Also, the variable \texttt{delta}
 represents $\delta_n$. The output $\delta_n$ is featured in Figure~\ref{fig:lollog1b}.  Finally the function \texttt{G} represents $g(n)$. 
The code below is also available on Github, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/brownian_var.py}{here}. 
Program name: \texttt{brownian\_var.py}. \\

%\pagebreak %

\begin{lstlisting}
import math

epsilon=0.05 
beta=0.45
alpha=1.00 
nMax=5001

Prob={}
Exp={}
Var={}
Prob[(0,0)] =1
Prob[(0,-1)]=0 
Prob[(0,1)] =0 
Prob[(0,-2)]=0 
Prob[(0,2)] =0 

def G(n):
 return(alpha*(n**beta))

def psi(n,m):
  p=0.0
  if m>G(n): 
    p=-1
  if m<-G(n):  
    p=1
  return(p)

Exp[0]=0
Var[0]=0
OUT=open("rndproba.txt","w")
for n in range(1,nMax):
  Exp[n]=0
  Var[n]=0
  delta=0
  for m in range(-n-2,n+3,1):
    Prob[(n,m)]=0
  for m in range(-n,n+1,1):
    Prob[(n,m)]=(0.5+epsilon*psi(n-1,m-1))*Prob[(n-1,m-1)]\
        +(0.5-epsilon*psi(n-1,m+1))*Prob[(n-1,m+1)]
    Exp[n]=Exp[n]+m*Prob[(n,m)]
    Var[n]=Var[n]+m*m*Prob[(n,m)]
    if m>G(n-1) and m<n: 
      delta=delta+8*epsilon*m*Prob[(n-1,m)]
  var1=Var[n]
  var2=Var[n-1]+1-delta
  string1=("%5d %.6f %.6f %.6f" % (n,var1,var2,delta))
  string2=("%5d\t%.6f\t%.6f\t%.6f\n" % (n,var1,var2,delta))
  print(string1)
  OUT.write(string2) 
OUT.close() 
\end{lstlisting}


\subsection{Path simulations}\label{paths}

This Python code performs all the simulations discussed in sections~\ref{qrrnd} and~\ref{azxb} and shown in Figure~\ref{fig:walk}. The option
 \texttt{deviations='Small'} is discussed in detail in section~\ref{qrrnd}, while \texttt{deviations='Large'} is
 explained in section~\ref{azxb}. The function \texttt{G} in the code corresponds to $g(n)$. Also, if you want to simulate a perfectly random walk, set $\epsilon$ (the parameter \texttt{eps} in the code) to zero. Finally, the code generates multiple realizations for any type of random walk. The number of realizations is determined by the parameter \texttt{Nsample}.
The code below is also available on Github, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/brownian_path.py}{here}. 
Program name: \texttt{brownian\_path.py}. \\

\begin{lstlisting}
import random
import math
random.seed(1) 

n=50000
Nsample=1
deviations='Large'
mode='Power' 

if deviations=='Large':
  eps=0.01
  beta=0.54
  alpha=0.3
elif deviations=='Small':
  eps=0.05
  beta=0.35 #beta = 1 for log
  alpha=1

def G(n):
  if mode=='Power':
    return(alpha*(n**beta))
  elif mode=='Log' and n>0:
    return(alpha*(math.log(n)**beta))
  else:
    return(0)

OUT=open("rndtest.txt","w")
for sample in range(Nsample):
  print("Sample: ",sample)
  S=0
  for k in range(1,n):
    x=1
    rnd=random.random()
    M=G(k)
    if deviations=='Large':
      if ((S>=-M and S<0 and rnd<0.5+eps) or (S<=M and S>0 and rnd<0.5-eps) or 
        (abs(S)>=M and rnd<0.5) or (S==0 and rnd<0.5)):
        x=-1
    elif deviations=='Small':
      if (S<-M and rnd<0.5-eps) or (S>M and rnd<0.5+eps) or (abs(S)<=M and rnd<0.5):
        x=-1
    print(k,M,S,x)
    S=S+x
    line=str(sample)+"\t"+str(k)+"\t"+str(S)+"\t"+str(x)+"\n" 
    OUT.write(line)   
OUT.close()      
\end{lstlisting}

%----------------------------------------------------------------------------------------------------------------------
\Chapter{Miscellaneous Topics}{}

Here I review some important or interesting topics not covered in the previous chapters. I start with turning your data into music to 
potentially gain unusual insights. The second topic is the production of videos and high quality plots in R, using the Cairo and AV libraries. Then I move to dual confidence regions which are analogous to Bayesian credible regions. The concept 
 is illustrated with a bivariate parameter estimated via the minimum contrast method, in the context of point processes. This procedure offers a mechanism to retrieve
 the parameters of interest using proxy statistics, when they are masked due to  hidden layers. In the process, I show how to produce 3D contour plots. The last sections cover \gls{gls:featureselection},  natural language 
 processing (the creation of a taxonomy with smart crawling) and automatically detecting the number of clusters in unsupervised clustering problems.


\hypersetup{linkcolor=red}

%\listoffigures

\section{The sound that data makes}\label{sound23}

It is common these days to read stories about the sound of black holes, deep space or the abyss. But what if you could turn your data into music? There are a few reasons one might want to do this. First, it adds extra dimensions, in top of those displayed in a scatter plot or a video of your data. Each observation in the sound track may have its own frequency, duration, and volume. That’s three more dimensions. With stereo sound, that’s six dimensions. Add sound texture, and the possibilities are limitless.

Then, sound may allow the human brain to identify new patterns in your data set, not noticeable in scatterplots and other visualizations. This is similar to scatterplots allowing you to see patterns (say clusters) that tabular data is unable to render. Or to data videos, allowing you to see patterns that static visualizations are unable to render. Also, people with vision problems may find sounds more useful than images, to interpret data.

Finally, another purpose of this article is to introduce you to sound processing in Python, and to teach you how to generate sound and music. This basic introduction features some of the fundamental elements. Hopefully, enough to get you started if you are interested to further explore this topic.

\subsection{From data visualizations to videos to data music}

We are all familiar with static data visualizations. Animated gifs such as \href{https://mltechniques.com/2022/04/20/computer-vision-shape-classification-via-explainable-ai/}{this one} brings a new dimension, but they are not new. Then, data represented as videos is something rather new,with examples on \href{https://www.youtube.com/c/VincentGranvilleVideos}{my YouTube channel}. However, I am not aware of any dataset represented as a melody. This article may very well feature the first example.

As in data videos, time is a main component. The concept is well suited to time series. In particular, here I generated two time series each with 
$n = 300$ observations, equally spaced in time. It represents pure, uncorrelated noises: the first one is Gaussian and represented by the sound frequencies; the second one is uniform and represented by the duration of the musical notes. Each note corresponds to one observation. I used the most standard musical scale, and avoided \textcolor{index}{half-tones}\index{half-tone (music)} [\href{https://en.wikipedia.org/wiki/Semitone}{Wiki}] -- the black keys on a piano -- to produce a pleasant melody. To listen to it, follow \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Images/sound.wav}{this GitHub link}, download the WAV file, and play it. Make sure your speakers are on. You may even play it in your office, as it is work-related after all.

Since it represents noise, the melody never repeats itself and has no memory. Yet it seems to exhibit patterns, the patterns of randomness. Random data is actually the most pattern-rich data, since if large enough, it contains all the patterns that exist. If you plot random points in a square, some will appear clustered, some areas will look sparse, some points will look aligned. The same is true in random musical notes. This will be the topic of a future article, entitled “The Patterns of Randomness”.

The next step is to create melodies for real life data sets, exhibiting auto-correlations and other peculiarities. The bivariate time series used here is pictured below: the red curve is the scaled Gaussian noise linked to note frequencies in the audio; the blue curve is the scaled uniform noise linked to the note durations. As for myself, I plan to create melodies for famous functions in number theory (the Riemann function) and blend the sound with the silent videos that I have produced so far, for instance here.


%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.56\textwidth]{sound_data2.jpg}  
\caption{Data linked to the melody: red curve for note frequencies, blue curve for note durations}
\label{fig:sound}
\end{figure}

%-------------------------


\subsection{References}
The musical scale used in my Python code is described in Wikipedia, \href{https://en.wikipedia.org/wiki/Piano_key_frequencies}{here}. An introduction to sound generation in Python can be found on StackOverFlow, \href{https://stackoverflow.com/questions/40782159/writing-wav-file-using-python-numpy-array-and-wave-module}{here}. For stereo sounds in Python, see \href{https://www.tutorialspoint.com/read-and-write-wav-files-using-python-wave}{here}. A more comprehensive article featuring known melodies with all the bells and whistles, is found \href{https://towardsdatascience.com/music-in-python-2f054deb41f4}{here} (part 1) and \href{https://towardsdatascience.com/music-in-python-part-2-4f115be3c781}{here} (part 2). However, I was not able to make the code work. See also here if you are familiar with  Python classes.

I think my very short code in section~\ref{cvbxc} offers the best bang for the buck. In particular, it assumes no music knowledge and does not use any library other than Numpy and Scipy.

%---------------
\subsection{Python code}\label{cvbxc}

In a WAV file, sounds are typically recorded as waves. These waves are produced by the \texttt{get\_sine\_wave function}, one wave per musical note. The base note has a $440$ frequency. Each octave contains $12$ notes including five half-tones. I skipped those to avoid dissonances. The frequencies double from one octave to the next one. I only included audible notes that can be rendered by a standard laptop, thus the instruction \texttt{in range(40,65)} in the code.

The last line of code turns wave values into integers, and save the whole melody as \texttt{sound.wav}. Now you can write your own code to listen to your data! Or you can use the code to test large sequences of random notes, to find if some short extracts might be good and original enough to integrate into your own music. You may also try non-sinusoidal waves. For instance, a mixture of waves to emulate harmonic pitches (two or more notes at the same time) and instruments other than piano. \\



\begin{lstlisting}
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import wavfile

def get_sine_wave(frequency, duration, sample_rate=44100, amplitude=4096):
    t = np.linspace(0, duration, int(sample_rate*duration))
    wave = amplitude*np.sin(2*np.pi*frequency*t)
    return wave

# Create the list of musical notes
scale=[] 
for k in range(40,65):
    note=440*2**((k-49)/12)
    if k%12 != 0 and k%12 != 2 and k%12 != 5 and k%12 != 7 and k%12 != 10:
        scale.append(note) # add musical note (skip half tones)
M=len(scale) # number of musical notes

# Generate the data
n=300
np.random.seed(101)
x=np.arange(n)
y=np.random.normal(0,1,size=n)
z=np.random.uniform(0.100,0.300,size=n)
min=min(y)
max=max(y)
y=0.999*M*(y-min)/(max-min)

plt.plot(x,y,color='red',linewidth=0.6)
plt.plot(x,15*z,color='blue',linewidth=0.6)
plt.show()
  
# Turn the data into music
wave=[]
for t in x: # loop over dataset observations, create one note per observation
    note=int(y[t])
    duration=z[t]
    frequency=scale[note]    
    new_wave = get_sine_wave(frequency, duration=duration, amplitude=2048)
    wave=np.concatenate((wave,new_wave))
wavfile.write('sound.wav', rate=44100, data=wave.astype(np.int16))
\end{lstlisting}


%\pagebreak

%-----

\section{Data videos and enhanced visualizations in R}\label{rprogravcx}

For a long time, charts produced by R looked pixelated and easily recognizable due to their poor quality. The problem was due
 to lack of \textcolor{index}{anti-aliasing}\index{anti-aliasing} mechanisms [\href{https://en.wikipedia.org/wiki/Spatial_anti-aliasing}{Wiki}] in the graphic libraries. Now with \texttt{ggplot2} [\href{https://ggplot2.tidyverse.org/}{Wiki}], the issue has been addressed. This package has a steep learning curve though. But if you are still using the old \texttt{plot} function,  you still face the problem. However, there is an easy workaround, with the Cairo library. I first explain how it works, and then move to the production of videos with the AV library.

%---------
\begin{figure}[H]
\centering
\includegraphics[width=0.48\textwidth]{cairo.png} %0.86
\caption{R plot before Cairo (left), and after (right)}
\label{fig:cairox}
\end{figure}
%---------

\subsection{Cairo library to produce better charts}\label{secvcare}

The problem is pictured in Figure~\ref{fig:cairox}. If you zoom in, the issue will be magnified. The fix consists of two lines of code in R. First, you need to install the Cairo library with the command \texttt{\textcolor{black}{install.packages('Cairo')}}. The first two lines in your R script would look like: \\

\noindent \textcolor{white}{000000}\texttt{\textcolor{black}{library('Cairo');}} \\
\textcolor{white}{000000}\texttt{\textcolor{black}{CairoWin(5,5);}}

\noindent The second line is to create a high resolution window on your screen, to replace the standard R graphics window. For a bigger window, try \texttt{CairoWin(6, 6)}. If instead you would like to save the image as a PNG file, replace the second line of code by something like \\

\noindent \textcolor{white}{000000}\texttt{\textcolor{black}{CairoPNG(filename="c:/Users/yourname/nice.png", width=600, height=600);}} \\

\noindent To actually generate the PNG image, \texttt{add dev.off()} at the bottom of your script. See \href{https://www.cairographics.org/}{CairoGraphics.org} for details, or the Wikipedia entry \href{https://en.wikipedia.org/wiki/Cairo_(graphics)}{here}. 
The full version of my R script is available on my GitHub repository, \href{https://github.com/VincentGranville/Point-Processes/blob/main/Source\%20Code/PP_NN_arrows.r}{here}. It uses an input file, also available in the same repository, \href{https://github.com/VincentGranville/Point-Processes/blob/main/Data/PB_r.txt}{here}.



Besides the Cairo library, you can use optimum \textcolor{index}{color palettes}\index{palette} in the 
 \textcolor{index}{RGB color scheme}\index{color model!RGB} to further improve the visual rendering. For details about the mathematical technicalities,
 see \href{https://mathoverflow.net/questions/415618/lattice-like-structure-with-maximum-spacing-between-vertices}{here}.  
Now, with the \textcolor{index}{RGBA color scheme}\index{color model!RGBA}, you can also add \textcolor{index}{color transparency}\index{color transparency} [\href{https://en.wikipedia.org/wiki/Alpha_compositing}{Wiki}] to better visualize
 overlapping objects, such as in Figure~\ref{fig:screen2}.

\subsection{AV library to produce videos}

The sample code below is also on my GitHub repository, \href{https://github.com/VincentGranville/Point-Processes/tree/main/Videos}{here}. The output videos and the data sets used to produce  them are in the same folder. Look out for filenames
 starting with \texttt{av}.  I used the Cairo library described in section~\ref{secvcare} for better rendering.


%---------
\begin{figure}%[H]
\centering
\includegraphics[width=0.8\textwidth]{vframe2.png} %0.86
\caption{Intermediate (left) and last frame (right) of the video}
\label{fig:vfr2}
\end{figure}
%---------

The input file \texttt{av\_demo\_vg2.txt} is a comma-separated text file. The input file has $20 \times 500 = \num{10000}$ rows. The R program joins $(x, y)$ to $(x_2, y_2)$ via the arrows function; each frame adds $20$ consecutive undirected arrows to the previous frame. I chose the colors using the \texttt{rgb} parameter in the arrows function. The call to the 
\texttt{CairoPNG} function (requiring the Cairo library) produces the $500$ PNG files (the frames) each with 
$600 \times 600$ pixels. Figure~\ref{fig:vfr2} shows two of these frames. Each row in the input data set consists of

\begin{itemize} 
\item the index $k$ of a vector,
\item the coordinates $x, y$ of the vector in question,
\item the coordinates $x_2, y_2$ of the next vector to be displayed,
\item the index \texttt{col} of that vector (used in the randomized version).
\end{itemize}
\noindent I used cosine functions for the RGB (red/green/blue) colors, with small integer multiples of a base period. These cosine waves, called harmonics in signal processing, make the colors harmonious. The argument \texttt{framerate} specifies the number of frames per second. \\

\begin{lstlisting}[language=R]
CairoPNG(filename = "c:/Users/vince/tex/av_demo%03d.png", width = 600, height = 600);  
data<-read.table("c:/Users/vince/tex/av_demo_vg2b.txt",header=TRUE);

k<-data$k;
x<-data$x;   
y<-data$y;  
x2<-data$x2;   
y2<-data$y2; 
col<-data$col; 

for (n in 1:500) {
    plot(x,y,pch=20,cex=0,col=rgb(0,0,0),xlab="",ylab="",axes=FALSE  );
    rect(-10, -20, 50, 50, density = NULL, angle = 45,
       col = rgb(0,0,0), border = NULL);
    a<-x[k <= n*20];
    b<-y[k <= n*20];
    a2<-x2[k <= n*20];
    b2<-y2[k <= n*20];
    c<-col[k <= n*20];
    arrows(a, b, a2, b2, length = 0, angle = 10, code = 2,
        col=rgb(  0.9*abs(sin(0.00200*col)),0.6*abs(sin(0.00150*col)),
        abs(sin(0.00300*col))  ));
}
dev.off();

png_files <- sprintf("c:/Users/vince/tex/av_demo%03d.png", 1:500)
av::av_encode_video(png_files, 'c:/Users/vince/tex/av_demo2b.mp4', framerate = 12)
\end{lstlisting}





% https://www.datasciencecentral.com/data-animation-much-easier-than-you-think/

%index CR / sub index dual region


%------
\section{Dual confidence regions}\label{dualcr1wqa}

This tutorial explains how to build \glspl{gls:cr} (the 2D version of a confidence interval) using as little statistical theory as possible. I also avoid the traditional terminology and notation such as $\alpha$, $Z_{1-\alpha}$, critical value, confidence level, significance level and so on. These can be confusing to beginners and professionals alike.

Instead, I use simulations and two keywords only: confidence region, and confidence level. The purpose is to explain the concept using a framework that will appeal to machine learning professionals, software engineers and non-statisticians. My hope is that you will gain a deep understanding of the technique, without headaches. I also introduce an alternative type of confidence region, called 
dual confidence region. It is asymptotically equivalent to the standard definition. In my opinion, it is more intuitive.


\subsection{Case study}\label{sdxcxza}

This example comes from a real-life application discussed in section 3.1 in my book on stochastic processes~\cite{vgsimulnew}. In this section I provide the minimum amount of material necessary to illustrate the methodology.
The full problem is described  section~\ref{orfucv}, for the curious reader. In its simplest form, we are dealing with independent \textcolor{index}{bivariate Bernoulli trials}\index{Bernoulli trials}. The data set has $n$ observations. Each observation consists of two measurements 
$(u_k, v_k)$, for $k=1,\dots, n$. Here $u_k = 1$ if some interval $B_k$ contains zero point (otherwise $u_k = 0$).
 Likewise, $v_k = 1$ if the same interval contains one point (otherwise $v_k = 0$).

The interval $B_k$ can contain more than one point, but of course it can not simultaneously contain one and two points. The probability that $B_k$ contains zero point is $p$; the probability that it contains one point is $q$, with $0< p+q <1$. The goal is to estimate $p$ and $q$. The estimators (proportions computed on the observations) are denoted as $p_0$ and $q_0$.

Since we are dealing with Bernoulli variables, the standard deviations are $\sigma_p = \sqrt{p(1-p)}$ and $\sigma_q = \sqrt{q(1-q)}$. Also the correlation between the two components $u_k, v_k$ of the observation vector is $\rho_{p,q} = -pq / \sigma_p \sigma_q$. Indeed the probability to observe $(0, 0)$ is $1-p–q$, the probability to observe $(1, 0)$ is $p$, the probability to observe $(0, 1)$ is $q$, and the probability to observe $(1, 1)$ is zero.

\subsection{Standard confidence region}

A \gls{gls:cr}\index{confidence region} of level $\gamma$ is a domain of minimum area that contains a proportion $\gamma$ of the potential values of your estimator $(p_0, q_0)$, based on your $n$ observations. When $n$ is large, $(p_0, q_0)$ approximately has a \textcolor{index}{bivariate normal distribution}\index{Gaussian distribution}\index{probability distribution!Gaussian} (also called Gaussian), thanks to the 
\textcolor{index}{central limit theorem}\index{central limit theorem}. The \textcolor{index}{covariance matrix}\index{covariance matrix} of this normal distribution is specified by $\sigma_p, \sigma_q$ and $\rho_{p,q}$ measured at $p = p_0$ and $q = q_0$. For a fixed $\gamma$, the optimum shape -- the one with minimum area -- necessarily has a boundary that is a contour level of the distribution in question. In our case, that distribution is bivariate Gaussian, and thus contour levels are ellipses.

\noindent Let us define 

\begin{equation}
H_n(x,y,p,q)=\frac{2n}{1-\rho_{p,q}^2}
\Big[\Big( \frac{x-p}{\sigma_p}\Big)^2 
-2\rho_{p,q}\Big(\frac{x-p}{\sigma_p}\Big)\Big(\frac{y-q}{\sigma_q}\Big) 
+ \Big(\frac{y-q}{\sigma_q}\Big)^2\Big],\label{gauss2d}
\end{equation}
with
\begin{equation}
\sigma_p
=\sqrt{p(1-p)},
\quad \sigma_q=\sqrt{q(1-q)},
\quad \rho_{p,q}=-\frac{pq}{\sqrt{pq(1-p)(1-q)}}.\label{cvcxcc}
\end{equation}


This is the general elliptic form of the contour line. Essentially, it does not depend on $n, p, q$ when $n$ is large. The standard confidence region is then the set of all $(x, y)$ satisfying $H_n(x, y, p_0, q_0)\leq G_\gamma$. Here you choose $G_\gamma$ to guarantee that the \textcolor{index}{confidence level}\index{confidence level} is $\gamma$. Replace $\leq$ by $=$ to get the boundary of that region.

In this case $G_\gamma$ is a \textcolor{index}{quantile}\index{quantile} of the \textcolor{index}{Hotelling distribution}\index{Hotelling distribution}\index{probability distribution!Hotelling} [\href{https://en.wikipedia.org/wiki/Hotelling\%27s_T-squared_distribution}{Wiki}].
In section~\ref{simulpetes}, I show how to compute $G_\gamma$. The simulations apply to any setting, whether $G_\gamma$ is a Hotelling, Fisher or any quantile. Or whether the limit distribution of your estimator $(p_0, q_0)$ is Gaussian or not, as $n$ — the sample size — increases. These simulations provide a generic framework to compute confidence regions.

\subsection{Dual confidence region}

The \textcolor{index}{dual confidence region}\index{confidence region!dual region} is simply obtained by swapping the roles of $(x, y)$ and $(p, q)$ in $H_n(x, y, p, q)$. It is thus defined as the set of $(x, y)$ satisfying $H_n(p, q, x, y) \leq H_\gamma$. Again, you choose $H_\gamma$ to guarantee that the confidence level is $\gamma$. Also, $(p, q)$ is replaced by $(p_0, q_0)$. This is no longer the equation of an ellipse. In practice, both confidence regions are very similar. Also, $H_\gamma$ is almost identical to $G_\gamma$. The interpretation is as follows. A point $(x, y)$ is in the dual confidence region of $(p_0, q_0)$ if and only if $(p_0, q_0)$ is in the standard confidence region of $(x, y)$. I use the same $n$ and confidence level $\gamma$ for both regions. You can use the same principle to define dual confidence intervals.

%---------
\begin{figure}%[H]
\centering
\includegraphics[width=0.865\textwidth]{dual.png} %0.86
\caption{Example of 90\% dual confidence region for $(p, q)$}
\label{fig:pbcixccx}
\end{figure}
%---------

Dual confidence regions are based on the same principle as \textcolor{index}{credible regions}\index{credible region (Bayesian)} [\href{https://en.wikipedia.org/wiki/Credible_interval}{Wiki}] in Bayesian statistics. Other methods producing non-elliptic regions are described in \cite{ploshu2013}.


\subsection{Simulations} \label{simulpetes}

The simulations consist of generating $N$ data sets, each with $n$ observations. Use the joint Bernoulli model described in  section~\ref{sdxcxza}, for the simulations. The purpose is to create data sets that have the same statistical behavior as your observations: 
 in other words, \gls{gls:syntheticdata}\index{synthetic data}. In particular, I simulate the bivariate Bernoulli model using some pre-specified $p_0, q_0$, the true but unknown values that we want to estimate.  I now describe how to proceed.

For each simulated dataset, compute the proportions, standard deviations and correlations. They are denoted as 
$x , y, \sigma_x, \sigma_y$ and $\rho_{x,y}$ (one set of values per data set). Use the standard formulas, but this time with $x,y$ observed, and $p_0,q_0$ the variables. That is, 
$$
H_n(p_0,q_0,x,y)=\frac{2n}{1-\rho_{x,y}^2}
\Big[\Big( \frac{p_0-x}{\sigma_x}\Big)^2 
-2\rho_{x,y}\Big(\frac{p_0-x}{\sigma_x}\Big)\Big(\frac{q_0-y}{\sigma_y}\Big) 
+ \Big(\frac{q_0-y}{\sigma_y}\Big)^2\Big],
$$
$$\sigma_x = \sqrt{x(1-x)}\quad, \sigma_y = \sqrt{y(1-y)},\quad \rho_{x,y}=-\frac{xy}{\sqrt{xy(1-x)(1-y)}}.$$ 
Also compute $G(x, y) = H_n(x, y, p_0, q_0)$ and $H(x, y) = H_n(p_0, q_0, x, y)$ for each data set. Put the results in a table with $N$ rows and $7$ columns. Proceed as follows. \vspace{1ex}

\begin{itemize}
\item {\bf Standard confidence region}: sort the table by $G(x, y)$.
\item {\bf Dual confidence region}: sort the table by $H(x, y)$.
\end{itemize}\vspace{1ex}

\noindent The first $\lfloor\gamma N\rfloor$ rows in your sorted table determines your confidence region of level $\gamma$. All the $(x, y)$ in those rows belong to your confidence region. Here $\lfloor\cdot\rfloor$ represents the integer part function. In the first $\lfloor\gamma N\rfloor$ rows, the last value of $H(x, y)$ -- if sorted by $H(x, y)$ -- is $H_\gamma$. Likewise, if sorted by $G(x, y)$, the last value of $G(x, y)$ is $G_\gamma$. See example in Figure~\ref{fig:pbcixccx}, with 
$N = \num{10000}$ and $n = \num{20000}$. As $N$ increases, your simulations yield regions closer and closer to the theoretical ones. The spreadsheet with these simulations is available on my GitHub repository, \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{here}.

\subsection{Original problem with minimum contrast estimators}\label{orfucv}

The original problem introduced in section~\ref{sdxcxza} consisted of estimating the two parameters 
 $\lambda, s$ of a perturbed lattice point process: the intensity and the scale. These stochastic processes have applications in sensor locations and cell network optimization. Rather than a direct estimation which is not possible in this case (these parameters are attached to a hidden lattice process), I used proxy statistics $p, q$ instead. This method, called 
\textcolor{index}{minimum contrast estimation}\index{minimum contrast estimation}, requires a one-to-one-mapping between the original parameter space, and the proxy space. Minimum contrast estimation is a general technique encompassing 
 \textcolor{index}{maximum likelihood estimation}\index{maximum likelihood estimation} [\href{https://en.wikipedia.org/wiki/Maximum_likelihood_estimation}{Wiki}]. It is used in the context
 of point processes by Tilman Davies \cite{hghf} 
 and Jesper Møller \cite{momo55}. See also slides 114-116 \href{https://cimpatogo2018.sciencesconf.org/data/pages/Handout_Moller_CIMPA_Togo_2018.pdf}{here} or
\href{https://drive.google.com/file/d/1y5TZXvAL8fP9G5UkmV3npKgoVB0YWtXk/view?usp=sharing}{here}.



The point count statistic discussed in section~\ref{sdxcxza} measures the number of points of this process that are in a specific interval $B_k$. I used $n$ non-overlapping intervals $B_1,\dots, B_n$, each one yielding one observation vector $(u_k,v_k)$ for $k=1,\cdots,n$. The observation vectors are almost identically and independently distributed across the intervals. However, the first and second components of the vectors are negatively correlated. This explains the choice of the bivariate Bernoulli distribution for the model. 

%---


More specifically, the situation is almost identical (and asymptotically identical) to the following: we observe a bivariate Bernoulli sequence of independently and identically distributed $(U_k, V_k)$ but for each $k$, $U_k$ and $V_k$ are negatively correlated with 
the same correlation as in Formula~(\ref{cvcxcc}). The random variables have the following joint distribution:
\begin{align}
 P(U_k=0,V_k=0) & =1-p-q,\nonumber \\
 P(U_k=1,V_k=0) & =p,\nonumber \\
 P(U_k=0,V_k=1) & =q, \nonumber \\
 P(U_k=1,V_k=1) & =0.\nonumber 
\end{align}
The following must be satisfied: $0<p,q<1$ and $p+q<1$. 


%------------------------------
%---------
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{PB-ci2.PNG}  %0.77
\caption{Minimum contrast estimation for $(\lambda,s)$ using $(p,q)$ as proxy stats}
\label{fig:pbcixzas}
\end{figure}
%---------

\noindent The scatterplot in Figure~\ref{fig:pbcixzas} illustrates the estimation procedure, using minimum contrast estimators. The X-axis represents $p$, and the Y-axis represents $q$. There are two main features: \vspace{1ex}
%\quad \\
\begin{itemize}
\item {\bf Observed data}. The three purple dots correspond to estimated values of $(p,q)$ derived from three  sets of observations,  each with $n=\num{10000}$. 

\item {\bf Theoretical model}. The four overlapping clusters show, based on simulations, the distribution of $(p,q)$ for four different theoretical values
of $(\lambda,s)$. Each cluster -- identified by its color -- has $100$ points corresponding to $100$ simulations. Each simulation within a same cluster uses 
the same hand-picked $(\lambda,s)$.  The purpose of these simulations is to find the inverse mapping $(p, q) \mapsto (\lambda,s)$
via numerical approximations, to retrieve the hidden parameter $(\lambda,s)$ when $(p,q)$ is observed. Four colors is just a small beginning. In  Table~\ref{tab81232}, each cluster is summarized by two statistics: its computed center in the $(p,q)$--space, associated to the hand-picked parameter vector $(\lambda,s)$.
\end{itemize}\vspace{1ex}


\noindent Now let us focus on the rightmost purple dot in Figure~\ref{fig:pbcixzas}, corresponding to one of the three observation sets. Its coordinates vector 
 is denoted as $(p_0,q_0)$.
The $(p,q)$--space is called the \textcolor{index}{proxy space}\index{proxy space}. In this case, it is a subset of $[0,1]\times [0,1]$. If
the proxy spaced contained only the four points $(p,q)$ listed in Table~\ref{tab81232}, the estimated value $(\lambda_0,s_0)$ of $(\lambda,s)$ would be the center of the orange cluster.  That is, $(\lambda_0,s_0)=(1.4, 0.6)$ because $(0.3275,0.4113)$  is the closest cluster center to the purple dot $(p_0,q_0)$ in the proxy space. 

But let's imagine that I hand-picked $10^5$ vectors $(\lambda,s)$ instead of four, thus generating $10^5$ cluster centers and a very large Table~\ref{tab81232} with $10^5$ entries. Then again, the best estimator of $(\lambda,s)$ would still  
be the one obtained by minimizing the distance between the purple dot $(p_0,q_0)$ computed on the observations, and the $10^5$ cluster centers. In practice, the hand-picking is automated 
(computerized) and leads to a 
black-box implementation of the estimation procedure. 

\begin{table}[H]
\[
\begin{array}{ccc}
\hline
 \mbox{Cluster} &  (\lambda,  s) & (p, q) \\
\hline
 \mbox{Orange} & (1.4, 0.6) & (0.3275,  0.4113)\\
\mbox{Gray} & (1.4, 0.5) & (0.3186, 0.4216) \\
\mbox{Yellow} & (1.6,  0.7) & (0.3321, 0.3995)\\
\mbox{Blue} & (1.8, 0.6)&  (0.3371,  0.4007)\\
\hline
\end{array}
\]
\caption{\label{tab81232}Extract of the mapping table used to recover $(\lambda,s)$ from $(p,q)$}
\end{table}

\noindent Finally, the glow effect in Figure~\ref{fig:pbcixzas} may be used for classification or clustering purposes. It generates cluster boundaries
 given observed points, in a way similar to the method described in chapter~\ref{chapterfastclassif}.


\subsection{General shape of confidence regions}\label{generi}

Before establishing the fundamental result, I briefly discus how to plot \glspl{gls:cr} in 3D. Figure~\ref{fig:pbcixsds}
 shows an atypical non-elliptic example, arising from a \textcolor{index}{mixture model}\index{mixture model}. The \textcolor{index}{contour levels} \index{contour level}
correspond to \textcolor{index}{confidence levels}\index{confidence level}. This type of chart is called \textcolor{index}{contour plot}\index{contour plot}. However, in the 
literature, most contour plots are 2D. And those that are 3D usually feature vertical rather than horizontal contours, as the horizontal ones
are more difficult to produce. In this case, I produced it with 
\textcolor{index}{Mathematica}\index{Mathematica} [\href{https://www.wolframalpha.com/}{Wiki}]. See code below.  \\

\begin{lstlisting}[language=Mathematica]
Plot3D[Exp[-(Abs[x]^3.5 + Abs[y]^3.5 )] + 
    0.8*Exp[-4*(Abs[x - 1.5]^4.2 + Abs[y - 1.4]^4.2 )], {x, -2, 3}, 
    {y, -2, 3}, MeshFunctions -> {#3 &}, Mesh -> 25, 
    Exclusions -> None, PlotRange -> {Automatic, Automatic, {0, 1}}, 
    ImageSize -> 600] 
\end{lstlisting}

%---------
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{contours.png}
\caption{Non-elliptic confidence regions with various confidence levels}
\label{fig:pbcixsds}
\end{figure}
%---------

\noindent Now let's get to the core of the subject. In our particular case, the standard confidence region is asymptotically elliptic
 because the underlying distribution of the statistic $(p,q)$ -- itself a random vector depending on the $n$ observations -- approaches a
multivariate Gaussian law as $n\rightarrow\infty$. See exercise 27 in my book on stochastic processes~\cite{vgsimulnew} for details. Since contour levels of Gaussian distributions are ellipses, the result follows immediately. 

More generally, the confidence region of level $\gamma$ is the minimum set covering a proportion $\gamma$ of a the mass of the distribution attached to the estimated parameters. 
Let  $S_\gamma$ be the set in question, and $f(x,y)$ be the density attached to the distribution. I assume 
that the density has one maximum only, and that it is continuous everywhere on $\mathbb{R}^2$. Thus the problem consists of finding 
the set $S_\gamma$ of minimum area, such that
\begin{equation}
\int\int_{S_\gamma} f(x,y) dxdy = \gamma.\label{zinal}
\end{equation}
It is easy to see that the boundary of $S_\gamma$ is a contour line of $f(x,y)$. To build $S_\gamma$, you start at the maximum of the density, and to keep the area minimum, the set must progressively be expanded, strictly following contour lines, until (\ref{zinal}) is satisfied. So 
$$S_\gamma = \{(x, y) \in\mathbb{R}^2 \mbox{ such that } f(x,y)\leq G_\gamma\},$$
where $G_\gamma$ must be chosen so that (\ref{zinal}) is satisfied. Assuming $\max f(x,y)=M$, the volume covered by $S_\gamma$
 is 
\begin{equation}
\gamma = z_\gamma \cdot |S_\gamma| + \int_{z_\gamma}^M |R(z)| dz, \label{zinal2}
\end{equation}
where $R(z) = \{(x, y) \in\mathbb{R}^2 \mbox{ such that } f(x,y) =z\}$, and $|\cdot|$ denotes the area of a 2D domain. Clearly, 
$|S_\gamma|=|R(z_\gamma)|$. So there is only one unknown in Equation~(\ref{zinal2}), namely $z_\gamma$. Finally, $G_\gamma=z_\gamma$, and thus the value of $G_\gamma$ is found by solving (\ref{zinal2}). The area of $S_\gamma$ is
thus $|S_\gamma|=|R(G_\gamma)|$. 

%------------------------------------

\section{Fast feature selection based on predictive power} 

In all machine learning problems, deciding which metrics to use is one of the core problems. This section addresses this topic. I propose a simple metric to measure \gls{gls:predictivepower}. It is used for combinatorial \gls{gls:featureselection}\index{feature selection}, when a large number of feature combinations need to be ranked automatically and very fast, for instance in the context of transaction scoring, in order to optimize predictive models. You can easily implement it with
 a \textcolor{index}{parallel architecture}\index{distributed architecture} [\href{https://en.wikipedia.org/wiki/Parallel_computing}{Wiki}], such as \textcolor{index}{Map-Reduce}\index{Map-reduce} [\href{https://en.wikipedia.org/wiki/MapReduce}{Wiki}]. I used this methodology for credit card fraud detection, keyword scoring (assessing the commercial value of keyword for keyword bidding purposes) and Internet traffic quality scoring. 

Feature selection is used to detect the best subset of features, out of dozens or hundreds of features (also called independent variables). By ``best", I mean with highest predictive power as defined in section~\ref{secdr}. In short, you want to remove duplicate features, correlations between features, and features lacking predictive power, or features (sometimes called rules) that are rarely triggered -- except if they are excellent predictors of rare but costly fraud for instance.

The problem is combinatorial in nature. You want a manageable, small set of features (say $20$ features) selected from (say) a set of $500$ features, to run machine learning algorithms  in a way that is statistically robust. But there are $2.7 \times 10^{35}$ combinations of $20$ features out of $500$, and you need to compute all of them to find the feature set with maximum predictive power. This problem is computationally intractable, and you need to find an alternate solution. The good thing is that you don’t need to find the absolute maximum; you just need to find a subset of $20$ features that is good enough.

One way to proceed is to compute the predictive power of each feature. Then, add one feature at a time to the subset (starting with zero feature) until either you reach 
$20$ features (your limit), or adding a new feature does not significantly improve the overall predictive power of the feature subset (in short, convergence has been attained). At each iteration, choose the feature to be added, among the two remaining features with the highest predictive power: you will choose (among these two features) the one that increases the overall predictive power (of the subset under construction) most. Now you have reduced your computations from 
$2.7 \times 10^{35}$ to $40 = 2 \times 20$. A possible improvement  consists in removing one feature at a time from the subset, and replace it with a feature randomly selected from the remaining features. If this new feature boosts the overall predictive power of the feature subset, keep it, and otherwise switch back to old subset. Repeat this step $\num{10000}$ times or until no more gain is achieved (whichever comes first).

Finally, you can add two or three features at a time, rather than one. Sometimes, combined features have better predictive power than isolated features. For instance if feature $A$ is the country, with values in $\{\text{USA}, \text{UK}\}$ and feature $B$ is the hour of the day, with values in $\{\text{``Day - Pacific Time"}, \text{``Night - Pacific Time"}\}$, both features separately have little predictive power. But when you combine both of them, you have a much more powerful feature: 
``UK/Night" is good, ``USA/Night" is bad, ``UK/Day" is bad, and ``USA/Day" is good, if your response (what you are predicting) is Internet traffic quality in the US. Using these two features together also reduces the risk of false positives and false negatives.

Also, in order to avoid highly granular features, use feature \gls{gls:binning}\index{binning}. So instead of having country as feature $A$ (with $200$ potential country values) use country group, with 3 list of countries (high risk, low risk, neutral). These groups can change over time. And instead of (say) 
 ``IP address" as feature $B$ (with billions of potential values), use type of IP address instead, with $6$ or $7$ types, one being for instance ``IP address is in some whitelist".


\subsection{How cross-validation works}

I illustrate the concept of predictive power on a subset of two features. Let’s say that you have two binary features A and B taking two possible values $0$ or $1$. Also, in the context of fraud detection, one would assume that each observation in the \gls{gls:trainingset} is either Good (no fraud) or Bad (fraud). The fraud status (G or B) is called the response or dependent variable in statistics. The features $A$ and $B$ are also called rules or independent variables.

\Gls{gls:crossvalid}\index{cross-validation} works as follows. First, split your \textcolor{index}{training set}\index{training set} (the data where the response B or G is known) into two parts: 
\gls{gls:validset}\index{validation set} and training data. Make sure that both parts are data-rich: if the validation set is big (millions of observations) but contains only one or two clients out of $200$, it is data-poor and your statistical inference will be negatively impacted (low robustness) when dealing with data outside the training set. It is a good idea to use two different time periods for training and validation. You are going to compute the predictive power (including rule selection) on the training data. When you have decided on a final, optimum subset of features, you then compute the predictive power on the validation set. If the drop in predictive power is significant in the validation set (compared with training data), something is wrong with your analysis: detect the problem, fix it, start over. You can use multiple validation and training sets: this will give you an idea of how the predictive power varies from one validation set to another one. Too much variance is an issue that should be addressed.

\subsection{Measuring the predictive power of a feature}\label{secdr}

Standard methods are based on classic  \gls{gls:goodnessoffit}\index{goodness-of-fit} metrics [\href{https://en.wikipedia.org/wiki/Goodness_of_fit}{Wiki}], such as various ratios computed on the \textcolor{index}{confusion matrix}\index{confusion matrix} 
[\href{https://en.wikipedia.org/wiki/Confusion_matrix}{Wiki}]. Examples are discussed in this Wikipedia article, and include false positive and false negative rates.
Here I describe an original approach to compute the \textcolor{index}{predictive power}\index{predictive power}. Using our above example with two binary features $A, B$ taking on two values $0$, $1$, we can break the observations from the control data set into $8$ bins.  Let's denote as $n_1, n_2,\dots,n_8$ the number of observations in each of these $8$ bins
shown in Table~\ref{tabnbv45}. 

\renewcommand{\arraystretch}{1.2} %%%
%\renewcommand{\arraystretch}{1.2} %%%
\begin{table}[H]
\small
\[
\begin{array}{cccc}
\hline
 \mbox{Bin} &  \text{Feature } A & \text{Feature } B & \text{Response} \\
\hline
 1  & 0 & 0 & \text{G} \\
2  & 0 & 1 & \text{G} \\
 3  & 1 & 0 & \text{G} \\
4  & 1 & 1 & \text{G} \\
5  & 0 & 0 & \text{B} \\
6  & 0 & 1 & \text{B} \\
 7  & 1 & 0 & \text{B}\\
8  & 1 & 1 & \text{B} \\
\hline
\end{array}
\]
\caption{\label{tabnbv45} Eight bins: $2$ features $(A, B)$ times $2$ outcomes (Good/Bad)}
\end{table}
\noindent Now let us introduce the following quantities:
$$
P_{00} = \frac{n_5}{n_1 + n_5}, \quad P_{01} = \frac{n_6}{n_2 + n_6}, \quad P_{10} = \frac{n_7}{n_3 + n_7}, \quad P_{11} = \frac{n_8}{n_4 + n_8},\quad 
p = \frac{n_5 + n_6 + n_7 + n_8}{n_1 + n_2 +\cdots + n_8}.
$$
Let’s assume that $p$, measuring the overall proportion of fraud, is less than $50\%$ (that is, $p < 0.5$, otherwise we can swap between fraud and non-fraud). For any 
$0<r<1$,  define the $W$ function (shaped like a W), based on a parameter $0<a<1$ (typically $a = 0.5 - p$) as follows:

\[   
W(r) = 
     \begin{cases}
       1 - r / p, &\quad\text{if }\text{ } 0 < r < p, \\
        a (r - p) / (0.5 - p), &\quad\text{if }\text{ } p < r < 0.5,\\
       a (r - 1 + p) / (p - 0.5),&\quad\text{if }\text{ } 0.5 < r < 1 - p,\\
       (r - 1 + p) / p, &\quad\text{if }\text{ }  1- p < r < 1.\\ 
     \end{cases}
\]
 
\noindent Typically, $r = P_{00}, P_{01}, P_{10}$ or $P_{11}$. The  function $W$ has the following properties:\vspace{1ex}
\begin{itemize}
	\item It is minimum and equal to $0$ when $r  = p$ or $r = 1 - p$, that is, when $r$ does not provide any information about fraud / non fraud,
	\item It is maximum and equal to $1$ when $r = 1$ or $r = 0$, that is, when we have perfect discrimination between fraud and non-fraud, in a given bin.
	\item It is symmetric: $W(r) = W(1 - r)$ if $0 < r < 1$. So if you swap Good and Bad (G and B), it still provides the same predictive power.
\end{itemize}\vspace{1ex}

\noindent Now let’s define the \textcolor{index}{predictive power}\index{predictive power} as
$$
H = P_{00} W(P_{00}) + P_{01} W(P_{01}) + P_{10} W(P_{10}) + P_{11} W(P_{11}).
$$
The function $H$ is the predictive power for the feature subset $\{A, B\}$ with four bins $``00", ``01", ``10", ``11"$ corresponding to 
$(A = 0, B = 0), (A = 0, B = 1), (A = 1, B = 0), (A = 1, B = 1)$. Although $H$ is remotely related to the 
\textcolor{index}{entropy metric}\index{entropy}, it has specific properties of its own. Unlike entropy, $H$ is not based on physical concepts or models; it is actually a \textcolor{index}{synthetic metric}\index{synthetic metric}.


The weights $P_{00},P_{01},P_{1,0},P_{11}$  guarantee that bins with low count  have low impact on $H$. Set $W(r)$ to $0$ for any bin that has less than $20$ observations. 
For instance, the frequency of bin $``00"$ is $(n_1 + n_5) / (n_1 +\cdots + n_8)$, its size or bin count is $n_1 + n_5$, and 
$r = P_{00} = n_5 / (n_1 + n_5)$ for this bin. If $n_1 + n_5 = 0$, set $P_{00}$ to $0$ and $W(P_{00})$ to 0. 
I actually recommend doing this not just if $n_1 + n_5 = 0$, but also whenever $n_1 + n_5 < 20$, especially if $p$ is low. If $p$ is very low, say $p < 0.01$, you need to over-sample bad transactions when building your training set, and adjust the counts accordingly. 
Of course, the same rules applies to $P_{01}, P_{10}$ and $P_{11}$. 

Also, you should avoid feature subsets resulting in a large proportion of observations spread across a large number of almost empty bins, as well as feature subsets that produce a large number of empty bins: observations outside the training set are likely to belong to an empty or almost empty bin, and it leads to high-variance predictions. To avoid this drawback, stick to binary features and use fewer than $20$ features if possible. 
Finally, $P_{ij}$  is the estimator of the probability $P(A = i, B = j)$ for $i, j = 0,1$ in 
\textcolor{index}{naive Bayes classification}\index{naive Bayes}\index{Bayesian inference!naive Bayes} [\href{https://en.wikipedia.org/wiki/Naive_Bayes_classifier}{Wiki}].


The technique easily generalizes to more than two features, and the predictive power $H$ has interesting properties: $0\leq H\leq 1$, with $H= 0$ if the feature subset has no predictive power, and $H=1$ if it has maximum predictive power. If  $p = 0.5$, then the function $W$ is shaped like a V rather than a W. You may try $p=0.5$ and check whether it provides good enough predictions.


\subsection{Efficient implementation} \label{effsdxc}

Given a subset of $20$ binary features, you can pre-compute all the bin counts in any extended \textcolor{index}{flag vector}\index{flag vector}, and store them in a hash table \texttt{Hash} -- also called 
 ``associative array"  or ``dictionary" in Python. Such an extended vector has $21$ components: 
$20$ for the features, with value  $0$ or $1$, and one for the response, with value G or B. 


%---------------


In Python, an entry in this \textcolor{index}{hash table}\index{hash table} [\href{https://en.wikipedia.org/wiki/Hash_table}{Wiki}] would look like \texttt{Hash[(v,y)]=56}
 where  (say)  $y=\text{``G"}$ and $v=``01101001010110100100"$.
The hash table is made of  \textcolor{index}{key-value pairs}\index{key-value pair}. In this example, the value is $56$ and the key is $(v,y)$.
It means that the flag vector $v$ has 56 observations with response G. More precisely, a flag vector is a binary string showing which rules are triggered.  Here a rule is a feature, and ``triggered" means that the value of the feature in question is equal to one. Non-binary flag vectors or responses are not discussed here, but they are also widely used.  This framework is sometimes referred to as \textcolor{index}{association rule learning}\index{association rule} [\href{https://en.wikipedia.org/wiki/Association_rule_learning}{Wiki}]. 

The hash table is produced by parsing your training set one time, sequentially: for each observation, compute the flag vector $v$, check if the response is Good (G) or Bad (B), and update the associated key-value pairs accordingly, with the following instruction:
\texttt{Hash[(v,G)]++} if the response is Good, or \texttt{Hash[(v,B)]++} if the response is Bad.



Then whenever you need to measure the \gls{gls:predictivepower} of a subset of these $20$ features, you don’t need to parse your big data set again (potentially billion of observations), but instead, just access this small hash table: this table contains all you need to build your flag vectors and compute predictive scores, for any combination of features that is a subset of the top $20$. You can  do even better than top $20$, maybe top $30$. While this would create a hash table with $2$ billion keys, most of these keys would correspond to empty bins and thus would not be in the hash table. Your hash table might contain only $200$ million keys, maybe too big to fit in  memory, but easily manageable with a distributed architecture such as Map-Reduce.



Even better: build this hash table for the top $40$ features. However now, your hash table could have up to $2$ trillion keys. But if your dataset has only $100$ billion observations, then of course your hash table cannot have more than $100$ billion keys. In this case,  you create a training set with $20$ million observations, so that your hash table will have at most 20 million keys (and probably less than $5$ million due to empty bins). Thus, it can fit in memory, because
 you are working with a \textcolor{index}{sparse hash table}\index{hash table!sparse}.



You can now estimate the predictive power of many different feature subsets. To do it, parse the hash table obtained in the previous step. For each  
key $(v,y)$ in this input  hash table, loop over the desired feature subsets to create new bin counts: these counts are stored / updated in an output hash table. The key in the newly created output hash table has two components: the ``subset ID" (a number representing the feature subset)) and 
 the key $(v, y)$ of the input hash table.  
When the output hash table is created, you then loop over its keys to compute the predictive power for each feature subset. 

%---
\section{Natural language processing: taxonomy creation}\label{nlp21}

This is all about structuring unstructured data. I used the techniques described in this section to automatically enhance online directories such as Wikipedia, Yelp, Amazon or DMOZ [\href{https://en.wikipedia.org/wiki/DMOZ}{Wiki}]. 
It involves extensive web crawling. One of the noteworthy results obtained  by analyzing online user queries was a better breakdown of the ``restaurant" category into more meaningful subcategories: romantic restaurant, dinner, wine pub, downtown or nearby restaurant, restaurant with a view or by the river, cheap, ethnic, casual or upscale restaurant, restaurant chefs, recipes, jobs and furniture. The classic breakdown is by type of cuisine, but it does not fit as well with what users are looking for. 

Cleaning the data using \textcolor{index}{stop words}\index{stop word (NLP)} [\href{https://en.wikipedia.org/wiki/Stop_word}{Wiki}], \textcolor{index}{n-grams}\index{n-gram (NLP)} 
 permutations [\href{https://en.wikipedia.org/wiki/N-gram}{Wiki}], synonymous, fixing typos, handling special characters,  
 \textcolor{index}{text normalization}\index{text normalization} [\href{https://en.wikipedia.org/wiki/Text_normalization}{Wiki}] using \textcolor{index}{regular expressions}\index{regular expression} [\href{https://en.wikipedia.org/wiki/Regular_expression}{Wiki}] and so on, is not discussed here. It is covered in my online classes. 



\subsection{Designing a keyword taxonomy} 

Here I discuss an algorithm to perform fast clustering on big data sets, as well as the graphical representation of such complex clustering structures. By fast, I mean a \textcolor{index}{computational complexity}\index{computational complexity} [\href{https://en.wikipedia.org/wiki/Computational_complexity}{Wiki}] of order $O(n)$. This is much faster than 
\textcolor{index}{hierarchical agglomerative clustering}\index{hierarchical clustering} [\href{https://en.wikipedia.org/wiki/Hierarchical_clustering}{Wiki}] which are typically $O(n^2 \log n)$. By big data, I mean several millions, possibly billions of observations.

The application in mind is the creation of a keyword, product or document taxonomy. In particular, I want to create a keyword taxonomy from scratch, based 
on crawling billions of webpages to extract and cluster keywords into categories. This is a typical unsupervised \gls{gls:nlp}\index{natural language processing} (NLP) problem.
The proposed algorithm is as follows: \vspace{1ex}

\noindent {\bf Step 1: Preprocessing}\label{ppioppoo}

\noindent You gather billions of keywords over the Internet by crawling (say) Wikipedia or Google results, clean the results, and compute the frequencies for each keyword and for each ``keyword pair". A ``keyword pair" is two keywords found on a same webpage, or close to each other on a same web page. Also by keyword, I mean an entity like ``California insurance", so a keyword usually contains more than one token, but rarely more than three. You then can create a keyword table,  where each entry is a pair of keywords followed by three counts, such as: %\vspace{1ex}
\begin{center}
A=``California insurance", B=``home insurance", $x=543$, $y=998$, $z=11$
\end{center}
where
\begin{itemize}
	\item $x$ is the number of occurrences of keyword A in all the web pages crawled
	\item $y$ is the number of occurrences of keyword B in all the web pages crawled
	\item $z$ is the number of occurrences where A and B form a pair (e.g. they are found on a same page)
\end{itemize}
You can build the keyword table using a distributed architecture. The vast majority of keywords A and B do not form a 
``keyword pair". In other words, $z=0$ most of the time. So by ignoring these null entries, your final keyword table can be stored in memory 
as a \textcolor{index}{hash table}\index{hash table} (see section~\ref{effsdxc}) where the key is $(\text{A}, \text{B})$ and the value
 attached to a key is $(x, y, z)$. Let's name this table \texttt{Keyword}.\vspace{1ex}


\noindent{\bf Step 2: Clustering}

\noindent To create a taxonomy, you want to group the keywords into similar clusters. One way to do it is to compute a \textcolor{index}{dissimilarity metric}\index{dissimilarity metric} 
$d(\text{A}, \text{B})$ between two keywords A, B. For instance $d(\text{A}, \text{B}) = z / \sqrt{xy}$, although other choices are possible. Note that the denominator prevents extremely popular keywords (for instance ``free") from being close to all the keywords, and from dominating the entire keyword relationship structure: indeed, it favors better keyword bonds, such as ``lemon" with ``law" or ``pie", rather than ``lemon" with ``free". The larger $d(A, B)$, the closer the keywords A and B. 
I describe the clustering part in section~\ref{cvbsdwq}.



\subsection{Fast clustering algorithm for keyword data}\label{cvbsdwq}


Here  I have $n = 10^7$ unique keywords and $m=10^8$  keyword pairs $\{\text{A}, \text{B}\}$ 
 where $d(A,B)>0$. That is, an average of $r = 10$ related keywords attached to each keyword. These keyword pairs are stored in the hash table \texttt{Keyword} created in the preprocessing step section~\ref{ppioppoo}. 
The algorithm builds the new hash tables \texttt{Hash} (the category table) and \texttt{Weight}. It proceeds incrementally  as follows:\vspace{1ex}



\noindent {\bf Initialization} -- The small data (or seeding) step. Select $\num{10000}$ seed keywords, create (say) $100$ categories and create a hash table \texttt{Hash} where the key is one of the 
$\num{10000}$ seed keywords, and the value is a list of categories the keyword is assigned to. For instance, 
\begin{center}
\texttt{Hash['cheap car insurance']=\{'automotive','finance'\}}
\end{center}
The choice of the initial $\num{10000}$ seed keywords is very important. I suggest to pick up the top $\num{10000}$ keywords, in terms of number of associations: that is, keywords A with many B's where $d(\text{A}, \text{B}) > 0$. This will speed up the convergence of the algorithm.\vspace{1ex}



\noindent {\bf The big data step}. Browse the hash table \texttt{Keyword} from beginning to end. 
We now build the tables \texttt{Hash} and \texttt{Weight}. Let $(\text{A}, \text{B})$ be the current keyword pair in \texttt{Keyword}. \vspace{1ex}\\
%\begin{itemize}
%\item 
\textcolor{white}{000}If \texttt{Hash[A]} exists and \texttt{Hash[B]} does not, do: \\
\textcolor{white}{000000}\texttt{Hash[B]=Hash[A]} \\
\textcolor{white}{000000}\texttt{Weight[B]=d(A,B) }\\
%\item 
\textcolor{white}{000}Else If \texttt{Hash[A]} does not exist and \texttt{Hash[B]} exists, do: \\
\textcolor{white}{000000}\texttt{Hash[A]=Hash[B]} \\
\textcolor{white}{000000}\texttt{Weight[A]=d(A,B) } \\
%\item 
\textcolor{white}{000}Else If \texttt{Hash[A]} and \texttt{Hash[B]} exist, re-categorize: \\
\textcolor{white}{000000}If \texttt{d(A,B)>Weight[B]} do:\\
\textcolor{white}{0000000000}\texttt{Hash[B]=Hash[A]} \\
\textcolor{white}{0000000000}\texttt{Weight[B]=d(A,B)}\\
\textcolor{white}{000000}Else If \texttt{d(A,B)>Weight[A]} do: \\
\textcolor{white}{0000000000}\texttt{Hash[A]=Hash[B]} \\
 \textcolor{white}{0000000000}\texttt{Weight[A]=d(A,B)}\vspace{1ex} \\
%\end{itemize}
\noindent You could replace \texttt{Hash[A]=Hash[B]} by \texttt{Hash[A]=Concatenate(Hash[A],Hash[B])}, and the other way around. This will
 increase the number of categories a keyword is assigned to. I did not test that option.
\vspace{1ex}

\noindent {\bf The loop}. Repeat the ``big data step" $6$ or $7$ times: \texttt{Hash} and \texttt{Weight} are kept in memory and keep growing at each subsequent iteration of the loop.
\vspace{1ex}



\subsubsection{Computational complexity}

The computational complexity is asymptotically $(N+1)m = O(n)$, where $N$ is the number of iterations in the loop. This is very fast. However, accessing the hash tables slows it down a bit  as \texttt{Hash} and \texttt{Weight} grow bigger at each new iteration.

Pre-sorting the \texttt{Keyword} hash table  by the $d(\text{A}, \text{B})$ values allows you to  reduce the number of hash table accesses, by making all the re-categorizations not needed anymore.
You can also improve the computational complexity by keeping the most important keys -- based on count and $d(\text{A},\text{B})$) -- and deleting the other ones. In practice, deleting $65\%$ of the \texttt{Keyword} hash table (the long keyword tail) has  little impact on the performance: you will have a large bucket of un-categorized keywords, but in terms of volume, these keywords might represent less than $0.1\%$ of the Internet traffic.

Finally, one could use \textcolor{index}{Tarjan's  algorithm}\index{Tarjan's  algorithm} [\href{https://en.wikipedia.org/wiki/Tarjan\%27s_strongly_connected_components_algorithm}{Wiki}] to perform the clustering, based on strongly \textcolor{index}{connected components}\index{connected components} [\href{https://en.wikipedia.org/wiki/Component_(graph_theory)}{Wiki}]. To proceed, you first bin the distances: 
$d(\text{A}, \text{B})$ is set to $1$ if it is above some pre-specified threshold, and to $0$ otherwise. This is a \gls{gls:graphmodel} theory algorithm: each keyword represents a node, each pair of keywords where $d(\text{A}, \text{B}) = 1$, represents an edge. The computational complexity of the algorithm is $O(n + m)$, where $n$ is the number of keywords and $m$ is the number of keyword pairs (edges). To take advantage of this algorithm, you might want to store the \texttt{Keyword} hash table in a 
\textcolor{index}{graph database}\index{graph database} [\href{https://en.wikipedia.org/wiki/Graph_database}{Wiki}]. For those interested in graphical representations of the cluster structure, see the 
\textcolor{index}{Fruchterman and Rheingold algorithm}\index{Fruchterman and Rheingold algorithm} [\href{https://en.wikipedia.org/wiki/Force-directed_graph_drawing}{Wiki}]. However its computational complexity is $O(n^3)$.  


\subsubsection{Smart crawling of the whole Internet and a bit of graph theory}

Crawling the web must be done in parallel. You need a to create a log file, updated in real time, to store all the pages that you already visited along with the status (successful crawl or not, number of bytes downloaded, domain/subdomain, and time spent to access the page). That way, if your computer crashes, you can resume from where it stopped without losing any data. You may limit the amount of data extracted by page to $16$ kilobytes, and limit the number of web pages visited per website to (say) $\num{1000}$. Pages that were not successfully crawled may be re-crawled later at least one more time. Identify duplicate URLs such as
 \texttt{web.com}, \texttt{web.com/} and \texttt{web.com/?source=Facbook} so you crawl only one of them.

You also need to avoid getting stuck in an infinite loop, crawling the same pages again and again. Keep a hash table of all the pages already crawled: a copy of the log file, in memory. If done well, in a few months you can crawl billions of web pages, covering (in traffic volume) most of the Internet pages ever browsed by human beings.

At level 1, you start with a list of (say) $\num{10000}$ web pages obtained by extracting data from online directories such as Wikipedia or Dmoz, or search result pages from Google, based on a list of thousands of top search keywords. All links gathered at level 1 (links found on the web pages you visited) are stored in a list used for level 2 crawling. From that list, remove links already visited at level 1. Likewise, all links found at level 2 constitute the target URLs to crawl at level 3. Again, remove from that list pages already crawled at level 1 or 2. Move on to level 4, 5 and 6 using the same principles.  A practical application of this methodology is tested in my training sessions. Table~\ref{tabert4} shows the amount of data (in megabytes) collected at each level. This distribution is typical.



\renewcommand{\arraystretch}{1.2} %%%
%\renewcommand{\arraystretch}{1.2} %%%
\begin{table}[H]
\small
\[
\begin{array}{cr}
\hline
 \mbox{Level} &  \text{Data (GB)}  \\
\hline
1 & 0.002877 \\
2 & 0.456084 \\
3 & 8.722723 \\
4 & 26.942508 \\
5 & 39.443366 \\
6 & 42.429041 \\
7 & 13.175749 \\
\hline
\end{array}
\]
\caption{\label{tabert4} Amount of data collected at each level, when crawling the Internet}
\end{table}

It is interesting to note the connection to the \textcolor{index}{six degrees of separation}\index{six degrees of separation} problem in graph theory [\href{https://en.wikipedia.org/wiki/Six_degrees_of_separation}{Wiki}].  Most web pages with some traffic are connected to any other one by a path involving at most $6$ or $7$ intermediate links (called ``levels" here).
The \textcolor{index}{Watts and Strogatz model}\index{Watts and Strogatz model} [\href{https://en.wikipedia.org/wiki/Watts\%E2\%80\%93Strogatz_model}{Wiki}] shows that the average path length between two nodes in a random network is equal to $\log N / \log K$, where $N$ is the number of
 nodes (a web page here) and $K$ the number  of acquaintances per node (that is, the number of links on any given web page). 

In the case of friend connections, if $N = 3\times 10^8$ (the US population) and $K = 30$ (the number of friends per individual), then  the 
number of degrees of separation between any two people is $5.7$. Now if $N = 6\times 10^9$, it is equal to $6.6$. The Python code below perform the simulations to study these distributions.

The algorithm below is rudimentary and can be used for simulation purposes by any programmer: It does not even use tree or \gls{gls:graphmodel} structures.  Applied to a population of 2,000,000 people, each having 20 friends, we show that there is a path involving 6 levels or intermediaries between you and anyone else. Note that the shortest path typically involves fewer levels, as some people have far more than 20 connections. 
Starting with you, at level one, you have twenty friends or connections. These connections in turn have 20 friends, so at level two, you are connected to 400 people. At level three, you are connected to 7,985 people, which is a little less than 20 x 400, since some level-3 connections were already level-2 or level-1. And so on. \\


\begin{lstlisting}
import random

n=2000000       # total population
nfriends=20  # number of friends per individual
ConnectedPeople={}
newConnections={}
newConnections[0]=1 
TotalConnectedPeople=0

for level in range(1,8): 
    newConnections[level]=0
    for k in range (0, newConnections[level-1]): 
        for m in range(0, nfriends): 
            human=random.randint(0,n-1) 
            if human not in ConnectedPeople:
                ConnectedPeople[human]=True 
                newConnections[level]=newConnections[level]+1
    TotalConnectedPeople=TotalConnectedPeople + newConnections[level]
    print("Connected people at level",level,": ",TotalConnectedPeople)
\end{lstlisting}

\noindent A previous version of this program used a faulty random generator that could not produce $2$ million distinct integers. 
Thus I could never achieve full connectivity no matter how many levels I used. Watch out for issues like that when doing this type of simulations. You could actually use
 this code to test \gls{gls:prng} generators\index{pseudo-random numbers}.


%--------------------------------------------------------------------------------------------------------------------
\section{Automated detection of outliers and number of clusters}\label{bbcl}

In the context of unsupervised clustering, one of the most popular recipes to identify the number of clusters, is the 
\textcolor{index}{elbow rule}\index{elbow rule} [\href{https://en.wikipedia.org/wiki/Elbow_method_(clustering)}{Wiki}]. It is usually performed manually. Here, I show how it can be automated and applied to other problems, such as outlier detection. The idea is a follows: a clustering algorithm (say K-means 
[\href{https://en.wikipedia.org/wiki/K-means_clustering}{Wiki}]) can identify a cluster structure with any number of clusters on a given data set; typically, a function $v(m)$ provides a statistical summary of the best cluster structure consisting of $m$ clusters, for $m=1,2,3$ and so on. For instance, $v(m)$ is the sum of the squares of the distances from any observed point to its assigned cluster center.
The function $v(m)$ is decreasing, sharply initially for small values of $m$, then much more slowly for larger values of $m$, creating an elbow in its graph.  The value of $m$ corresponding to the elbow is deemed to be the optimal number of clusters. See Figure~\ref{fig:pbelbow1}. Instead of $v(m)$, I use the standardized version $v'(m)=v(m)/v(1)$. 

I illustrate how to use the elbow rule to detect \textcolor{index}{outliers}\index{outliers} in section~\ref{elbtr45}. The same methodology applies to detect the number of clusters. 
 The data consists of a realization of a 2D Brownian motion. I am interested in the increments $R_k$, measuring the distance between
 a point of the process, and the next one. For practical purposes, the simulated realization can be interpreted as a 2D random walk, and $R_k$ is the length of the segment joining two successive points in Figure~\ref{fig:pbelbow1}. These segments are visible if you zoom in on the picture. The model used to produce this \gls{gls:syntheticdata} is described in section~\ref{lvfgf}.


%\begin{equation}
%R_k=\frac{1}{\lambda}\Big(-\log(1-U_k)\Big)^\gamma, \label{gam11}
%\end{equation}

\subsection{Black-box elbow rule to detect outliers}\label{elbtr45}

\noindent Figure~\ref{fig:pbelbow1} shows a realization of a \textcolor{index}{Brownian motion}\index{Brownian motion} with $10^4$ points, using $\gamma=2$ and
$\lambda=\Gamma(1+\gamma)$ in Formula~(\ref{gam11}).  The goal is to detect the number of values, among the top $R_k$'s, that significantly outshine all the others. Here, they are not technically outliers in the sense that they are still deviates of the same distribution; rather, they are called extremes. 
The first step is to rank these values. The ordered values (in reverse order) are denoted as $R_{(1)},R_{(2)}$ and so on, with $R_{(1)}$ being the largest one. I
used $v(m)=R_{(m)}$ as the criterion for the elbow rule, that is, after standardization, $v'(m)=v(m)/v(1)$. 

On the right plot in Figure~\ref{fig:pbelbow1}, the Y axis on the left represents $v'(m)$, the X axis represents $m$, and the $Y$ axis on the right represents the strength of the elbow signal (the height of the red bar; I discuss later how it is computed). The top 10 values of $v'(m)$ ($m=1,\dots, 10)$ are
$$1.00, \quad
0.92, \quad
0.77,\quad
0.76,\quad
0.71,\quad
0.69,\quad
0.63,\quad
0.61,\quad
0.60,\quad
0.56, \quad
0.55,\quad
0.55.$$
Clearly, the third value $0.77$ is pivotal, as the next ones stop dropping sharply, after an initial big drop at the beginning of the sequence. So the ``elbow signal" is strongest at $m=3$, and the conclusion is that the first two values ($2=m-1$) outshine all the other ones. The purpose of the black-box elbow rule algorithm, is to automate the decision process: in this case deciding that the optimum is $m=3$. 

Note that in some instances, it is not obvious to detect an elbow, and there may be none. In my example, the elbow signal is very strong, because I chose a rather large value $\gamma=2$ in Formula~(\ref{gam11}), causing the Brownian process to exhibit an unusually strong cluster structure, and large disparities among the top $v(m)$'s.
A larger $\gamma$ would generate even stronger disparities. A negative value of $\gamma$, say $\gamma=-0.75$, also causes strong disparities, well separated clusters, and an easy-to-detect elbow. The resulting process is not even Brownian anymore if $\gamma=-0.75$, since in that case, $\mbox{Var}[R_k]=\infty$. The
standard  Brownian motion corresponds to $\gamma=0$ and can still exhibit clusters depending on the realization. Finally, in our case, $m=3$ also corresponds to the number of clusters on the left plot in Figure~\ref{fig:pbelbow1}. This is a coincidence, one that happens very frequently, because the top $v(m)$'s (left to the elbow) correspond to unusually large values of $R_k$. Each of these very large values typically gives
 rise to the building of a new cluster, in the simulations. 

The elbow rule can be used recursively, first to detect the number of ``main" clusters in the data set, then to detect the number of sub-clusters within each cluster. The strength of the signal (the height of the red bar) is typically very low if the $v'(m)$'s have a low variance. In that case, there is no set of values outshining all the other ones, that is, no true elbow. For an application of this methodology to detect the number of clusters, see a recent article of Chikumbo \cite{vg5}. An alternative to the elbow rule, to detect the number of clusters, 
is the silhouette method [\href{https://en.wikipedia.org/wiki/Silhouette_(clustering)}{Wiki}].

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{PB_elbow1.PNG} 
\caption{Elbow rule (right) finds $m = 3$ clusters in Brownian motion (left)}
\label{fig:pbelbow1}
\end{figure}

I now explain how the strength of the elbow signal (the height of the red bars in Figure~\ref{fig:pbelbow1}) is computed. First, compute the first and second order differences of the function $v'(m)$: 
$\delta_1(m)=v'(m-1)-v'(m)$ for $m>1$, and $\delta_2(m)=\delta_1(m-1)-\delta_1(m)$ for $m>2$. The strength of the elbow signal, at position $m>1$, 
is  $\rho_1(m)=\max[0,\delta_2(m+1)-\delta_1(m+1)]$. I used a dampened version of $\rho_1(m)$, namely $\rho_2(m)=\rho_1(m)/m$, to favor cluster
structures with few large clusters, over many smaller clusters. Larger clusters can always be broken down into multiple clusters, using the same clustering algorithm. 
The data, including formulas, charts, and simulation of the Brownian motion is in the spreadsheet 
 \texttt{PB\_inference.xls} on my GitHub repository, \href{https://github.com/VincentGranville/Point-Processes/tree/main/Spreadsheets}{here}. See the \texttt{Elbow\_Brownian} tab.  You can modify the parameters highlighted in orange in the spreadsheet: in this case, $\gamma$ in cell 
\texttt{B16}. Note that
$\lambda$ is set to $\Gamma(1+\gamma)$ in cell \texttt{B17}. 

%----------------------------------------------------------------------------------------------------------------------
\section{Advice to beginners}

In this section, I provide guidance to machine learning beginners. After finishing the reading and the accompanying classes (including successfully completing the student projects), you should have a strong exposure to the most important topics, many covered in detail in this book. At this point, you should be able to pursue the learning on your own -- a never ending process even for top experts --  in particular with the advice provided in section~\ref{staerqlk}. 

\subsection{Getting started and learning how to learn}\label{staerqlk}

The first step is to install Python on your laptop. While it is possible to use Jupyter notebooks instead [\href{https://jupyter.org/}{Wiki}], this option is limited and won't give you the full experience of writing and testing serious code as in a professional, business environment. Installing Python depends on your system. See the official Python.org website \href{https://www.python.org/downloads/}{here} to get started and download Python. On my Windows laptop, I first installed the Cygwin environment (see \href{https://www.cygwin.com/}{here} how to install it) to emulate a Unix environment. That way, I can use Cygwin windows instead of the Windows command prompt [\href{https://en.wikipedia.org/wiki/Cmd.exe}{Wiki}]. The benefits is that it recognizes Unix commands. An alternative to Cygwin is
\href{https://ubuntu.com/download}{Ubuntu}. You could also use the \href{https://www.anaconda.com/products/distribution}{Anaconda} environment. 

Either way, you want to save your first Python program as a text file, say \texttt{test.py}. To run it, type in \texttt{Python test.py} in the command window. You need to be familiar with basic file management systems, to create folders and sub-folders as needed. Shortly, you will need to install Python libraries on your machine. Some of the  most common ones are pandas, scypy, seaborn, numpy, random and matplotlib. You can create your own too, as illustrated in section~\ref{fc223}. In your Python script, only use the needed libraries. Typically, they are listed at the beginning of your code, as in the following example:

\begin{lstlisting}{frame=none}
import numpy as np
import matplotlib.pyplot as plt
import moviepy.video.io.ImageSequenceClip  # to produce mp4 video
from PIL import Image  # for some basic image processing 
\end{lstlisting}

\noindent To install (say) the numpy library, type in \texttt{pip install numpy} in the Windowns command prompt. 

\subsubsection{Getting help} 

One of the easiest ways to learn more and solve new problems using Python or any programming language is to use the Internet. For instance, when I designed my 
 sound generation algorithm in Python (see section~\ref{sound23}), I googled keywords such as ``Python sound processing". I quickly discovered a number of libraries and tutorials, ranging from simple to advanced.  Over time, you discover websites that consistently offer solutions suited to your needs, and you tend to stick with them, until you ``graduate" to the next level of expertise and use new resources. 

It is important to look at the qualifications of people posting their code online, and how recent these posts are. You have to discriminate between multiple sources, and identify those that are not good or outdated. Usually, the best advice comes from little comments posted in discussion forums, as a response to solutions offered by some users. Of course, you can also post your own questions. Two valuable sources here to stay are GitHub and StackExchange. There are also numerous Python groups on LinkedIn and Reddit. In the end, after spending some time searching for sound libraries in Python, I've found solutions that do not require any special library: Numpy can process sound files. It took me a few hours to discover all I needed on the Internet. 

Finally, the official documentation that comes with Python libraries can be useful, especially if you want to use special parameters and understand the inner workings (and limitations) rather than using them as black-boxes with the default settings. For instance, when looking at model-free parameter estimation for time series (using optimization techniques), I quickly discovered the 
  \texttt{curve\_fit} function from the Scipy library. It did not work well on my unusual datasets (see section~\ref{poihgf}). I discovered, in the official online documentation, several settings 
 to improve the performance. Still unsatisfied with the results (due to numerical instability in my case), I searched for alternatives and discovered that the swarm optimization technique (an alternative to \texttt{curve\_fit}) is available in the Pyswarms library. In the end, testing these libraries on rich synthetic data allows you to find what works best for your data.  

I will soon offer classes related to this book. This is another option to learn more and get answers to your questions.

\subsubsection{Beyond Python}

Python has become the standard language for machine learning. Getting familiar with the R programming language will make you more competitive on the job market. 
 Section~\ref{rprogravcx} shows you how to create videos and better-looking charts in R. Finally, machine learning professionals should know at least the basics of SQL, since many jobs still involve working with traditional databases. 

In particular, in one of the companies I was working for, I wrote a script that would accept SQL code as input (in text file format) to run queries against the Oracle databases, and trained analysts on how to use it in place of the dashboard they were familiar with. They were still using the dashboard (Toad in this case) to generate the SQL code, but run the actual queries with my script. The queries were now running $10$ times faster: the productivity gain was tremendous.  

To summarize, Python is the language of choice for machine learning, R is the language of statisticians, and SQL is the language of business and data analysts.

\subsection{Automated data cleaning and exploratory analysis}

It is said that data scientists spend $80\%$ of their time on data cleaning and \textcolor{index}{exploratory analysis}\index{exploratory analysis} [\href{https://en.wikipedia.org/wiki/Exploratory_data_analysis}{Wiki}]. This should not be the case. To the beginner, it looks like each new dataset comes with a new set of challenges.
 Over time, you realize that there are only so many potential issues. Automating the data cleaning step can save you a lot of time, and eliminate boring, repetitive tasks. A good Python script allows you to automatically take care of most problems. Here, I review the most common ones. 

First, you need to create a summary table for all features taken separately: the type (numerical, categorical data, text, or mixed). For each feature, get the top $5$ values, with their frequencies. It could reveal a wrong or unassigned zip-code such as 99999. Look for other special values such as NaN (not a number), N/A, an incorrect date format, missing values (blank) or special characters. For instance,  accented characters, commas, dollar, percentage signs and so on can cause issues with text parsing  and \textcolor{index}{regular expression}\index{regular expression} [\href{https://en.wikipedia.org/wiki/Regular_expression}{Wiki}]. Compute the minimum, maximum, median and other percentiles for numerical features. Check for values that are out-of-range: if possible, get the expected range from your client before starting your analysis. Use 
\textcolor{index}{checksums}\index{checksum} [\href{https://en.wikipedia.org/wiki/Checksum}{Wiki}] 
if possible, with encrypted fields such as credit card numbers or ID fields. 

A few Python libraries can take care of this. In particular: Pandas-Profiling, Sweetviz and D-Tale. See \href{https://medium.com/@karteekmenda93/exploratory-data-analysis-tools-83ef538c879f}{here} for details. 

The next step is to look at interactions between features. Compute all cross-correlations, and check for redundant or duplicate features that can be ignored. Look for IDs or keys that are duplicate or almost identical. Also two different IDs might have the same individual attached to them. This could reveal typos in your data. Working with a table of common typos can help. Also, collect data using 
pre-populated fields in web forms whenever possible, as opposed to users manually typing in their information such as state, city, zip-code, or date. Finally, check for misaligned fields. This happens frequently in NLP problems, where data such as URLs are parsed and stored in CSV files before being uploaded in databases. Now you can standardize your data. 

Sometimes, the data has issues beyond your control. When I was working at Wells Fargo, internet session IDs generated by the Tealeaf software were broken down into multiple small sessions, resulting in 
 wrong userIDs and very short Internet sessions. Manually simulating such sessions and looking how they were tracked in the database, helped solve this mystery, leading to correct analyses. Sometimes, the largest population segment is entirely missing in the database. For instance, in Covid data, people never tested who recovered on their own (the vast majority of the population in the early days) did not show up in any database, giving a lethality rate of $6\%$ rather than the more correct $1\%$, with costly public policy implications. Use common sense and out-of-the-box thinking to detect such issues, and let stakeholders known about it. Alternate data sources should always be used whenever possible. In this case, sewage data -- a proxy dataset -- provides the answer.


\subsection{Example of simple analysis: marketing attribution}

Sometimes, a simple solution that requires a few days of work as opposed to several weeks, easy to understand as in explainable AI, is good enough and makes everyone happy. When working for NBC, I was asked to perform some \textcolor{index}{marketing attribution}\index{marketing attribution} analysis. The project, also referred to as marketing mix modeling [\href{https://en.wikipedia.org/wiki/Marketing_mix_modeling}{Wiki}] consisted of assessing the individual impact of $20$ different channels -- in this case specific TV shows -- on Internet traffic growth. 

Each week, about $12$ TV shows were used to boost traffic to the target website. Collected data included the GRP (gross rating point, measuring the size of the TV audience attached to a particular TV show) and the number of unique and new users on the website. The selected TV shows were booked well in advance based on inventory availability, and could not be tested separately: in short, it was not possible to use \textcolor{index}{experimental design}\index{experimental design} techniques [\href{https://en.wikipedia.org/wiki/Design_of_experiments}{Wiki}] such as \textcolor{index}{A/B testing}\index{A/B testing} [\href{https://en.wikipedia.org/wiki/A/B_testing}{Wiki}]. 

I created a \textcolor{index}{flag vector}\index{flag vector} for each week, with $20$ components: one for each potential TV show. The component associated to a specific TV show was set to $1$ if it was used during the week in question, and to $0$ otherwise. I then compared weeks that were in a similar time period (to avoid seasonality effects) after excluding weeks with major holidays.  My summary table included pairs of weeks, say weeks A and B, as well as the increase or decrease in Internet traffic between A and B. Also, I selected pairs \{A,  B\}  with the same mix of TV shows except for one that was either absent in A but not in B, or the other way around.  

It was then possible, for a given TV show, to see if using it in a specific week was associated with traffic growth more frequently than traffic decline. This led to the identification of the best and worst performing TV shows. Eventually ``Law \& Order" was found to be the winner, and used more frequently whenever possible.

\subsection{Upcoming books and courses}

The present book offers a solid foundation of modern machine learning, and provide in-depth coverage of many important topics.  My other book ``Stochastic Processes and Simulations -- A Machine Learning Perspective" \cite{vgsimulnew}  focus on a number of statistical aspects including a large collection of univariate and multivariate probability distributions, mixture models, new tests of independence, spatial statistics and covering problems with applications to cellular networks and optimizing the location of sensor devices.  

\noindent Future books currently being written will cover the following topics: \vspace{1ex}

\begin{itemize}
\item Bootstrapping and unusually powerful model-free confidence intervals
\item Natural language generation (shape generation is covered in this book)
\item More synthetic data and adversarial learning
\item Fast density estimation techniques on the grid, applied to clustering 
\item Time series with long-range dependencies
\item Dynamical systems and chaos modeling, including in higher dimensions
\item Brownian-related processes with applications to finance and physics
\item Survival analysis and growth models
\item Diverging fixed-point algorithms that solve difficult problems
\item Change point detection in time series
\item Alternatives to standard location metrics
\item Polynomial regression and regression methods in unusual spaces
\end{itemize} \vspace{1ex}

\noindent To not miss them, subscribe to my newsletter, \href{https://mltechniques.com/resources/}{here}. Also, I will be offering machine learning courses, with several modules: NLP, clustering, regression and optimization, synthetic data,  
 visualizations and data animations (videos, sound),  graphs and nearest neighbor models, LaTeX,   
  Python for scientific computing and GitHub, explainable AI, stochastic processes, dynamical systems, experimental number theory and more. Visit 
\href{https://mltechniques.com/courses/}{MLTechniques.com/courses} for details about the upcoming programs.

 

%----------------------------------------------------------------------------------------------------------------------

\setlength{\glsdescwidth}{0.75\hsize}
\pagebreak
\printnoidxglossary[type=gloss,style=long,title={Glossary},sort=def] %%%%
\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{refstats} % Entries are in the refs.bib file in same directory as the tex file

%\pagebreak

\printindex

\hypersetup{linkcolor=red} % red %
\hypersetup{linkcolor=red}



\end{document}