\documentclass[oneside,10pt]{book}
\usepackage{amsmath} % for "\cfrac" macro
\usepackage[export]{adjustbox}
\usepackage{relsize}
\usepackage{array}
\usepackage{enumitem}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
\pagestyle{plain}
\newcommand\Chapter[2]{
  %\chapter[#1: {\itshape#2}]{#1\\[2ex]\Large\itshape#2}
  \chapter[#1]{#1\\[2ex]\Large\itshape#2}
}

%\usepackage{fontspec}
%\setmainfont{Times New Roman} %Times New Roman
%\setmonofont{Consolas}

%\usepackage{selinput}
%\SelectInputMappings{Euro={€}}

%\usepackage[utf8]{inputenc}
\usepackage[titles]{tocloft}
\setlength{\cftbeforechapskip}{7pt} %%%%%%%%%% 6pt
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{amsmath}    % need for subequations
\usepackage{amsfonts}
\usepackage{amssymb}  % needed for mathbb  OK
\usepackage{bigints}
\usepackage{graphicx}   % need for figures
\usepackage{subfig}
\usepackage{verbatim}   % useful for program listings
\usepackage{color}      % use if color is used in text
%\usepackage{subfigure}  % use for side-by-side figures
\usepackage{parskip}
\usepackage{float}
\usepackage{courier}
%\usepackage{artemisia} %%%
\usepackage{exercise}
\usepackage{sistyle}
\usepackage{textcomp} 
%

%%%\usepackage[utf8]{luainputenc}
%\usepackage{luatextra}
%
%%\usepackage[utf8]{inputenc}
%%\usepackage[T1]{fontenc}
%%\usepackage{textcomp,upgreek}
%\usepackage{fontspec} %,xltxtra}
%\usepackage{unicode}
\usepackage[euler]{textgreek}
%%\DeclareUnicodeCharacter{3B8}{\ensuremath{\uptheta}}
%

\SIthousandsep{,}
%\usepackage{numprint}
\setlength\parindent{0pt}

\newtheorem{prop}{Proposition}

\renewcommand{\DifficultyMarker}{}
\newcommand{\AtBeginExerciseHeader}{\hspace{-21pt}}  %-0.2pt
\renewcommand{\ExerciseHeader}{\AtBeginExerciseHeader\textbf{\ExerciseName~\ExerciseHeaderNB} \ExerciseTitle}
\renewcommand{\AnswerHeader}{\large\textbf{\AnswerName~\ExerciseHeaderNB}\smallskip\newline}
\setlength\AnswerSkipBefore{1em}

\usepackage{xspace}
\usepackage{imakeidx}
\makeindex

\usepackage[nottoc]{tocbibind}

\usepackage[colorlinks = true,
          linktocpage=true,
            pagebackref=true, % add back references to bibliography
            linkcolor = red,
            urlcolor  = blue,
            citecolor = red,
 %           refcolor  =red,
            anchorcolor = blue]{hyperref}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{gray2}{rgb}{0.35,0.35,0.35}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{index}{rgb}{0.88,0.32,0}

%------- source code settings
\usepackage{listings} 
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

%-----------------------------------------------------------------

\usepackage{blindtext}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }


\setlength{\baselineskip}{0.0pt} 
\setlength{\parskip}{3pt plus 2pt}
\setlength{\parindent}{20pt}
\setlength{\marginparsep}{0.0cm}
\setlength{\marginparwidth}{0.0cm}
\setlength{\marginparpush}{0.0cm}
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.4} %%%
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

\usepackage[symbols,nogroupskip,acronym]{glossaries-extra}
%\usepackage[xindy,symbols,nogroupskip,sort=def,acronym]{glossaries}
\makenoidxglossaries %%%%%%%%%%%%%%%
%\setlength{\glsdescwidth}{1.3\hsize}

\begin{document}

\hypersetup{linkcolor=blue}
%inserting a glossary entry in gloss: \gls{gls:keyword1} \\

\baselineskip=2\baselineskip
\thispagestyle{empty}
\hspace{0pt}
\vfill
%\hrulefill
\begin{center}
\rule{0.90\textwidth}{.4pt}
\end{center}

\begin{center}
{\Huge \bf{Gentle Introduction To Chaotic Dynamical Systems} }  
\end{center}


\baselineskip=0.5\baselineskip
\addvspace{2cm}
\begin{center}
%\includegraphics[width=0.7\textwidth]{linear.png}  \\
%\addvspace{1cm}
%\includegraphics[width=0.6\textwidth]{imgpyRiemannFinalOrbits-v2-small2.jpg} 
\includegraphics[width=0.75\textwidth]{pillow.png}  
\end{center}
\addvspace{2cm}
\begin{center}
\rule{0.90\textwidth}{.4pt}
\end{center}
\begin{center}
Vincent Granville, Ph.D. $|$ \href{https://mltechniques.com/}{www.MLTechniques.com} $|$ Version 2.0, May 2023 
\end{center}
%\hrulefill

\hypersetup{linkcolor=red} % red %

\vfill
\hspace{0pt}
\pagebreak

\chapter*{Preface} %\clearpage

In less than 100 pages, the book covers all important topics about discrete chaotic dynamical systems and
 related time series and stochastic processes, ranging from introductory to advanced, in one and two dimensions. State-of-the art methods and new results are
  presented in simple English. Yet, some mathematical proofs appear for the first time in this book: for instance,
 about the full autocorrelation function of the logistic map, the absence of cross-correlation between digit sequences
  in a family of irrational numbers, and a very fast algorithm to compute the digits of quadratic irrationals. These are not just new important if not seminal theoretical developments: it leads to  
 better algorithms in random number generation (PRNG), benefiting applications such as 
  data synthetization, security, or heavy simulations. In particular, you will find an implementation of a very fast, simple PRNG based on
 millions of digits of millions of quadratic irrationals, producing strongly random sequences superior in many respects to those available
 on the market. 

Without using measure theory, the invariant distributions of many systems are discussed in details, with numerous closed-form expressions for classic and new
 maps, including the logistic, square root logistic, nested radicals, generalized continued fractions (the Gauss map), the ten-fold and dyadic maps, and more. The concept of bad seed, rarely discussed in the literature, is explored in details. It leads to singular fractal distributions with
 no probability density function, and sets similar to the Cantor set. Rather than avoiding these monsters, you will be able to leverage them
 as competitive tools for modeling purposes, since many evolutionary processes in economy, fintech, physics, population growth and so on, do not always behave nicely. 

A summary table of  numeration systems serves as a useful, quick reference on the subject. Equivalence between different maps is also discussed. In a nutshell, this book is dedicated to the study of two numbers: zero and one, with
 a wealth of applications and results attached to them, as well as some of the toughest mathematical conjectures. It will appeal in particular to busy practitioners  
 in fintech, security, defense, operations research, engineering, computer science, machine learning, AI, as well as consultants
 and professional mathematicians. For students complaining about how hard this topic is, and deterred by the amount of advanced mathematics, this book will help them get jump-started. While the mathematical level remains high in some sections, it is explained as
 simply as possible, focusing on what is needed for the applications.

Numerous illustrations including beautiful representations of these systems (generative art), a lot of well documented Python code, and nearly 20 off-the-beaten-path exercises complementing the theory, will help you navigate through this fascinating field. 
 You will see how even the most basic systems offer such an incredible variety of configurations depending on a few parameters, allowing you
 to model a very large array of phenomena.  A surprising application -- a synthetic stock exchange and lottery -- is described in detail in chapter~\ref{pouti}, including full business model and legal aspects.

Finally, chapter~\ref{ch1}
 also covers time-continuous processes including unusual clustered, reflective, constrained, and integrated Brownian-like processes, random walks and time series, with little math and jargon-free. In the end, my goal is to 
get you to you use these systems fluently, and see them as gentle, controllable chaos. In short, what real life should be! Quantifying the amount
 of chaos is also one of the topics discussed in the book.



\section*{About the author}

Vincent Granville is a pioneering data scientist and machine learning expert, co-founder of Data Science Central  (acquired by TechTarget), founder of \href{https://mltechniques.com/}{MLTechniques.com}, former VC-funded executive, author and patent owner.
\begin{wrapfigure}{l}{0.14\textwidth}
\vspace{-1ex}
\includegraphics[width=0.98\linewidth]{vgr3.png} 
%\caption{Caption1}
%\label{fig:wrapfig}
\end{wrapfigure}
\vspace{-2ex}\quad \\
\noindent Vincent’s past corporate experience includes Visa, Wells Fargo, eBay, NBC, Microsoft, and CNET. 
Vincent is also a former post-doc at Cambridge University, and the National Institute of Statistical Sciences (NISS).  
He  published in {\em Journal of Number Theory}, {\em Journal of the Royal Statistical Society} (Series B), and {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}. He is also the author of multiple books, available \href{https://mltechniques.com/resources/}{here}. He lives  in Washington state, and enjoys doing research on stochastic processes, dynamical systems, experimental math and probabilistic number theory.

%\renewcommand{\baselinestretch}{0.97}\normalsize
\listoffigures
%\renewcommand{\baselinestretch}{1.00}\normalsize
%\listoftables


\hypersetup{linkcolor=red}
\renewcommand{\baselinestretch}{1.00}\normalsize
\tableofcontents 
\renewcommand{\baselinestretch}{1.00}\normalsize

\chapter{Random Walks, Brownian Motions, and Related Stochastic Processes}\label{ch1}

I introduce these stochastic processes, routinely used by Wall Street quants, with a simple approach consisting of rescaling random walks to make them time-continuous, with a finite variance, based on the central limit theorem. 
Stochastic processes have many applications, including in finance and physics. It is an interesting model to represent many phenomena. 
One of the most simple examples is a random walk, and indeed easy to understand with no mathematical background. However, time-continuous stochastic processes are usually defined and studied using advanced and abstract mathematical tools such as measure theory, martingales, and filtration. 

In this chapter, it is explained in simple terms, accessible to first-year college students with exposure to calculus and basic probability principles. At least initially, I discuss the one-dimensional case. After introducing standard Brownian motions, I dicuss other related processes
 with interesting properties. It leads to a functional integral equation with exact solution, for the equilibrium distribution of the system: in this case, non-Gaussian unlike Brownian motions.

\section{From random walks to Brownian motions}\label{bmrwp}

The \textcolor{index}{Brownian motion}\index{Brownian motion} [\href{https://en.wikipedia.org/wiki/Brownian_motion}{Wiki}] is among the most basic time-continuous processes: it is simply an integrated \textcolor{index}{white noise}\index{white noise} [\href{https://en.wikipedia.org/wiki/White_noise}{Wiki}]. It is sometimes referred to as the \textcolor{index}{Wiener process}\index{Wiener process} [\href{https://en.wikipedia.org/wiki/Wiener_process}{Wiki}]. 
To build these processes, I use one of  the most basic time-discrete stochastic processess:  a \textcolor{index}{random walk}\index{random walk} [\href{https://en.wikipedia.org/wiki/Random_walk}{Wiki}].  The random walk is defined by $X_{k+1}$ equal to either $X_k + 1$ or $X_k -1$ with equal probabilities. Typically, one starts with $X_0=0$, and $k$ denotes the time. 

Thus, a random walk is sequence of auto-correlated random variables indexed by time. Let $(U_k)$ with $k=1,2$ and so on be a sequence of independent binary random variables taking the values $+1$ or $-1$
 with equal probability $\frac{1}{2}$. Then $X_n$ can be written as $X_n = U_1 + \dots + U_n$, and $-n\leq X_n\leq n$. Let's use $U_1$ as the representative for the family $(U_k)$ of random variables, since they all have the same distribution.

%xxx showing random walk, brownian motion

What happens if we change the time scale (horizontal axis) say from daily to hourly, or to every millisecond? We then also need to rescale the values (vertical axis) appropriately; otherwise the process exhibits massive oscillations in very short time periods. At the limit, if we consider infinitesimal time increments, the process becomes a continuous one. Much of the complex mathematics needed to define these continuous processes amount to performing the correct rescaling of the vertical axis, to make the limiting process meaningful. 

You can define these time-continuous processes as the limit of their time-discrete version: using the correct rescaling is straightforward. Let us define $\{Y_t(n)\}$ as the same process as $\{X_k\}$, but with small time increments of $1/n$ instead of $1$.  In other words, $Y_{k/n}(n) = X_k$. I just rescaled the horizontal time axis. I also introduced the notation $t=k/n$ for the more granular time. Note that $Y_t(n)$ can  take on very large values, between $-n$ and $+n$ when $t = 1$. Thus we also need to rescale the vertical axis. We have:
$$
\text{Var}[Y_1(n)] =\text{Var}[X_n]  = \sum_{t=1}^n \text{Var}[U_t] = n\cdot \text{Var}[U_1].
$$
The only way to make the right-hand side of the equation not depending on $n$ is to rescale the vertical axis as follows. Define
$$
Z_t(n) = \frac{Y_t(n)}{\sqrt{n}}, \quad Z_t = \lim_{n\rightarrow\infty} Z_t(n).
$$
Then
$$
\text{Var}[Z_1(n)] = \text{Var}[U_1], \quad \text{Var}[Z_t(n)] = t\cdot \text{Var}[U_1], 
$$
which leads to 
\begin{equation}
\text{Var}[Z_t] = t \cdot \text{Var}[U_1].\label{eq1}
\end{equation}
Also, because of the central limit theorem, by construction for any real value $t$, $Z_t$ has a Gaussian distribution, regardless of the distribution of $U_1$. The final process $Z_t$ is both time-continuous and continuous on the vertical axis, though nowhere differentiable. It looks like a fractal and it is known as a Brownian motion -- the standard time-continuous stochastic process -- from which many other processes are derived. 

Note that if instead of using a binary random variable for $U_1$, you use a Gaussian one, then the limiting process 
$Z_t$ is identical, but we are dealing with Gaussian variables throughout the construction, making it easier to study the covariance structure and other properties. It then becomes a simple exercise to derive the covariance between $Z_t$ and $Z_s$ for any $t,s$. The covariance can also be estimated using simulations. Finally, note that $Z_0 = 0$ and $\text{E}[U_1] = 0$. 

%%
\section{General Properties}

The Brownian motion can also be viewed as a Gaussian stationary time series, characterized by its covariance or auto-correlation structure. It is also related to deterministic dynamical systems  that exhibit a fractal behavior. Under appropriate transformations, many of these processes can be made equivalent.  

One question is whether the above construction (the limit of a time-discrete random walk) covers all types of Brownian motions, or only a few particular cases. One way to investigate this is to check whether this construction can generate any kind of covariance structure that characterizes these processes. The answer is positive, making advanced mathematical theory unnecessary to build and study Brownian motions, as well as the numerous complex stochastic processes derived from this base process.  However, if you allow the random variables $U_k$ used in our construction to {\em not} be independent, then you can build more sophisticated time-continuous stochastic processes, that are not Brownian motions. 


All the stochastic processes introduced so far, whether time-discrete or time-continuous, share the following properties. In most cases, it is easy to turn a stochastic process into one that satisfies these properties, using simple transformations, as illustrated later in this section.\vspace{1ex} 
\begin{itemize}
\item \textcolor{index}{Stationarity}\index{stationarity} [\href{https://en.wikipedia.org/wiki/Stationary_process}{Wiki}]: It means that there is no trend, drift, or more precisely, the fact that the properties of the process in question do not explicitly depend on the time parameter $t$. Such processes are usually characterized by their auto-correlation structure alone. 
\item \textcolor{index}{Ergodicity}\index{ergodicity} [\href{https://en.wikipedia.org/wiki/Ergodicity}{Wiki}]: This means that one instance of the process is enough to derive all its properties. You don't need to make hundreds of simulations to study the process' properties or compute estimates: one simulation (also called {\em instance}) over a very long time period will do.  

\item \textcolor{index}{Scale-invariant}\index{scale-invariant} [\href{https://en.wikipedia.org/wiki/Scale_invariance}{Wiki}] and fractal behavior: If you zoom in or out on any single realization of these processes, you will get a new process with the exact same properties and behavior, indistinguishable from the parent process. Two different time windows provide two versions of the process that are identical with respect to their statistical properties. 

\item \textcolor{index}{Memoryless}\index{memoryless property} [\href{https://en.wikipedia.org/wiki/Memorylessness}{Wiki}]: The future observations depend on the present value only, not on past observations. This is sometimes referred to as 
the \textcolor{index}{Markov property}\index{Markov property} [\href{https://en.wikipedia.org/wiki/Markov_property}{Wiki}].  
\end{itemize}\vspace{1ex}

\noindent It is sometimes possible to transform a process so that it satisfies some of the above properties. For instance, if $X_k$ is a time series with a linear trend and discrete time increments, the differences $X_k - X_{k-1}$ may represent a stationary  time series. Likewise, if 
$X_k$ depends on $X_{k-1}$ and $X_{k-2}$, then the vector $(X_k, X'_k)$ with $X'_k = 
X_{k-1}$ represents a bivariate memoryless time series: the next bivariate value depends only on the current one.

Another interesting topic with numerous applications is the distribution of records (minimum or maximum), and their arrival times. A summary of the most important results  is found in my book on synthetic data~\cite{vgsynthetic}, in the section 
entitled ``Record Distances Between a Point and its Vertex". It deals with 2D processes, but the results also apply to 1D random walks. The analysis of records is the main subject of \textcolor{index}{extreme value theory}\index{extreme value theory} [\href{https://en.wikipedia.org/wiki/Extreme_value_theory}{Wiki}]. 

Finally, two important results are the \textcolor{index}{arcsine law}\index{arcsine law} [\href{https://en.wikipedia.org/wiki/Arcsine_laws_(Wiener_process)}{Wiki}] and chance to return to the origin. The former states that for 1D random walks and Brownian motions, the proportion of the time that the process is positive follows an arcsine law. The latter states the following: The probability of a random walk returning to its origin is 1 in one or two dimensions but only 34\% in three dimensions: 
this is \textcolor{index}{Pólya's theorem}\index{Pólya's theorem} [\href{https://mathworld.wolfram.com/PolyasRandomWalkConstants.html}{Wiki}]. 

\begin{Exercise} {\em Statistical estimations} -- Simulate $10^6$ realizations of a Brownian motion with $0\leq t \leq 1$, using the random walk construction described previously. Study the distribution of the following quantities, using computations averaged across all your simulations. In particular, what is the mean and variance, for the following quantities:
\begin{itemize}
	\item Extreme values: $\min Z_t$, $\max Z_t$, 
	\item Proportion of the time when $Z_t > 0$ (note that $Z_0 = 0$),
	\item Number of times when the sign of $Z_t$ changes.
\end{itemize}
Keep in mind that the $Z_t$'s are auto-correlated. Given a particular realization of a stochastic process, these statistics can be used to check if it is a Brownian motion or not. Another interesting exercise is to study the process in question if the variable $U_1$ does not have a variance, for instance if $U_1$ has a Cauchy distribution. 
\end{Exercise}


\section{Integration, differentiation, moving averages}\label{movbc}

Let's use the construction scheme in section~\ref{bmrwp} to build a Brownian motion $\{Z_t\}$.
The underlying time-discrete random walk $\{X_k\}$ is referred to as the base process. 
I introduce two transformations:\vspace{1ex}
\begin{itemize}
	\item The cumulative or \textcolor{index}{integrated process}\index{integrated process} $\{ S_t \}$ derived from $\{ Z_t \}$,
	\item The theoretical \textcolor{index}{moving average process}\index{moving average process} $\{ M_t \}$ derived from $\{ Z_t \}$.
\end{itemize}\vspace{1ex}
The inverse of integration is differentiation: the \textcolor{index}{differentiated}\index{differentiated process} $\{S_t\}$ is $\{Z_t\}$. In practice, the smoother process (integrated or moving average) is easier to study and sometimes displays patterns that can't be identified in the original process. 
To build the processes in question, proceeed as follows.
In the construction of the Brownian motion described in section~\ref{bmrwp}, replace 
$X_n = U_1 + \cdots + U_n$ by 
$X_n = V_1 + \cdots + V_n$, where $V_k$ is described below for each transformation. \vspace{1ex}
\begin{itemize}
	\item Integration: $V_k = U_1 + \cdots + U_k$.
	\item Differention: $V_k = U_{k+1} - U_k$. If $\{ Z_t \}$ is a Brownian motion  then the resulting process is a white noise: nowhere continuous, nowhere differentiable.
	\item Moving average: $V_k = U_k + U_{k + 1} + \cdots + U_{k + h(k)}$ where $h(k)$ is as small as possible to make the resulting process continuous and differentiable everywhere.  
\end{itemize}\vspace{1ex}
For moving averages applied to a Brownian motion, $h(k) = \lfloor \sqrt{k}\rfloor$ works. Here $\lfloor \cdot \rfloor$ stands for the integer part function. Does $h(k) = \lfloor \log k \rfloor$ work? This would make the resulting process far more similar to the original one, but maybe barely (if at all) continuous -- in other words, more chaotic than with $h(k) =  \lfloor\sqrt{k} \rfloor$.

\begin{figure}[H]
\centering
\includegraphics[width=0.78\textwidth]{ma2.PNG} %0.86
\caption{Brownian motion (green), integrated (orange) and moving average (red)}
\label{fig:trc}
\end{figure}

As in section~\ref{bmrwp}, you need to use the correct rescaling of the vertical axis to obtain meaningful variances that do not depend on $n$.
 I illustrate this for the integrated process. In this case we have:
$$
X_n = \sum_{k=1}^n V_k =\sum_{k=1}^n k\cdot U_{n-k+1},
$$
thus
$$
\text{Var}[X_n] = \sum_{k=1}^n k^2 \text{Var}[U_1] =\frac{n(n+1)(2n+1)}{6}\text{Var}[U_1]\sim \frac{n^3}{3}\text{Var}[U_1].
$$
The rescaling of the horizontal time axis is the same. Thus, the proper rescaling factor for the vertical axis, as $n$ tends to infinity, is  
$X_n / \sqrt{n^3/3}$. This leads to 
\begin{equation}
\text{Var}[S_t] = t^3 \cdot \text{Var}[U_1].\label{eq2}
\end{equation}

The same logic applies to compute $\text{Var}[M(t)]$. The details are left as an exercise. A more complicated exercise consists of computing the covariance between $S_t$ and $S_{t + s}$ for $s > 0$, and proving that $\{S_t\}$ is {\em not} a Brownian motion itself (being differentiable everywhere unlike Brownian motions.) 

Figure~\ref{fig:trc} was produced with the Python code in this section. It represents one realization of a Brownian motion $\{Z_t\}$, together with its integration $\{S_t\}$ and moving average $\{M_t\}$. The time period is $0\leq t \leq 5$. The Python program \texttt{brownian.py} is also on my
GitHub directory, \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/Brownian.py}{here}. 
The Brownian motion, integrated Brownian and moving average are denoted respectively as \texttt{X}, \texttt{S} and \texttt{M} in the code,
 while \texttt{T} is the time. 

Different metrics exist to characterize the smoothness stochastic process is. The 
\textcolor{index}{Hurst exponent}\index{Hurst exponent} [\href{https://en.wikipedia.org/wiki/Hurst_exponent}{Wiki}] is a well known one, measuring the amount of long-term memory in your process. It is equal to $\frac{1}{2}$ for a Brownian motion or 
 \textcolor{index}{Brown noise}\index{Brown noise} [\href{https://en.wikipedia.org/wiki/Brownian_noise}{Wiki}], and to 0
 for a \textcolor{index}{pink noise}\index{pink noise} [\href{https://en.wikipedia.org/wiki/Pink_noise}{Wiki}] and for a 
one-dimensional \textcolor{index}{white noise}\index{white noise} [\href{https://en.wikipedia.org/wiki/White_noise}{Wiki}]. The higher the value, the smoother the process.  When the parameter \texttt{smooth} in the Python code is increased,
 the resulting simulation is smoother.  Another methodology to generate integrated (smooth) processes is discussed in my book on synthetic data~\cite{vgsynthetic}, in the chapter dealing with linear algebra and auto-regressive time series. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.9\textwidth]{linear.png}  
\caption{Integrated Brownian (top left), Brownian (top right) and nowhere continuous (bottom)}
\label{fig:lollog1xx}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

There are many different ways to simulate Brownian motions and related processes. Figure~\ref{fig:lollog1xx} is based on simple 
\textcolor{index}{auto-regressive time series}\index{autoregressive models} [\href{https://en.wikipedia.org/wiki/Autoregressive_model}{Wiki}], properly scaled to make them time-continuous. The technique is described in the chapter ``Gentle Introduction to Linear Algebra -- Synthetic Time Series" in my book on synthetic data~\cite{vgsynthetic}. The originality here consists in choosing auto-regressive models where  some of the roots of the \textcolor{index}{characteristic polynomial}\index{characteristic polynomial} 
 [\href{https://en.wikipedia.org/wiki/Characteristic_polynomial}{Wiki}] are multiple, in particular roots with 
modulus equal to 1 (the ``largest" roots). The processes in the top part of Figure~\ref{fig:lollog1xx} are standard: roots with multiplicity 1 yield Brownian motions, multiplicity 2 yields integrated Brownian, multiplicity 3 yields doubly integrated Brownian and so on. In the bottom part, the underlying time series used in the simulation has $X_n$ depends on $X_{n-2}$, $X_{n-3}$ and so on, but not
 on $X_{n-1}$. The resulting processes are non-Brownian. Indeed they look like a ``derivative" of a Brownian: a non-standard function that densely fills some peculiar domain, resulting in a shape with a \textcolor{index}{fractal dimension}\index{fractal dimension}
 [\href{https://en.wikipedia.org/wiki/Fractal_dimension}{Wiki}] between 1 and 2: something intermediate between a curve and a surface.

The following Python script \texttt{Brownian.py} is used to produce Figure~\ref{fig:trc}.  It is on 
GitHub, \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/Brownian.py}{here}. \vspace{1ex}

%https://www.youtube.com/watch?v=W9jktqV3_Mc brownian wolfram alpha look for api

\begin{lstlisting}
import numpy as np
import matplotlib.pyplot as plt

n = 10000
m = 5*n
T = []
X = []
T.append(0.0)
X.append(0.0)
np.random.seed(1979)
for k in range(1,m):
    u = np.random.uniform(0,1)
    if u < 0.5:
        X.append(X[k-1]-1)
    else:
        X.append(X[k-1]+1)
    T.append(T[k-1] + 1/n)

S = []
S.append(0.0)
for k in range(1,m):
    S.append(X[k]+S[k-1])

M = []
smooth = 2.5  # the larger, the smoother the moving average
M.append(0.0)
hn = int(smooth*np.sqrt(n))
for k in range(1,m):
    sum = 0.0
    for h in np.arange(-hn, hn+1):
        idx = k + h
        if idx >= m:  # fix for index outside the array range
            idx = m - 1 - (idx % n)
        elif idx < 0: # fix for index outside the array range
            idx = -idx
        sum += X[idx]
    sum /= (2*hn + 1)
    M.append(sum)

for k in range(1,m):
    X[k] = X[k]/(n**0.5)
    S[k] = S[k]/(n**1.5)
    M[k] = M[k]/(n**0.5)

axes = plt.axes()
axes.tick_params(axis='both', which='major', labelsize=8)
axes.tick_params(axis='both', which='minor', labelsize=8)
for axis in ['top','bottom','left','right']:
    axes.spines[axis].set_linewidth(0.5) 
plt.plot(T, X, linewidth = 0.4, color = 'green', alpha = 0.2)   # Brownian motion
plt.plot(T, S, linewidth = 0.8, color = 'orange', alpha = 0.8)  # integrated Brownian motion
plt.plot(T, M, linewidth = 0.8, color = 'red', alpha = 1.0)     # moving average process
plt.axhline(y = 0.0, color = 'grey', linestyle = '--', linewidth = 0.4)
plt.show()
\end{lstlisting}

\section{Reflected random walks}\label{rflectr}

The goal here is to introduce the reader to a bounded process, where moves up or down are allowed or not based on the current state. It mimics an environment where constraints prevent the system from going too far up too quickly, or the other way around. Thus such processes do not exhibit massive explosions or implosions. 

\begin{figure}%[H]
\centering
\includegraphics[width=0.85\textwidth]{ma4.PNG} %0.86
\caption{Reflected random walk with $a=b=\frac{1}{2}$}
\label{fig:ivf}
\end{figure}



Our \textcolor{index}{reflected random walk}\index{reflected random walk} [\href{https://en.wikipedia.org/wiki/Reflected_Brownian_motion}{Wiki}] is defined as follow. Start with $X_0=0$ and then

\begin{equation}
X_k = \begin{cases}X_{k-1} + U_k\cdot k^{-a} & \text{if } X_{k-1} < 0, \\
                      X_{k-1} - U_k\cdot k^{-a}                                   & \text{if }   X_{k-1} \geq 0,%
        \end{cases}\label{poputres}
\end{equation}
%\begin{align}
%X(k)   & =  X(k-1) + \frac{U(k)}{k^a} \quad \text{ if } X(k-1) < 0,  \nonumber \\
% X(k)  & = X(k-1) - \frac{U(k)}{k^a} \quad \text{ if } X(k-1) \geq 0, \nonumber 
%\end{align}
where $U_k = V^b_k$, the $V_k$'s are uniform independent deviates on $[0, 1]$ and $a\geq 0, b>0$ are two parameters.
Also define $Z_k=k^a X_k$.
The distribution of $Z_k = k^a X_k$ is stable over time, in contrast to Brownian motions that lack stationarity unless normalized. For the reflected random walk, the limiting distribution of $Z_k$ -- called the \textcolor{index}{invariant measure}\index{invariant measure} [\href{https://en.wikipedia.org/wiki/Invariant_measure}{Wiki}] 
 or \textcolor{index}{attractor distribution}\index{attractor distribution} in dynamical systems -- is not Gaussian. This is another indication that the process is not Brownian. 

\begin{figure}[H]
\centering
\includegraphics[width=0.80\textwidth]{ma3.PNG} %0.86
\caption{Invariant measure (density function) of reflected random walk with $a=b=\frac{1}{2}$}
\label{fig:iv}
\end{figure}

However it satisfies
 the \textcolor{index}{stochastic integral equation}\index{stochastic integral equation}~(\ref{seqa}), which has an exact solution. For this type of
 \textcolor{index}{functional equations}\index{functional equation} [\href{https://en.wikipedia.org/wiki/Functional_equation}{Wiki}], the unknown is a probability distribution: here, the attactor distribution. The solution is also referred to as a 
\textcolor{index}{fixed point}\index{fixed point algorithm} [\href{https://en.wikipedia.org/wiki/Fixed_point_(mathematics)}{Wiki}], in this case of infinite dimension. It may not be unique.


 Let $Z$ be the limit of $Z_k$ as $k\rightarrow\infty$.  What is the
 distribution of $Z$? Let $F_Z$ and $f_Z$ denote respectively the CDF (cumulative distribution function) and density function attached to $Z$.
 Likewise, $F_U$ and $f_U$ is the common CDF and density attached to the $U_k$'s. To find $f_Z$ you need to solve the following integral equation where $f_Z$ is the unknown:
\begin{equation}
f_Z(z) = \int_0^1 \Big[f_U(x+z) + f_U(x-z)\Big]\cdot f_Z(x) dz. \label{seqa}
\end{equation}

Figure~\ref{fig:iv} shows an approximation to the solution computed on $2\times 10^6$ iterates of Formula~(\ref{poputres}) with 
$a=b=\frac{1}{2}$. In this case, the exact density function is known and equal to 
$$
f_Z(z) = \frac{b+1}{2}\cdot \Big(1-|z|^{1/b}\Big) = \frac{1-F_U(z)}{2 \text{E}[U]}, \quad -1\leq z \leq 1.
$$
This is the correct solution if $b=1,b=\frac{1}{2}$, or $b\rightarrow 0$, regardless of $0<a<1$. It can be verified by plugging this solution in Formula~(\ref{seqa}). I haven't checked if the formula is still valid for other values of $b$. It is easy to empirically obtain the following result, 
 based on observations over a very large time period:
$$
\text{Var}[Z] = \frac{b+1}{3(3b+1)}.
$$
Of course $\text{E}[Z]=0$. Figure~\ref{fig:ivf} shows one realization of a reflected random walk (the first $2000$ values of $Z_k$) when $a=b=\frac{1}{2}$. The Python code to produce Figures~\ref{fig:ivf} and ~\ref{fig:iv} is on my GitHub repository 
 \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/brownian_reflective.py}{here}, and also in 
 section~\ref{dupuis}.

\subsection{Exercises}

The purpose of this section is to explore particular cases of reflective random walks, corresponding to specific values of $a$ and $b$. 
In Exercise~\ref{knorr}, the formula for the integral equation is established. 

\begin{Exercise} {\em Reflective random walks: special cases} -- Prove that if $0<a < 1$, then  $X_k\rightarrow 0$ as
$k\rightarrow\infty$. Under the same condition, prove that the limiting distribution of $Z$
\begin{itemize}
\item	always exists and its support domain is $[-1, 1]$,
\item is symmetric, with mean and median equal to 0,
\item does not depend on $a$, but only on $b$.
\end{itemize}
For instance, if $b =1$, the distribution of $Z$ is triangular regardless of $a$. If $a < 1$ and $b = 0$, (the non-stochastic case) prove that 
$Z$ can only take on 3 values: $-1,+1$ and $0$ respectively with probability $\frac{1}{4},\frac{1}{4}$ and $\frac{1}{2}$.
Also show that when $b\rightarrow 0^{+}$,  the distribution of Z converges to a uniform distribution on $[-1, 1]$. 
When $b=0$ (the non-stochastic case), we also have the following cases:
\begin{itemize}
\item if $a=1$, $X_k\rightarrow 0$,
\item 	if $a = 3$, $X_k\rightarrow \zeta(3) -\frac{5}{4}\approx -0.048$,   
\item	if $a = 4$, $X_k\rightarrow \zeta(4) -\frac{9}{8}\approx -0.043$.
\end{itemize}
Here $\zeta(\cdot)$ is the \textcolor{index}{Riemann zeta function}\index{Riemann zeta function} [\href{https://en.wikipedia.org/wiki/Riemann_zeta_function}{Wiki}]. 
\end{Exercise}

\begin{Exercise}\label{knorr} {\em Establishing the integral equation} -- Prove that the density $f_Z(z)$ attached to the limiting distribution must
satisfy~(\ref{seqa}). \vspace{1ex} \\ 
{\bf Solution} \\
Based on Formula~(\ref{poputres}), we have
$$
P(X_k<z) = \int_{-1}^1 P(X_k < z | X_{k-1} = x) f_{X_{k-1}}(x) dx.
$$
Let $U$ be a random variable with the same distribution as any $U_k$, and let $k\rightarrow\infty$. Separating the cases $x<0$ and $x\geq 0$, we have:
$$
P(Z<z) = \int_0^1 P(U > x-z )f_Z(x)dx + \int_{-1}^0 P(U < z-x )f_Z(x)dx.
$$  
Taking advantage of the symmetries of the problem, this can be further simplified to
$$
F_Z(z) = \frac{1}{2} + \int_0^1 \Big[F_U(x+z)-F_U(x-z)\Big] f_Z(x)dx.
$$
Finally, taking the derivative with respect to $z$ on both sides of the previous equality, we obtain the desired integral equation for $f_z$. \qed

We will encounter more of these integral equations with an exact solution in the next chapters. Many are related to dynamical systems that represent new numeration systems, leading to interesting invariant measures. But in general, these equations do not have an exact solution. They are typically solved numerically with an iterative algorithm converging to the solution.  

\end{Exercise}

\subsection{Python code}\label{dupuis}

The Python code in this section is used to produce Figures~\ref{fig:ivf} and~\ref{fig:iv}. It is also on my GitHub repository 
 \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/brownian_reflective.py}{here}. I included it in this chapter because it illustrates the Seaborn and Statsmodels libraries: the former to produce empirical density plots, and the latter to compute empirical
 distributions. \vspace{1ex}

\begin{lstlisting}
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.distributions.empirical_distribution import ECDF

m = 2000 
a = 0.5
b = 0.5
T = []
X = []
Z = []
T.append(0.0)
X.append(0.0)
Z.append(0.0)
np.random.seed(1979)
for k in range(1,m):
    u = np.random.uniform(0,1)
    u = u**b
    if X[k-1] < 0:
        X.append(X[k-1] + u/(k**a))
    else:
        X.append(X[k-1] - u/(k**a))
    Z.append((k**a) * X[k])
    T.append(T[k-1] + 1)
   
axes = plt.axes()
axes.tick_params(axis='both', which='major', labelsize=8)
axes.tick_params(axis='both', which='minor', labelsize=8)
for axis in ['top','bottom','left','right']:
    axes.spines[axis].set_linewidth(0.5) 
plt.plot(T, Z, linewidth = 0.4, color = 'green', alpha = 1)  
plt.axhline(y = 0.0, color = 'black', linestyle = '--', linewidth = 0.4)
plt.show()

ecdf = ECDF(Z[10:len(Z)])
sns.set_context("paper",font_scale=0.8, rc={"lines.linewidth": 0.8})
sns.displot(Z[10:len(Z)], kind="kde",linewidth=0.5) ### , bins=150)
plt.show()
\end{lstlisting}


\section{Constrained random walks}

%xxxx

%properties of RW / records distribution of records

%xxx random walk never comes back to the origin in 3D ???
%x to add to previous section
%arc sine law

%xxxx link to [or add entire chapter] to "subtle departures to randomness" where I study 
%  return to zero and so on. or link to extreme value theory in same book
%xxx add chapter on long term autocorrel time series
%xxx pillow article / article with solution to dynamical system
%xxx 2D numeration system 
%xxxx appendic=x from stats book
% add chapter: PRNG based on quadratic irrationals

The standard symmetric 1D \textcolor{index}{random walk}\index{random walk} [\href{https://en.wikipedia.org/wiki/Random_walk}{Wiki}] fundamental to this section is a sequence $\{S_n\}$ with $n\geq 0$, starting at $S_0=0$, and recursively defined by 
$S_{n}=X_{n}+S_{n-1}$, for $n>0$.  Here $X_1,X_2$ and so on are independent random variables with $P[X_n=1]=P[X_n=-1]=\frac{1}{2}$. 
 Thus $\{S_n\}$ is a time-discrete stochastic process, and indeed the most basic one. In sections~\ref{azxa} and~\ref{azxb}, I drop the assumption
 of independence, leading to modified random walks such as those described in~\cite{nkrn2018,lanwu2012}.
More general references include~\cite{gtm2021,peresbrown}.

With proper rescaling, a random walk becomes a time-continuous stochastic process $S_t$ called \textcolor{index}{Brownian motion}\index{Brownian motion} [\href{https://en.wikipedia.org/wiki/Brownian_motion}{Wiki}], with $t\in\mathbb{R}^+$. See the time series in gray in Figure~\ref{fig:walk}, displaying a particular instance: it shows the first $\num{50000}$ values of
 $S_n$ in a short window, giving the appearance of a Brownian motion. 
By contrast, each of the orange, red and gray time series represents one instance of a specific type of non-Brownian motion. Sections~\ref{azxa} and~\ref{azxb} focuses on these three types of processes,  which are quasi, but not fully random.


\subsection{Three fundamental properties of pure random walks}\label{poyt}

The standard random walk $\{S_n\}$ (illustrated in gray in Figure~\ref{fig:walk}) is the base or reference process, used to build more sophisticated models. It has too many properties to list in this short chapter. However, the following are the most relevant to our discussion.

\begin{itemize}
\item \textcolor{index}{Law of the iterated logarithm}\index{law of the iterated logarithm}\index{iterated logarithm} [\href{https://en.wikipedia.org/wiki/Law_of_the_iterated_logarithm}{Wiki}]. 
 In our context, it is stated as follows:
\begin{equation}
\lim \sup \frac{|S_n|}{\sqrt{2nv\log \log n}} = 1 \quad \text{as } n\rightarrow \infty.\label{lil12}
\end{equation}
Here, as per the 
 \textcolor{index}{Hartman–Wintner theorem}\index{Hartman–Wintner theorem} [\href{https://encyclopediaofmath.org/wiki/Law_of_the_iterated_logarithm}{Wiki}],  $v=\text{Var}[X_1]=1$. See~\cite{peresbrown} pages 118--123 for a version adapted to Brownian motions.
\item Expected number of \textcolor{index}{zero crossings}\index{random walk!zero crossing} in $S_1,\dots,S_n$, denoted as $N_n$. Here a zero-crossing is an index $0<k\leq n$
 such that $S_k=0$. For $n>0$, we have (see \href{https://math.stackexchange.com/questions/1684576/expected-of-returns-in-a-symmetric-simple-random-walk}{here}): 
$$
\text{E}[N_{2n}]=-1+\frac{2n+1}{4^n} \binom{2n}{n}  \sim  \frac{2}{\sqrt{\pi}}\cdot \sqrt{n}  \quad \text{as } n\rightarrow \infty.
$$
\item Distribution of \textcolor{index}{first hitting time}\index{random walk!first hitting time} to zero [\href{https://en.wikipedia.org/wiki/First-hitting-time_model}{Wiki}], or
 first zero crossing after $S_0=0$, also called time of first return. The random variable in question is denoted as $T$. It is defined as follows: 
$T=n$ (with $n>0$) if and only if $S_{n}=0$ and $S_k\neq 0$ if $0<k<n$. We have $P[T=n]=0$ if $n$ is odd, and $\text{E}[T]=\text{Var}[T]=\infty$. Yet, our random walks cross the X-axis infinitely many times.   We also have the following
 \textcolor{index}{probability generating function}\index{probability generating function} [\href{https://en.wikipedia.org/wiki/Probability-generating_function}{Wiki}] (see \href{https://math.stackexchange.com/questions/64919/biased-random-walk-and-pdf-of-time-of-first-return}{here}):
$$
\sum_{n=1}^\infty  (2x)^{2n} P[T=2n] =1-\sqrt{1-4x^2} \quad \text{if } x\leq \frac{1}{4}.
$$
 From there, one can obtain 
\begin{align}
P[T=2n] & =\frac{1}{(2n-1)4^n}\binom{2n}{n}\sim \frac{1}{\sqrt{4\pi}}\cdot n^{-3/2} \quad \text{as } n\rightarrow \infty,\nonumber \\
\text{E}[T^{-1}] & = \int_{0}^{1/2} \frac{1-\sqrt{1-4x^2}}{x}dx = 1-\log 2. \nonumber
\end{align}
\end{itemize}

\noindent Note that $\text{E}[T^{-1}]$ is finite, while $\text{E}[T]$ is infinite. The fact that
 $\text{E}[T]=\infty$ explains why the sequence $S_n$ can stay above or below the X-axis for incredibly long time periods, as shown
 in Figure~\ref{fig:walk} for the gray curve. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.8\textwidth]{walk.png}  
\caption{Typical path $S_n$ with $0\leq n\leq \num{50000}$ for four types of random walks}
\label{fig:walk}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

The above three statistics $|S_n|/\sqrt{2n\log\log n}, N_{2n}$ and $T^{-1}$can be used to design tests of randomness for 
\textcolor{index}{pseudorandom number generators}\index{pseudo-random numbers}. Indeed, the \textcolor{index}{prime test}\index{pseudo-random numbers!prime test}\index{prime test (of randomness)}   relies on a number theoretic version of the law of the iterated logarithm (LIL). See chapter ``High Quality Random Numbers for Simulations and Data Synthetization" in~\cite{vgsynthetic}.
The purpose is to detect very weak departures from randomness, even in sequences that are random enough to pass the classic LIL test, yet not fully random. 
 
I now describe special types of modified random walks that lack true independence in the sequence $\{X_n\}$. In particular, I discuss why
 they are special and of great interest, with a focus on applications.


\subsection{Random walks with more entropy than pure random signal}\label{azxa}

One way to introduce dependencies in the sequences is to increase the frequency of oscillations (and thus the entropy) in the gray curve in 
 Figure~\ref{fig:walk}. The gray curve represents a realization of a pure random walk. 
 To achieve this goal, you may want the sequence to violate the law of the iterated logarithm: you want to build a sequence that would satisfy a modified law of the iterated logarithm with $\sqrt{2n\log\log n}$ in Formula~(\ref{lil12}) replaced by (say) $n^{2/5}$. 

To accomplish this, you need to add constraints when simulating the sequence in question.  Yet you want to preserve quasi randomness: 
the absence of drifts and auto-correlations in 
 the sequence $\{X_n\}$, even though there is some modest lack of independence. So modest indeed that most statistical tests would fail to catch it, even though it can made highly visible to the naked eye: see the red, and especially the blue curve in Figure~\ref{fig:walk}. 

\subsubsection{Applications}

Such sequences can be used to generate \textcolor{index}{synthetic data}  or to model barely constrained stochastic processes, such as stock price fluctuations in an almost perfect market. See also~\cite{pac203}. Another application is  
 to introduce an undetectable backdoor in some encryption systems without third parties (government or hackers) being able to notice it, depending on the strength of the dependencies. This type of backdoor can help the encryption company decrypt a message when requested by a 
 legitimate user who lost his key, even though the encryption company has no standard mechanism to store or retrieve keys (precisely to avoid government interference). 

This assumes that there is a mapping between the  type of weak dependencies introduced in a specific sequence, and the 
 type of algorithm (or the key) used to decrypt the sequence in question. The mapping can be made too loose for full decryption even by the parent company, but helpful to retrieve partial data, such as where the sequence originates from: in this case, the type of dependencies is a proxy for a signature. All that is needed is to add some extra bits so that the sequence has the desired statistical behavior.

Ironically, you need a very good, industrial-grade \textcolor{index}{pseudo-random number generator}\index{pseudo-random numbers} (PRNG) to generate almost perfectly random sequences. PRNG's that are not good enough -- such as the \textcolor{index}{Mersenne twister}\index{Mersenne twister}\index{pseudo-random numbers!Mersenne twister} -- may introduce irregularities that can interfere with the ones you want to introduce. This is discussed 
 in detail in chapter 10 about PRNG's, in~\cite{vgsynthetic}.

\subsubsection{Algorithm to generate quasi-random sequences}\label{qrrnd}

One way to generate such sequences is as follows: \vspace{1ex}

\noindent\textcolor{white}{00} $S=0$ \\
\textcolor{white}{00} {\bf For} $n = 1, 2,\dots$\\
\textcolor{white}{0000}  Generate random deviate $U$ on $[0,1]$\\
\textcolor{white}{0000}  $M=g(n)$\\
\textcolor{white}{0000}  {\bf If} ($S< -M$ and $U < \frac{1}{2}-\epsilon$) 
or ($S> M$ and $U < \frac{1}{2}+\epsilon$) or ($|S|\leq M$ and $U<\frac{1}{2}$) \\ 
\textcolor{white}{0000} {\bf Then} \\
\textcolor{white}{000000} $X_n=-1$ \\
\textcolor{white}{0000} {\bf Else} \\
\textcolor{white}{000000} $X_n=1$\\
\textcolor{white}{0000} $S=S+X_n$\\
\textcolor{white}{0000} $S_n=S$ \vspace{1ex}

\noindent Here $0<\epsilon<\frac{1}{2}$ and $\alpha>0$.  The function $g(n)$ is positive and 
 growing more slowly than $\sqrt{n}$. Typically, $g(n)=\alpha n^\beta$ with $0\leq \beta\leq\frac{1}{2}$, or $g(n)=\alpha (\log n)^\beta$
 with $\beta\geq 0$. 
The Python code in section~\ref{paths} performs this simulation: choose the option \texttt{deviations='Small'}. You can customize the function
 $g(n)$, denoted as \texttt{G} in the code. The option \texttt{mode='Power'} corresponds to $g(n)=\alpha n^\beta$,
 while \texttt{mode='Log'} corresponds to $g(n)=\alpha (\log n)^\beta$. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.9\textwidth]{iteratedlog1b.png}  
\caption{$\delta_n=1-\text{Var}[S_{n+1}]+\text{Var}[S_n]$ for four types of random walks, with $0\leq n\leq\num{5000}$}
\label{fig:lollog1b}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------



\noindent Results are displayed in 
 Figure~\ref{fig:walk}. The color scheme is as follows:
\begin{itemize}
\item Gray curve: $\epsilon=0$, corresponding to a pure random walk.
\item Blue curve: $g(n)=\log n$, $\epsilon=0.05$.
\item Red curve: $g(n)=n^\beta$ with $\beta=0.35$, $\epsilon=0.05$.
\end{itemize}
The yellow curve represents a very different type of process, discussed in section~\ref{azxb}.

\subsubsection{Variance of the modified random walk}

The symmetric nature of the modified random walk $\{S_n\}$ defined in section~\ref{qrrnd} results in several
identities.  Let $p_n(m)=P(S_n=m)$, with $-n\leq m \leq n$. Also, let $S_0=0$ and $p_0(0)=1$. Then $p_n(m)$ can be recursively computed using some modified version of the Pascal triangle recursion:
\begin{equation}
p_{n+1}(m)=\Big[\frac{1}{2}+\epsilon \cdot A_n(m-1)\Big]p_n(m-1)+\Big[\frac{1}{2}-\epsilon\cdot A_n(m+1)\Big]p_n(m+1),\label{zxzdc}
\end{equation}
where $A_n(m)=\chi[m<-g(n)]-\chi[m>g(n)]$. Here $\chi$ is the indicator function: $\chi(\omega)=1$ if $\omega$ is true, otherwise $\chi(\omega)=0$.
Some of the identities in question include:
$$
\sum_{m=-n}^n m \cdot p_n(m)=\text{E}[S_n]=0,\quad
\sum_{m=-n}^n A_n(m)p_n(m)=0,\quad
\sum_{m=-n}^n m^2 A_n(m)p_n(m)=0,
$$
$$
\sum_{m=-n}^n m^2 \Big[A_{n-1}(m-1)p_{n-1}(m-1)+A_{n-1}(m+1)p_{n-1}(m+1)\Big]=0.
$$
From these identities, it is easy to establish a recursion for the variance:
\begin{equation}
\text{Var}[S_{n+1}]=\text{Var}[S_n]+1-\delta_n, \quad \text{with }\delta_n=8\epsilon\cdot \sum_{m>g(n)} m \cdot p_n(m).\label{varzes}
\end{equation}
The sum for $\delta_n$ is finite since $p_n(m)=0$ if $m>n$.   Of course, 
$\text{Var}[S_0]=0$. Also, if $\epsilon=0$, the sequence is perfectly random: $\delta_n=0$, $\text{Var}[S_n]=n$ and
 $S_n/\sqrt{n}$ converges to a normal distribution. In turn, the law of the iterated logarithm is satisfied. Conversely, 
 this is violated if $\epsilon>0$. Formula~(\ref{varzes}) 
 combined with \textcolor{index}{Hoeffding's inequality}\index{Hoeffding inequality} [\href{https://en.wikipedia.org/wiki/Hoeffding\%27s_inequality}{Wiki}], may
 provide some bounds for $\text{Var}[S_n]$. 

\noindent Figure~\ref{fig:lollog1} shows $\delta_n$ for four types of modified random walks, using the following color scheme:
\begin{itemize}
\item Yellow: $g(n)=10,\epsilon=0.05$
\item Red: $g(n)=n^\beta,\beta=0.50, \epsilon=0.05$
\item Blue: $g(n)=n^\beta,\beta=0.45, \epsilon=0.05$
\item Purple: $g(n)=n^\beta,\beta=0.55, \epsilon=0.05$
\end{itemize}
The curve that coincides with the X-axis ($\delta_n = 0$) corresponds to $\epsilon=0$, that is, to  pure randomness regardless of $g(n)$. It is not
 colored in Figure~\ref{fig:lollog1}. Finally, the Python code in section~\ref{pypy1bv} computes $\text{Var}[S_n]$ exactly (not via simulations) using two different methods, proving that
 Formula~(\ref{varzes}) is correct. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.9\textwidth]{iteratedlog1.png}  
\caption{Same as Figure~\ref{fig:lollog1b}, using a more aesthetic but less meaningful chart type}
\label{fig:lollog1}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

\subsection{Random walks with less entropy than pure random signal}\label{azxb}

In section~\ref{azxa}, I focused on creating sequences with higher oscillation rates than dictated by randomness, resulting in lower amplitudes.
Doing the opposite -- decreasing the oscillation rate -- is more difficult. For instance, using $g(n)=n^\beta$ with $\beta>\frac{1}{2}$ won't work. 
 You can't do better than $\sqrt{n}$ because of the law of the iterated logarithm: boosting $\beta$ beyond the threshold $\frac{1}{2}$  is
 useless.

\noindent A workaround is to use the following algorithm: \vspace{1ex}

\noindent\textcolor{white}{00} $S=0$ \\
\textcolor{white}{00} {\bf For} $n = 1, 2,\dots$\\
\textcolor{white}{0000}  Generate random deviate $U$ on $[0,1]$\\
\textcolor{white}{0000}  $M=g(n)$\\
\textcolor{white}{0000}  {\bf If} ($-M< S< 0$ and $U < \frac{1}{2}+\epsilon$) 
or ($0<S< M$ and $U < \frac{1}{2}-\epsilon$) or ($S=0$ and $U<\frac{1}{2}$) \\ 
\textcolor{white}{0000} {\bf Then} \\
\textcolor{white}{000000} $X_n=-1$ \\
\textcolor{white}{0000} {\bf Else} \\
\textcolor{white}{000000} $X_n=1$\\
\textcolor{white}{0000} $S=S+X_n$\\
\textcolor{white}{0000} $S_n=S$ \vspace{1ex}

\noindent The Python code in section~\ref{paths}, with the option \texttt{deviations='Large'}, performs this simulation. The yellow time series in 
 Figure~\ref{fig:walk} is a realization of such a modified random walk, in this case with $g(n)=\alpha n^\beta$, with 
 $\alpha=0.30,\beta=0.54$ and $\epsilon=0.01$. It is unclear if the yellow curve will ever cross again the horizontal axis after
 $\num{50000}$ iterations, but it is expected to do so. To the contrary, the other three curves (gray, red, blue) are guaranteed to cross the
 horizontal axis infinitely many times, even though the random variable $T$ measuring the spacing between two crossings 
 (referred to as the \textcolor{index}{hitting time}\index{random walk!first hitting time} in section~\ref{poyt}) has infinite expectation.

For pure random walks (the gray curve in Figure~\ref{fig:walk}), the average number of times that $S_k=0$ when $0<k\leq 2n$
 is asymptotically equal to $\sqrt{4n/\pi}$ , as discussed in section~\ref{poyt}. One would expect this value to be about $178$
 when $2n=\num{50000}$. For the gray curve, the observed value is $243$. Keep in mind that huge variations are expected between different realizations of the same random walk, due to the fact that $\text{E}[T]=\infty$. Indeed, averaged over three realizations, the value $243$ was down to 
$185$. Also, a faulty pseudo-random number generator could easily lead to results that are off, in this case.

One would expect much larger 
 values for the ``non-random" red and blue curves. The observed 
values are respectively $747$ and $1783$, based on a single realization in each case. Likewise, the yellow curve is expected to have a much smaller value: in Figure~\ref{fig:walk}, 
that value is $105$.

\subsection{Python code: computing probabilities and variances attached to $S_n$}\label{pypy1bv}

This Python code is related to section~\ref{qrrnd}, where you can find more details.
It computes the variance of $S_n$ for $n=1, 2$ and so on, using two different methods:
 one based on the standard definition of the variance (denoted as \texttt{var1} in the code), and one based 
 on Formula~\ref{varzes}. The latter is denoted as \texttt{var2} in the code.  Also, the variable \texttt{delta}
 represents $\delta_n$. The output $\delta_n$ is featured in Figure~\ref{fig:lollog1b}.  Finally the function \texttt{G} represents $g(n)$. 
The code below is also available on Github, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/brownian_var.py}{here}. 
Program name: \texttt{brownian\_var.py}. \\

%\pagebreak %

\begin{lstlisting}
import math

epsilon=0.05 
beta=0.45
alpha=1.00 
nMax=5001

Prob={}
Exp={}
Var={}
Prob[(0,0)] =1
Prob[(0,-1)]=0 
Prob[(0,1)] =0 
Prob[(0,-2)]=0 
Prob[(0,2)] =0 

def G(n):
   return(alpha*(n**beta))

def psi(n,m):
    p=0.0
    if m>G(n): 
        p=-1
    if m<-G(n):  
        p=1
    return(p)

Exp[0]=0
Var[0]=0
OUT=open("rndproba.txt","w")
for n in range(1,nMax):
    Exp[n]=0
    Var[n]=0
    delta=0
    for m in range(-n-2,n+3,1):
        Prob[(n,m)]=0
    for m in range(-n,n+1,1):
        Prob[(n,m)]=(0.5+epsilon*psi(n-1,m-1))*Prob[(n-1,m-1)]\
            +(0.5-epsilon*psi(n-1,m+1))*Prob[(n-1,m+1)]
        Exp[n]=Exp[n]+m*Prob[(n,m)]
        Var[n]=Var[n]+m*m*Prob[(n,m)]
        if m>G(n-1) and m<n: 
            delta=delta+8*epsilon*m*Prob[(n-1,m)]
    var1=Var[n]
    var2=Var[n-1]+1-delta
    string1=("%5d %.6f %.6f %.6f" % (n,var1,var2,delta))
    string2=("%5d\t%.6f\t%.6f\t%.6f\n" % (n,var1,var2,delta))
    print(string1)
    OUT.write(string2) 
OUT.close() 
\end{lstlisting}


\subsection{Python code: path simulations}\label{paths}

This Python code performs all the simulations discussed in sections~\ref{qrrnd} and~\ref{azxb} and shown in Figure~\ref{fig:walk}. The option
 \texttt{deviations='Small'} is discussed in detail in section~\ref{qrrnd}, while \texttt{deviations='Large'} is
 explained in section~\ref{azxb}. The function \texttt{G} in the code corresponds to $g(n)$. Also, if you want to simulate a perfectly random walk, set $\epsilon$ (the parameter \texttt{eps} in the code) to zero. Finally, the code generates multiple realizations for any type of random walk. The number of realizations is determined by the parameter \texttt{Nsample}.
The code below is also available on Github, \href{https://github.com/VincentGranville/Machine-Learning/blob/main/Source\%20Code/brownian_path.py}{here}. 
Program name: \texttt{brownian\_path.py}. \\

\begin{lstlisting}
import random
import math
random.seed(1) 

n=50000
Nsample=1
deviations='Large'
mode='Power' 

if deviations=='Large':
    eps=0.01
    beta=0.54
    alpha=0.3
elif deviations=='Small':
    eps=0.05
    beta=0.35 #beta = 1 for log
    alpha=1

def G(n):
    if mode=='Power':
        return(alpha*(n**beta))
    elif mode=='Log' and n>0:
        return(alpha*(math.log(n)**beta))
    else:
        return(0)

OUT=open("rndtest.txt","w")
for sample in range(Nsample):
    print("Sample: ",sample)
    S=0
    for k in range(1,n):
        x=1
        rnd=random.random()
        M=G(k)
        if deviations=='Large':
            if ((S>=-M and S<0 and rnd<0.5+eps) or (S<=M and S>0 and rnd<0.5-eps) or 
                  (abs(S)>=M and rnd<0.5) or (S==0 and rnd<0.5)):
                x=-1
        elif deviations=='Small':
            if (S<-M and rnd<0.5-eps) or (S>M and rnd<0.5+eps) or (abs(S)<=M and rnd<0.5):
                x=-1
        print(k,M,S,x)
        S=S+x
        line=str(sample)+"\t"+str(k)+"\t"+str(S)+"\t"+str(x)+"\n" 
        OUT.write(line)   
OUT.close()      

\end{lstlisting}


\section{Two-dimensional Brownian motions}



There are countless types of random walks or quasi-Brownian motions that are -- on purpose and by design -- not perfectly random.  A good reference is the book by Mörters and Peres~\cite{peresbrown}, published in 2010. Here I discuss a 2B Brownian motion generated using some specific probability distributions. Depending on the parameters, these distributions may or may not have an infinite expectation or variance. Things start to get interesting when the expectation becomes infinite (and the Brownian motion is no
 longer Brownian), resulting in a system exhibiting a strong clustering structure. 

Note that all the simulations performed here consist of discrete random walks rather than 
 space-continuous Brownian motions.
 They approach Brownian motions very well: when increments become infinitesimal, with proper rescaling the result is a continuous process
 as illustrated in Figure~\ref{fig:lolbrown}.
   
  In one dimension, we start with $S_0=0$ and $S_n=S_{n-1}+R_n\theta_n$, for $n=1,2$ and so on. If the $R_n$'s are independently and identically distributed (iid) with an exponential distribution of expectation $1/\lambda$ and $\theta_n=1$, then the resulting process is a stationary 
\textcolor{index}{Poisson point process}\index{Poisson point process} [\href{https://en.wikipedia.org/wiki/Poisson_point_process}{Wiki}] of 
intensity function $\lambda$ on $\mathbb{R}^{+}$; the $R_n$'s are the successive 
interarrival times \textcolor{index}{interarrival times}\index{interarrival times}. If the $\theta_n$'s are iid with $P(\theta_n=1)=P(\theta_n=-1)=\frac{1}{2}$, and independent from the $R_n$'s, then we get a totally different type of process, which, after proper re-scaling, represents a time-continuous 
 \textcolor{index}{Brownian motion}\index{Brownian motion} in one dimension. For general references, see \cite{daleyA2002,daleyB2008}.


%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{brownian.png}  
\caption{Clustered Brownian process}
\label{fig:lolbrown}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

I generalize it to two dimensions, as follows. Start with $(S_0,S'_0)=(0,0)$. Then generate the points $(S_n, S'_n)$, with $n=1,2$ and so on, using the recursion
\begin{align}
S_n &  =  S_{n-1}+R_n \cos(2\pi\theta_n) \label{brown10} \\
S'_n & = S'_{n-1}+ R_n\sin(2\pi\theta_n) \label{brown11}
\end{align}
where $\theta_n$ is uniform on $[0, 1]$, and the radius $R_n$ is generated using the formula
\begin{equation}
R_n=\frac{1}{\lambda}\Big(-\log(1-U_n)\Big)^\gamma, \label{gam11}
\end{equation}
where $U_n$ is uniform on $[0,1]$. Also, $\lambda>0$, and the random variables $U_n,\theta_n$ are all independently distributed. If $\gamma>-1$, then
$\mbox{E}[R_n]=\frac{1}{\lambda}\Gamma(1+\gamma)$ where $\Gamma$ is the \textcolor{index}{gamma function}\index{Gamma function} 
[\href{https://en.wikipedia.org/wiki/Gamma_function}{Wiki}]. In order to standardize the process, I use
$\lambda=\Gamma(1+\gamma)$. Thus, $\mbox{E}[R_n]=1$ and if $\gamma>-\frac{1}{2}$,
$$\mbox{Var}[R_n]=\frac{\Gamma(1+2\gamma)}{\Gamma^2(1+\gamma)}-1.$$
We have the following cases:


\begin{itemize}
\item If $\gamma=1$, then $R_n$ has an exponential distribution.
\item If $-1<\gamma<0$, then $R_n$ has a \textcolor{index}{Fréchet distribution}\index{Fréchet distribution}\index{distribution!Fréchet}. If in addition, $\gamma>-\frac{1}{2}$, then its variance is finite. 
\item If $\gamma>0$, then $R_n$ has a \textcolor{index}{Weibull distribution}\index{Weibull distribution}\index{distribution!Weibull}, with finite variance. 
\end{itemize}
Interestingly, the Fréchet and Weibull distributions are two of the three 
\textcolor{index}{attractor distributions}\index{attractor distribution} in \textcolor{index}{extreme value theory}\index{extreme value theory}.  

The two-dimensional process consisting of the points $(S_n,S'_n)$ is a particular type of random walk. The random variables $R_n$ represent the (variable) lengths of the successive increments. Under proper re-scaling, assuming the variance of $R_n$ is finite, it tends to a time-continuous
two-dimensional Brownian motion. However, if $\mbox{Var}[R_n]=\infty$, it may not converge to a Brownian motion. Instead, it is very similar to a 
\textcolor{index}{Lévy flight}\index{Lévy flight}\index{Brownian motion!Lévy flight} [\href{https://en.wikipedia.org/wiki/L\%C3\%A9vy_flight}{Wiki}], and produces a strong cluster structure, with well separated clusters. See Figure~\ref{fig:lolbrown}, based on $\gamma=-\frac{1}{2}$, $\lambda=8$, and featuring the first $\num{10000}$ points
 of the bivariate sequence $\{(S_n,S'_n)\}$. 

The 
Lévy flight uses a \textcolor{index}{Lévy distribution}\index{Lévy distribution}\index{distribution!Lévy}
[\href{https://bit.ly/3rV7mrq}{Wiki}] for $R_n$, which also has infinite expectation and variance. Along with 
the \textcolor{index}{Cauchy distribution}\index{Cauchy distribution}\index{distribution!Cauchy} (also with infinite expectation and variance), it is one of the few \textcolor{index}{stable distributions}\index{stable distribution} [\href{https://en.wikipedia.org/wiki/Stable_distribution}{Wiki}]. Such distributions are attractors 
for an adapted version of the \textcolor{index}{Central Limit Theorem}\index{central limit theorem} (CLT), just like the Gaussian distribution is the attractor
for the CLT. A well written, seminal book on the topic, is ``Limit Distributions for Sums of Independent Random Variables", by Gnedenko and Kolmogorov \cite{gk1954}.


For a simple introduction to Brownian and related processes, see the website RandomServices.org by Kyle Siegrist, especially the chapter on
standard Brownian motions, \href{https://www.randomservices.org/random/brown/Standard.html}{here}. The processes discussed here are further investigated in my book ``Stochastic Processes and Simulations: A Machine Learning Perspective"~\cite{vgsimulnew}.


\chapter{Introduction to Discrete Chaotic Dynamical Systems}


In chapter~\ref{ch1}, I introduced the concept of invariant measure satisfying a stochastic integral equation, in the context of stochastic
 discrete dynamical systems such as reflected random walks. In this chapter I establish a general formula to find the invariant measure -- also called invariant or equilibrium distribution -- for a class of well-known models. I then discuss several examples where 
 an exact solution is known. I also discuss the inverse problem:  finding a dynamical system satisfying a given invariant measure. The last section deals with the two-dimensional case.

\section{Definitions, properties, and examples}\label{oifvcd}

One fascinating problem is the following.  Given a random number, what is the probability distribution of the coefficients of its continued fraction expansion? For instance, we have 
$$
%e=2+\cfrac{1}{1+\cfrac{1}{2+\cfrac{1}{ 1+\cfrac{1}{1+\cfrac{1}{4+\cfrac{1}{1+\cfrac{1}{1+\cfrac{1}{6+\cdots}}}}}}}}
\pi = 3 + \cfrac{1}{7 + \cfrac{1}{15 +\cfrac{1}{1 +\cfrac{1}{292 +\cfrac{1}{1 + \dots}}}}}
$$
In this case the first five coefficients are 
 $7, 15, 1, 292, 1$. The method presented here provides an immediate solution to this historical problem: the Gauss–Kuzmin distribution. In particular, the frequency of the value $n=1,2$ and so on among the coefficients in the continued fraction expansion is 
\begin{equation}
\pi_n = 2\log_2 (n+1) - \log_2 (n+2) - \log_2 n > 0.\label{tatar}
\end{equation}
The expectation is infinite, but the logarithm of these coefficients have a finite expectation. Note that using the first three coefficients, you get the very good approximation $\pi \approx 355/113  \approx 3.15159292$. This is due to the fact that the fourth coefficient $292$ is unusually large. The same method allows you to solve many other problems not related to continued fractions, with various applications.
 
I will dig deeper into this example later in this section. But first, I need to introduce the general framework, some terminology, and more examples. I start with the one-dimensional case.  A 
 \textcolor{index}{discrete dynamical system}\index{dynamical systems!discrete} [\href{https://en.wikipedia.org/wiki/Dynamical_system}{Wiki}] is an iteration $X_{k+1} = g(X_k)$, where the initial value
 $X_0$ is called the \textcolor{index}{seed}\index{seed (dynamical systems)}. The index $k$ is usually referred to as the time or the 
\textcolor{index}{evolution parameter}\index{evolution parameter}. The system maps a domain $D\in \mathbb{R}$ (typically an interval) onto itself. That is, if $x\in D$, then $g(x)\in D$. Thus $X_0, X_1$ and so on are all in $D$, and $g(D) = D$. Typically, the function $g$ is
 not invertible to allow for mixing. The domain $D$ is called the \textcolor{index}{state space}\index{state space}, and 
the function $g$ is called a \textcolor{index}{mapping}\index{map}.

One of the most well-known examples is the 
\textcolor{index}{logistic map}\index{logistic map}\index{map!logistic} [\href{https://en.wikipedia.org/wiki/Logistic_map}{Wiki}]. In this case, $g(x) = r x(1-x)$ where $0<r \leq 1$ is a parameter and $D=[0, 1]$. There is a time-continuous version of it, governed by a differential equation rather than a recurrence. It is fully chaotic -- the case we are interested in -- when $r=4$. It has properties shared by all the systems studied in this book. 


\subsection{Properties of discrete chaotic dynamical systems}\label{thorium}
The systems studied in this book have the following properties. They are stated in the context of the logistic map as an illustration. \vspace{1ex}
\begin{itemize}
\item For the vast majority of seeds $X_0$, 
 the sequence $\{X_k\}$ is dense in $D$: it approaches any value in $D$ arbitrarily closely if $k$ is large enough. 
However, some seeds (infinitely many yet very rare) produce a periodic sequence and thus result in non-chaotic behavior. An example is 
 $X_0=\frac{1}{2}$ for the logistic map. Such seeds are called \textcolor{index}{bad seeds}\index{seed (dynamical systems)!bad seed}. Their set is small enough that it has zero \textcolor{index}{Lebesgue measure}\index{Lebesgue measure} [\href{https://en.wikipedia.org/wiki/Lebesgue_measure}{Wiki}].
\item The sequence can not be inverted: given an observed value $X_k$, there are always two values $X_{k-1}$ and $X'_{k-1}$ such
 that $X_k=g(X_{k-1}) = g(X'_{k-1})$. In some cases other than the logistic map, you actually have more than two potential previous iterates leading to the same $X_k$.
\item The sequence $\{X_k\}$ is \textcolor{index}{ergodic}\index{ergodicity} [\href{https://en.wikipedia.org/wiki/Ergodicity}{Wiki}]: 
 the distribution and other statistical properties (mean, variance, autocorrelations) attached to the $X_k$'s can be approximated with arbitrary precision  using one single infinite sequence (called realization or instance) starting with a good seed $X_0$, or using infinitely many realizations, each starting with a good seed -- say a different random seed for each realization -- but using only the first few iterates in each realization.
\item The limiting distribution $F(x)=P(X_k<x)$ as $k\rightarrow\infty$ (previously referred to as the invariant measure) is  also called
 the \textcolor{index}{invariant distribution}\index{invariant distribution}. It is  solution to the functional 
 equation $P(X_k<x) = P(g(X_k)<x)$, where the unknown $F$ is the distribution of $X_k$. It coincides 
with the limiting \textcolor{index}{empirical distribution}\index{empirical distribution} of $X_k$ observed on any sequence as
 $k\rightarrow\infty$ if you start with a \textcolor{index}{good seed}\index{seed (dynamical systems)!good seed} or a random seed. The probability that a random seed is a good one is $100\%$. Good seeds lead to a non-periodic sequence $\{X_k\}$, chaotic yet well behaved. I provide examples later in this book.
\item Computation of $X_k$ based on the recurrence $X_k=g(X_{k-1})$  is bound to generate totally wrong values after as little as 45 iterations. Due to the \textcolor{index}{ergodicity}\index{ergodicity} of the sequence, it is not a problem in many cases: it amounts to starting with a brand new seed every 45 iterations or so. It is an issue when computing long-range autocorrelations.
\item Two sequences $\{X_k\}, \{X'_k\}$ starting with two different good seeds $X_0, X'_0$ that are very close to each other, will over time completely diverge. This property is sometimes used to define chaos. It also explains the previous point.
\end{itemize} \vspace{1ex}

\noindent One important result to find an exact solution (assuming there is one) to the functional equation $F(x) = F(g(x))$, and thus directly identify the main invariant measure, is the following. Here I assume that $g(x) = p(x) - \lfloor p(x)\rfloor$ where the brackets represent the integer part function. In addition, $p(x)$ is assumed to be positive, monotonic, continuous and decreasing (thus bijective and invertible) with $p(1)=1$ and $p(0)=\infty$. Thus, $D = [0, 1]$. Despite the restrictions, it covers a large class of important dynamical systems, for instance
 the \textcolor{index}{Gauss map}\index{Gauss map}\index{map!Gauss}\index{dynamical systems!Gauss map} [\href{https://en.wikipedia.org/wiki/Gauss\%E2\%80\%93Kuzmin\%E2\%80\%93Wirsing_operator}{Wiki}] if $p(x) = 1/x$  leading to the \textcolor{index}{continued fractions}\index{continued fractions} [\href{https://en.wikipedia.org/wiki/Continued_fraction}{Wiki}] discussed in the introduction. In this case, $g(0)=0$.

In a number of important and interesting cases, there is a function $r(\cdot)$ such that the \textcolor{index}{invariant distribution}\index{invariant distribution}\index{invariant distribution!see invariant measure} $F$ with support domain
 $D=[0, 1]$ can be written as $F(x)=r(x+1)-r(1)$.  Thus $r(x)$ must be increasing on $[1,2]$ and $r(2)=1+r(1)$. Not any function can be an invariant distribution, also called 
\textcolor{index}{attractor distribution}\index{attractor distribution} in probability theory. Define $R(x)=r(x+1)-r(x)$. Then we can retrieve $p(x)$ (under some conditions) using the formula
\begin{equation}
p(x)=R^{-1}\Big(r(x+1)-r(1)\Big)=R^{-1}(F(x)). \label{ch2eq1}
\end{equation}
The approach here is backward: we solve the inverse problem where $F$ is known and we try to retrieve $p(x)$. But in the end, it helps build a table of dynamical systems with known solution, in the same way that a table of integrals helps solve a number of integrals. 

If you only know $p(x)$, to retrieve $F(x)$ or its derivative $f(x)$, you need to solve the following functional equation, whose unknown is the function $f$. 
\begin{equation}
f(x)=\sum_{k=1}^\infty \Bigg| \frac{dF(q(x+k))}{dx} \Bigg| = \sum_{k=1}^\infty |q'(x+k)| f(q(x+k)).\label{ch2eq2}
\end{equation}
Here $q(x)=p^{-1}(x)$. Note that $R(x) = F(q(x))$ or alternatively, $R(p(x)) = F(x)$, with $p(q(x)) = q(p(x)) = x$. Also, $0\leq x\leq 1$. 

In practice, you get a good approximation if you use the first 1000 terms in the sum. Typically, the invariant density $f$ is bounded, and the weights $|q'(x+k)|$ are decaying relatively fast as $k$ increases. 
The theory behind this is based on the 
\textcolor{index}{transfer operator}\index{transfer operator} [\href{https://en.wikipedia.org/wiki/Transfer_operator}{Wiki}], and related to
the \textcolor{index}{Perron-Frobenius theorem}\index{Perron-Frobenius theorem} [\href{https://en.wikipedia.org/wiki/Perron\%E2\%80\%93Frobenius_theorem}{Wiki}].
The invariant density is the eigenfunction of the transfer operator, corresponding to the eigenvalue 1. A much simpler approach is discussed in Exercise~\ref{exch2ex1}. Also, if $x$ is replaced by a vector (for instance, if working with bivariate dynamical systems), the above formula can be generalized, involving two variables $x, y$, and the derivative of the (joint) distribution is replaced by a Jacobian. 

\begin{Exercise}\label{exch2ex1} {\em Establishing the functional equation} -- Assume $p(x) = \lambda/x^\alpha$ with $\alpha,\lambda >0$. Prove that the invariant density $f$ satisfies~(\ref{ch2eq2}). \vspace{1ex}

\noindent {\bf Solution} \\ 
Let $x'$ be a back image of $x$, in the sense that $g(x')=x$. For any $x\in [0,1]$, there are infinitely many  $x_k'$ such that
 $g(x'_k)=x$. I denote them as

$$q_k(x) = q(x+k) = \Big(\frac{\lambda}{x+k}\Big)^{1/\alpha}, \quad k=1,2,\cdots$$

\noindent where $q(x)=p^{-1}(x)$ and $q_k(x)=q(x+k)$ by definition. Now the back image of the whole interval $[x,x+\epsilon]$ with $\epsilon\approx 0$ is obtained as
$$ T_\epsilon(x) = \bigcup_{k=1}^\infty \Big[q_k(x+\epsilon),q_k(x)\Big].$$
The length of the $k$-th interval on the right-hand side is approximately $\epsilon |q'_k(x)|$, where $q'$ denotes the derivative with respect to $x$. Thus, since  $F([x,x+\epsilon]) = F(g([x,x+\epsilon])) = F(T_\epsilon(x))$, dividing by $\epsilon$ and letting $\epsilon\rightarrow 0$, we have the following fundamental functional equation:
$$f(x)=\sum_{k=1}^\infty |q'_k(x)|\cdot f(q_k(x))$$
where $f$ is the derivative of $F$.\qed

In particular, if $\alpha=\lambda=1$ (the standard continued fraction), it is easy to verify that $F(x)=\log_2(1+x)$ satisfies the functional equation. This is a well known result, and the proof provided here is probably one of the shortest ones. An immediate consequence is Formula~(\ref{tatar})
 involving the \textcolor{index}{Gauss–Kuzmin distribution}\index{Gauss-Kuzmin distribution} [\href{https://en.wikipedia.org/wiki/Gauss\%E2\%80\%93Kuzmin_distribution}{Wiki}]. This discrete distribution represents the probability that 
$\lfloor  p(X_k) \rfloor =n$ for $n=1,2$ and so on, starting with a random seed $X_0$, and $k\rightarrow\infty$. It has an infinite expectation. If $\alpha<1$ the resulting distribution has a finite
 expectation, and if $\alpha\leq\frac{1}{2}$, its variance is finite. The strictly positive integer $\lfloor  p(X_k) \rfloor$ representing the $k$-th coefficient in 
 the continued fraction expansion of $X_0$, is also called the $k$-th \textcolor{index}{digit}\index{digit} in the system in question.
\end{Exercise}

\subsection{Shortlist of dynamical systems with known solution}\label{bckdigt}

In section~\ref{rflectr}, I discussed a system with an exact solution to the functional equation: reflected random walks. I discuss more systems with an exact  solution in the next chapters. The logistic map is one of them. Here I focus on systems where the exact solution can be derived from the results in section~\ref{thorium}. Before starting, I introduce the concept of digits and numeration system attached to the 
chaotic dynamical systems in question. It is important and used throughout this book, with interesting applications in cryptography and random number generation.

\subsubsection{Digits and numeration system attached to a dynamical system}\label{digits}

The \textcolor{index}{numeration systems}\index{numeration system} described here are a generalization of the classic ones. Traditional \textcolor{index}{digits}\index{digit} in base $b$
are themselves attached to the 
the map $g(x)= bx - \lfloor bx \rfloor$, known as the \textcolor{index}{ten-fold map}\index{ten-fold map}\index{map!ten-fold} if $b=10$, and the \textcolor{index}{dyadic map}\index{map!dyadic} [\href{https://en.wikipedia.org/wiki/Dyadic_transformation}{Wiki}] if  $b=2$. 
I discuss theses cases, including when $b$ is a bivariate or \textcolor{index}{irrational base}\index{irrational base}, later in this book. The full
 infinite sequence of digits (also called \textcolor{index}{codes}\index{code (dynamical systems)}) allows you to retrieve the seed $X_0$, that is, the number that it represents.

Let us now focus on the dynamical systems described in section~\ref{thorium} using the map $g(x) = p(x) - \lfloor p(x) \rfloor$.
The $k$-th digit $a_k$ attached to the seed $X_0=x_0$ is defined as $a_k=\lfloor p(X_k)\rfloor$ for $k=0,1$ and so on. Even though the dynamical systems discussed here are not invertible, it is possible to compute $x_0$ if you only known its digits, thanks to the fact that $p(x)$ is invertible. To compute $x_0$ based on its digits, start with $N$ large (say $N=20$), $u_N=0$, and proceed iteratively backwards starting with $k=N-1$, back down to $k=0$, using the recursion 

$$u_k=q(u_{k+1} + a_k)-\lfloor q(u_{k+1} + a_k)\rfloor.$$

Here $q(x)=p^{-1}(x)$. If you start with $N=\infty$, then $x_0=u_0$. For the \textcolor{index}{Gauss map}\index{Gauss map}\index{map!Gauss}, the digits are simply the coefficients that appear in the continued fraction expansion $x_0=1/(a_0+1/(a_1+1/(a_2+\dots)))$. Note that $x_0\in[0,1]$ and the digits can take on any integer value greater than $1$. For the Gauss map, numbers $x_0$ that are rational are excluded. Similar restrictions apply to other maps. The expectation of the digits (the average of all the digits of $x_0$ for any $x_0$ but a set of Lebesgue measure zero), is finite if and only if $\int_0^\epsilon p(x)f(x)dx<\infty$ for any $\epsilon\in ]0,1]$. Here $f(x)$ is the invariant density, that is, the derivative of the invariant distribution. Likewise, the variance of the digits is finite if and only if $\int_0^\epsilon p^2(x)f(x)dx<\infty$ for any $\epsilon\in ]0,1]$. If the expectation and variance are finite, then it is possible to compute the autocorrelation between two successive digits. Obviously, both the expectation and variance are infinite for the digits of the Gauss map (continued fractions). Below we discuss a few dynamical systems, some with finite and some with infinite expectation / variance for the digits.

\subsubsection{Examples with exact solution in closed form}\label{cf4366}

Besides the Gauss map associated to continued fractions, I provide 5 examples with exact solution in closed form for the invariant distribution $F$. Again, $a_k=\lfloor p(X_k)\rfloor$ is the $k$-th digit attached to the system in question, as discussed in section~\ref{digits}. I use the functions 
 $r(x)$, $R(x)$ and $q(x)=p^{-1}(x)$ introduced in section~\ref{thorium}. \vspace{1ex}

\noindent{\bf Example 1}

\noindent Let $r(x)=-2(x+1)/x$. Then
$$ F(x)=\frac{2x}{x+1}, \quad R(x)=\frac{2}{x(x+1)},\quad p(x)=\frac{-1+\sqrt{5+4/x}}{2}, \quad q(x)=\frac{4}{(2x+1)^2-5}.$$
In this case 
\begin{align}
P(a_k = n)  =F(q(n))-F(q(n+1))=\frac{4}{n(n+1)(n+2)}, \quad
\text{E}[a_k]  =\sum_{n=1}^\infty k P(a_k=n)=2. \nonumber
\end{align}

\noindent{\bf Example 2}

\noindent Let
$$r(x)=\lambda\log\frac{\alpha+x}{\alpha+1} \mbox{ with }\lambda=\Big(\log\frac{\alpha+2}{\alpha+1}\Big)^{-1}.$$
Then
$$F(x)=\lambda\log\Big(\frac{\alpha+x+1}{\alpha+1}\Big), \quad
p(x)=\frac{\alpha+1}{x}-\alpha, \quad q(x)=\frac{\alpha+1}{\alpha+x}.$$
Here $\alpha\geq 0$. If $\alpha=0$, then this is just the Gauss map and we are dealing with standard continued fractions.
 Thus this example deals with \textcolor{index}{generalized continued fractions}\index{continued fractions!generalized}. \vspace{1ex}

\noindent{\bf Example 3}

\noindent Let
$$r(x)=\lambda\log\frac{x(x+1)}{2} \mbox{ with }\lambda=\frac{1}{\log 3}.$$
Then $$F(x)=\lambda\log\frac{(x+1)(x+2)}{2}, \quad
p(x)=\frac{4}{x(x+3)}, \quad q(x)=\frac{3}{2}\Bigg(-1+\sqrt{1+\frac{16}{9x}}\Bigg).$$

\noindent{\bf Example 4}

\noindent Let
$$r(x)=\lambda \alpha^{w(x)} \mbox{ with }\lambda=\frac{1}{\alpha^4-\alpha^2},
 \text{ and }w(x)=2^x.
$$
Then
$$ F(x)=\lambda(\alpha^{w(x+1)}-\alpha^2), \quad
p(x)=\log_2\log_\alpha\Bigg(\frac{1-\sqrt{1+4(w^2-\alpha^2)}}{2}\Bigg).$$

\noindent Here $0<\alpha<\sqrt{2}/2$. Other values of $\alpha$ don't work. The more simple case $r(x)=\lambda \alpha^x$ leads to nowhere. \vspace{1ex}

\noindent{\bf Example 5}

\noindent Let $\psi$ be a positive, monotonic decreasing function with $\psi(0)=\infty$. 
Let
$$r(x)=-\frac{1}{\psi(1)}\cdot \sum_{k=0}^\infty \psi(x+k), \quad \int_1^\infty\psi(x)dx<\infty.$$ 
Then
$$F(x)= \frac{1}{\psi(1)}\cdot \sum_{k=1}^\infty \Big(\psi(k)-\psi(x+k)\Big),\quad
R(x)=\frac{\psi(x)}{\psi(1)}, \quad p(x)=\psi^{-1}\Bigg(\sum_{k=1}^\infty \Big[\psi(k)-\psi(x+k)\Big]\Bigg).$$
Also, $P(a_k=n) = (\psi(n)-\psi(n+1))/\psi(1)$, for $n=1,2$ and so on. An interesting case involving the 
\textcolor{index}{Hurwitz}\index{Hurwitz function} and 
\textcolor{index}{Riemann zeta}\index{Riemann zeta function} functions  [\href{https://en.wikipedia.org/wiki/Hurwitz_zeta_function}{Wiki}] is when $\psi(x)=x^{-s}$ with $s>1$. It
 corresponds to a non-standard version of the \textcolor{index}{Hurwitz map}\index{Hurwitz map}.


\subsubsection{Gauss map: expectation, variance and autocorrelation}

For the Gauss map $X_{k+1}=1/X_k-\lfloor 1/X_k\rfloor$, the invariant distribution obtained in Exercise~\ref{exch2ex1} is $F(x)=\log_2(1+x)$ with $0\leq x \leq 1$. The invariant density $f(x)$ is the derivative of the distribution in question.  The expectation and variance of $X_k$ are easy to compute, for instance $\text{E}[X_k]=\int_0^1 x f(x)dx$. The 
covariance $\text{Cov}[X_k,X_{k+1}]=E[X_k X_{k+1}]-E[X_k]E[X_{k+1}]$ is a bit more delicate:  

$$\text{E}[X_k X_{k+1}]=\int_0^1 x\Bigg(\frac{1}{x}-\Bigg\lfloor\frac{1}{x}\Bigg\rfloor\Bigg)f(x)dx=1-\int_0^1 x\Bigg\lfloor\frac{1}{x}\Bigg\rfloor f(x)dx.$$

\noindent I tried Mathematica to evaluate the rightmost integral, to non-avail. Manual computations lead to

$$\int_0^1 x\Bigg\lfloor\frac{1}{x}\Bigg\rfloor f(x)dx =\frac{1}{\log 2}\sum_{n=1}^\infty \Bigg[\frac{1}{n+1}-n\log\Bigg(\frac{(n+1)^2}{n(n+2)}\Bigg)\Bigg] =\frac{\gamma}{\log 2} $$
where $\gamma$ is the Euler–Mascheroni constant. Putting everything together (I spare you the details), the lag-1 autocorrelation between $X_k$ and $X_{k+1}$, that is the limit of the empirical lag-1 autocorrelation computed on the first $k$ iterates as $k\rightarrow\infty$, is 
\begin{equation}
\rho=2\cdot \frac{(2-\gamma)\log 2 - 1}{3\log 2 - 2} \approx -0.347. \label{fritesdur}
\end{equation}
This is confirmed by empirical evidence, and it is true for almost all $x_0\in [0, 1]$ (in the Lebesgue sense). Exceptions to the rule include numbers such as $x_0=\sqrt{2}-1$, the golden ratio, rational and quadratic irrational numbers. By comparison, the lag-$n$ autocorrelation for the multiplicative system $X_{k+1}=bX_k-\lfloor bX_k\rfloor$ attached the the numeration system in integer base $b>1$, is $b^{-n}$. The additive system $X_{k+1}=X_k+\alpha - \lfloor X_k +\alpha\rfloor$ with $\alpha$ irrational, exhibits very strong long-range autocorrelations, making it one of the least chaotic dynamical systems among the chaotic ones. Unlike the Gauss map or the dyadic map ($b=2$), it does not have exceptions (that is, bad seeds or numbers $x_0$ not following the dominant invariant distribution) because its invariant distribution is unique. The exceptions (bad seeds) to the dyadic map are all the numbers $x_0$ that are not \textcolor{index}{normal}\index{normal number} 
[\href{https://en.wikipedia.org/wiki/Normal_number}{Wiki}] in base $2$.

To read more about the autocorrelation structure attached to the continued fraction system, see the article authored by Báez-Duarte~\cite{autocorr2005}, written in French.


\subsection{Exercises: Gauss map and continued fractions}

The exercises in this section focus on addressing the numerical instability of the sequence $\{X_k\}$, the bivariate digits of a 
 bivariate dynamical system, and how Khinchin's constant related to the coefficients of continued fractions, is obtained. 

\begin{Exercise} {\em A bivariate numeration system} -- How would you define the digits ($a_k, b_k)$ of a seed $(X_0,Y_0)=(x_0,y_0)$ in the following bivariate dynamical system, and reconstruct $(x_0,y_0)$ based on its digits?
$$X_{k+1}=\frac{1}{Y_k}-\Bigg\lfloor\frac{1}{Y_k}\Bigg\rfloor,\\
Y_{k+1}=\frac{1}{X_k}-\Bigg\lfloor\frac{1}{X_k}\Bigg\rfloor.$$
% \vspace{1ex}

\noindent {\bf Solution} \\
The orbit $(Y_k,Y_k)$ is dense on the unit square for almost all initial conditions $(X_0,Y_0)$. Of cause  the marginals of the invariant joint distribution are just the invariant distributions of the 1D case for such a basic system. Yet it is not obvious to obtain the full invariant joint distribution and understand its intricacies. In short, $F(x,y)\neq F(x)F(y)$. 

The digits of $(x_0,y_0)$ are $(a_k,b_k)$ for $k=0,1$ and so on, defined by $a_k=\lfloor 1/Y_k\rfloor$ and $b_k=\lfloor 1/X_k\rfloor$. If you know the digits, you can reconstruct $(x_0,y_0)$ as follows. Start with $N$ large enough, say $N=20$ if you want about 20 digits of accuracy. Set $(u_N,v_N)=(0,0)$. Proceed backwards as follows, from $k=N-1$ down to $k=0$:

$$u_k = \frac{1}{v_{k+1}+b_k} - \Big\lfloor \frac{1}{v_{k+1}+b_k} \Big\rfloor,\quad
v_k = \frac{1}{u_{k+1}+a_k} - \Big\lfloor \frac{1}{u_{k+1}+a_k} \Big\rfloor.
$$
If $N=\infty$ then $(u_0,v_0)=(x_0,y_0)$. Another bi-dimensional numeration system (an extension of the standard base-$b$ numeration system) is discussed in section~\ref{vcxzaq}. 
\end{Exercise}

\begin{Exercise} \label{exensd}{\em Numerical instability of chaotic dynamical systems} --- As discussed in section~\ref{thorium}, the computation
 of $X_k$ using the iterative map $X_{k+1}=g(X_k)$ quickly results in total loss of accuracy after as little as 45 iterations, as rounding errors propagate exponentially fast. Indeed, the problem is similar to computing the first $1000$ binary digits of $\sqrt{2}$ if your machine precision is limited to 32 bits. How would you test the accuracy of your computations, especially when working with very high (yet finite) precision? 
\vspace{1ex}

\noindent {\bf Solution} \\
One way assess \textcolor{index}{numerical instability}\index{numerical stability} is to pretend -- by truncating and rounding the values of $X_k$ -- that your machine has a precision of only 3 digits. Then do the same, pretending the precision is 6 digits, then 12 digits. Comparing the results obtained with 3-digit versus 6-digit or 12-digit precision, you can tell how far (that is, how many iterations) you can go to preserve the desired accuracy in the sequence $\{X_k\}$. The idea is to incrementally increase the precision until you see no more improvements. This is the actual limit of your machine or library function that you use. In my case, I found in some older system that increasing the precision from 180 to 200 or 300 digits had no effect. This meant that no matter what is advertised in the documentation (or possibly due to some glitches of my own), the accuracy of my results was limited to 180 digits.
\end{Exercise}

\begin{Exercise} {\em Khinchin's constant} -- Let $a_1,a_2$ and so on be the coefficients or digits of the continued fraction expansion of any  
good seed $x_0\in [0,1]$. How do you determine the value of $(a_1 a_2\cdots a_k)^{1/k}$ as $k\rightarrow\infty$?
 \vspace{1ex}

\noindent {\bf Solution} \\
The existence and determination of \textcolor{index}{Khinchin's constant}\index{Khinchin's constant} [\href{https://en.wikipedia.org/wiki/Khinchin\%27s_constant}{Wiki}] is a well-know and fascinating problem. It is a painfully slow process to obtain an empirical estimation and assess its accuracy, but
 the theory helps solve this problem. Here $P(a_k = n)$ is easy to obtain from the invariant distribution, see~(\ref{tatar}). Let $K_0$ be the constant in question. We have:
$$\log K_0 = \lim_{k\rightarrow\infty} \frac{1}{k}\sum_{i=1}^k \text{E}\Big[\log a_i\Big] =\lim_{k\rightarrow\infty} 
\text{E}\Big[\log a_k\Big] = 
\sum_{n=1}^\infty (\log n) P(a_k=n).$$
From there, it is easy to obtain
$$K_0 = \prod_{n=1}^\infty \Bigg(1+\frac{1}{n(n+2)}\Bigg)^{\log_2 n} \approx 2.6854520010.$$

\noindent The values of $\log a_k = \log \lfloor p(X_k)\rfloor$ for $k$ between 1 and $\num{20000}$ are plotted in Figure~\ref{fig:picf},
 for $X_0 =\pi$. You can download these values from the
``Online Encyclopedia of Integer Sequences" (OEIS), \href{https://oeis.org/A001203}{here}.  The first few ones are shown at the very beginning of section~\ref{oifvcd}.
\end{Exercise}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{picf.png}  
\caption{First \num{20000} coefficients (their logarithm) in the continued fraction of $\pi$}
\label{fig:picf}
\end{figure}

\begin{Exercise} {\em Digits of generalized Gauss map} -- Let $X_{k+1} = 1/X_k^\alpha  -\lfloor 1/X_k^\alpha\rfloor$, and
$a_k = \lfloor 1/X_k^\alpha\rfloor$, with $\alpha>0$. When $\alpha=1$ (the Gauss map), $\text{E}[a_k]=\infty$. For which values
 of $\alpha$ is the expectation finite?
\end{Exercise}

\subsection{Random numbers based on a bivariate numeration system}\label{vcxzaq}

The following sequence is rather interesting. The initial condition is a \textcolor{index}{bivariate seed}\index{seed (dynamical systems)!bivariate} $(X_0, X_1)$ and $(b_0,b_1)$ is a vector of integer numbers, positive or negative, called \textcolor{index}{bivariate base}\index{base (numeration systems)!bivariate}. The iterated map is defined as follows:
$$
X_{k+2} = b_1 X_{k+1} + b_0 X_k - \lfloor b_1 X_{k+1} + b_0 X_k \rfloor, \quad k=0,1,\dots
$$
Here $X_0\in [0, 1]$ is a constant, and $X_1$ is uniformly distributed on $[0, 1]$. So in all cases $X_k\in [0, 1]$. If $b_0=0$, particular cases include the ten-fold map ($b_1=10$) and the dyadic map ($b_1=2$) discussed earlier. Non-integer and \textcolor{index}{irrational bases}\index{base (numeration systems)!irrational} are discussed later in this book.  Also, if $b_0=0$, then $a_k = \lfloor b_2 X_k\rfloor$ is the $k$-th digit of $X_1$ in base $b_1$. 

In addition, if $b_1=0$, the lag-$m$ autocorrelation
 in the sequence $\{X_k\}$ is $b_1^{-m}$ for $m=1,2$ and so on. Furthermore,
 all vectors $(X_k,X_{k+1})$ lie in a small, finite number of parallel lines. Thus, unless $b_1$ is large in absolute value, this sequence is not a good candidate to generate random numbers on $[0,1]$. The goal here is to show that with two bases $b_0,b_1$ (instead of one), we can reduce the 
 interdependencies between successive $X_k$'s, of course without completely eliminating them. We can do even better with 3 or more bases, but this is not discussed here.  See \href{https://math.stackexchange.com/questions/4308185/period-or-lack-of-for-a-probably-new-pseudo-random-number-generator}{this post} for an example with deeper recursions.

\subsubsection{Properties of the bivariate numeration system}\label{ssda}

Before exploring how to increase randomness in the sequence $\{X_k\}$, I want to share a few properties. Let $b_0,b_1 >0$
and $X_0, X_1\in [0,1]$. Define $d_k = b_0 X_k + b_1 X_{k+1}-X_{k+2}$ as the $k$-th digit of $(X_0,X_1)$ in base $(b_0, b_1)$. 
The main properties are:\vspace{1ex}
\begin{itemize}
\item Two different vectors
 $(X_0, X_1)$ and $(X'_0,X'_1)$ can not have the same digits in base $(b_0,b_1)$ if $b_0>b_1 + 1$ and $b_0,b_1>0$. See 
proof \href{https://mathoverflow.net/questions/372712/hybrid-numeration-system-on-0-12}{here}.

\item Assume $b_0>b_1 + 1$ and $b_0,b_1>0$. Let $b=b_0+b_1$. Then to each $(X_0, X_1)\in [0,1]\times [0,1]$ corresponds a unique number
$h(X_0,X_1)$ defined by its expansion in base $b$ as follows:
$$
h(X_0, X_1)=\sum_{k=0}^\infty \frac{d_k}{b^k} \in [0, b].
$$
This creates an order on $[0, 1]\times [0, 1]$: $(X_0, X_1) < (X'_0,X'_1)$ if and only if
$h(X_0, X_1) < h(X'_0,X'_1)$.
\item Unlike in univariate base systems, the distribution of the digits of a random number in base $(b_0,b_1)$ may not be uniformly distributed. 
For instance, if $b_0=b_1=3$ then the digits 0 and 5 of a random number $(X_0,X_1)$ 
appear with a frequency of $1/18$, the digits 1 and 4 with a frequency of $3/18$,
 and the digits 2 and 3 with a frequency of $5/18$.

\item Another interesting property, assuming $X_1\neq 0$, is the following: $X_k = A_k X_1 - \lfloor A_k X_1\rfloor$ with $A_0=X_0/X_1$, $A_1=1$, and
$A_{k+2} = b_1 A_{k+1} + b_0 A_k$. This result is based on empirical evidence. The proof or disproof is left to the reader.

\item Finally, all triplets $(X_k, X_{k+1},X_{k+2})$ lie on a finite number of parallel planes. The parametric equation for  this family of planes is 
$b_0 x + b_1 y-z = d$, where the parameter $d$ is an integer. This parameter can only take a few different values, depending on $b_0$ and $b_1$. The values in question correspond to the potential values that the digits of any arbitrary $(X_0,X_1)$ can take.  
 \end{itemize}\vspace{1ex}

\noindent It is interesting to note that the system described here with second-order recurrence can be re-written as a 
\textcolor{index}{bivariate dynamical system}\index{dynamical systems!bivariate} with first-order recurrence, as follows. In this case, the seed is $(X_0,Y_0)$. The technique used here for the transformation works with all recurrences of any order: the reduction to first order is achieved by increasing the dimension of the system.

\begin{equation}
    \begin{cases}
      X_{k+1} = Y_k, \nonumber\\[3pt]
     Y_{k+1} = b_1Y_k + b_0X_k - \lfloor b_1Y_k + b_0X_k \rfloor.\nonumber
%      \left\lvert\frac{k_{p\omega}s+k_{i\omega}}{s}\cdot\frac{1}{Ts+1}\right\rvert_{S=\mathrm{j}\cdot2\pi}=1
    \end{cases} %\,.
\end{equation}

\subsubsection{Increasing point spread and randomness}\label{oidfjb6}

As discussed in section~\ref{ssda}, given a base $(b_0, b_1)$, each triplet $(X_k,X_{k+1},X_{k+2})$ lies in a family of parallel planes. Thus, these triplets are not randomly scattered in the entire 3-dimensional unit cube, regardless of the seed. Indeed the number of parallel planes can be quite small. This means that we are far from a uniform distribution on the unit cube. Using very large integers (in absolute value)  for $b_0$ and $b_1$ significantly increases the number of parallel planes in the family, and thus the randomness. 

The same problem exists 
 with 
\textcolor{index}{congruential random number generators}\index{pseudo-random numbers!congruential PRGN} (PRNG) [\href{https://en.wikipedia.org/wiki/Linear_congruential_generator}{Wiki}], though usually involving more than 3 dimensions and possibly more hyperplanes (thus more randomness) depending on the PRNG and its seed.  Unlike PRNG sequences though, the sequence $\{X_k\}$ is typically non-periodic. Better random number generators are discussed in my book on synthetic data~\cite{vgsynthetic}, in particular a very fast PRNG involving digits of millions of quadratic irrational numbers.

Now, let's get back to the sequence $\{X_k\}$ and assume that $b_0, b_1>0$. There is a simple technique to dramatically boost the number of planes, and thus the randomness: work with the sequence $\{Y_k\}$ instead of $\{X_k\}$, where
 $Y_k = X_{3k}$. For the sequence $\{Y_{k}\}$, the number of potential parallel planes where any $(Y_k,Y_{k+1},Y_{k+2})$ can be found, is $M = b_0^3 + 3b_0 b_1 + b_1^3$. These planes satisfy the equation
 $b_0^3 x +b_1(b_1^2 +3b_0) y - z =d$, where the parameter $d$ can take on $M$ distinct values: $d = 0, 1,\dots,M-1$.  The first step to prove this result is to realize that due to the recurrence relation defining $X_k$ and consequently $Y_k$, we always have
$$
b_0^3 Y_k +b_1(b_1^2 +3b_0) Y_{k+1} - Y_{k+2} =d,
$$ 
where $d$ is an integer between $0$ and $M-1$. This fact assumes that $b_0, b_1 > 0$. Another solution is to use a deeper recurrence,
 say 
$$
X_{k+3} = b_2 X_{k+2} + b_1 X_{k+1} + b_0 X_k - \lfloor b_2 X_{k+2} + b_1 X_{k+1} + b_0 X_k \rfloor, \quad k=0,1,\dots
$$
 where $(X_0, X_1, X_2)$ is the seed, $(b_0, b_1, b_2)$ is the base, and $Y_k=X_{4k}$. Now instead of parallel planes, we have 
 parallel hyperplanes in 4 dimensions, and a much larger number of them. In short, a lot more randomness. And of course, this can be extended to higher dimensions, further boosting the randomness.

\begin{figure}%[H]
\centering
\includegraphics[width=0.85\textwidth]{diag2.png}  
\caption{Base $(-5,4)$ leads to better randomness [blue fit] than $(3,-2)$ [orange fit]}
\label{fig:diag}
\end{figure}

There is another way to look at the lack of randomness, and how to increase it. For the sequence $\{X_k\}$, not all bases are created equal. Some lead to more randomness than others. The following \textcolor{index}{Kolmogorov-Smirnov statistic}\index{Kolmogorov-Smirnov statistic} [\href{https://en.wikipedia.org/wiki/Kolmogorov\%E2\%80\%93Smirnov_test}{Wiki}] measures the distance to randomness:
\begin{equation}
\Delta = \max_{\alpha} \Delta(\alpha), \, \text{ with } \Delta(\alpha) = \Bigg|\Big(\pi(\alpha)\Big)^{1/3}-\Big(\pi_0(\alpha)\Big)^{1/3}\Bigg| \label{maxaop}
\end{equation}
where 
\begin{itemize}
\item In the 3-D version of the test, $\alpha=(\alpha_1,\alpha_2,\alpha_3) \in [0,1]^3$, 
though the test is more revealing the higher the dimension; the maximum in~(\ref{maxaop}) is over all $\alpha \in [0,1]^3$.
 In $d$ dimension, $1/3$ is replaced by $1/d$.
\item $\pi(\alpha)=\pi(\alpha_1,\alpha_2,\alpha_3)$ is the \textcolor{index}{empirical distribution}\index{empirical distribution} 
[\href{https://en.wikipedia.org/wiki/Empirical_distribution_function}{Wiki}] of $P(X_k<\alpha_1, X_{k+1}<\alpha_2,X_{k+2}<\alpha_3)$,
 computed on many $k$'s over a large number of good seeds $(X_0,X_1)$, for a large number $n$ of random triplets $(\alpha_1,\alpha_2,\alpha_3)$
 in the unit cube.
\item $\pi_0(\alpha)=\pi_0(\alpha_1,\alpha_2,\alpha_3)=\alpha_1\alpha_2\alpha_3$ is the exact distribution (CDF) if the $X_k$'s were independently 
 and uniformly distributed on $[0,1]$. Here we known that they aren't. 
\end{itemize} \vspace{1ex}
The larger $\Delta$, the less random the sequence $\{X_k\}$ is. 
The number of $k$'s is denoted as \texttt{m} in the Python code in section{pulkh}. The fractional part of a real number $x$ positive or negative, that is $x-\lfloor x\rfloor$,
 is denoted as \texttt{x\%1}  in the code. In other words, $x$ modulo 1. It is between 0 and 1. Also, despite the appearance of using only one seed in the Python code, the fact that errors grow exponentially in the iterative computation of  $X_k$ means that 
 the sequence $\{X_k\}$ is implicitly and randomly re-seeded every 45 iterations or so, for a total of $m=10^5$ iterations per base $(b_0, b_1)$.

Figure~\ref{fig:diag} produced by the code in section~\ref{pulkh} shows $\Delta(\alpha)$ computed for $n=100$ triplets $\alpha$. Thus each set (blue, orange) consists of $100$ dots. The further away the dots are
 from the main diagonal, the less random the sequence $\{X_k\}$ is on average. Orange 
corresponds to $(b_0,b_1)=(3, -2)$, blue to $(b_0,b_1)=(-5,4)$. The Python code is also on my GitHub repository, 
\href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/chaos_2D_base.py}{here}. Special cases such as
 $(b_0,b_1)=(0,2)$ don't work not because of a glitch in the code, but because of the way hardware architecture (related to arithmetic 
 operations) is designed. In this case, replace $2$ by $2\pm 2^{-30}$, and it will fix the issue even though the base no longer
 consists of integers. 

\subsubsection{Python implementation of distance to randomness}\label{pulkh}

This code described in section~\ref{oidfjb6} is also on GitHub, \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/chaos_2D_base.py}{here}. It is used to produce Figure~\ref{fig:diag}. 
 The purpose is to measure the quality of a base $(b_0,b_1)$, in terms of its ability to generate randomness in the sequence
 $\{X_k\}$, regardless of the (good) seed $(X_0, X_1)$. \vspace{1ex}


\begin{lstlisting}
# chaos_2D_base.py | compute Kolmogorov-Smirnov Delta distance in test of independence

import numpy as np
import matplotlib.pyplot as plt

def simulation(b0,b1,n,m):

    Delta    = 0.0; # Kolmogorov-Smirnov distance
    arr_pi   = [0] * n
    arr_pi_0 = [0] * n
    
    for iter in range(0, n): 
        alpha1 = np.random.uniform(0.0, 1.0)
        alpha2 = np.random.uniform(0.0, 1.0)
        alpha3 = np.random.uniform(0.0, 1.0)
        count = 0
        for k in range (2, m): 
            x[k] = (b1 * x[k-1] + b0* x[k-2]) % 1
            if x[k-2]<alpha1 and x[k-1]<alpha2 and x[k]<alpha3:
                count += 1
        pi   = (count/(m-1))**(1/3)
        pi_0 = (alpha1 * alpha2 * alpha3)**(1/3)
        arr_pi.append(pi)
        arr_pi_0.append(pi_0)
        diff = abs(pi - pi_0)
        if diff > Delta:
            Delta = diff
        OUT.write("%3d\t%3d\t%.4f\t%.4f\t%.4f\t%6.5f\t%6.5f\n" \
             % (b0,b1,alpha1,alpha2,alpha3,pi,pi_0))

    return(Delta, arr_pi, arr_pi_0) 


n = 100
m = 100000
seed = 543
np.random.seed(seed)
x = [0] * m   # initialize array of size m
x[0] = 0.5       # seed: X_0
x[1] = np.log(2) # seed: X_1

OUT=open("base2d.txt","w")
OUT.write("b0\tb1\talpha0\talpha1\talpha2\tpi\tpi_0\n")
axes = plt.axes()
[axx.set_linewidth(0.2) for axx in axes.spines.values()]
axes.tick_params(axis='both', which='major', labelsize=7)
axes.tick_params(axis='both', which='minor', labelsize=7)
axes.set_xlim(0.0, 1.0)
axes.set_ylim(0.0, 1.0)
plt.plot([0,1], [0,1], 'k-', linewidth=0.2)

b0 = -5; b1 =4
(Delta, arr_pi, arr_pi_0) = simulation(b0, b1, n, m)
print("base = (%2d, %3d) | Kolmogorov-Smirnov distance: %6.4f" % (b0, b1, Delta))
plt.scatter(arr_pi, arr_pi_0,s=1.0)

b0 = 3; b1 =-2
(Delta, arr_pi, arr_pi_0) = simulation(b0, b1, n, m)
print("base = (%2d, %3d) | Kolmogorov-Smirnov distance: %6.4f" % (b0, b1, Delta))
plt.scatter(arr_pi, arr_pi_0,s=1.0)

plt.show()
OUT.close()
\end{lstlisting}



\section{Iterative method to find the invariant distribution}

Now I discuss an iterative algorithm to solve the 
\textcolor{index}{stochastic integral equation}\index{stochastic integral equation}~(\ref{ch2eq2}) to find the invariant distribution $F$. Usually, there is no direct, exact solution. Even when there is one as in the preceding examples, the ability to retrieve it via an iterative algorithm tells you how fast your algorithm converges, and what the error looks like. Remember that here, the unknown is a function $F$ or its derivative (the density) $f$. Nevertheless, the 
\textcolor{index}{fixed-point algorithm}\index{fixed-point algorithm} [\href{https://en.wikipedia.org/wiki/Fixed-point_iteration}{Wiki}] 
still works to get a very accurate approximation of this function. It is implemented as follows. 

We start with a rough approximation $f_0$ of the invariant density. Then we build successive approximations using the fixed-point iteration
\begin{equation}
f_{n+1}(x) = \sum_{k=1}^\infty |q'(x+k)| f_n(q(x+k)).\label{poxzqa}
\end{equation}
%where $\lambda$ is typically called the \textcolor{index}{learning rate}\index{learning rate} [\href{https://en.wikipedia.org/wiki/Learning_rate}{Wiki}] in machine learning. 
The algorithm is a direct application of Formula~(\ref{ch2eq2}). As an illustration, I use it to find the invariant distribution (a good approximation) for the dynamical system
defined by $g(x)=p(x)-\lfloor p(x)\rfloor$, where $q(x)=p^{-1}(x)$ and $p(x) = \lambda / x^\alpha$. Here $\lambda,\alpha>0$
 and $\alpha\leq 1$. The
 exact solution is known at least when $\alpha=\lambda=1$. In this case, we are dealing with the Gauss map studied earlier. In all the cases,
 the domain of the invariant density is $[0, 1]$. I~start with the uniform density $f_0(x) = 1$ for $x\in [0, 1]$. The implementation is in the
Python code in section~\ref{pawqsx}.

\subsection{Results and discussion about convergence}

Figure~\ref{fig:diagcvg} shows the successive approximations $f_1$, $f_2$ and so on, converging very fast towards the exact solution: the
 invariant density $f(x) = \kappa/(1+x)$ with $\kappa = 1/\log 2$ (in red). This is true even when starting with the uniform density $f_0(x) = 1$.
 The function $f_1$ is in blue, $f_2$  in orange, $f_3$ in green, and $f_4$ is already indistinguishable from the exact solution. I used
 the first 500 terms in the right-hand side of formula~(\ref{poxzqa}), with $\num{5000}$ evenly spaced locations (interpolating nodes) on the X-axis for the
 approximations. You can do very well with much fewer terms and nodes. The Python implementation is in section~\ref{pawqsx}.



\begin{figure}[H]
\centering
\includegraphics[width=0.62\textwidth]{converg.png}  %0.77
\caption{Convergence of iterated functions towards the invariant density in red}
\label{fig:diagcvg}
\end{figure}

Later in this book, I discuss other dynamical systems where the exact solution to the functional equation is known in closed form. It leads to interesting and sometimes awkward,
 non-standard probability distributions. We already established the invariant distribution attached to the Gauss map, as well as the
 related \textcolor{index}{Gauss-Kuzmin distribution}\index{Gauss-Kuzmin distribution}\index{distribution!Gauss-Kuzmin}  attached to the digits or coefficients of continued fractions.  We will see that we also have a special, simple invariant distribution attached
 to the ``digits" of \textcolor{index}{nested radicals}\index{nested radicals} [\href{https://en.wikipedia.org/wiki/Nested_radical}{Wiki}]. We will also discover that the well-known invariant measure of the 
\textcolor{index}{logistic map}\index{logistic map}\index{map!logistic} defined by $g(x)=4x(1-x)$ is 
 a \textcolor{index}{beta distribution}\index{beta distribution}\index{distribution!beta}. Even more surprising, we will find (in closed form) the invariant measure of the square-root logistic map
 defined by $g(x) =\sqrt{4x(1-x)}$.

\subsection{Python code to numerically solve the functional equation}\label{pawqsx}

The code is also available on GitHub, \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/chaos_solveFunctional.py}{here}. I use the first \texttt{N} terms in formula~(\ref{poxzqa}), with \texttt{M} evenly spaced locations (interpolating nodes) on the X-axis for the
 approximations. The function $f_{n+1}$ is stored in the array \texttt{f\_new} with \texttt{M+1} entries, and computed based on values of
 $f_{n}$ stored in \texttt{f\_old}. These arrays are updated at each iteration. The exact solution is stored in \texttt{f\_exact}.
\vspace{1ex}

\begin{lstlisting}
# chaos_solveFunctional.py | fixed-point iteration to compute invariant density f
# f_exact corresponds to alpha = llambda = 1: f(x) = (1/log(2)) * 1/(1+x)

import numpy as np
import matplotlib.pyplot as plt

M = 5000     # granularity
N = 500      # number of terms in the sum for functional equation 
llambda = 1  # must be <= 1
alpha   = 1
beta    = 1/alpha

OUT=open("solve.txt","w")

f_old = [] 
f_new = []
f_exact = []
x = []
for k in range(M+1):
    f_old.append(1.0)  # initialize f_0 to uniform[0,1]
    f_new.append(0.0)
    f_exact.append( (1/np.log(2)) / (1+k/M) )
    x.append(k/M)      # locations on X-axis
axes = plt.axes()
[axx.set_linewidth(0.2) for axx in axes.spines.values()]
axes.tick_params(axis='both', which='major', labelsize=7)
axes.tick_params(axis='both', which='minor', labelsize=7)

for iter in range(0,10):

    print("Fixed-point iteration:",iter)
    sum=0

    for k in range(M+1): 
        # loop over equally spaced arguments of f
        f_new[k]=0
        for n in range(1,N+1):  
            # loop over the terms in functional equation 
            y=(llambda/(x[k] + n))**beta  # x[k] = k/M
            y=int(0.5 + M*y)              # must have: 0 <= y <= M
            if y <= M+1: 
                f_new[k] += f_old[y]*beta*(llambda**beta)/((x[k]+n)**(1+beta))
            else:
                print("Out of range error (ignored):",iter,k,y)
        sum += f_new[k]

    for k in range(0,M+1):
        f_new[k] = M*f_new[k]/sum  # must integrate to 1 (it's a density)
        f_old[k] = f_new[k] 
    
    for k in range(M+1): 
        OUT.write("%6d\t%6d\t%6.4f\t%6.4f\n" % (iter,k,x[k],f_new[k]))
    plt.plot(x,f_new,linewidth=0.3) 

OUT.close()

plt.plot(x,f_new,linewidth=0.3)
plt.plot(x,f_exact,color='red',linewidth=0.6)
plt.show()
\end{lstlisting}

\subsection{Curious, very accurate approximation of $\pi$}

The dynamical system discussed here, namely with $p(x)=\lambda/x^\alpha$, leads to a curious approximation of $\pi$, in just one iteration:
$$
\pi - 3 \approx (6189766)^{-1/8} 
$$
correct to 9 decimal places: the error is about $5.19\times 10^{-11}$. It is obtained with $\lambda=1$ and $\alpha=1/8$. The digits of such systems are defined in 
section~\ref{digits}, and $6189766$ turns out to be the first digit of $\pi-3$ in that system. 

By contrast, the
 approximation $\pi \approx 355/113$ is obtained using 3 digits of $\pi-3$ in the continued fraction system (the Gauss map, with
 $p(x)=1/x$). It was
already considered a pretty spectacular approximation, correct to 6 decimal places, with the error
 around $2.67\times 10^{-7}$. Its history dates back to the 5th century 
 when it was named the Milü number (see \href{https://en.wikipedia.org/wiki/Mil\%C3\%BC}{here}). Note that the Gauss map is a particular case of the more general dynamical system in question, with
 $\lambda=\alpha=1$. 


\section{Measuring the amount of chaos}

Before discussing chaos, I want to emphasize the fact that many dynamical systems have a non-chaotic version depending on the parameter.
 For instance, for the \textcolor{index}{logistic map}\index{logistic map}\index{map!logistic} $g(x)=r x(1-x)$ discussed earlier, 
  there is no chaos in the sequence $\{X_k\}$ if $r<3$. Chaos starts to kick-in at $r\geq 3$. The sequence is fully chaotic when
 $r=4$. In this book, I only consider fully chaotic sequences. 

In the logistic map, when $r\geq 3$, the sequence exhibits a number of \textcolor{index}{bifurcations}\index{bifurcation} [\href{https://en.wikipedia.org/wiki/Bifurcation_theory}{Wiki}]:
 2 when $r=r_1=3$, then 4 starting at $r=r_2 \approx 3.449$, then 8 starting at $r=r_3 \approx 3.544$, 
 then 16 starting at $r =r_4 \approx  3.564$ and so on, up
 to an infinite number at $r=4$. Successive ratios $(r_{n-1}-r_{n-2})/(r_n - r_{n-1})$ tend to some constant when $n\rightarrow \infty$.
 That constant, also found in many other dynamical systems, is the universal constant of chaos. It is known
 as \textcolor{index}{Feigenbaum's constant}\index{Feigenbaum's constant} [\href{https://en.wikipedia.org/wiki/Feigenbaum_constants}{Wiki}], and its value is approximately $4.669$.

\subsection{Hurst exponent and related metrics}

In the context of \textcolor{index}{Brownian motions}\index{Brownian motion}, 
 \textcolor{index}{random walks}\index{random walk} and related chaotic processes discussed in chapter~\ref{ch1}, chaos is
 often measured using some kind of moving average. Here I illustrate some of these metrics applied to different types of stochastic processes  
 ranging from very smooth (barely chaotic) to highly unpredictable (very chaotic). These processes are built using the following method: 
\vspace{1ex}
\begin{itemize}
\item[]Step 1: Use an autocorrelated \textcolor{index}{stationary process}\index{stationarity} $\{X_k\}$, with $k=0,1$ and so on.
\item[]Step 2: The standardized $\{X_k\}$ is $\{Y_k\}$ with mean and variance equal to 0 and 1 respectively. 
\item[]Step 3: $\{Z_k\}$ is the final process with $Z_k = (Y_0 + \cdots + Y_{k})/\sqrt{k+1}$.
\end{itemize}\vspace{1ex}
In practice, we might be more interested in time-continuous processes than in the discrete version. So a time series featuring $\{Z_k\}$ 
for $k=0,1$ and so on up to $k=N$ may be re-scaled (say) with $t=k/750$ so that it looks time-continuous with the time now ranging from $t=0.00$ to $t=N/750$ (with time increments of $1/750$). This technique is described in section~\ref{bmrwp}. 

Now I provide 4 examples of such processes, each featuring a class of patterns in terms of chaotic behavior. Details about the construction is found in
 my spreadsheet \texttt{Chaos\_BrownianFractional.xlsx} available on GitHub, \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/chaos_BrownianFractional.xlsx}{here}. The spreadsheet illustrates the construction 
 of $\{X_k\}, \{Y_k\}, \{Z_k\}$ for series A, as well as the computation of smoothness metrics.
 I used it to produce Figure~\ref{fig:hurst}. In the spreadsheet, $N = \num{22212}$.\vspace{1ex}

\begin{itemize}
\item[] Series A: $X_{k+1} = b X_k - \lfloor bX_k \rfloor$ with $X_0 = \log 2$ and $b = (1 + \sqrt{5})/2$.
\item[] Series B: $X_k$ is either $0$ or $1$ with probability $\frac{1}{2}$. The $X_k$'s are independent.
\item[] Series C: $X_{k+1} = b + X_k - \lfloor b + X_k \rfloor$, with $X_0 = \log 2$, $b = (1+\sqrt{5})/2$.
\item[] Series D: for $\{X_k\}$, use $\{Z_k\}$ from series C.
\end{itemize} \vspace{1ex}
 Interestingly, in series A,
 $\lfloor b X_k \rfloor$ is the $k$-th digit of $X_0$ in the irrational 
\textcolor{index}{Golden ratio base}\index{golden ratio base}\index{base (numeration systems)!golden ratio base} [\href{https://en.wikipedia.org/wiki/Golden_ratio_base}{Wiki}], discussed later in this book.  The underlying 
 weakly autocorrelated dynamical system $\{X_k\}$ is in the same family as the
 \textcolor{index}{dyadic map}\index{dyadic map} corresponding to $b=2$, and related to the binary numeration system. Of course in series B, $\{X_k\}$ is not autocorrelated. Series C and D have long-range autocorrelations.

The operation that consists of transforming $\{X_k\}$ into $\{Z_k\}$ is known as integration. The resulting sequence $\{Z_k\}$ is smoother (less chaotic) than
 $\{Y_k\}$. The inverse operation is known as differentiation: it increases chaos. Note that $\{Z_k\}$ is a fully variance-normalized Brownian motion if
 you start with a white noise for $\{X_k\}$. So it is not really a Brownian motion, as oscillations stay within a standard range instead of growing in amplitude over time. It is related to 
\textcolor{index}{fractional Brownian motions}\index{fractional Brownian motion}\index{Brownian motion!fractional} 
[\href{https://en.wikipedia.org/wiki/Fractional_Brownian_motion}{Wiki}] when the original
 sequence $\{X_k\}$ exhibits long-range autocorrelations. 

Finally, Series A an B result in the same category of processes. 
Despite the fact that $\{X_k\}$ is autocorrelated in series A and not in series B, that autocorrelations are weak enough as to have no impact on $\{Z_k\}$ when turned into a time-continuous process. So I did not include series B in my illustrations. To the contrary,
 autocorrelations in series C and D are strong and long-range, and have a noticeable impact on the time-continuous version of
 $\{Z_k\}$.

\begin{figure}%[H]
\centering
\includegraphics[width=0.72\textwidth]{hurstACD.png}  %0.77
\caption{Series A, C, D (left) with corresponding chaos measurements (right)}
\label{fig:hurst}
\end{figure}

\subsubsection{Detrending moving averages and spreadsheet illustration}

The traditional metric to measure the smoothness of $\{Z_k\}$ is the 
\textcolor{index}{detrending moving average}\index{detrending moving average}, and it is abbreviated as DMA. It is the mean squared error between your observations and its various moving averages  of order $m = 1, 2, 3$, and so on. 
The exact definition can be found in ``Statistical test for fractional Brownian motion based on detrending moving 
average algorithm"~\cite{grz2018} and in many other articles. Other criteria are also used, such as FA and DFA. A comparison of these metrics can be found in ``Comparing the performance of FA, DFA and DMA using different synthetic long-range correlated time series"~\cite{ying2018}. In my spreadsheet, I use DMA, along with autocorrelations of various lags. The better notation DMA($m$)  emphasizes the fact that it depends on $m$. Now we have this well-known result:
$$
\text{DMA}(m) \sim C(h) \cdot m^{2H},
$$
where $C(\cdot)$ is a bounded function. 

This is an asymptotic result, meaning that it becomes more accurate as $m$ grows to infinity. 
The constant $H$ is known as the \textcolor{index}{Hurst exponent}\index{Hurst exponent} [\href{https://en.wikipedia.org/wiki/Hurst_exponent}{Wiki}]. It takes on values between 0 and 1, with $H = \frac{1}{2}$ corresponding to the Brownian motion. Higher values correspond to smoother time series, and lower ones to more chaotic data. In the spreadsheet, since I represent time-continuous
 processes where $750$ increments of discrete time correspond to one increment or unit of continuous time (say a second), I use large values 
$m=100,200,\dots, 500$. 

Also, in order to work with stabilized variances and avoid the volatility present at the beginning of $\{Z_k\}$ (due to the fact that we start at 0), I study the time series and perform all the computations assuming we start at time $t=4.00$, that is,
 after skipping the first $4\times 750$ observations in discrete time. This is noticeable in Figure~\ref{fig:hurst}, where the X-axis
 on the left plots represents the time, and starts at $t=4.00$.   

The main takeaway from my spreadsheet and analysis, summarized in Figure~\ref{fig:hurst}, is the fact that the curvature
 of the function $\text{DMA}(m)$ is a good indicator of the amount of chaos present in $\{Z_k\}$. If you look at the right plots in Figure~\ref{fig:hurst}, the curve with the yellow dots represent $\text{DMA}(m)$, and the X-axis represents $m$.




\begin{Exercise} {\em A very smooth time series} -- Create series E as follows: use $\{Z_k\}$ obtained in series D as your starting point $\{X_k\}$ to produce a new $\{Z_k\}$, smoother than that  in series D. \vspace{1ex} 

\noindent {\bf Solution} \\
Using my Excel spreadsheet, this can be done with one easy step: copy column $\{Z_k\}$ (the values only) onto column $\{X_k\}$ and all the summary
 statistics, the new $\{Z_k\}$, and the plot will automatically be updated. See solution in Figure~\ref{fig:hurst2}. Note that as the smoothness
 increases (or in other words, chaos and entropy is reduced), the curvature of  the curve with yellow dots on the right-hand side in Figure~\ref{fig:hurst} and~\ref{fig:hurst2} is also increasing. Other examples are provided in section~\ref{movbc}: see Figure~\ref{fig:lollog1xx}.
\end{Exercise}

\begin{figure}[H]
\centering
\includegraphics[width=0.72\textwidth]{hurstE.png}  %0.77
\caption{Series E (left) with corresponding chaos measurements (right)}
\label{fig:hurst2}
\end{figure}

\subsection{Lyapunov exponent and related metrics}\label{fredur}

The \textcolor{index}{Lyapunov exponent}\index{Lyapunov exponent} [\href{https://en.wikipedia.org/wiki/Lyapunov_exponent}{Wiki}] is used to detect a different feature of chaos: sensitivity to initial conditions, or the fact that
  two very close seeds $X_0$ and $X'_0$ lead to totally different trajectories $\{X_k\}$ and $\{X'_k\}$ over time, typically after
 very few iterations. For instance, the \textcolor{index}{ten-fold map}\index{ten-fold map} $g(x)=10 x - \lfloor 10 x\rfloor$ leads to the digits of $X_0$ in base 10: the $k$-th digit is $\lfloor 10 X_k\rfloor$. If instead of (say) $X_0=\log 2$, you start with $X'_0 = X_0 + \pi \times 10^{-300}$, after $k=300$ iterations, the digits become totally unrelated and the two sequences completely separate.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{rh.png}  %0.77
\caption{Non-periodic orbit with fractal dimension and basin of repulsion}
\label{fig:rheye}
\end{figure}

Another metric is the \textcolor{index}{approximate entropy}\index{entropy!approximate entropy} [\href{https://en.wikipedia.org/wiki/Approximate_entropy}{Wiki}].
It is used to quantify regularity and predictability in time series fluctuations. Applications include medical data, finance, physiology, human factors engineering, and climate sciences. 
It should not be confused with \textcolor{index}{entropy}\index{entropy} [\href{https://en.wikipedia.org/wiki/Entropy}{Wiki}], which measures the amount of information attached to a specific probability distribution -- with the uniform distribution on $[0, 1]$ achieving maximum entropy among all continuous distributions on $[0, 1]$, and the normal distribution achieving maximum entropy among all continuous distributions defined on the real line, given a specific variance. 



You can also use the \textcolor{index}{autocorrelation function}\index{autocorrelation function} of $\{X_k\}$ [\href{https://en.wikipedia.org/wiki/Autocorrelation}{Wiki}] as an indicator of chaos. Autocorrelations that are very weak (close to zero) or
 decaying extremely fast are associated with increased chaos. To the contrary, long-range, strong autocorrelations are associated with higher predictability and less chaos, as illustrated in Figure~\ref{fig:hurst2} and the bottom part of Figure~\ref{fig:hurst}. 
For instance, in the ten-fold map just discussed, the lag-$m$ autocorrelation is $10^{-m}$ for $m=1,2$ and so on. It is decaying exponentially fast as $m$ increases, making this system quite chaotic.   Methods based on the autocorrelation structure fit in the field of \textcolor{index}{spectral theory}\index{spectral theory} \cite{spectral2020}.

Finally, the \textcolor{index}{fractal dimension}\index{fractal dimension} [\href{https://en.wikipedia.org/wiki/Fractal_dimension}{Wiki}] is also used, especially to characterize chaos in the 
\textcolor{index}{orbit}\index{orbit (dynamical systems)} of 2D dynamical systems. In a 2D system,
 the time-continuous path or orbit $(X_t, Y_t)$ may densely fill an entire domain. This domain is sometimes called the 
\textcolor{index}{basin of attraction}\index{basin of attraction}, and zones that are never visited are referred to as
 \textcolor{index}{repulsion basins}\index{basin of repulsion}. The cover of this book feature such basins.  Figure~\ref{fig:rheye} illustrates
 the concept, using some orbit of the \textcolor{index}{Riemann zeta function}\index{Riemann zeta function} $\zeta(\sigma,t)$. Depending on the parameter $\sigma$, some hole around the origin is never visited (when $\zeta$ has no zero), and the area being visited may extend to the entire complex plane. More about this in my book on stochastic processes~\cite{vgsimulnew}. 
 The fractal dimension of the orbit, even though it is a one-dimensional curve, is a real number between 1 and 2. It generalizes to higher dimensions.

\section{The pillow map: a fascinating bivariate system} 

This section is a brief introduction to 2D discrete chaotic dynamical systems. I focus exclusively on the 
\textcolor{index}{sine map}\index{sine map}\index{map!sine}, which I renamed the pillow map for reasons that will soon become obvious.
 In one dimension, it is defined by $g(x)=-\rho x + \lambda \sin x$. In two dimensions, by 
 $g(x,y)=(-\rho x + \lambda \sin y, -\rho y + \lambda \sin x)$, or equivalently:

\begin{equation}
    \begin{cases} 
      X_{k+1} = -\rho X_k + \lambda \sin Y_k, \\[3pt]
     Y_{k+1} = -\rho Y_k + \lambda \sin X_k. \label{gfda}
%      \left\lvert\frac{k_{p\omega}s+k_{i\omega}}{s}\cdot\frac{1}{Ts+1}\right\rvert_{S=\mathrm{j}\cdot2\pi}=1
    \end{cases} %\,.
\end{equation}
It is governed by two parameters $\rho,\lambda$. Before exploring the 2D version, I discuss results related to the one-dimensional case.

\subsection{The one-dimensional version: Arnold's tongues}

Here, let $\rho=1$. The one-dimensional case is related to 
\textcolor{index}{Arnold's tongues}\index{Arnold's tongues}\index{map!Arnold's tongues} [\href{https://en.wikipedia.org/wiki/Arnold_tongue}{Wiki}] and the \textcolor{index}{circle map}\index{circle map}\index{map!circle}.
You would expect the corresponding sequence $\{X_k\}$ to depend on $X_0$
 and to exhibit a chaotic, Brownian-type behavior, and indeed it does that pretty much all the time.
However, if $\lambda=8$ (also true if $\lambda$  is very close to 8), we have $X_k \sim \pm 2k\pi$ as $k\rightarrow\infty$.
The sign depends on the initial value $X_0$, and the symbol $\sim$ means asymptotically equal.
Assuming $X_0=2$ and $\lambda=8$, as $k\rightarrow\infty$ we have 
\begin{align}
X_{2k}-X_{2k-1} & \rightarrow \alpha\approx 7.939712, \nonumber \\
X_{2k-1}-X_{2k-2} & \rightarrow \beta\approx -1.65653, \nonumber 
\end{align}
with $\alpha + \beta =2\pi$ and $\alpha$ solution of
$2\pi = \alpha + \alpha\cos\alpha -\sqrt{\lambda^2-\alpha^2}\sin\alpha.$

If $X_0$ is large (say $X_0=67$) and $1<\lambda<3$, then $X_k$ converges very rapidly so the sequence looks flat (non-chaotic). If $X_0=67,\lambda=7.99$, the sequence is chaotic. 
If $X_0=67,\lambda=8$, we have the behavior described earlier. And with 
$\lambda>8.02$ we are back to chaotic behavior. 
Now if $X_0=67,\lambda=4$, then $X_k$ stays in a flat, narrow band, constantly oscillating.


\subsection{Two-dimensional case}\label{sinere23jh}

For the 2D version, references include  
``Chaotic Synchronization and Antisynchronization in Coupled Sine Maps"~\cite{mama221} and 
``Basins and Critical Curves Generated by A Family of Two-Dimensional Sine Maps"~\cite{mavcx2}.
Here I discuss the non-chaotic case when $|\rho|<1$ and $|\lambda|$ is not too large, so that the 
 iterates in~(\ref{gfda}) converge to a limit $(x,y)$ depending on the seed, as $k\rightarrow\infty$.
 If the system~(\ref{gfda}) convergences, the limit vector must satisfy 
\begin{equation}
    \begin{cases} 
      x = -\rho x + \lambda \sin y, \\[3pt]
     y = -\rho y + \lambda \sin x. \label{gfdk}
%      \left\lvert\frac{k_{p\omega}s+k_{i\omega}}{s}\cdot\frac{1}{Ts+1}\right\rvert_{S=\mathrm{j}\cdot2\pi}=1
    \end{cases} %\,.
\end{equation}
Solutions to system~(\ref{gfdk}) must satisfy
\begin{equation}
 x=\eta\sin(\eta\sin x),\quad y=\eta\sin(\eta\sin y),  \quad\text{ with } \eta = \frac{\lambda}{1+\rho}. \label{vcvb} 
\end{equation}
Solutions also satisfy $y\sin y = x\sin x$.
There are 7 solutions to~(\ref{vcvb}) for $x$, and 7 for $y$, thus a combined total of $7\times 7 = 49$ solutions for $(x,y)$. Among 
 these 49 solutions, four of them are the limit vector of the fixed-point iteration~(\ref{gfda}), for some seed
  satisfying $-4\leq X_0,Y_0\leq 4$. Their  approximate values are
$$
(x, y)\approx \pm (1.3588, 2.6069) \text{ or }
\pm(2.6069, 1.3588).
$$
These four solutions, also called \textcolor{index}{fixed points}\index{fixed-point algorithm} or \textcolor{index}{attractors}\index{attactor} [\href{https://en.wikipedia.org/wiki/Attractor}{Wiki}], 
 lead to four \textcolor{index}{bifurcations}\index{bifurcation} (determined by the seed) in the orbit of the system. 
In Figure~\ref{fig:rheypll}, all seeds $(X_0,Y_0)$ in $ [-4,4]\times [-4, 4]$ leading to a same fixed-point are painted in the same color.
More details are provided in section~\ref{pytgr}. Figure~\ref{gholdust} shows the basins with
 a different set of parameters: $\lambda=0.5,\rho=-1$.


\begin{figure}%[H]
\centering
\includegraphics[width=0.75\textwidth]{pillow.png}  %0.77
\caption{Basins of attraction of the 2D sine map ($\lambda=2,\rho=-0.25$)}
\label{fig:rheypll}
\end{figure}

\subsection{Python code to generate the basins of attraction}\label{pytgr}

The Python code is also on GitHub, \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/basin.py}{here}.
The implementation is based on $\lambda=2$ and $\rho=-0.25$. Different parameters may result in more basins: in that case, you need to
add enough colors to the \texttt{color} table, so that a color is assigned to each basin. When the sequence does not converge,
 the corresponding seed $(X_0,Y_0)$ is attached to a virtual basin with \texttt{basin\_ID=-1}, and
 painted in white: the color entry \texttt{(1,1,1)}. 

There are 4 large attraction basins (black, red, yellow, blue) in addition to the white area, each corresponding to a root of~(\ref{gfdk}).
 Each seed vector belongs to one of these basins, or to the white area.
 The components $x,y$ of the root attached to the basin \texttt{basin\_ID} 
are stored in \texttt{basin\_x[basin\_ID]} and \texttt{basin\_y[basin\_ID]}. 
A few seeds do not belong to any of these basins, nor to
 the white area: they constitute tiny separate ``artificial basins", so small that they are not visible in Figure~\ref{fig:rheypll}. Typically,
 they are located at the border between two basins, and caused by convergence issues. All basins (real or artificial) are listed, along with their size and color,  in the output produced
 by the Python code. The 4 real basins are easy to identify due to their large size; each one contains $25\%$ of the seeds not in the white area.
\vspace{1ex}

\begin{lstlisting}
# basins.py | find and plot basins of attraction of 2D sine map

import numpy as np
import matplotlib.pyplot as plt

llambda  = 2 
theta    = 1     
rho      = -0.25  # rho replaced by c-1 in older version   
eps      = 0.00001

n_basins = 0
basin_count = {}
basin_color = {}
basin_x     = {}
basin_y     = {}
list_x = []
list_y = []
list_color = []

seed = 102 
np.random.seed()

# first 10 colors are pre-set
color = []
color= [(0.0, 1.0, 0.0),
        (1.0, 0.0, 1.0),
        (0.0, 0.0, 0.0), 
        (1.0, 0.0, 0.0), 
        (1.0, 1.0, 1.0), 
        (1.0, 1.0, 0.0), 
        (0.0, 0.0, 1.0), 
        (0.5, 0.5, 0.0),
        (0.0, 0.0, 0.7), 
        (0.0, 0.6, 0.3)
       ] 
n_cols   = 10

OUT=open("basins.txt","w")

for X_0 in np.arange(-4, 4, 0.01): 
    print("X_0 = %5.3f" % (X_0))
    for Y_0 in np.arange(-4, 4, 0.01):
        x = X_0
        y = Y_0
        k = 0
        delta = 999999.9
        for k in range(100): 
            old_x = x
            old_y = y
            x = -rho*old_x + llambda*np.sin(theta*old_y)
            y = -rho*old_y + llambda*np.sin(theta*old_x)
            delta = max(abs(x-old_x), abs(y-old_y))
        if delta>0.2: 
            basin_ID = -1   # non-convergence zone (oscillating) 
        else:
            basin_ID = int(100 + 10*x + y + 0.5)
        if basin_ID in basin_count:
            basin_count[basin_ID] += 1 
        else:
            basin_count[basin_ID] = 1
            n_basins = n_basins + 1
            if n_basins > n_cols - 1:
                # add color to color table for the new basin
                red   = np.random.rand()
                green = np.random.rand()
                blue  = np.random.rand()
                rgb = (red, green, blue)
                color.append(rgb)
                n_cols = n_cols + 1
            basin_color[basin_ID] = color[n_basins]
            basin_x[basin_ID] = x
            basin_y[basin_ID] = x
        list_x.append(X_0)
        list_y.append(Y_0)
        list_color.append(basin_color[basin_ID])

OUT.close()

for basin_ID,count in basin_count.items():
    col = str(basin_color[basin_ID])
    x   = basin_x[basin_ID]
    y   = basin_x[basin_ID]
    print("basinID: %4d count: %6d color: %8s x: %7.4f y: %7.4f" \
         % (basin_ID,count,col,x,y))

axes = plt.axes()
[axx.set_linewidth(0.2) for axx in axes.spines.values()]
axes.set_facecolor("white")   # background color
axes.margins(x=0)
axes.margins(y=0)
axes.tick_params(axis='both', which='major', labelsize=7)
axes.tick_params(axis='both', which='minor', labelsize=7)
plt.scatter(list_x, list_y, c=list_color, s=0.1)
plt.xlim(-4, 4)
plt.ylim(-4, 4)
plt.show()
\end{lstlisting}

%---------------------------------------------

\chapter{Special Probability Distributions Associated to Chaos}

In this chapter, I cover more foundational concepts related to discrete chaotic dynamical systems. In particular, the equivalence between two systems -- for instance the logistic map and the binary digits of any number  -- as well as a simple formula to compute the autocorrelation function. Applications include random number generators and the 
 distribution of the digits of some special irrational numbers.  

In the last few years, the number of papers on the former topic 
 has exploded. The logistic map is frequently cited, and various authors show that its successive iterates are barely if at all correlated based on
 empirical measurements: a desirable property in this context. Here I prove using elementary mathematical arguments that the 
autocorrelation is actually zero, in contrast to many other dynamical systems. The simple formula actually  allows you to build other systems 
 that share this property.  

Surprisingly, new random number generators based on dynamical systems are closely linked to binary
 digits of irrational numbers as shown in section~\ref{puutrew}, even though the authors may be unaware of this fact and actually look down on irrational numbers 
 as a method to 
 synthesize randomness. The myth that these random number generators (PRNGs) are slow and easy to reverse-engineer is completely
 debunked in chapter~\ref{chapterPRNG}, where I introduce a very fast type of PRGN based on millions of digits of millions of
 quadratic irrationals, starting at arbitrary positions in the digit expansions, and having no cross- or auto-correlations.

Along the way, I investigate some special probability distributions attached to dynamical systems starting with a ``bad seed". They
 are  nowhere differentiable and thus singular. It offers new modeling tools to deal
 with chaos. One typical example involves a fractal invariant distribution and cantor sets.
Curiously enough, by using symbolic manipulations of the derivatives as if they really existed, you can compute the mean, variance and other moments which truly exist, despite the fact that the probability density function does not exist. 

Finally, in 
section~\ref{2dsinemp}, I revisit the 2D sine map and the Riemann zeta function, showcasing a few spectacular visualizations. Section~\ref{sunbvc} features a table summarizing probabilistic properties of common numeration systems. It includes the base $b$ system whether $b$ is integer or not, the logistic and square root logistic maps, continued fractions (the Gauss map) and nested
 radicals.


\section{Equivalence between logistic and dyadic map}

In section~\ref{digits}, I introduced the concept of numeration system and digits associated to a dynamical system.
For instance, the map $g(x)=bx - \lfloor bx \rfloor$ leads to the standard numeration system in base $b$, where 
 $a_k=\lfloor bX_k\rfloor$ is the $k$-th digit of the seed $X_0$ in base $b>1$.  This also works if $b$ is non-integer, even if $b$ is irrational. 
In section~\ref{cf4366}, I extended the concept of \textcolor{index}{digit}\index{digit} to a broader class of systems, including the Gauss map attached to continued fractions, and generalizations of the Gauss map.

Let $\{a_k\}$ be the digit sequence attached to the dynamical system
$\{X_k\}$ defined by $X_{k+1}=g(X_k)$. 
These digit sequences are one of the core concepts studied in this chapter. 
In general, a numeration system should satisfy the following properties: \vspace{1ex}

\begin{itemize}
\item Two different digit sequences $\{a_k\}, \{a'_k\}$ correspond to two different seeds $X_0=x_0$ and $X_0=x'_0$.
\item Two different seeds $X_0=x_0,X_0=x'_0$ have two different digit sequences.
\item All seeds have a unique digit representation in the numeration system attached to the mapping in question.
\item A digit sequence uniquely characterizes a seed.
\end{itemize}\vspace{1ex}

\noindent By ``all seeds", I mean all seeds in the definition domain $D$ attached to the mapping $g(\cdot)$. For instance, for the logistic, dyadic, tenfold, and Gauss maps, $D=[0, 1]$. A desirable property is as follows: given a digit sequence $\{a_k\}$, it is very easy to reconstruct the seed $X_0=x_0$ that it represents, using simple operations. I describe how to do it for the generalized Gauss map in section~\ref{digits}. For the numeration system in base $b$, the formula is
\begin{equation}
x_0 = \sum_{k=0}^\infty \frac{a_k}{b^{k+1}},\label{ddggf}
\end{equation}
with
$a_k = \lfloor bX_k\rfloor \in \{0, 1,\dots,  \nu_b\}$,  $X_k = b^k x_0 - \lfloor b^k x_0\rfloor$,
and  $\nu_b = b-1$ if $b$ is an integer, otherwise $\nu_b=\lfloor b\rfloor$.
Any number $x_0$ in $D$ can be written using~(\ref{ddggf}).
 This property is called \textcolor{index}{completeness}\index{completeness (numeration systems)} of the numeration system. 

Also, any number can be represented in two different ways. For instance the digit sequences $(1, 0,0,0,\dots)$
 and $(0,1,1,1,\dots)$ both represent $\frac{1}{2}$ in base $b=2$. However the latter is not legit. Formula~(\ref{ddggf}) with the conditions attached to it can only produce the former. This feature is present in all numeration systems. It is indeed the  requirement
 for the system to be complete yet without being redundant 
(\textcolor{index}{redundancy}\index{redundancy (numeration systems)} means that a number can have multiple legit digit representations). The mechanism at play is as follows: the $k$-digit must always be the largest possible integer such that the truncated sum~(\ref{ddggf}) based on the first digits $a_0,\dots,a_k$ is always the largest possible number less than or equal to $x_0$.  

Generally speaking, the construction of the successive digits is an application of the
 \textcolor{index}{greedy algorithm}\index{greedy algorithm} [\href{https://en.wikipedia.org/wiki/Greedy_algorithm}{Wiki}]. The oldest version of this algorithm, dating back to 1500 BC, was used to produce very fast approximations of real numbers with
 successive \textcolor{index}{Egyptian fractions}\index{Egyptian fractions} [\href{https://en.wikipedia.org/wiki/Egyptian_fraction}{Wiki}]. 
The same principle applies to all numeration systems discussed in this book. All of them are based on chaotic dynamical systems.

\subsection{Homomorphism between two dynamical systems}\label{hohok}

 I am particularly interested in the binary digits of the number
 $x_0=(2\pi)^{-1}\arcsin(\sqrt{y_0})$ with $y_0=1/3$.
The number $x_0$ is irrational, see \href{https://mathoverflow.net/questions/441714/does-x-0-1-3-lead-to-periodicity-in-the-logistic-map-x-k1-4x-k1-x-k/}{here}. Yet, studying the distribution of its binary digits seems hopeless. One question is whether
 on average, the proportions of 0 and 1 are identical. But there is something special to this number: thanks to the mapping between the dyadic and logistic map, it is possible to study the digits in question via the logistic map. And in the logistic map, $x_0$ is mapped onto $y_0 = 1/3$. This is the most trivial seed that leads to chaos in the logistic map, unlike $y_0=1/2$. 

Before digging into this in section~\ref{puutrew}, I explain the concept of
 homomorphism between two dynamical systems, central to solving this problem. In addition, it is also useful to determine the 
\textcolor{index}{invariant distribution}\index{invariant distribution} attached to one system if it is know for the other system. Indeed,
 I use it to obtain the invariant distribution of the logistic map.

An \textcolor{index}{homomorphism}\index{homomorphism} [\href{https://en.wikipedia.org/wiki/homomorphism}{Wiki}]  between 
 two dynamical systems, from $\{X_k\}$ to $\{Y_k\}$,  is a map $H$ such that $Y_k=H(X_k)$ for all $k$. 
 As an immediate consequence, if $\{X_k\}$ is defined by $X_{k+1}=g(X_k)$ and $H$ is invertible, then $\{Y_k\}$ is defined
 by 
$$
Y_{k+1}=h(Y_k), \, \text{ with } h(y)= H(g(H^{-1}(y))).
$$


Now, let $\{X_k\}$ be the dyadic map, and $\{Y_k\}$ be the logistic map. That is, $g(x) = 2x - \lfloor 2x \rfloor$ and
 $h(y) = 4y(1-y)$, with both functions defined on $[0, 1]$. Then the mapping is $H(x) =  \sin^2(2\pi x)$. The proof is in 
 Exercise~\ref{exlm3}. In practice, $H^{-1}$ is usually a multivalued function, and this is the case here.

\begin{Exercise}\label{exlm3} {\em Homomorphism between the dyadic and logistic map} -- Prove that  $H(x) =  \sin^2(2\pi x)$, and thus $H^{-1}(y) = (2 \pi)^{-1}\arcsin(\sqrt{y})$ may be used for the reverse operation. The mapping
 is not one-to-one though, as discussed in section~\ref{puutrew}. 
\vspace{1ex}

\noindent{\bf Solution} \\
The first step is to prove that for the logistic map, $Y_k=\sin^2(2^k\theta\pi)$ where $\theta= \pi^{-1}\arcsin\sqrt{y_0}$
and $Y_0=y_0$ is the seed. It is true for $k=0$, and if true for $k$, then 
\begin{align}
Y_{k+1} & = 4Y_{k}(1-Y_{k})  \nonumber \\
      & = 4 \sin^2(2^{k}\theta\pi)\Big(1 - \sin^2(2^{k}\theta\pi)\Big) \nonumber \\
      & =  \Big[2\sin(2^k\theta\pi)\cos(2^k\theta\pi)\Big]^2 \nonumber \\
     & = \sin^2(2^{k+1}\theta\pi). \nonumber
\end{align}
So it is also true for $k+1$. Now for the dyadic map, 
$X_k = 2^k x_0 - \lfloor 2^k x_0\rfloor$ where $X_0=x_0$ is the seed. Thus 
$$H(X_k)=\sin^2(2\pi X_k)=\sin^2\Big(2\pi\cdot  2^k x_0 -2\pi\lfloor 2^k x_0\rfloor\Big) = \sin^2(2\pi \cdot 2^k x_0)=\sin^2(2^k\theta\pi) = Y_k,
$$
assuming $\theta=2x_0$,  that is,  $x_0 = \theta/2 = (2\pi)^{-1}\arcsin\sqrt{y_0}$. This concludes the proof. \qed
\end{Exercise}


\subsection{Autocorrelation function: exact computation}\label{cxcdsojhg}
 

We are now in a position to obtain the invariant distribution of the logistic map. For the dyadic map $\{X_k\}$, it is easy to verify that the
 invariant distribution $P(X_k < x)$ is uniform on $[0,1]$. Let $F(y) = P(Y_k <y)$ be the invariant
 distribution of the logistic map $\{Y_k\}$. Based on the homomorphism established in section~\ref{hohok}, we have
\begin{equation}
F(y)  = P\Big(\sin^2(2\pi X_k) < y\Big) = 4 \cdot P\Big(X_k< \frac{1}{2\pi}\arcsin\sqrt{y}\Big)
 = \frac{2}{\pi}\arcsin\sqrt{y}, \quad 0\leq y \leq 1.\label{pawq}
\end{equation}
The factor $4$ in~(\ref{pawq}) is due to symmetries and the fact that the squared sine is not
 a one-to-one function on $[0, 1]$. In practice, it is more convenient to work with
the invariant density especially since the arcsine function has multiple definitions. This leads to
\begin{equation}
f(y)=\frac{dF(y)}{dy} = \frac{1}{\pi \sqrt{y(1-y)}}, \quad 0\leq y \leq 1.  \label{logdfs} 
\end{equation}
This is a \textcolor{index}{beta distribution}\index{beta distribution}\index{distribution!beta} [\href{https://en.wikipedia.org/wiki/Beta_distribution}{Wiki}] of mean $\frac{1}{2}$ and variance $\frac{1}{8}$.

The following result applies to all dynamical systems $Y_{k+1} = h(Y_k)$ with an invariant density $f$.  Here $h(\cdot)$ is a mapping from a domain $D$ onto itself. For instance, for the logistic map, $h(y)=4y(1-y)$ and $D=[0, 1]$, with the invariant density
 specified by~(\ref{logdfs}). Before stating the main theorem, I introduce some notations. Given a seed $Y_0=y_0$, 
I define the theoretical and empirical mean, variance, and lag-$m$ covariance  as
\begin{align}
 \mu  & =  \int_D y f(y)dy,                    & \mu_0  & =  \lim_{n\rightarrow\infty} \frac{1}{n}\sum_{k=1}^n Y_k, \nonumber \\
\sigma^2  & =  \int_D(y-\mu)^2f(y)dy,  & \sigma_0^2  & =  \lim_{n\rightarrow\infty} \frac{1}{n}\sum_{k=1}^n (Y_k -\mu_0)^2. \nonumber \\
\gamma(m) & =\int_D (y-\mu) \Big(h_m(y)-\mu\Big)f(y)dy & \gamma_0(m) & = \lim_{n\rightarrow\infty} \frac{1}{n}\sum_{k=1}^n (Y_k -\mu_0)(Y_{k+m}-\mu_0) \nonumber \nonumber
%\rho(m) & = xxxx & \rho_0(m) & = xxx \nonumber
\end{align}
where $h_0(y)=y$, $h_1(y)=h(y)$, $h_2(y)=h(h(y))$ and  so on.  The
 empirical metrics $\mu_0,\sigma^2_0$ and $\gamma_0$ depend on $x_0$. However the main theorem states that this is no longer the case
 if $y_0$ is a \textcolor{index}{good seed}\index{seed (dynamical systems)!good seed} and $n=\infty$. Of course, the standard simplifications apply, such as 
$$
\sigma^2   =  \int_D y^2f(y)dy - \mu^2, \quad\quad  \gamma(m)  =\int_D y h_m(y) f(y)dy -\mu^2.
$$
Finally, the theoretical and empirical lag-$m$ autocorrelations are  
$\rho(m)=\gamma(m)/\sigma^2$ and  $\rho_0(m)=\gamma_0(m)/\sigma_0^2$. Now I can state the main result:

\begin{theorem}\label{vgfd}
When $n=\infty$, the empirical and theoretical metrics coincide if $Y_0=y_0$ is a good seed. In particular, the lag-$m$ empirical and theoretical autocorrelations coincide: $\rho(m)=\rho_0(m)$ for all $m$, as $n\rightarrow\infty$.
\end{theorem}

Theorem~\ref{vgfd} results from the \textcolor{index}{ergodicity}\index{ergodicity} of the dynamical system $\{Y_k\}$,
 which is equivalent to having a (main) invariant density. Again, I emphasize the fact that this result is true for the vast majority of seeds, but there are infinitely many exceptions. For instance,
 in the numeration system in base $b$ with $b$ an integer, that is when $h(y)=by - \lfloor by \rfloor$, any seed $X_0=x_0$ that is a rational number is a \textcolor{index}{bad seed}\index{seed (dynamical systems)!bad seed}: the digits are eventually periodic, and thus theorem~\ref{vgfd} does not apply. Likewise, if you duplicate the digits in any good seed, say turning $x_0=0.5239\dots$ into $x'_0=0.55223399\dots$, the resulting seed is a bad seed: the observed coefficient
 $\rho_0(1)$ is different from the theoretical value $\rho(1)$. That said, the set of bad seeds has Lebesgue measure zero. So if you pick up a seed randomly, the chance that it is a bad seed is zero. To put it differently, there may be infinitely many invariant  distributions attached to a dynamical system, but there is one that dominates -- the mainstream invariant distribution -- and that's the one associated to good seeds  
 and featured in theorem~\ref{vgfd}. 

For the logistic map, we have $h_2(y)=h(h(y))=16 y(1-y)(1-2y)^2$. So with Mathematica it is easy to compute $\rho(2)$ and a fortiori $\rho(1)$. Since $\mu=\frac{1}{2}$, we obtain $\gamma(1)=\gamma(2)=0$.  See computation \href{https://mltblog.com/3EFVKQo}{here} for $\rho(2)$. Thus
the lag-1 and lag-2 autocorrelations are zero. This is confirmed based on computing $\rho_0(1)$ and $\rho_0(2)$ using any good seed, for instance $y_0=\frac{1}{3}$. Likewise, for the map attached to the base-$b$ system, if $b$ is an integer, we have
$\rho(m)=b^{-m}$. For good seeds, the actual digits are uncorrelated. 

One question is whether or not all $\gamma(m)$ are zero for $m=1,2$ and so on in the logistic map. We know so far  
 that $\gamma(1)=\gamma(2)=0$. Theorem~\ref{vgfd2} answers that question.

 \begin{theorem}\label{vgfd2}
Iterates $Y_k$ of the logistic map $Y_{k+1}=h(Y_k) = 4Y_k(1-Y_k)$ are uncorrelated if $Y_0=y_0$ is a good seed. 
In other words, $\rho(m)=\gamma(m)/\sigma^2 = 0$ for all strictly positive integers $m$.
\end{theorem}

\begin{proof} 
\quad \\
Let $f(y)$ be as in~(\ref{logdfs}). With the change of variable $y=\sin^2 2\pi z$ and $0\leq y\leq 1$, assuming $m>0$, we have
\begin{equation}
\gamma(m)+\mu^2 = \int_0^1 y h_m(y) f(y) dy = 4 \int_0^{1/4} \sin^2 (2\pi z) h_m(\sin^2 2\pi z) dz.\label{diaz}
\end{equation}
Proceeding iteratively we successively establish:
\begin{align}
h_1(\sin^2 2\pi z) & = h(\sin^2 2\pi z) = 4 \sin^2 (2\pi z) (1- \sin^2 \pi z) = \sin^2 4\pi z, \nonumber \\
h_2(\sin^2 2\pi z) & = h_1(h_1(\sin^2 2\pi z)) = h_1(\sin^2 4\pi z)= \sin^2 8\pi z, \nonumber \\
h_m(\sin^2 2\pi z) & = h(h_{m-1}(\sin^2 2\pi z)) = \sin^2(2^{m+1}\pi z). \nonumber
\end{align}
Thus with the change of variable $2\pi z=\theta$, Formula~(\ref{diaz}) becomes
$$
\gamma(m)+\mu^2 = 4 \int_0^{1/4} \sin^2 (2\pi z)\sin^2 (2^{m+1}\pi z)dz = \frac{2}{\pi}\int_0^{\pi/2}\sin^2\theta \sin^2 (2^m\theta)d\theta.
$$
It is well known that for any integer $n>1$, 
$$
\int_0^{\pi/2} \sin^2\theta \sin^2 (n \theta)d\theta =\frac{\pi}{8}.
$$
Combined together with $n=2^m$, and since $\mu=\frac{1}{2}$, we finally obtain
$\gamma(m)+\frac{1}{4} = \frac{1}{4}$ if $m>0$. Thus $\gamma(m)=0$, and $\rho(m)=\gamma(m)/\sigma^2=0$ for $m=1,2$ and so on. So all the autocorrelations vanish.
\qed
\end{proof}

In other words, if $Y_k$ has a $\text{Beta}[\frac{1}{2}, \frac{1}{2}]$ distribution, then $\text{E}[Y_k Y_{k+m}]=\text{E}[Y_k]\text{E}[Y_{k+m}]$ for all $m>0$. Another proof can be found \href{https://stats.stackexchange.com/questions/608109/autocorrelation-function-of-the-logistic-map-x-n1-4x-n1-x-n/}{here}.
The absence of autocorrelations does not mean that the $Y_k$'s are independent, assuming you start with a random seed $Y_0$.
 Indeed they are highly dependent: successive vectors $(Y_k,Y_{k+1})$ all lie on a same curve in $[0, 1]\times [0, 1]$, namely
 $y=h(x)= 4x(1-x)$. However, iterates that are far away from each other, such as $Y_k$ and $Y_{k+m}$ 
 with $m\geq 20$, are almost independent. See~\cite{cryptods2020} for an empirical study of the very small dependencies when $m$ is large
 enough. It makes the sequence $\{Y_k\}$, after proper rescaling and ignoring the first few hundred iterates, suitable for 
\textcolor{index}{random number generation}\index{pseudo-random numbers}. 

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{rescale.png}  
\caption{Logistic map: raw $(Y_k,Y_{k+20})$ on the left, versus rescaled on the right}
\label{fig:picfrsc}
\end{figure}

The rescaling in question consists of working with the transformed variable $Z_k= 2\pi^{-1}\arcsin\sqrt{Y_k}$ instead of $Y_k$. As discussed, $Y_k$ has a beta distribution specified by~(\ref{logdfs}) as $k\rightarrow\infty$. However, $Z_k$ has a uniform distribution on $[0, 1]$. This is pictured in Figure~\ref{fig:picfrsc}: both plots show near-independence in the point distribution, but the right plot (after rescaling and starting with $Y_0=\frac{1}{3}$) turns the non-uniform distribution into a uniform one. Note that here, $m=20$.
The
  \textcolor{index}{invariant joint distribution}\index{invariant distribution!joint distribution} of $(Y_k,Y_{k+1})$, 
$(Y_k, Y_{k+1},Y_{k+2})$ or $(Y_k,Y_{k+m})$ can be obtained, at least
 a close approximation based on a good seed such as $Y_0 =\frac{1}{3}$. However, the theoretical formula is of no practical interest.
See also section~\ref{oidfjb6} in the context of bivariate numeration systems, for another illustration of working with well spaced-out iterates $Y_{k},Y_{k+m},Y_{k+2m}$ and so on, to increase randomness.

 In general, the theoretical value of the 
autocorrelation function, at least the first lag,
 is usually easy to obtain for many dynamical systems, thanks to the theoretical formula for $\gamma(m)$. This in turn can be 
leveraged to find maps with zero autocorrelation, to
 build random number generators. Or to find an homomorphism to turn an autocorrelated system into one with no autocorrelation: this is what we accomplished here with the logistic map using the homomorphism with the dyadic map. 
% xxx add image before/after rescaling with lag m=20  (Z_k, Z_{k+m})


Finally, due to rounding errors spreading exponentially fast, $Y_{k+m}$ will be completely erroneous with $m>50$ if you know $Y_k$ with an accuracy of 15 decimal digits. See Exercise~\ref{exensd} where I discuss \textcolor{index}{numerical instability}\index{numerical stability}  of these systems (not just the logistic map) in more details. 

Another interesting result is the following. This is a consequence of  a theorem proved 
\href{https://stats.stackexchange.com/questions/422354/correlations-between-two-sequences-of-irrational-numbers}{here}. Again, this is based on the homomorphism between the dyadic map and the fully chaotic logistic map. 
\begin{theorem}

Given two seeds $Y_0=y_0$ and $Y_0=y'_0$ in $[0, 1]$, the two sequences 
$Y_{k+1}=4Y_k(1-Y_k)$ and $Y'_{k+1}=4Y'_k(1-Y'_k)$ corresponding respectively to $y_0$ and $y'_0$, are uncorrelated if 
$x_0=\pi^{-1}\arcsin\sqrt{y_0}$ 
and $x'_0=\pi^{-1}\arcsin\sqrt{y'_0}$ are linearly independent over the set of rational numbers.

\end{theorem}
This result can be leveraged to build \textcolor{index}{random number generators}\index{pseudo-random numbers} with a combination of various seeds, as long as their images in the dyadic map 
are linearly independent over $\mathbb{Q}$. Linear independence over $\mathbb{Q}$ means that for any $p,q\in\mathbb{Q}$, if $p x_0 + q x'_0=0$ then $p=q=0$.
Uncorrelated means the following:
$$
\lim_{n\rightarrow\infty}\frac{1}{n}\sum_{k=1}^n  \Big(Y_k-\frac{1}{2}\Big)\Big(Y'_k-\frac{1}{2}\Big) = 0.
$$


%-------------------------------------------------------------------------------------------------------------------------------------------

\subsection{Random numbers based on digits of irrational numbers}\label{puutrew}

I still use the notation $\{Y_k\}$ for the logistic map $Y_{k+1}=4Y_k(1-Y_k)$, and $\{X_k\}$ for the 
dyadic map $X_{k+1}=2X_k- \lfloor 2X_k\rfloor$. The homomorphism between the two maps
 (discussed in section~\ref{hohok}) makes it possible to link the digits of both systems. However, the mapping between the two is not one-to-one. More specifically, if $X_0 = (2\pi)^{-1}\arcsin \sqrt{Y_0}$ or $Y_0 = \sin^2(2\pi X_0)$ then $Y_k = \sin^2(2\pi X_k)$. But retrieving
 $X_k$ from $Y_k$ is not possible unless you have some information about $X_k$. 

\noindent The reverse map is as follows. Let $Z_k = (2\pi)^{-1}\arcsin\sqrt{Y_k}$, thus $0\leq Z_k\leq\frac{1}{4}$. We have four cases: \vspace{1ex}
\begin{itemize}
\item   If $X_k < \frac{1}{4}$  then $X_k=Z_k$,  % int(2x_k) = 0, int(2y_k) = 0 | let z_k = y_k
\item   If $X_k > \frac{3}{4}$  then $X_k = 1 - Z_k$,  %int(2x_k) = 1, int(2y_k) = 0 | let  z_k to 1-y_k
\item   If $\frac{1}{4} < X_k < \frac{1}{2}$ then $X_k  = \frac{1}{2} - Z_k$, %int(2x_k) =0, int(2y_k) = 0 | let z_k = 0.5 - y_k
\item   If $\frac{1}{2} < X_k < \frac{3}{4}$ then  $X_k=\frac{1}{2} + Z_k$. %  y_k  = x_k - 0.5, %int(2x_k) =1, int(2y_k) = 0 | let x_k = 0.5 + y_k
\end{itemize}\vspace{1ex}
Again, $a_k=\lfloor 2X_k\rfloor$ is the $k$-th binary digit of $X_0$. This makes the connection between random generators based on the logistic map, and binary digits of real numbers. 
Now I focus exclusively on the case where $Y_0=\frac{1}{3}$, resulting in 
$X_0=(2\pi)^{-1}\arcsin \sqrt{1/3}$ being an irrational number. These seeds
 are believed to be \textcolor{index}{good seeds}\index{seed (dynamical systems)!good seed} in their respective systems. 
 In particular, the sequence $\{Y_k\}$ has zero autocorrelation of any order, while the lag-$m$ autocorrelation in the sequence
 $\{X_k\}$ is $2^{-m}$. 
Thus the mapping $Y_k \mapsto X_k$ turns an autocorrelation-free sequence into an autocorrelated one,
 while the inverse mapping $X_k \mapsto Y_k$ (using the four cases in the bullet list) does the opposite. 
Also, $Y_0=\frac{1}{3}$ leads to all $Y_k$'s being rational numbers, even though
 the corresponding $X_k$'s are all irrational.

Starting with $Y_0=\frac{1}{3}$ leads to $Y_k = 2\cdot 4^k \cdot 3^{-2^k} Q_k$ where $Q_k$ is an integer if
 $k>0$. Now let $S_k = \sqrt{Q_k/Q_{k-1}}$. Using arguments similar to those in Exercise~\ref{tonnefer}, it is easy to see
 that $S_k$ is an integer if $k>0$. The sequence $\{S_k\}$ grows much more slowly than $\{Q_k\}$, albeit still dramatically fast. 
  The hope is that $\{S_k\}$ can shed some lights on the digit distribution of the logistic map with the seed $Y_0=\frac{1}{3}$. Any insight 
 might be valuable to check whether $\frac{1}{3}$ is a \textcolor{index}{good seed}\index{seed (dynamical systems)!good seed} or not. As of this writing, nobody knows, though it is
 conjectured to be a good seed. Obtaining a recurrence relation for $S_k$ is trivial and would help analyze the digit distribution.
  The first few values are $1, 7, 17, 5983, 28545857$. Also $S_k = 3^{2^{k-1}} 2^{-1}R_k$, where
 $R_k$ is defined in Exercise~\ref{tonnefer}.
\begin{Exercise} \label{tonnefer} {\em Transformation of the logistic map} -- Let $R_k = \sqrt{Y_{k+1}/Y_k}$. Prove that if $Y_0$ is a rational number,
  then $R_k$ is rational if $k>0$. Also prove that $R_k = |R_{k-1}^2 - 2|$ and $0\leq R_k\leq 2$.\vspace{1ex} \\
\noindent{\bf Solution} \\
\noindent We have $Y_{k+1}=4Y_k(1-Y_k) = 4Y_k(1-2Y_{k-1})^2$. Thus $R_k = 2|1-2Y_{k-1}|$ is rational if $k>0$ and
 $Y_0$ is rational (because then, all $Y_k$ are rational). And since $0\leq Y_{k-1}\leq 1$ for all $k>0$, we have $0\leq R_k\leq 2$.
Also, the relationship $Y_{k+1}=4Y_k(1-Y_k)$ leads to 
$$
\frac{1}{4}\frac{Y_{k+1}}{Y_k} - 1 = -Y_k, \quad \frac{1}{4}\frac{Y_{k}}{Y_{k-1}} - 1 = -Y_{k-1}\quad .
$$
Taking the ratio of the above expressions, one obtains 
$ (\frac{1}{4} R_k^2-1) / (\frac{1}{4} R_{k-1}^2-1) = R_{k-1}^2$.  The remaining of the proof consists of simple mathematical manipulations.\qed
\end{Exercise}




\subsection{Inverting a dynamical system: cryptography application}

Two of the arguments against using digits of irrational numbers to generate random numbers is the deterministic nature of 
 these digits, and the fact that it is time consuming to compute them. We shall see in chapter~\ref{chapterPRNG} that these two arguments
 are fallacious. Regarding the former, my technique involves a large number of irrational numbers (that is, a large number of seeds),
  and I do not start at the first digit, but rather at arbitrary positions. Congruential generators are of course
 deterministic as well. 

Taken a face value, it makes the logistic map worse than irrational numbers to generate randomness, despite its popularity.  
First, obtaining $Y_{k+1}$ requires computations equivalent to squaring $Y_k$, resulting in very large numerators and denominators if starting with a rational seed such as $\frac{1}{3}$, as seen in section~\ref{puutrew}. To the contrary, digits of irrational numbers such as
 $\sqrt{2}/2$ require working with integer numbers only, that increase in size by just one bit from one iteration to the next.

Then, if you know $Y_{k}$ in the logistic map, you can retrieve $Y_{k-1}$ in one shot with a $50\%$ chance of success, as there are only two potential values of $Y_{k-1}$ such that
 $Y_{k}=4Y_{k-1}(1-Y_{k-1})$. By trial and error, you can retrieve $Y_{k-10}$ with $2^{10}$ trials. Now, if you don't work with exact 
 arithmetic to produce the random numbers, it is a lot faster. The successive iterates will be totally wrong after 40 iterations or so (assuming 32-bit arithmetic) due
 to rounding errors growing exponentially fast, but this is the same as starting with a new seed every now and then. This also makes your system a lot more secure, but there is one big drawback: 
  your random numbers are now a lot more difficult to replicate on other machines: they depend on how your hardware and software implement arithmetic operations given a limited amount of machine precision.

\subsubsection{Systems harder to reverse-engineer}

The dyadic and the logistic maps are somewhat weak. If you know $Y_{k+1}$, you can retrieve $Y_k$ by testing 2 values. For the \textcolor{index}{nested radicals}\index{nested radicals} system, you need to test 3 values.  The base-$b$ system (a generalization of the dyadic system with $Y_{k+1}=bY_k - \lfloor bY_k\rfloor$) requires $b$ attempts maximum.  For the 
 \textcolor{index}{Gauss map}\index{Gauss map}, that number is infinite. The number in question is equal to the number of potential 
values taken by the digits. In the Gauss map, digits 1, 2 and 3 are much more frequent than the other digits. Thus despite
 infinitely many potential values for the digits, the Gauss map is only remotely stronger than the logistic, in terms of its ability to withstand inversion. The digits in the Gauss map are the coefficients in the continued fraction expansion of the seed $Y_0$. 

The logistic map is the only system with no autocorrelation of any order,  among those listed in Table~\ref{ttyttuchi}. However,
  it has strong non-linear interdependencies, just as strong as the linear ones found in the dyadic map. Also there are simple techniques
 to decorrelate the sequences in question, or to make the iterates $Y_k$ uniformly distributed (a requirement for security) if the invariant
 distribution of the underlying system is not uniform. One powerful method to substantially increase randomness and thus security 
 is to only use sampled values in the sequence, for instance $Y_3, Y_6, Y_9$ and so on. See section~\ref{oidfjb6} for an illustration. The bigger the spread, 
  the higher the randomness. 

Typically the digit sequence is a lot more random than the original sequence $\{Y_k\}$. For good seeds in the dyadic map, 
 it emulates independent Bernoulli trials each with a $50\%$ chance of success. In short, absolute, perfect randomness. 

 



\section{Probabilistic properties of numeration systems}

I covered several dynamical systems so far, including the logistic, dyadic and Gauss maps. The goal here is to introduce two new ones that lead
 to interesting numerations systems. Properties of all these systems are summarized in table~\ref{ttyttuchi}. 


\subsection{Square root logistic map}

The square root logistic map is defined by $X_{k+1}=\sqrt{4X_k(1-X_k)}$. 
The map is also from $[0, 1]$ onto itself, with the seed $X_0$ in $[0, 1]$. Unlike its sister -- the logistic map -- it has strong
 autocorrelations.  

Theorem~\ref{thm87h} establishes several properties of this map, in particular a closed-form formula for its invariant distribution $F$. Let $X$ be a random variable whose distribution $F$ is the invariant distribution of the square root logistic map. In other words,
 if $P(X_k<x) = F(x)$, then $P(X_{k+1}<x)=F(x)$, or equivalently, $F(x)=F(g(x))$. As usual, $F$ is the limit distribution
 of $X_k$ as $k\rightarrow\infty$, if you start with a \textcolor{index}{good seed}\index{seed (dynamical systems)!good seed} $X_0$.

\begin{theorem}\label{thm87h}
The invariant distribution of
 the square root logistic map  is $F(x)=P(X < x) = 1 - \sqrt{1 - x}$. Now let $Y = 1 - \sqrt{1 - X}$. Then $Y$ has a uniform distribution on [0, 1]. Finally, $P(X < 2x - x^2) = x$.
\end{theorem}
\begin{proof} 
\quad \\
Once the first result is established, the second one is an application of the 
\textcolor{index}{probability integral transform}\index{probability integral transform}
 [\href{https://en.wikipedia.org/wiki/Probability_integral_transform}{Wiki}]. The third one is left as an exercise. Here I focus on the 
 main result: $F(x)=1 - \sqrt{1 - x}$.  To prove it, I check if the 
 \textcolor{index}{functional equation}\index{functional equation} $P(X<x) = P(g(X)<x)$ is satisfied if $P(X<x)=F(x)=1 - \sqrt{1 - x}$.

\noindent First, we have $g(x)=\sqrt{4X(1-X)}<x$  if and only if $X< g_1(x)$ or $X>g_2(x)$ with 

$$g_1(x) = \frac{1-\sqrt{1-x^2}}{2}, \quad g_2(x) = \frac{1+\sqrt{1-x^2}}{2}.$$
Thus we have
\begin{align}
P(g(X)<x) & = P[X<g_1(X)] + P[X>g_2(X)] \nonumber \\
  & = P[X<g_1(X)] + 1 - P[X<g_2(X)] \nonumber \\
 & = 1 - \sqrt{1-g_1(x)} + \sqrt{1-g_2(x)} \nonumber \\
 & = 1 - \sqrt{\frac{1+\sqrt{1-x^2}}{2}} + \sqrt{\frac{1-\sqrt{1-x^2}}{2}} \nonumber
\end{align}
and the functional equation to satisfy becomes, after successive squaring and rearrangements:
\begin{align}
P(X<x) & = P(g(X)<x) \nonumber \\
 & \Leftrightarrow 1-\sqrt{1-x} = 1 - \sqrt{\frac{1+\sqrt{1-x^2}}{2}} + \sqrt{\frac{1-\sqrt{1-x^2}}{2}} \nonumber \\
 & \Leftrightarrow 1-x = \frac{1+\sqrt{1-x^2}}{2} + \frac{1-\sqrt{1-x^2}}{2}
 -2 \cdot \sqrt{\frac{1+\sqrt{1-x^2}}{2}} \cdot \sqrt{\frac{1-\sqrt{1-x^2}}{2}} \nonumber \\
& \Leftrightarrow 1-x = 1 - 2\cdot\sqrt{\frac{1-(1-x^2)}{4}} = 1-x.\nonumber
\end{align}
Both sides of the equality are confirmed to be identical, and this completes the proof. \qed
\end{proof}
Now we can compute the autocorrelation $\rho$. Let $f$ be the invariant density, that is, the derivative of the invariant distribution. We have
$\text{E}[X] = 2/3$, $\text{Var}[X] = 4/45$, $\text{E}[g(X)] = \text{E}[X]$, $\text{Var}[g(X)] = \text{Var}[X]$, and thus
$$
E[X g(X)] =\int_0^1 x g(x)f(x)dx = \int_0^1 \frac{xg(x)}{2\sqrt{1-x}}dx=  \int_0^1 x^{3/2}dx = \frac{2}{5}.
$$
Thus $\text{Covar}[X, g(X)] = \text{E}[X g(X)] - 4/9 = -2/45$, and  
$\rho=\text{Covar}[X, g(X)] / \text{Var}[X] = -1/2$. The lag-$m$ authocorrelation 
 is very well approximated by $(-1/2)^m$. Indeed this may very well be the exact value. The approximation is exact if $m=1$ or $m=2$.
 By contrast, the lag-$m$ autocorrelation of the \textcolor{index}{dyadic map}\index{dyadic map}\index{map!dyadic} is $(1/2)^m$.



\subsection{Nested radicals}\label{prasoil}

We all know how to compute the digits of a real number in base $b$ when $b$ is an integer. 
\textcolor{index}{Nested radicals}\index{nested radicals} [\href{https://en.wikipedia.org/wiki/Nested_radical}{Wiki}]  
offer an alternative. In this system, digits can only take on  3 values: 0, 1 or 2. In the traditional numeration system, the 
 digits of a random number -- a seed $X_0$ with a uniform distribution on $[0, 1]$ -- are distributed with the same frequencies. For instance, in the binary system, $50\%$ are zero, and $50\%$ are one. Also, the successive digits are not autocorrelated. 

This is no longer true with nested radicals. In this case, digits 0, 1, and 2 of random seeds have approximate frequencies of 
$46\%$, $30\%$, and $23\%$ respectively. The autocorrelation in the digit sequence (when starting with a good seed or a random one) is about 0.03, close to but different from zero. The nested radicals system is defined by $X_{k+1}=X_k^2 - \lfloor X_k^2 \rfloor + 1$,
  starting with a seed $X_0$ in $[1,2]$. Thus, it is a mapping from $[1, 2]$ onto itself. The $k$-th digit is $a_k =\lfloor X_k^2 \rfloor -1$. 
We can reconstruct the seed $X_0 = x_0$ backward with the same method as in section~\ref{bckdigt} leading to the formula
\begin{equation}
x_0 = \sqrt{a_0 + \sqrt{a_1 + \sqrt{a_2+\dots}}}.\label{09u712e}
\end{equation}
As usual, two different seeds have two different digit sequences, and any digit sequence corresponds to a seed, namely the one specified
 by~(\ref{09u712e}). Any number in $[1,2]$ has a unique digit representation. There are no gaps -- numbers that can not be represented in this way. This would be the case if we restrict ourselves to two rather than three digits. A system satisfying
 this property is said to be \textcolor{index}{complete}\index{completeness (numeration systems)}. To verify that this is the case here,
 note that for instance, when $\epsilon>0$ is very close to zero, $x_0=\sqrt{2}-\epsilon$ and $x'_0=\sqrt{2}+\epsilon$ have the following
digit sequences:\vspace{1ex} 
\begin{itemize}
\item for $x_0$, $a_0=0$ with all subsequent digits equal to 2 (the maximum value) as $\epsilon\rightarrow 0$,
\item for $x'_0$, $a_0=1$ followed by more and more subsequent digits all equal to 0 as $\epsilon\rightarrow 0$.
\end{itemize}\vspace{1ex}
 This is the same as in the decimal system, where for instance $0.23999\dots$ $=0.24$. Another obvious property is the backward recurrence $X_k=\sqrt{a_k +X_{k+1}}$.

As for the invariant distribution $F(x)$, we have this remarkable
approximation: $F(x) \approx -2 +\sqrt{5x-1}$. It leads to a very good approximation for the digit frequencies, featured in
 Table~\ref{ttyttuchi} in the row labeled $p_n$. The quantity $p_n$ is the probability for a digit $a_k$ to be equal to $n$. It corresponds to the empirical frequencies if $x_0$ is a good seed, when computed over all the infinitely many digits. Here, $n=0,1$ or $2$. The remaining of
 this discussion is to show that the approximation of $F(x)$, although excellent, is not exact. This is not obvious, since you would
 need a tremendous amount of simulations, with a very good random number generator and high accuracy everywhere, to notice that there is a tiny error.

\begin{figure}[H]
\centering
\includegraphics[width=0.60\textwidth]{error1.PNG} %0.86
\caption{Error due to using an approximation for the invariant distribution}
\label{err32}
\end{figure}

The goal is to show that if $F(x)=P(X_k<x) = -2+\sqrt{5x-1}$, then the functional equation $P(X_k<x) = P(g(X_k)<x)$ is not satisfied.
I start with the obvious fact that
$$ P\Big[g(X_k)< x\Big] = P\Big[1<X_k<\sqrt{x}\Big] + P\Big[\sqrt{2}<X_k<\sqrt{x+1}\Big] + P\Big[\sqrt{3}<X_k<\sqrt{x+2}\Big].$$
Then proving that $F(x) \neq -2+\sqrt{5x-1}$ amounts to proving that in general, if $x\in [1, 2]$,
\begin{equation}
\sqrt{5x-1} \neq -\lambda + \sqrt{5\sqrt{x}-1} + \sqrt{5\sqrt{x+1}-1} + \sqrt{5\sqrt{x+2}-1},\label{popi}
\end{equation}
where $\lambda = \sqrt{5\sqrt{2}-1} +  \sqrt{5\sqrt{3}-1}$. Take for instance $x=1.29$, then
the left side of the non-equality in~(\ref{popi}) is $2.33452\dots$, and the right side is $2.33453\dots$. Note that 
 if $x=1$ or $x=2$, both sides are equal. The difference between the left and right side (the error) when $x\in[1,2]$ is pictured 
 in Figure~\ref{err32}.

For a comprehensive reference on nested radicals, also called continued square roots by analogy to continued fractions, see the 98-page article published in 2017 by 
Dixon~\cite{sccf17}, featuring a chronology of the research on this topic. 


\subsection{Summary table for common systems}\label{sunbvc}

Here $\{x_k\}$ represents a sequence defined by $x_0$ (the \textcolor{index}{seed}\index{seed (dynamical systems)}) and $x_{k+1}=g(x_k)$. The associated \textcolor{index}{digits}\index{digit} is the sequence $\{a_k\}$,
 while $F$ is the \textcolor{index}{invariant distribution}\index{invariant distribution} attached to $\{x_k\}$. 
 The probability distribution 
 of the digits, denoted as $p_n = P(a_k=n)$, is the invariant density of the digit sequence $\{a_k\}$. 
 Thus it does not depend on $k$. Potential value for $n$ are the same as in the ``digits" row. For $F$,
 the domain is specified in the ``domain" row. The row labeled $x_0$ shows the formula 
 to reconstruct $x_0$ if you only know the digits.
The symbol $\rho$ denotes the autocorrelation.

For continued fractions, it is customary to start with $a_0$ being the integer part of $x_0$. Here the integer part is always $0$ because of the choice of the domain, and thus $a_0$ is the first non-zero element in the expansion of~$x_0$. The autocorrelation in the digit sequence does not exist due to infinite expectation. For $\{x_k\}$, the symbol $\gamma$ in the autocorrelation function is the Euler-Mascheroni constant,
 see Formula~(\ref{fritesdur}).

Missing values in the table could easily be computed. I haven't had the time to work on this yet. For instance, 
 the reconstruction of the seed $x_0$ based on its digits in the logistic map, can be done using the homomorphism with the dyadic map. 
 Autocorrelations not shown in the table can be computed using the same method as in section~\ref{cxcdsojhg} for the logistic map. Not only for the lag-1 autocorrelation, but also lag-$m$ for any~$m$. 


\begin{table}[H]
\[
\begin{array}{|l|c|c|c|c|c|c|}
\hline %\\[-12pt]%\\[-14pt]
   %\shortstack{ \Large{\textcolor{white}{L}}\\ \textcolor{white}{P} }
 & 
  \shortstack{\vspace{2ex}\quad \\ logistic } & \shortstack{\vspace{-0.5ex} square root \\ logistic} &	
  \shortstack{nested \\ radicals} 
    & \shortstack{base $b$ \\ integer} & \shortstack{base $b$ \\ non-integer} & \shortstack{continued  \\ fractions} \\
 %&  & logistic &	radicals & b integer & b not integer & fractions \\
\hline
\hline
\text{domain} & [0, 1] &  [0, 1] & [1, 2] &  [0, 1] & [0, 1] &  [0, 1] \\
\text{digits} & 0, 1 &  0, 1 & 0, 1, 2 &  0,\dots,b-1 & 0,\dots,\lfloor b\rfloor &  1,2,3,\dots \\
g(x) & 4x(1-x) & \sqrt{4x(1-x)} & x^2 -\lfloor x^2\rfloor + 1 & bx - \lfloor bx \rfloor & bx - \lfloor bx  \rfloor& x^{-1}-\lfloor x^{-1}\rfloor\\
a_k & \lfloor 2x_k\rfloor & \lfloor 2x_k\rfloor & \lfloor x_k^2 \rfloor - 1 & \lfloor bx_k\rfloor & \lfloor bx_k\rfloor & \lfloor x_k^{-1}\rfloor\\
x_0 & - & - & \sqrt{a_0+\sqrt{a_1+\cdots}} & \sum a_k b^{-k} & \sum a_k b^{-k} &
%\frac{1}{a_0+\frac{1}{a_1+\dots}} \\
% 1/(a_0+1/(a_1+\dots)) \\
%     \cfrac{1}{a_0 + \cfrac{1}{a_1 +\dots}} \\
%(a_0 +\frac{1}{a_1+\dots})^{-1}\\
\Big(a_0 + \cfrac{1}{a_1 + \dots\,}\Big)^{-1} \\[8pt]
\shortstack{$p_n$ \\ \textcolor{white}{xx}}
 &  \shortstack{$1/2$\\ \textcolor{white}{xx}} 
  &\shortstack{$p_1=\sqrt{2}/2$\\$p_0 = 1 -p_1$} 
  & \shortstack{$r_{n+2}-r_{n+1},$ \\ $r_n =\sqrt{5\sqrt{n}-1}$}
  &  \shortstack{$1/b$\\ \textcolor{white}{xx}}
  &  \shortstack{$-$ \\ \textcolor{white}{xx}}
  % & \log_2\Big[\frac{(n+1)^2}{n(n+2)}\Big]\\ 
  & \log_2\mathlarger{\frac{(n+1)^2}{n(n+2)}}\\[4pt]
 % & \shortstack{$2\log_2(n+1) - $ \textcolor{white}{xx} \\ \textcolor{white}{xx}  $\log_2[n(n+2)]$} \\[4pt]
F(x) & \text{Beta}\Big[\frac{1}{2},\frac{1}{2}\Big] 
 & 1-\sqrt{1-x} 
 &  -2 + \sqrt{5x-1}
 & x & - & \log_2(1+x)\\[4pt]
 \rho(x_k, x_{k+1}) & 0 & -1/2 & - & 1/b & - 
 &  \mathlarger{ \frac{(4-2\gamma)\log 2 - 2}{3\log 2 - 2} }\\
\rho(a_k,a_{k+1}) & 0 & -1/4 & - & 0 & - & - \\
\hline
\end{array}
\]
\caption{\label{ttyttuchi} Common numeration systems ($F$ and  $p_n$ approximated for nested radicals)}
\end{table}





\section{Special probability distributions}\label{gorexcres}

The goal here is to explore some dynamical systems in details, especially the invariant distribution and the digit sequences.
 The focus is on systems that are relatively simple and come with closed-form solutions depending on the parameters.
 I discuss another homomorphism with the dyadic map in section~\ref{scrotew}, this time one-to-one. The one previously studied was with the logistic map. In section~\ref{nodensity}, I focus on cases that lead to singular distributions, nowhere differentiable, and how to handle them.
 In the process, I introduce another version of the nested radicals system.


\subsection{A different version of the binary numeration system}\label{scrotew}

In this section, I explore another interesting chaotic system $\{Z_k\}$. 
I focus in the digit sequence only,  denoted as $\{X_k\}$. It constitutes a dynamical system in itself. 
I start with introducing 
\begin{equation}
Z = X_0 + X_0 X_1 +X_0 X_1 X_2 + \dots, \quad Z_k = X_k + X_k X_{k+1} +X_k X_{k+1} X_{k+2} + \dots,\label{digere}
\end{equation}
where the $X_k$ are iid (independently and identically distributed) random variables. Thus $Z=Z_0$ is the \textcolor{index}{seed}\index{seed (dynamical systems)}.

The connection to numeration systems is made clear later in this section and as well as in Exercise~\ref{polcjon} and~\ref{polcjonb}. For simplicity, I use the notation $X$ to represent any of the $X_k$. For convergence reasons, I also assume that $-1<\text{E}[X]< 1$ and $\text{E}[X^2]<1$.
Let $F$ be the cumulative distribution
 function (CDF). 
The following \textcolor{index}{functional equation}\index{functional equation} must be satisfied: 
$F_Z = F_{X(1+Z)}$, that is, $P(Z<z) = P[X(1+Z)<z]$. We also have
\begin{equation}
\text{E}[Z^n] =\text{E}[X^n]\cdot \text{E}[(1+Z)^n] = \frac{\text{E}[X^n]}{1-\text{E}[X^n]} 
 \cdot\sum_{j=0}^{n-1} \binom{n}{j} \text{E}[\text{Z}^j] \label{johndick} 
\end{equation}
 for $n=1,2$ and so on. Using Formula~(\ref{johndick}) recursively, one can retrieve 
$\text{E}[Z^n]$. For instance,
$$
\text{E}[Z] = \frac{\text{E}[X]}{1-\text{E}[X]}, \quad \text{Var}[Z] = \frac{\text{Var}[X]}{(1-\text{E}[X^2])(1-\text{E}[X])^2}.
$$
Likewise, if you know the distribution of $Z$, you can retrieve $\text{E}[X^n]$. For instance, 
with the notation $\mu=\text{E}[Z]$, $\sigma^2=\text{Var}[Z]$ and $\nu=\mu/(1+\mu)$, we have:
$$
\text{E}[X] = \nu, \quad \text{Var}[X] = \frac{(1-\nu^2)(1-\nu)^2 \sigma^2}{1 + (1-\nu)^2\sigma^2}.
$$
A consequence is that $\text{E}[Z]> -1/2$, so the distribution $F_Z(z) = P(Z<z)$ can not be arbitrary. Finally,
  if the distribution of $X$ is specified, than the \textcolor{index}{invariant distribution}\index{invariant distribution} 
 of $\{Z_k\}$ is simply $F_Z$. It is obtained by solving the functional equation $F_Z = F_{X(1+Z)}$, where $F_X$ is known
  and $F_Z$ is the unknown.

In sections~\ref{geoger} to~\ref{pornicsew}, I discuss different examples, with a focus on the invariant distribution $F_Z$.
 Finally, exercises in section~\ref{jeporei} complement the material and explore additional systems, along with
 more fundamental methodology and Python implementations.

\subsubsection{Geometric invariant distribution}\label{geoger}

Let us consider the case when both $X$ and $Z$ are discrete random variables, with
$p_n=P(X = n)$ and $q_n = P(Z=n)$, for $n=0,1$ and so on. We have:\vspace{1ex}

\begin{itemize}
\item $q_0 = p_0 =P(X_0=0)$,
\item $q_1 = p_1p_0 = P(X_0=1)P(X_1=0)$,
\item $q_2 = (p_1^2+p_2)p_0 =P(X_0=X_1=1, X_2=0)+P(X_0=2, X_1=0)$,
\item $q_3 = (p_1^3+2p_1p_2 +p_3)p_0$,
\item $q_4 = (p_1^4+ 3p_1^2p_2 + 2p_1p_3+p_2^2 + p_4)p_0$.
\end{itemize}\vspace{1ex}

\noindent If $X$ is a Bernoulli random variable with $P(X=0) = p_0$ and $P(X=1)=p_1 = 1-p_0$, then $q_n= p_0 p_1^n$. Thus 
 $Z$ has a \textcolor{index}{geometric distribution}\index{geometric distribution}.
The converse is also true.
To prove this, let us assume that $P(Z = n) = q_n = q_0 (1 - q_0)^n$, and that $P(X = n) = p_n$, for $n = 0, 1$, and so on. 
The equation $p_1p_0 = q_1 =  q_0(1-q_0)$ combined with $p_0 = q_0$ yields $p_1 = 1 - q_0$.  
As a result, $p_0 + p_1 = q_0 + (1 - q_0) = 1$. Thus if $n > 1$, then $P(X = n) = p_n = 0$. 
This corresponds to a Bernoulli distribution for $X$. 

\subsubsection{Gaussian-like invariant distribution}

Let us consider the case where $X$ has takes on the values $-1,-\frac{1}{2},+\frac{1}{2},+1$, each with probability $\frac{1}{4}$. 
Simulations show that the distribution of $Z$ is Gaussian-like. Let $f_Z$ be the invariant density, that is, the derivative of $F_Z$.
It satisfies $f(z) = f(-z)$ and $f(0)=f(1)=f(-1)$. Furthermore,
$$
f_Z(z)= \frac{1}{4} \Big[f_Z(-1-z) + 2f_Z(-1-2z) + 2f_Z(-1+2z) + f_Z(-1+z) \Big] .
$$
We have $\text{E}[Z]=0$ and $\text{Var}[Z]=5/3$. The moments $\text{E}[Z^n]$ satisfy
$$
\text{E}[Z^n] = \int_{-\infty}^\infty z^n f_Z(z)dz = \frac{1}{2}\Big(1+\frac{1}{2^n}\Big)\int_{-\infty}^\infty (1+z)^n f_Z(z)dz.
$$
The third moment $\text{E}[Z^3]$, easy to compute thanks to formula~(\ref{johndick}), is different from that of  
 a Gaussian distribution with zero mean and variance $5/3$. So the Gaussian is only an approximation.
Similar distributions have been analyzed by David Bailey in his book ``Experimental Mathematics in Action"~\cite{nt1}. In particular, sections 5.2 and 5.3 (pages 114--137) are very relevant to this context.  

\subsubsection{Connection to the binary numeration system}\label{pornicsew}

In section~\ref{hohok} I discussed the \textcolor{index}{homomorphism}\index{homomorphism} between the dyadic and the logistic map. 
 We saw that it is not one-to-one. Here I discuss the homorphism between the dyadic map, and the $\{Z_k\}$ map
 defined by~\ref{digere}. This time, it is one-to-one. This happens if $X_k$ is equal to either $\frac{1}{2}$ or $-\frac{1}{2}$, each value occurring with same probability~$\frac{1}{2}$. In that case, the invariant distribution $F_Z$ is uniform on $[-1, 1]$ as 
 shown in Exercise~\ref{paasweqasde}.

\noindent Let $Z_0 = z_0$ be the seed. We then have
$$
z_0 + 1 = \sum_{k=1}^\infty \frac{a_k}{2^k}, \quad  a_k\in\{0, 1\},
$$
where $\{a_k\}$ is the digit sequence of the number $z_0+ 1$ in base 2. The two inversion formulas are
$$
a_k  = \frac{1}{2}+ 2^k \cdot \prod_{j=0}^k X_j, \text{ and }
X_k  = \frac{1}{2}\cdot \frac{2a_k -1}{2a_{k-1} -1} \text{ if } k>0, \text{ with } X_0 = a_0 -\frac{1}{2}. 
$$

\subsection{Singular distributions with no density function}\label{nodensity}

I start with systems similar or identical to those in section~\ref{gorexcres}, but this time looking at what happens when working
  with \textcolor{index}{bad seeds}\index{seed (dynamical systems)!bad seed}. Usually, the invariant distribution is solution 
 to a \textcolor{index}{functional equation}\index{functional equation} as seen previously. Typically that equation has
 infinitely many solutions, with one of them -- the ``mainstream" solution -- covering all the cases (the good seeds) except a small
 set of Lebesgue measure zero (the bad seeds, sometimes called the \textcolor{index}{exception set}\index{exception set}). 

As an illustration, in the dyadic map (the binary numeration system), all seeds that are rational numbers are bad seeds: it results
 in periodic orbits. Good seeds result in a very specific, exponentially decaying autocorrelation function, incompatible with the behavior of rational numbers. There are far more exceptions than just the irrational numbers. However these exceptions are far outnumbered by good seeds. After looking at the (usually singular or non-existent) invariant distributions attached to bad seeds in the dyadic map, I focus on bad seeds in 
 the nested radical system. In all cases, the seed is a random variable this time (as in section~\ref{gorexcres}), thus leading to stochastic systems. One typical example involves a fractal invariant distribution and cantor sets.

\subsubsection{Bad seeds in the stochastic dyadic map}

Possibly the most simple example is the binary numeration system attached to the 
 \textcolor{index}{dyadic map}\index{dyadic map}. Using the same framework and notation as in section~\ref{gorexcres}, let
$$Z =\sum_{k=0}^\infty \frac{X_k}{2^{k+1}},$$ 

\noindent with  $P(X_k=1) = p, P(X_k=0) = 1-p .$
Here the $X_k$'s are independently and identically distributed Bernoulli random variables of parameter $p$. For good seeds,
 $p=\frac{1}{2}$. But what happens if $p\neq \frac{1}{2}$? For instance, if $p=\frac{3}{4}$, you end up with an invariant distribution 
 $F_Z(z) = P(Z<z)$ very similar in shape to 
 that discussed in Exercise~\ref{polcjonb} and plotted in Figure~\ref{fig:r1lkn2x}.
 It is nowhere differentiable: the corresponding density function $f_Z$ does not exist.

Now let us pretend that $f_Z$ exists, and let's compute the moments.
To see how this works, I start with the 
\textcolor{index}{functional equation}\index{functional equation}.  In section~\ref{gorexcres}, that equation
 was $F_Z=F_{X(1+Z)}$. Here it is  $F_Z=F_{(X+Z)/2}$, where $F_Z$ is the unknown. Thus we have $\text{E}[Z^n] = 2^{-n} \text{E}[(X+Z)^n]$ which leads to the following recursion formula for the moments:
$$\text{E}[Z^n]=\frac{p}{2^n-1 + p} 
\cdot\sum_{j=0}^{n} \binom{n}{j}\text{E}[Z^{n-j}].$$
This yields
$$\text{E}[Z] = p, \quad
\text{E}[Z^2] = \frac{p}{3}(1+2p), \quad
\text{E}[Z^3] = \frac{p}{7}(1+4p+2p^2),\quad
\text{E}[Z^4] = \frac{p}{105}(7+46p + 44p^2+8p^3).
$$
Simple simulations show that estimated (empirical) moments converge to the  theoretical values. In a nutshell, we could write integrals such 
 as 
$$
E[Z^4] = \int_0^1 z^4 f_Z(z) dz = \frac{p}{105}(7+46p + 44p^2+8p^3),
$$
even though $f_Z$ does not exist when $p\neq\frac{1}{2}$. Indeed, this can be extended to positive, non-integer values of $n$. In the 
 above example, $n=4$.

Interestingly, the set of (bad) seeds that have a binary expansion with the above invariant distribution together with $p\neq \frac{1}{2}$,
 has Lebesgue measure zero. However it is dense in $[0, 1]$ but full of microscopic holes. It consists of numbers that do not have the same 
 proportion of 0 and 1 in their base 2 representation. This type of sets fits in the
 same category as the \textcolor{index}{Cantor set}\index{Cantor set} [\href{https://en.wikipedia.org/wiki/Cantor_set}{Wiki}].
 Indeed, the Cantor sets consists of numbers in $[0, 1]$ that have no digit equal to 2 in their base 3 representation. That set
 also has macroscopic holes visible to the naked eye.

%[DONE] https://stats.stackexchange.com/questions/439528/parameter-estimation-when-the-likelihood-function-does-not-exist


\subsubsection{Nested radicals and fractal invariant distributions}

I now come back to the system with the functional equation  $F_{Z^2} = F_{X+Z}$, that is,
 $P(Z^2<z) = P(X+Z<z)$. This is a stochastic version of the system studied in section~\ref{prasoil}, since the seed $Z$ is now a
 random variable. It is defined by  
$$Z=\sqrt{X_0+\sqrt{X_1+\sqrt{X_2+\dots}}}$$
With the notation $p_n=P(X=n)$, let's use the following distribution for $X$ : 
\begin{equation}
p_0 = \frac{1}{2}, \quad p_1 = \frac{1}{1 + \sqrt{5}}, \quad p_2 = \frac{3 - \sqrt{5}}{4}. \label{pxz1}
\end{equation} 

The choice for the distribution is not arbitrary: it makes the system smoother and possibly easier to solve. To the contrary 
$p_0 = p_1= p_2 = \frac{1}{3}$ yields a far more chaotic system, and the 
case $p_0 = p_2 = \frac{1}{2}$, $p_1=0$ is so wild that the support domain of $Z$ has large visible gaps, making it strikingly  
similar to the \textcolor{index}{Cantor set}\index{Cantor set}.  Good seeds have yet a different distribution for the digits
 $X_k$, specified in row $p_n$ in Table~\ref{ttyttuchi} (look for the column labeled ``nested radicals").

Wherever the density $f_Z$ exists (the derivative of $F_Z$), we have:
\begin{equation}
f_Z(z)= 
\begin{cases}
 2p_0 z f_Z(z^2), & 1<z<\sqrt{2}      \nonumber    \\[2pt]  
 2p_1 z f_Z(z^2 -1), & \sqrt{2}<z<\sqrt{3}   \nonumber \\[2pt]  
 2p_2 z f_Z(z^2 -2), & \sqrt{3}<z<2    \nonumber \\ 
\end{cases}
\end{equation}

Thus, for $f_Z(z)$ to be properly defined at $z = (1+\sqrt{5})/2$, we must have $p_1=1/(1+\sqrt{5})$. Likewise, 
 a similar argument applied to the other extremities leads to~(\ref{pxz1}). The idea is to apply each of these 3 formulas
 backwards and recursively. The first case suggests that maybe $f_Z(z)=f(1)\cdot z^{-1}$ if the density existed. Hence trying
 $F_Z(z)=\log_2 z$ as the potential solution to the functional equation seems reasonable.  It 
turns out to be a pretty good approximation as seen in Figure~\ref{fig:botul}.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{nr.png}  
\caption{$F_Z(z)$ vs $\log_2 z$ on the left; approximation error on the right}
\label{fig:botul}
\end{figure}




There is more to the story. The invariant distribution, despite looking smooth on a macroscopic scale, is actually pretty chaotic.
The right plot in Figure~\ref{fig:botul} shows the approximation error $F_Z(z)-\log_2 z$ with $z\in [1,2]$. It behaves exactly like a fractal! 
The figure was produced with \texttt{invariant\_nested\_radical.py}, also available
 on GitHub,  \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/invariant_nested_radicals.py}{here}.\vspace{1ex}

\begin{lstlisting}
# invariant_nested_radical.py
import numpy as np
from matplotlib import pyplot as plt

seed = 453
np.random.seed(seed)
p0 = 1/2
p1 = 1/(1 + np.sqrt(5))
p2 = (3 - np.sqrt(5))/4
z_values = []

def my_plot_init():
    axes = plt.axes()
    [axx.set_linewidth(0.2) for axx in axes.spines.values()]
    axes.margins(x=0)
    axes.margins(y=0)
    axes.tick_params(axis='both', which='major', labelsize=7)
    axes.tick_params(axis='both', which='minor', labelsize=7)
    return()

for number in range(1000000):
    z = 0.0
    prod = 1.0
    for k in range(60):
        rnd = np.random.rand()
        if rnd < p0:
            x_k = 0
        elif rnd < p0 + p1:
            x_k = 1
        else:
            x_k = 2
        z = np.sqrt(x_k + z)
    z_values.append(z)

x = np.sort(z_values)
y = np.arange(len(x))/float(len(x))
approx = []
for arg in x:
    val = np.log(arg)/np.log(2)
    approx.append(val) 

my_plot_init()
plt.plot(x, y, linewidth = 0.4)
plt.plot(x, approx, linewidth = 0.4)
plt.show()

my_plot_init()
plt.plot(x, y-approx, linewidth = 0.4)
plt.show()
\end{lstlisting}

\subsubsection{Generalizations and characteristic function}
%---

In some systems,  the random variables $X$ and $Z$
are linked by the formula $Z_{k} = (X_k +Z_{k+1})^\alpha$, where $\alpha$ is a parameter, $\{X_k\}$ the digit sequence (random here)
 and $Z_0$ is the seed (also random). In my notation, $Z=Z_0$ and $X$ is a random variable with same distribution as any $X_k$.
Also, the $X_k$'s are independent. The \textcolor{index}{invariant distribution}\index{invariant distribution} is
 the limit distribution (if it exists) of $Z_k$ as 
$k\rightarrow\infty$, 
 or the distribution that the seed $Z=Z_0$ must have, if we want $Z_1$ to have the same distribution as $Z_0$, and by extension, the same distribution as all $Z_k$.

For the \textcolor{index}{nested radicals}\index{nested radicals}, $a=1/2$. For continued fractions 
(the \textcolor{index}{Gauss map}\index{Gauss map}), $\alpha = -1$. The additive block $X_k + Z_{k+1}$  of two independent components suggests
 that \textcolor{index}{characteristic functions}\index{characteristic function}
 [\href{https://en.wikipedia.org/wiki/Characteristic_function_(probability_theory)}{Wiki}] may help solve the problem (finding $F_Z$ when $F_X$ is known) or
 the inverse problem (finding $F_X$ when $F_Z$ is known). The latter usually has no solution unless 
 $F_Z$ is one of the very rare \textcolor{index}{attractor distributions}\index{attractor distribution} of the system.  
For nested radicals, we have $\varphi_{Z^2} = \varphi_X\cdot \varphi_Z$ where $\varphi$ denotes the characteristic function.
  For continued fractions,
 $\varphi_{1/Z} = \varphi_X\cdot \varphi_Z$. This formula may be useful to solve the inverse problem. 

For the numeration system in base $b$,  we have $Z_k = (X_k +Z_{k+1})/b$, resulting 
 in $\varphi_{bZ} = \varphi_X \cdot \varphi_Z$. This is true even if $b$ is not an integer, and regardless of the distribution 
 of $X_k$ as long as it takes integer values between $0$ and $b-1$ inclusive (the upper bound is $\lfloor b\rfloor$ if $b$ is not an integer). 
However, in most cases related to \textcolor{index}{bad seeds}\index{seed (dynamical systems)!bad seed}, $Z$ is singular and may not have a characteristic function, regardless of the system. 
For good seeds in the base $b$ system, $Z$ has a uniform distribution on $[0, 1]$ and $X$ has a discrete uniform distribution. 
The \textcolor{index}{dyadic map}\index{dyadic map} corresponds to $b=2$. 


%-----
% xxxxxxx xxx99 https://www.datasciencecentral.com/new-family-of-generalized-gaussian-distributions/
%   in [DSS recovered]
% xxx contact author of IEEE paper / conferences on dynamical systems
% xxxxxxxxxx MLT article: copula with parametric approach / gradient based and remove non-influential obs in dataset
% xx dynamical system to generate sequence of rgb colors: dyadic ; 3 successive iterates yiels one RGB color
%--------------------------
% xxx Contact author e-mail: avlad@racai.ro, adriana_vlad@yahoo.com
%The autocorrelation function of the logistic map chaotic signal in relation with the statistical independence issue
%   contact authors, contact special group on LinkedIn
%----------------------
%https://math.stackexchange.com/questions/3445421/limiting-distributions-attractors-associated-with-the-discrete-difference-oper
%xxx continued fraction with lower / higher exponent
% rabbit number


% https://math.stackexchange.com/questions/3330127/non-standard-solution-to-fx-frac12-bigf-fracx2-f-frac1x2


\subsection{Exercises}\label{jeporei}

The following original exercises complement the material discussed in sections~\ref{scrotew}
  and~\ref{nodensity}. They focus on important aspects of the methodology that could be skipped in a first reading. If you don't have time to solve them, you should at least read the solutions: they cover additional techniques
 and methodology that are fundamental for a full understanding of the subject, including Python implementations. 

The underlying system
 assumed in this section is that governed by~(\ref{digere}). Here $X$ is a random variable with the same
 distribution as any $X_k$, since  all the $X_k$'s are  
independently and identically distributed.

\pagebreak

\begin{Exercise}\label{paasweqasde} {\em Case when the invariant distribution is uniform} -- If 
 $X$ has a \textcolor{index}{Rademacher distribution}\index{Rademacher distribution} [\href{https://en.wikipedia.org/wiki/Rademacher_distribution}{Wiki}], that is $P(X=\frac{1}{2}) =
 P(X=-\frac{1}{2}) =\frac{1}{2}$, prove that $Z$ is uniform on $[-1, 1]$.
\vspace{1ex}

\noindent {\bf Solution} \\
Is it easy to compute the moments $\texttt{E}[Z^n]$ for $n=1,2$ and so on, using
Formula~(\ref{johndick}). The formula can be generalized to non-integer $n$. The moments of $Z$ coincide with those of a uniform distribution on $[-1, 1]$.
In certain cases (if $X$ and $Z$ are positive and continuous), 
you can also use the \textcolor{index}{Mellin transform}\index{Mellin transform} [\href{https://en.wikipedia.org/wiki/Mellin_transform}{Wiki}] 
to solve the functional equation $F_Z = F_{X(1+Z)}$, where the unknown is $F_Z$.
If $\text{M}(\cdot)$ denotes the Mellin transform, we have $\text{M}[Z] =\text{M}[X(1+Z)] = \text{M}[X]\cdot \text{M}[1+Z]$, which simplifies
 the computations. The use of Mellin transforms in this context dates back to 1948, see~\cite{mellin48}.
\end{Exercise}

\begin{Exercise}\label{p} {\em An almost perfect identity} -- This time, let us assume that the $X$ has a continuous distribution.
More specifically,  $X$ is distributed as $\sin\pi Y$ where $Y$ has a Gaussian distribution with zero mean and unit variance. 
Using simulations, compute $\text{Var}[Z]$. Does $Z$ also have a unit variance?
\vspace{1ex}

\noindent {\bf Solution} \\
We have $\text{Var}(Z) = 1.00000020\dots$. So it is different from 1. It is almost impossible to notice the discrepancy
 using simulations alone. Note that $\text{E}[X] = 0$, thus $\text{Var}[Z] = \text{E}[X^2] / (1 - \text{E}[X^2])$.  Then,
$$
\text{E}[X^2] =\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty x^2 \sin^2 \pi x dx
 = \frac{1+(4\pi^2 - 1)\exp(-2\pi^2)}{2}
= 0.500000051\dots
$$
See computations performed by Mathematica, \href{https://mltblog.com/3JqDagR}{here}. Thus the announced result.
\end{Exercise}

\begin{Exercise}\label{p} {\em A model fitting problem} --  You collected some data. The
 features in your dataset (also called predictors or independent variables) are $X_0, X_1,\dots$ and the
 response $Z$. You try to fit a model to your data, the two options being
$Z = X_0 + X_0X_1+X_0X_1 X_2+\dots$ and $Z = X_0 + X_1X_2+X_3X_4 X_5+\dots$, where the $X_k$ are independently and identically
 distributed in both cases. How to determine which model is a better fit?
\vspace{1ex}

\noindent {\bf Solution} \\
One easy way is to look at $\text{Var}[Z]$. The first and second models respectively have
$$
 \text{Var}[Z] = \frac{\text{Var}[X]}{(1-\text{E}[X^2])(1-\text{E}[X])^2}, 
\quad \text{Var}[Z]=\frac{\text{Var}[X]}{(1 - \text{E}[X^2]) (1 - \text{E}^2[X])}.
$$
The statistic for your test can be $T =  (1 - \text{E}[X^2]) (1 - \text{E}^2[X]) \text{Var}[Z]/ \text{Var}[X]$. It is expected
 to be equal to 1 (regardless of the distributions of $X$ and $Z$) if the second model is the correct one, assuming $0 < |\, \text{E}[X] \,| < 1$.
\end{Exercise}


\begin{Exercise}\label{polcjon} {\em Computing the digits} -- Given a seed $Z_0$, compute
  the digits $X_k$ defined by~(\ref{digere}). For simplicity, let us assume that $X_k$ can only take on two values: $a$ and $b$,
with $0<a<b$ and $a+b=1$.  \vspace{1ex}

\noindent {\bf Solution} \\
The following code \texttt{digits.py} computes the digit of $Z_0 = \log 2$ when $b = \sqrt{2}/2$. 
If $X_k=b$, the digit is shown as 1; if $X_k=a$, it is shown as 0. The variable \texttt{sum} computes the approximation
 to $Z_0$ when using 60 digits. The program is also on my GitHub repository, 
 \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/digits.py}{here}.\vspace{1ex}
\begin{lstlisting}
# digits.py
import numpy as np

b = np.sqrt(2)/2   
a = 1-b
z = np.log(2)  

prod = 1.0
beta = b/(1-a)
alpha = a/(1-b)

if z > beta: 
  sum   = b
  digit = 1
else:
  sum   = a
  digit = 0

prod = sum
print("digit %3d = %1d" % (0, digit))

for k in range(1, 60):
    beta  = sum + prod * b/(1-a)  
    alpha = sum + prod * a/(1-b)
    if alpha < z:
         prod  = prod * b
         digit = 1
    else: 
        prod  = prod * a
        digit = 0
    sum += prod 
    print("digit %3d = %1d" % (k, digit))
print ("sum = %14.13f, z = %14.13f" % (sum, z))
\end{lstlisting}
\end{Exercise}

\begin{Exercise}\label{polcjonb} {\em Computing the invariant distribution} -- 
Using the same system as in Exercise~\ref{polcjon}, let $P(X_k = b) = p$, and $P(X_k=a) = 1-p$. 
Plot the invariant distribution
  if $a=0.4$, $b=0.6$ and $p=0.8$. In which case (depending on $a,b,p$) is the invariant distribution uniform?\vspace{1ex}

\noindent {\bf Solution}

\noindent The random variable $Z$ can only take values in $[\alpha, \beta]$ with $\alpha = a/(1-a)$ and $\beta=b/(1-b)$. The invariant
 distribution $F_Z(z) = P(Z<z)$ is uniform on $[\alpha,\beta]$ if and only if $b=p$, and thus $a=1-p$. In all other cases, the 
 invariant distribution is singular and nowhere differentiable. See plot in Figure~\ref{fig:r1lkn2x}, 
 featuring the \textcolor{index}{empirical distribution}\index{empirical distribution} based on $\num{10000}$ values of $Z$. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.63\textwidth]{ecdf.png}  
\caption{Invariant distribution when $a=0.4, b=0.6, p=0.8$}
\label{fig:r1lkn2x}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

\noindent Below is the code \texttt{code invariant\_cdf.py} to produce Figure~\ref{fig:r1lkn2x}. 
It is also on GitHub, \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/invariant_cdf.py}{here}. 
\vspace{1ex} 

\begin{lstlisting}
# invariant_cdf.py
import numpy as np
from matplotlib import pyplot as plt

seed = 453
np.random.seed(seed)
p = 0.8
a = 0.4
b = 0.6
z_values = []

for number in range(10000):
    z = 0.0
    prod = 1.0
    for k in range(60):
        rnd = np.random.rand()
        if rnd < p:
            x_k = b
        else:
            x_k = a
        prod = prod * x_k
        z += prod
    z_values.append(z)

x = np.sort(z_values)
y = np.arange(len(x))/float(len(x))

axes = plt.axes()
[axx.set_linewidth(0.2) for axx in axes.spines.values()]
axes.margins(x=0)
axes.margins(y=0)
axes.tick_params(axis='both', which='major', labelsize=7)
axes.tick_params(axis='both', which='minor', labelsize=7)
plt.plot(x, y, linewidth = 0.4)
plt.show()
\end{lstlisting}

\end{Exercise}

%-----------------------------
%[DONE] Variance, Attractors and Behavior of Chaotic Statistical Systems      [see DSC recovered], 
%[DONE]  see https://math.stackexchange.com/questions/3477718/new-numeration-system-mapping-to-binary-numeration-system
%[DONE]  see https://stats.stackexchange.com/questions/435049/variance-of-z-x-1-x-1-x-2-x-1-x-2-x-3-cdots make this an exercise:
%[DONE]      https://math.stackexchange.com/questions/3477718/new-numeration-system-mapping-to-binary-numeration-system
%---------------------------------


\section{Cool synthetic images and generated art}\label{2dsinemp}

Chaotic dynamical systems can produce spectacular visualizations such as fractals. In this section, I feature a different type of images, arising
 from the orbits or basins of attraction of some of the systems explored in earlier chapters.

\begin{figure}[H]
\centering
\includegraphics[width=0.70\textwidth]{pillowspiral.PNG} %0.86
\caption{Basins of attraction of the 2D sine map ($\lambda=0.5,\rho=-1$)}
\label{gholdust}
\end{figure}

\subsection{Revisiting the 2D sine map}

I introduced the 2D sine map in section~\ref{sinere23jh}. In this version, I added one parameter $\theta$,
 though $\theta=1$ in all but one example. It is defined by 

\begin{equation}
    \begin{cases} 
      X_{k+1} = -\rho X_k + \lambda \sin \theta Y_k, \\[3pt]
     Y_{k+1} = -\rho Y_k + \lambda \sin \theta X_k. \label{gfda}
%      \left\lvert\frac{k_{p\omega}s+k_{i\omega}}{s}\cdot\frac{1}{Ts+1}\right\rvert_{S=\mathrm{j}\cdot2\pi}=1
    \end{cases} %\,.
\end{equation}

A \textcolor{index}{basin of attraction}\index{basin of attraction} consists of all the seeds $(X_0,Y_0)$ resulting in convergence
 of the sequence $(X_k,Y_k)$ to a same limit, in the non-chaotic mode (when $\lambda$ is small enough).  
 Figure~\ref{gholdust} features these basins when $\lambda=0.5$ and $\rho=-1$, with $-4\leq X_0,Y_0\leq 4$, each basin in a different color. Figure~\ref{fig:rheypll} shows how the basins look like when $\lambda=2$ and $\rho=-0.25$. 
The source code \texttt{basin.py} is in section~\ref{pytgr}, and also on GitHub, \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/basin.py}{here}.

In addition to the basins of attraction, the \textcolor{index}{orbits}\index{orbit (dynamical systems)} are of particular interest. 
 Given a set of parameters $(\lambda,\theta,\rho)$ and initial condition $(X_0, Y_0)$, the orbit also called path or trajectory,
 is the set of successive iterates $(X_k,Y_k)$, for $k=1,2$ and so on. It may converge, oscillate and reach a periodic state, or exhibit a chaotic behavior depending on the seed and parameters. It may avoid some areas for ever. These
 areas are called \textcolor{index}{basins of repulsion}\index{basin of repulsion}. 




%\pagebreak %%%

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.95\linewidth]{orbit1b.png}
  %\captionof{figure}{A figure}
  %\label{fig:test1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.95\linewidth]{orbit2b.png}
  %\captionof{figure}{Another figure}
  %\label{fig:test20}
\end{minipage}
\end{figure}

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{orbit3b.png}
 % \captionof{figure}{A figure}
  %\label{fig:test1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{orbit4b.png}
  %\captionof{figure}{Another figure}
  %\label{fig:test21}
\end{minipage}
\end{figure}

\begin{figure}[H]
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{orbit5b.png}
  %\captionof{figure}{A figure}
  %\label{fig:test1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{orbit6b.png}
  %\captionof{figure}{Another figure}
  %\label{fig:test22}
\end{minipage}
\caption{Six spectacular orbits of the 2D sine map}
\label{fig:b6b4c9u6}
\end{figure}

Figure~\ref{fig:b6b4c9u6} features 6 examples, corresponding to the first 6 values
 in the arrays \texttt{X\_0}, \texttt{Y\_0}, \texttt{llambda}, \texttt{theta}, \texttt{rho} in the source code. The number of iterations used 
 in each plot -- typically $\num{20000}$ -- is specified 
 by \texttt{n\_iter}. The first plot is a scatterplot. The other ones are standard Matplotlib plots, with 
 pairs of successive iterates $(X_k,Y_k)$ and $(X_{k+1},Y_{k+1})$ joined by a colored line segment, with the color depending on $k$.


\begin{lstlisting}
# sine2D_orbit.py | find and plot basins of attraction of 2D sine map

import numpy as np
import matplotlib.pyplot as plt

def orbit(X_0, Y_0, llambda, theta, rho, n_iter):
    list_x = []
    list_y = []
    list_color = []
    x = X_0
    y = Y_0
    for iter in range(n_iter): 
        old_x = x
        old_y = y
        x = -rho*old_x + llambda*np.sin(theta*old_y)
        y = -rho*old_y + llambda*np.sin(theta*old_x)
        list_x.append(x)
        list_y.append(y)
        red   = 1.0*abs(np.sin(2.7 - 5.4*iter/n_iter)) 
        green = 0.8*abs(np.sin(0.8 + 8.0*iter/n_iter)) 
        blue  = 0.6*abs(np.sin(5.2 - 4.7*iter/n_iter))  
        alpha = 1.0
        rgb = (red, green, blue, alpha)
        list_color.append(rgb)
    return(list_x, list_y, list_color)

llambda  = [2.0, 0.04, 1.5, 10, 2.5, 2.0, 2.0] 
theta    = [1.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]           
rho      = [-0.5, -1, -1, -1, -1, -1, -0.5] 
mode     = ['s','p','p','p','p','p','s']  # 's' for scatter, 'p' for plot
n_iter   = [50000, 20000, 20000, 20000, 20000, 20000, 50000]
X_0 = [0.0, 1.0, 3.0, 2.0, 1.0, 3.0, 0.0]  
Y_0 = [3.0, 4.0, 4.0, 4.0, 4.0, 4.0, 3.0]   
n_plots = len(X_0)    

for plot in range(n_plots):
    (list_x, list_y, list_color) = orbit(X_0[plot], Y_0[plot], \
            llambda[plot], theta[plot], rho[plot], n_iter[plot])
    axes = plt.axes()
    [axx.set_linewidth(0.2) for axx in axes.spines.values()]
    axes.set_facecolor("black")   # background color
    axes.margins(x=0)
    axes.margins(y=0)
    axes.tick_params(axis='both', which='major', labelsize=7)
    axes.tick_params(axis='both', which='minor', labelsize=7)
    if mode[plot] == 's':
        plt.scatter(list_x, list_y, c=list_color, s=0.1)
    elif mode[plot] == 'p':
        for idx in range(n_iter[plot]-1):
            col = list_color[idx]
            xx = [list_x[idx], list_x[idx +1]]
            yy = [list_y[idx], list_y[idx +1]]
            plt.plot(xx, yy, linewidth = 0.4, c=col) 
    plt.show()
\end{lstlisting}



%-------------------------------------------------------------------------------------

\subsection{The Riemann zeta function}

I briefly discuss the Riemann zeta function in section~\ref{fredur}
 and~\ref{iurhty}, and a lot more in my book on synthetic data~\cite{vgsynthetic}. In particular, Figure~\ref{fig:rheye} shows its ``eye". 
 Several spectacular videos about its orbits, depending on the parameters, can be
 found on my YouTube channel, \href{https://www.youtube.com/watch?v=MDBaJ8RsLA8}{here}
 and \href{https://www.youtube.com/watch?v=9IdkvifRFgU}{here}. 

\begin{figure}[H]
\centering
\includegraphics[width=0.90\textwidth]{rhp2.PNG} %0.86
\caption{Chaotic convergence path of the $\eta$ series in the complex plane}
\label{gholgurew}
\end{figure}

Figure~\ref{gholgurew}  is a frame from a 
  video  posted \href{https://www.youtube.com/watch?v=XI5MhyNc7us}{here}. The code 
 \texttt{image2R.py}
 to produce it is on my GitHub repository, \href{https://github.com/VincentGranville/Visualizations/blob/main/Source-Code/image2R.py}{here}. For convenience, I also included it in this section. The image represents successive iterates of the system 
$$X_{k+1} = X_{k} + \frac{(-1)^{k}} {(k+1)^{\sigma+ it}}$$
in the complex plane, starting with $X_0 = 0$. Each of the two branches in Figure~\ref{gholgurew} represents a path corresponding to a specific
 complex parameter $\sigma + it$. One branch has $t=5555555.0$, the other one $t=5555555.1$ In both cases
 $\sigma=0.75$. Small differences in $t$ leads to very different trajectories when $t$ is large. This indicates that
 the system, for all purposes, can be considered as chaotic when $t$ is large. The number of branches is determined by the parameter \texttt{m} in the code.

Despite the appearance of being chaotic, this system actually converges to $\eta(\sigma+it)$, where
 $\eta$ is the \textcolor{index}{Dirichlet eta function}\index{Dirichlet eta function} [\href{https://en.wikipedia.org/wiki/Dirichlet_eta_function}{Wiki}]. However, convergence is very slow and erratic if $t$ is large. Here I used $\num{20000}$ iterations, and this may not be enough. \vspace{1ex}

\begin{lstlisting}
# image2R.py

from PIL import Image, ImageDraw           # ImageDraw to draw ellipses etc.
import moviepy.video.io.ImageSequenceClip  # to produce mp4 video
from moviepy.editor import VideoFileClip   # to convert mp4 to gif

import numpy as np
import math
import random
random.seed(100)

#--- Global variables ---

m=2               # number of curves
nframe=20000      # number of images   ### 20000
count=0           # frame counter 
start=2000        # must be smaller than nframe
r=20              # one out of every r image is included in the video

width = 3200 # 1600 # 1600
height =2400 #1200 # 1200

images=[]

etax=[]
etay=[]
sigma=[]
t=[]
x0=[]
y0=[]
flist=[]  # filenames of the images representing each video frame

etax=list(map(float,etax))
etay=list(map(float,etay))
sigma=list(map(float,sigma))
t=list(map(float,t))
x0=list(map(float,x0))
y0=list(map(float,y0))
flist=list(map(str,flist))

#--- Initializing comet parameters ---

for n in range (0,m):
    etax.append(1.0)
    etay.append(0.0)
    t.append(5555555+n/10)
    sigma.append(0.75)
sign=1

minx= 9999.0
miny= 9999.0
maxx=-9999.0
maxy=-9999.0

for n in range (0,m):
    sign=1
    sumx=1.0
    sumy=0.0
    for k in range (2,nframe,1):
        sign=-sign
        sumx=sumx+sign*np.cos(t[n]*np.log(k))/pow(k,sigma[n])
        sumy=sumy+sign*np.sin(t[n]*np.log(k))/pow(k,sigma[n])
        if k >= start:
            if sumx < minx:
                minx=sumx
            if sumy < miny:
                miny=sumy
            if sumx > maxx:
                maxx=sumx
            if sumy > maxy:
                maxy=sumy
sign=1
rangex=maxx-minx
rangey=maxy-miny

img    = Image.new( mode = "RGB", size = (width, height), color = (255, 255, 255) )
pix = img.load()
draw = ImageDraw.Draw(img)

red=255
green=255
blue=255
col=(red,green,blue)
count=0

#--- Main Loop ---

for k in range (2,nframe,1): # loop over time, each t corresponds to a ideo frame
    if k%10 == 0:
        print("Building frame:",k)
    sign=-sign
    for n in range (0,m):    # loop over curves
        x0.insert(n,int(width*(etax[n]-minx)/rangex))
        y0.insert(n,int(height*(etay[n]-miny)/rangey))
        etax[n]=etax[n]+sign*np.cos(t[n]*np.log(k))/pow(k,sigma[n])
        etay[n]=etay[n]+sign*np.sin(t[n]*np.log(k))/pow(k,sigma[n])
        x=int(width*(etax[n]-minx)/rangex)
        y=int(height*(etay[n]-miny)/rangey)
        shape = [(x0[n], y0[n]), (x, y)]
        red    = int(255*0.9*abs(np.sin((n+1)*0.00100*k)))
        green= int(255*0.6*abs(np.sin((n+2)*0.00075*k)))
        blue = int(255*abs(np.sin((n+3)*0.00150*k)))

        if k>=start:
            # draw line from (x0[n],y0[n]) to (x_new,y_new)
            draw.line(shape, fill =(red,green,blue), width = 1)

    if k>=start and k%r==0:
        fname='imgpy'+str(count)+'.png'
        count=count+1
        # anti-aliasing mechanism
        img2 = img.resize((width // 2, height // 2), Image.LANCZOS) #ANTIALIAS)
        # output curent frame to a png file
        img2.save(fname)       # write png image on disk
        flist.append(fname)    # add its filename (fname) to flist
        images.append(img2)    # to produce Gif image

# output video file
clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(flist, fps=20) 
clip.write_videofile('riemann.mp4')
\end{lstlisting}




%\pagebreak


%----------------------------------------------------------------------------------------------------------------
\Chapter{Random Numbers Based on Quadratic Irrationals}{}\label{chapterPRNG}


In the previous chapters, I established the strong connection between discrete chaotic dynamical systems and numeration systems. 
I also discussed an example where binary digits of irrational numbers are used  to generate random numbers, see
 section~\ref{puutrew}. This is in the context of the logistic map,  popular again to generate randomness. Practitioners might not be aware that the logistic map method discussed in so many recent papers amounts to using digits of irrational numbers,
 thanks to the homomorphism with the dyadic map. Indeed they stay away from PRNGs (pseudo-random number generators) based on such digits due to how slow and supposedly how non-random they are. Yet they implicitly use them in some ways, without knowing it. 

In section~\ref{pivizintrobvbc}, 
 I discuss very fast PRNGs based on the digits of millions of quadratic irrationals. It dispels the myth of lack of randomness, while at the same time featuring an algorithm just as fast as modern PRNGs, thanks to new developments pertaining to quadratic irrational numbers. At the very core, all of this is based on one of the most fundamental dynamical systems: the dyadic map. 

In section~\ref{prozaxheliumgas}, I discuss a new test of randomness -- the prime test -- for pseudo-random number generators (PRNG), to detect subtle patterns in binary sequences. The test shows that congruential PRNGs, even the best ones, have flaws that can be exacerbated by the choice of the seed. This includes the Mersenne twister used in many programming languages including Python. I also show that the digits of some numbers such as $\sqrt{2205}$ fail this new test, despite the fact that they pass all the standard tests. I propose a methodology to avoid these flaws, with a Python implementation. The test is particularly useful when high quality randomness is needed. This includes cryptography and military-grade security applications, as well as
\textcolor{index}{synthetic data}\index{synthetic data} generation and simulation-intensive \textcolor{index}{Markov chain Monte Carlo}\index{Monte Carlo simulations}\index{Markov chain!MCMC} methods.  

The origin of this test is in number theory and connected to the Riemann 
Hypothesis. In particular, it is based on Rademacher stochastic processes. These random multiplicative functions are a number-theoretic version of Bernoulli trials.  This chapter features state-of-the-art research on this topic, as well as an original, simple, integer-based formula to compute square roots to generate random digits. It is offered with a Python implementation that handles integers with millions of digits. 

\hypersetup{linkcolor=red} 

\section{Introduction}\label{pivizintro}

Let $\chi(\cdot)$ be a function defined for strictly positive integers, with $\chi(1)=1$ and $\chi(ab)=\chi(a)\chi(b)$ for 
any integers $a,b>0$. Such a function is said to be
\textcolor{index}{completely multiplicative}\index{multiplicative function!completely multiplicative} [\href{https://en.wikipedia.org/wiki/Completely_multiplicative_function}{Wiki}]. 
Here we are interested in the case where $\chi$ takes on two possible values: $+1$ and $-1$. The core of my methodology is based on the following, well-known identity:
\begin{equation}
\sum_{k=1}^\infty \chi(k) k^{-z} = \prod_{p\in P} \frac{1}{1-\chi(p) p^{-z}}.\label{bore}
\end{equation}
The product is over all prime integers ordered by increasing values: $P=\{2,3,5,7,11,\dots\}$ is the set of all primes. Such a product is called an \textcolor{index}{Euler product}\index{Euler product} [\href{https://en.wikipedia.org/wiki/Euler_product}{Wiki}]. The series on the left-hand side is called
a \textcolor{index}{Dirichlet series}\index{Dirichlet series} [\href{https://en.wikipedia.org/wiki/Dirichlet_series}{Wiki}]. The argument $z=\sigma+it$ is a complex number. You don't need to know anything about complex numbers to understand this chapter. The only important fact is that the series or product converges only if $\sigma$ -- the real part of $z$ -- is large enough, typically larger than $0$, $\frac{1}{2}$ or $1$, depending on $\chi$. If $\chi$ is a constant function, thus equal to $1$, then the product and series converge to the
 \textcolor{index}{Riemann zeta function}\index{Riemann zeta function} $\zeta(z)$ [\href{https://en.wikipedia.org/wiki/Riemann_zeta_function}{Wiki}] if $\sigma>1$.

For primes $p$,  let the $\chi(p)$'s be independent random variables, with $\text{P}[\chi(p)=1] =  \text{P}[\chi(p)=-1] =\frac{1}{2}$.
The product, denoted as $L_P(z,\chi)$, is known as a \textcolor{index}{Rademacher random multiplicative function}\index{distribution!Rademacher}\index{multiplicative function!Rademacher}\index{random multiplicative function}\index{Rademacher function}, see \cite{harper2020bb}, \cite{harper2020} and \cite{yukkam2013}. If $z$ is a complex number, we are dealing with
 \textcolor{index}{complex random variables}\index{random variable!complex}\index{complex random variable} [\href{https://en.wikipedia.org/wiki/Complex_random_variable}{Wiki}]. From the product formula and the independence assumption, it is easy to obtain
\begin{equation}
\text{E} [L_P(z,\chi)]=\prod_{p\in P} \text{E}\bigg[\frac{1}{1-\chi(p)p^{-z}}\bigg]=\prod_{p\in P }\frac{1}{1-p^{-2z}}=\zeta(2z).\label{proofrn}
\end{equation}
Thus the expectation is finite if $\sigma=\Re(z)>\frac{1}{2}$. A similar argument can be used for the variances. 

Now let us replace the random variables $\chi(p)$ 
by \textcolor{index}{random numbers}\index{pseudo-random numbers}, 
taking the values $+1$
 or $-1$ with probability $\frac{1}{2}$. If these generated numbers are ``random enough'', free of dependencies, then one would expect them to 
 satisfy the laws of Rademacher random multiplicative functions. The remaining of this chapter explores this idea in
 details, with a focus on applications.  

\section{Pseudo-random numbers}\label{prozaxheliumgas}

There is no formal definition of pseudo-random numbers. Intuitively, a good set of pseudo-random numbers is a  
deterministic binary sequence of digits that satisfies all statistical tests of randomness. Of course, it makes no sense to talk about 
randomness if the sequence contains very few digits, say one or two. So pseudo-random numbers (PRN) are associated with
 infinite sequences, even though in practice one only uses finite sub-sequences.


A rigorous definition of PRN sequences requires the convergence of the 
%\textcolor{index}{empirical joint distribution}
multivariate \textcolor{index}{empirical joint distribution}\index{empirical distribution}\index{empirical distribution!multivariate}
 [\href{https://en.wikipedia.org/wiki/Empirical_distribution_function}{Wiki}]
 of any finite sub-sequence of $m$ digits, to the known theoretical value under the assumption of full randomness.  Let the PRN 
sequence be denoted as $\{d(k)\}$ with $k=1,2$ and so on. A sub-sequence of $m$ digits is defined by its indices, denoted as $i_1,i_2,\dots i_m$.  The convergence of the empirical distribution means that regardless 
of the indices $0\leq i_1 < i_2< \dots$ we have:
\begin{equation}
 \lim_{n\rightarrow\infty} \frac{1}{n}\sum_{k=1}^n I\Big[d(k+i_1)=k_1,d(k+i_2)=k_2,\dots,d(k+i_m)=k_m\Big] = 2^{-m} \label{eqrdv}
\end{equation}
for any $(k_1,k_2,\dots,k_m)$ in $\{-1,+1\}^m$. Here $I$ is the indicator function: $I[A]=1$ if $A$ is true, otherwise $I[A]=0$. The following number $\lambda$ is of particular interest:
\begin{equation}
\lambda=\sum_{k=1}^\infty d'(k) \cdot 2^{-k}, \quad \text{with } d'(k)=\frac{1+d(k)}{2} \in \{0, 1\}. \label{zzxdx}
\end{equation}
Thus the $d'(k)$'s are the binary digits of the number $\lambda$, with $0\leq \lambda\leq 1$. 

The connection between the multiplicative function $\chi(\cdot)$ in Formula~(\ref{bore}) and the $d(k)$'s is as follows. Let denote the $k$-th prime as $p_k$, with $p_1=2$. Then $d(k)=\chi(p_k)$. The traditional definition of PRN's is equivalent to requiring $\lambda$ to
be a \textcolor{index}{normal number}\index{normal number} in base $2$ [\href{https://en.wikipedia.org/wiki/Normal_number}{Wiki}]. I introduce a stronger
 criterion of randomness in section~\ref{sprng}


\subsection{Strong pseudo-random numbers}\label{sprng}

Convergence of the empirical joint distributions, as defined by Formula~(\ref{eqrdv}), has a few important implications. 
The \textcolor{index}{Kolmogorov-Smirnov test}\index{Kolmogorov-Smirnov test} [\href{https://en.wikipedia.org/wiki/Kolmogorov\%E2\%80\%93Smirnov_test}{Wiki}], the \textcolor{index}{Berry-Esseen inequality}\index{Berry-Esseen inequality} [\href{https://en.wikipedia.org/wiki/Berry\%E2\%80\%93Esseen_theorem}{Wiki}] 
and the \textcolor{index}{law of the iterated logarithm}\index{law of the iterated logarithm}\index{iterated logarithm} [\href{https://en.wikipedia.org/wiki/Law_of_the_iterated_logarithm}{Wiki}] can be applied to the PRN sequence $\{d(k)\}$. These three fundamental results provide strong limitations on the behavior of finite PRN 
 sequences. If a sequence $\{d(k)\}$ or its representation by the number $\lambda$ does no stay within these limits, 
  then it does not emulate pure randomness.  However, some quasi-random PRN sequences, with weak dependencies, meet these requirements yet are not truly ``random". For instance, a number can be normal in base $2$ yet have digits that exhibit some dependencies, 
 see \href{https://mathoverflow.net/questions/426815/normal-numbers-and-law-of-the-iterated-logarithm}{here}.  The purpose of this section is to introduce stronger requirements in order to catch some of these exceptions. This is where the multiplicative function $\chi(\cdot)$ comes into play. 

The function $\chi(\cdot)$, initially defined for primes $p$, is extended to all strictly positives integers via $\chi(ab)=\chi(a)\chi(b)$. Because the $\chi(p)$'s are independent among prime numbers (by construction), the full sequence $\{\chi(k)\}$ over all $k$ must behave in a certain way. Obviously, if $k$ is a square integer, $\chi(k)=1$. But if $k$ is not a square, we still have 
 $P[\chi(k)=1]=P[\chi(k)=-1]=\frac{1}{2}$. For instance, $\chi(4200)=\chi(4)\chi(25)\chi(6)\chi(7)=\chi(6)\chi(7)$.
 Since the product of two independent random variables with \textcolor{index}{Rademacher distribution}\index{Rademacher distribution}\index{distribution!Rademacher}
 [\href{https://en.wikipedia.org/wiki/Rademacher_distribution}{Wiki}] has a Rademacher distribution, it follows that 
 $\chi(6)=\chi(2)\chi(3)$ has a Rademacher distribution, and thus $\chi(4200)=\chi(6)\chi(7)$ also has a Rademacher distribution.
 So, the $\chi(k)$'s are identically distributed with zero mean, unless $k$ is a square integer. However, they are not independently distributed, even after removing square integers, or even if you only keep \textcolor{index}{square-free integers}\index{square-free integer} 
 [\href{https://en.wikipedia.org/wiki/Square-free_integer}{Wiki}].

I now define three fundamental functions, which are central to my new test of randomness. First, define the following sets:
\begin{itemize}
\item $S_1(n)$  contains all prime integers $\leq n$. 
\item $S_2(n)$ contains all positive square-free integers $\leq n$. 
\item $S_3(n)$ contains all positive non-square integers $\leq n$. 
\end{itemize}
So each of these sets contains at most $n$ elements. Then, define the three functions as
\begin{equation}
L_1(n)=\sum_{k\in S_1(n)} \chi(k),\quad L_2(n)=\sum_{k\in S_2(n)} \chi(k),\quad L_3(n)=\sum_{k\in S_3(n)} \chi(k).\label{oopi}
\end{equation}
 Now, I can introduce my new test of randomness.

\subsubsection{New test of randomness for PRNGs}\label{vcfprng}

Let $d(1),d(2),\dots$ be a sequence of integer numbers, with $d(k)\in \{-1,1\}$ and
$P=\{p_1,p_2,\dots\}$ be the set of prime numbers. The goal is to test how random the sequence $\{d(k)\}$ is, based on the first $n$ elements $d(1),\dots,d(n)$. The algorithm is as follows.
\begin{itemize}
\item Step 1: Set $\chi(p_k)=d(k)$, where $p_k$ is the $k$-th prime number ($p_1=2$). 
\item Step 2: For $k\notin P$ with
 prime factorization $k=p_1^{a_1}p_2^{a_2}\cdots$, set
$\chi(k)=\chi^{a_1}(p_1)\chi^{a_2}(p_2)\cdots$, with $\chi(1)=1.$
\item Step 3: Using Formula~(\ref{oopi}), compute $L^*_3(n) = |L_3(n)|/\sqrt{n\log\log n}$. 
\item Step 4: If $L^*_3(n)<0.5$ or $L^*_3(n)>1.5$, the sequence $\{d(k)\}$ (the first $n$ elements) lacks true randomness.
\end{itemize}
This test is referred to as the ``\textcolor{index}{prime test}"\index{pseudo-random numbers!prime test}\index{prime test (of randomness)}.
Let's illustrate step 2 with $k=4200$: since $4200=2^3\cdot 3\cdot 5^2 \cdot 7$, we have $\chi(4200)=\chi^3(2)\chi(3)\chi^2(5)\chi(7)
 = \chi(2)\chi(3)\chi(7)$. 

Some non-random sequences may pass the prime test. So you should never use this test alone to decide whether a sequence is good enough.
 Also, the standardization of $L_3(n)$, using the $\sqrt{n\log\log n}$ denominator, is not perfect, but good enough for all practical purposes, assuming $10^4<n<10^{15}$.  This test can detect departure from randomness that no other test is able to uncover. 
I discuss practical examples and
 a Python implementation later in this chapter.

A PRN sequence that satisfies~(\ref{eqrdv}) and passes all the existing tests, including the prime test, is called \textcolor{index}{strongly pseudo-random}\index{pseudo-random numbers!strongly random}. The
corresponding real number $\lambda$ defined by Formula~(\ref{zzxdx}) is called \textcolor{index}{strongly normal}\index{normal number!strongly normal}. It should not be difficult to
prove that almost all numbers are strongly normal. Thus almost all PRN sequences are strongly pseudo-random. Yet creating
one that can be proved to be strongly pseudo-random is as difficult as proving that a given number is normal (and a fortiori, strongly normal). Interestingly, none of the sequences produced by \textcolor{index}{congruential random number generators} 
[\href{https://en.wikipedia.org/wiki/Linear_congruential_generator}{Wiki}] are strongly pseudo-random,
for the same reason that no rational number is normal: in both cases, $d(k)$ is periodic. 


Modern test batteries
 include the \textcolor{index}{Diehard tests}\index{Diehard tests of randomness}\index{pseudo-random numbers!Diehard tests} [\href{https://en.wikipedia.org/wiki/Diehard_tests}{Wiki}] published in 1995, and the
 \textcolor{index}{TestU01}\index{pseudo-random numbers!TestU01} framework [\href{https://en.wikipedia.org/wiki/TestU01}{Wiki}], introduced in 2007. 



\subsubsection{Theoretical background: the law of the iterated logarithm}\label{iterlawsd}

The prime test checks whether the multiplicative function $\chi(k)$ derived from $\{d(k)\}$, satisfies a particular version of the 
 \textcolor{index}{law of the iterated logarithm}\index{law of the iterated logarithm}\index{iterated logarithm} [\href{https://en.wikipedia.org/wiki/Law_of_the_iterated_logarithm}{Wiki}]. Truly random sequences $\{d(k)\}$ satisfy that law. Since $\{\chi(k)\}$ is multiplicative and thus non-random, the law of the iterative algorithm
 must be adapted to take care of the resulting dependencies. In particular, the $\sqrt{n\log\log n}$ weight used to standardize
  $L_3(n)$  provides only an approximation, good enough for all practical purposes. The exact weight is discussed in 
  \cite{harper2020} and \cite{yukkam2013}.  

Many sequences $\{d(k)\}$ satisfy the basic law of the iterated logarithm without the prime number / Euler product apparatus introduced 
 in this chapter. This is the case for most of the sequences studied here. However, by looking at $\chi(k)$ rather than the original 
 $d(k)$, we are able to magnify flaws that are otherwise undetectable by standard means. An example is the Mersenne twister
  implemented in Python, passing the standard test, but failing the prime test when the seed is set to $200$. 

\subsubsection{Connection to the Generalized Riemann Hypothesis}\label{iurhty}

The \textcolor{index}{Generalized Riemann Hypothesis}\index{Riemann Hypothesis!Generalized} (GRH) [\href{https://en.wikipedia.org/wiki/Generalized_Riemann_hypothesis}{Wiki}] is one of the most famous unsolved problems in
mathematics. It states that the function $L(z,\chi)$ defined by Formula~(\ref{bore}) has no root if $\frac{1}{2}<\sigma=\Re(z)<1$. Here $z=\sigma+it$ is a complex number, and $\sigma=\Re(z)$ is its real part.
It applies to a particular class of functions $\chi(\cdot)$, those that are ``well behaved". Of course, without any restriction on $\chi(\cdot)$,
 there are \textcolor{index}{completely multiplicative functions}\index{multiplicative function!completely multiplicative} [\href{https://en.wikipedia.org/wiki/Completely_multiplicative_function}{Wiki}]
 known to satisfy GRH. For instance, the function defined by $\chi(p_{2k})=-1, \chi(p_{2k+1})=1$ where $p_k$ is the $k$-th prime number. The
 corresponding $L(z,\chi)$ has no root if $\sigma>\frac{1}{2}$ because the product converges for $\sigma>0$, and of course, the product has no root. The sequence $\{\chi(p_k)\}$ is obviously non-random as it perfectly alternates, and thus I labeled it \texttt{CounterExample} in Table~\ref{tabuchi}. 

If the Euler product in  Formula~(\ref{bore}) converges for some $\sigma>\sigma_0$, it is equal to its series expansion when $\sigma>\sigma_0$, 
 it converges for all $\sigma>\sigma_0$, and $L(z,\chi)$ satisfies GRH 
when $\sigma>\sigma_0$. When $\sigma<1$, the convergence is \textcolor{index}{conditional}\index{convergence!conditional} [\href{https://en.wikipedia.org/wiki/Conditional_convergence}{Wiki}], making things more difficult. Another example that trivially satisfies GRH if $\sigma>\frac{1}{2}$ is when $\chi(\cdot)$ is a random variable with
 $P[\chi(p)=1]=P[\chi(p)=-1]=\frac{1}{2}$ for primes $p$. In this case convergence means that $L(z,\chi)$ has finite expectation as proved by 
 Formula~(\ref{proofrn}), and finite variances. This function $\chi(\cdot)$ is called a 
\textcolor{index}{random Rademacher multiplicative function}\index{random multiplicative function!Rademacher}\index{Rademacher function!random}, see \cite{RH1002}. Here the $\chi(p)$'s are identically and independently distributed over the set of all primes, and the definition is extended to all strictly positive  integers with the formula $\chi(ab)=\chi(a)\chi(b)$.

So, if we were able to find a ``nice enough" pseudo-random yet deterministic function $\chi(\cdot)$ with convergence of the 
 Euler product when $\sigma>\sigma_0$ for some $\sigma_0<1$, a function $\chi(\cdot)$ that is random enough over the primes (like its sister, the truly stochastic version) to guarantee the convergence of the product, then it would be a major milestone towards proving GRH. Convergence of the product would imply that:
\begin{itemize} 
\item $L(z,\chi)$ is \textcolor{index}{analytic}\index{analytic function} [\href{https://en.wikipedia.org/wiki/Analytic_function}{Wiki}], because  the product is equal to its series expansion, which trivially satisfies the \textcolor{index}{Cauchy-Riemann equations}\index{Cauchy-Riemann equations} [\href{https://en.wikipedia.org/wiki/Cauchy\%E2\%80\%93Riemann_equations}{Wiki}],
\item $L(z,\chi)$ has no root if $\sigma>\sigma_0$ since the product has no root.
\end{itemize}
As discussed, examples that are ``not so nice" exist; ``nice enough" means that $L(z,\chi)$ satisfies a
 \textcolor{index}{Dirichlet functional equation}\index{Dirichlet functional equation} [\href{https://en.wikipedia.org/wiki/Functional_equation_(L-function)}{Wiki}]. Typically, ``nice enough" 
 means that $L(z,\chi)$ is a \textcolor{index}{Dirichlet-$L$ function}\index{Dirichlet-$L$ function} [\href{https://en.wikipedia.org/wiki/Dirichlet_L-function}{Wiki}]. Besides the \textcolor{index}{Riemann zeta function}\index{Riemann zeta function} where $\chi(p)=1$ is the trivial Dirichlet character, the most 
 fundamental example is when $\chi=\chi_4$ is the non-trivial \textcolor{index}{Dirichlet character modulo $4$}\index{Dirichlet character} [\href{https://en.wikipedia.org/wiki/Dirichlet_character}{Wiki}]. This example is featured in Figure~\ref{fig:rn2x} where $\sigma=\frac{1}{2}$ (left plot) and contrasted with a not so nice example on the right plot, corresponding to a pseudo-random sequence $\{\chi(p_k)\}$. The $\chi_4$ example 
 is referred to as \texttt{Dirichlet4} in Table~\ref{tabuchi}. Note that for $\sigma=\frac{1}{2}$, $L(z,\chi_4)$ has infinitely many roots just like the Riemann zeta function, though its roots are different: this is evident when looking at the left plot in Figure~\ref{fig:rn2x}. The (conjectured) absence of root is when $\frac{1}{2}<\sigma<1$.

I went as far as to compute the Euler product for $L(z,\chi_4)$ when $z=\sigma=0.99$. It nicely converges to the correct value, without the typical 
 growing oscillations associated to lack of convergence, 
 agreeing with the value computed using the series expansion in Formula~(\ref{bore}). It means that the product converges at least for all $z$ with
 $\sigma=\Re(z)\geq 0.99$. Thus there is no root for $L(z,\chi_4)$ if $\sigma\geq 0.99$. This would be an immense milestone compared to the best known result (no root if $\sigma\geq 1$) if it was theoretically possible to prove the convergence in question, supported only by empirical evidence. 
 Convergence implies that the sequence $\{\chi_4(p_k)\}$ is random enough over the primes $p_1,p_2$ and so on. That is, the gap between $+1$ and $-1$ in the sequence never grows too large (that is, runs of same value can only grow so fast), and the proportion of $+1$ and $-1$ tends to $\frac{1}{2}$ fast enough, despite the known \textcolor{index}{Chebyshev's bias}\index{Chebyshev's bias (prime numbers)} [\href{https://en.wikipedia.org/wiki/Chebyshev\%27s_bias}{Wiki}]. The fact that the proportion eventually converges to $\frac{1}{2}$ when using more and more terms in the sequence, is a consequence 
 of \textcolor{index}{Dirichlet's theorem}\index{Dirichlet's theorem} [\href{https://en.wikipedia.org/wiki/Dirichlet\%27s_theorem_on_arithmetic_progressions}{Wiki}]. This is how close we are -- or you may say how far -- to proving GRH.

%-----------------------------vince/riemann2and3.mp4
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{rn3.png}  
\caption{Orbit of $L(z,\chi)$ at $\sigma=\frac{1}{2}$, with $0<t<200$ and $\chi=\chi_4$ (left) versus pseudo-random $\chi$ (right)}
\label{fig:rn2x}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

There are other ``nice functions" $\chi(\cdot)$ that fit within the GRH framework. For instance, with primes that are not integers, such as Beurling primes~\cite{bzf2004} (discussed later in this chapter) or \textcolor{index}{Gaussian primes}\index{Gaussian primes} [\href{https://en.wikipedia.org/wiki/Gaussian_integer}{Wiki}]. For a general family of such functions, see the \textcolor{index}{Dedekind zeta function}\index{Dedekind zeta function} [\href{https://en.wikipedia.org/wiki/Dedekind_zeta_function}{Wiki}]. For a general introduction to the Riemann zeta function and related topics, see \cite{kconrad2018} and \cite{tdr1987}.




\subsection{Testing well-known sequences}\label{twist}

The binary sequences analyzed here are denoted as $\{d(k)\}$, with $d(k)\in\{-1,+1\}$ and $k=1,2$ and so on. 
The tests, restricted to $d(k)\leq n$, are based on 
$L^*_3(n)=L_3(n)/\sqrt{n\log\log n}$, with  $L_3(n)$ defined by~(\ref{oopi}). Again, $\chi(p_k)=d(k)$ for prime numbers,
and $\chi(\cdot)$ is extended to non-primes positive integers via $\chi(ab)=\chi(a)\chi(b)$. Finally,
 $p_k$ is the $k$-th prime with $p_1=2$. In my examples, $n=\num{20000}$ in Table~\ref{tabuchi}, and $n=\num{80000}$ 
 in Figures~\ref{fig:rn1} and~\ref{fig:rn2}.  

The fact that a sequence fails the test for a specific $n$ does not mean it fails for all $n$. The success or failure also depends on the seed (the initial conditions). Some seeds require many iterations -- that is, a rather large $n$ -- before randomness kicks in. The test should not be used for small $n$, say $n<1000$. Finally, passing the test does not mean that the sequence is random enough. I provide examples of poor PRNGs that pass the test. Typically, to assess the randomness character of a sequence, one uses a battery of tests, not just one test. However, the prime test can detect patterns that no other one can. In some sense, it is a last resort test.

\noindent The sequences investigated here fall into four types:

\begin{itemize}
\item Discrete chaotic \textcolor{index}{dynamical systems}\index{dynamical systems} [\href{https://en.wikipedia.org/wiki/Dynamical_system}{Wiki}]. 
 In one dimension, many of these systems are driven by a recursion $x_{k+1}=g(x_k)$, where $g$ is a continuous mapping from
$[0,1]$ onto $[0,1]$. The initial value $x_0$ is called the seed. Typically, $d(k)=1$ if $x_k<0.5$, otherwise $d(k)=-1$. The \textcolor{index}{logistic map}\index{logistic map}\index{dynamical systems!logistic map} [\href{https://en.wikipedia.org/wiki/Logistic_map}{Wiki}], with $g(x)=4x(1-x)$, is labeled \texttt{Logistic} in Table~\ref{tabuchi} as well as in the Pyhton code
 in section~\ref{prngpython}. The \textcolor{index}{shift map}\index{shift map}\index{dynamical systems!shift map} in base $b$, defined by $g(x)=bx-\lfloor bx\rfloor$ where the brackets represent the integer part function, here with $b=3$, is labeled  
\texttt{Base3}. The case $b=2$ is known as the 
 \textcolor{index}{dyadic map}\index{dyadic map}\index{dynamical systems!dyadic map} [\href{https://en.wikipedia.org/wiki/Dyadic_transformation}{Wiki}]. 
The number $\lfloor bx_k\rfloor$
  is the $k$-digit of the seed $x_0$, in base $b>1$. In particular, if $x_0$ is a rational number, then the sequence
$\{d(k)\}$ is periodic, and thus non random. Even in the best-case scenario (using a random seed), the sequence $\{d(k)\}$ is auto-correlated. 

\item \textcolor{index}{Mersenne twister}\index{Mersenne twister} [\href{https://en.wikipedia.org/wiki/Mersenne_Twister}{Wiki}] as
 implemented in the Python function \texttt{random.random()}. This congruential PRNG is also another type of dynamical system,
 though technically ``non-chaotic" because the sequence $\{x_k\}$ 
is periodic. It emulates randomness quite well as the period is very large.
 Likewise, the shift map with a large base $b$ and a bad seed $x_0$ (a rational number resulting in periodicity) will emulate
 randomness quite well if $x_0=q/p$, where $p,q$ are integers and $p$ is a very large prime. Both in the table and in the figures, I use the label \texttt{Python} for the Mersenne twister. It fails the prime test for various seeds, especially if the seed is set to $200$. See also section~\ref{fixp}.
\item Number theoretic sequences related to the distribution of prime numbers or
 \textcolor{index}{Beurling primes}\index{Beurling primes}. Sequences of this type are labeled \texttt{Dirichlet4}. The main one, with the seed set to $3$, corresponds to
 $\chi(p_k)=+1$ if $p_k \equiv 1 \bmod 4$ and $\chi(p_k)=-1$ if $p_k \equiv 3\bmod 4$. Here $\chi(2)=0$. This function, denoted as $\chi_4$, is the non-trivial \textcolor{index}{Dirichlet character modulo 4}\index{Dirichlet character} [\href{https://en.wikipedia.org/wiki/Dirichlet_character}{Wiki}]. The sequence barely fails the $L^*_3$ test; the full function $\chi_4$ defined over all positive integers (not just the non-square integers) is periodic. The sister sequence (with the seed set to $1$) has 
%the opposite sign of 
 $\chi(p)=-\chi_4(p)$ if $p$ is prime. Sequences based on Beurling primes (a generalization of prime numbers to non-integers) are not included here, but discussed in~\cite{vgsynthetic}. The Python code 
in section~\ref{prngpython} can easily be adapted to handle them. The \texttt{DirichletL.py} code posted 
 \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/dirichletL.py}{here} on my GitHub repository, includes Beurling primes.  These numbers are studied in 
 Diamond \cite{wen2016} and Hilberdink  \cite{bzf2004}.
\item Binary digits of \textcolor{index}{quadratic irrational}\index{quadratic irrational} numbers. I use a simple, original recursion to compute these 
digits: see code description in section~\ref{zw23}. The Python code painlessly handles the very large integers involved in these computations. Surprisingly, $\sqrt{2}$ passes the prime test as expected, but $\sqrt{2205}$ does not. Sequences based on these digits are labeled \texttt{SQRT} in this document.
\end{itemize}

\noindent The dyadic map is impossible to implement in Python due to the way computations are performed in the CPU: the iterates 
$\{x_k\}$ are (erroneously) all equal to zero after about $45$ iterations. This is why I chose the shift map with $b=3$. In this case, the iterates are also all wrong after $45$ iterations due to propagating errors caused by limited machine precision (limited to $32$ bits). Even with $64$ or any finite number of bits, the problem persists.  However, with $b=3$, the $x_k$'s keep oscillating properly and maintain their statistical properties forever (including randomness or lack of), due to the \textcolor{index}{ergodicity}\index{ergodicity}\index{dynamical systems!ergodicity} 
 [\href{https://en.wikipedia.org/wiki/Ergodicity}{Wiki}] of the system. The result is identical to using a new seed every $45$ iterations or so. 

The dyadic map with $b=2$, in principle, could be used to compute the binary digits of the seed  
$x_0=\sqrt{2}$, but because of the problem discussed, it does not work. Instead, I use a special recursion to compute these digits. If you replace $b=2$ by $b=2-2^{-31}$ (the closest you can get to avoid complete failure) the $x_k$'s produced by the Python code,  even though also completely wrong after $45$ iterations, behave as expected from a statistical point of view: this is a workaround to  using $b=2$. The same problem is present in other programming languages.

\subsubsection{Reverse-engineering a pseudo-random sequence}

Many of the sequences defined by a recursion $x_{k+1}=g(x_k)$, where $x_0$ is the seed, can be reverse-engineered, and are thus 
 unsafe to use when security is critical. This includes sequences produced by congruential PRNGs. By reverse engineering, I mean that if you observe $m$ consecutive digits, you can easily compute all the digits, and thus correctly ``guess" the whole sequence. In the case of the Mersenne twister, $m=624$ is conjectured to be the smallest possible value even though the period is $2^{\num{19937}}-1$, see \href{https://en.wikipedia.org/wiki/Mersenne_Twister}{here}. For the shift map in base $b$, while $x_k$ is asymptotically uniformly distributed on $[0, 1]$ if $x_0$ is random, the vectors $(x_k,x_{k+1})$ lie in a very specific configuration: $x_{k+1}-x_k$ is a small integer, making the sequence $\{x_k\}$ anything but random. As a result, for any positive integer $q$, the empirical \textcolor{index}{autocorrelation}\index{autocorrelation function} 
 [\href{https://en.wikipedia.org/wiki/Autocorrelation}{Wiki}] between $(x_1, x_2,x_3,\dots)$ 
and $(x_{q+1}, x_{q+2},x_{q+3},\dots)$ computed on the infinite sequence, is equal to $1/b^q$ if $b$ is an integer $\geq 2$. 
 A good sequence should have zero autocorrelations for all $q$. 

It is possible to significantly improve the base sequence $\{x_k\}$, to make it impossible to reverse-engineer. In the case of the shift map, using $d(k)=\lfloor bx_k\rfloor$ instead of $x_k$, results in zero autocorrelations and perfect randomness if the seed $x_0$ is random. A seed such as $x_0=\sqrt{2}/2$ or $x_0=\log 2$ 
is conjectured to achieve this goal. The explanation is as follows: $d(k)$ is the $k$-th digit of $x_0$ in base $b$. Even if you were to observe $m=10^{\num{50000}}$ consecutive digits of $\sqrt{2}/2$, there is no way to predict what the next digit will be, if you don't know that $x_0=\sqrt{2}/2$. Actually even if you have that information, it is still impossible to predict the next digit. Any sequence of $m$ digits is conjectured to occur infinitely many times at arbitrary locations for a seed such as $\sqrt{2}/2$. So given any such string of $m$ digits (no matter how large $m$ is), it is impossible to tell where it takes place in the infinite sequence of digits, and thus impossible to correctly predict all the subsequent digits.    

However, because of machine precision (the problem discussed in section~\ref{twist}), the $x_k$'s generated by a computer for the shift map (or any map for that matter), eventually become periodic. Thus $\{d(k)\}$ becomes periodic too. A workaround is the use exact arithmetic to compute $d(k)$, as in my Python code in section~\ref{zw23}. Another solution is to use 
 \textcolor{index}{Bailey–Borwein–Plouffe formulas}\index{Bailey–Borwein–Plouffe formulas} [\href{https://en.wikipedia.org/wiki/Bailey\%E2\%80\%93Borwein\%E2\%80\%93Plouffe_formula}{Wiki}]
 to compute the digits. There are many BBP formulas for various good \textcolor{index}{transcendental}\index{transcendental number} seeds [\href{https://en.wikipedia.org/wiki/Transcendental_number}{Wiki}]  
  such as $x_0=\frac{\pi}{4}$, but as far as I know, none for
  the subset of \textcolor{index}{algebraic numbers}\index{algebraic number} [\href{https://en.wikipedia.org/wiki/Algebraic_number}{Wiki}] such as $x_0=\sqrt{2}/2$. 

%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.69\textwidth]{rn1b.png}  
\caption{$L_3^*(n)$ test statistic for four sequences: Python[200] and SQRT[90,91] fail}
\label{fig:rn1}
\end{figure}
%-------------------------

%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.69\textwidth]{rn2b.png}  
\caption{$|L_3(n)|$  test statistic for four sequences: Python[200] and SQRT[90,91] fail}
\label{fig:rn2}
\end{figure}
%imgpy9979_2and3.PNG
%-------------------------

\subsubsection{Illustrations}

Figures~\ref{fig:rn1} and~\ref{fig:rn2} show the core statistics of the prime test,
 defined by Formula~(\ref{oopi}): $L^*_3(n)$ and $|L_3(n)|$, for $n$ between
 $\num{1000}$ and $\num{80000}$.  If $L^*_3(n)<0.5$ or $L^*_3(n)>1.5$, the sequence $\{d(k)\}$ (the first $n$ elements) lacks true randomness; it is not 
 \textcolor{index}{strongly pseudo-random}\index{pseudo-random numbers!strongly random}. Table~\ref{tabuchi} summarizes these findings for a larger collection of sequences, 
 computed at $n=\num{20000}$.
  The notation \texttt{Python[200]} corresponds to the Python implementation 
 of the Mersenne twister, using the \texttt{random.random()} function and the seed $200$, that is, \texttt{random.seed(200)}. 
 Similarly, \texttt{SQRT[90,91]} is for the binary digits of $\sqrt{2205}$, obtained using the bivariate seed $y=90, z=91$ in the code in section~\ref{zw23}. Not surprisingly, the sequence \texttt{Base3[0.72]} fails, as $0.72=18/25$ is a rational number with a small denominator. 
 Thus $d(k)=\chi(p_k)$ is periodic with a rather small period. The column labeled \texttt{Status} in Table~\ref{tabuchi} indicates if the sequence in question fails or passes the prime test.



For convenience, I also included a type of sequences called \texttt{CounterExample}. For this type of sequences, $\chi(p_k)$ perfectly alternates between $-1$ and $+1$. One of the two resulting sequences $\{d(k)\}$ barely passes the test, the other one fails.
Now, the \texttt{Dirichlet4} sequence with seed set to $3$, has perfectly alternating $d(k)$'s and is thus non-random.  It fails the prime test, but barely.
This  means that passing this test is not a guarantee of randomness. Only failing the test is a guarantee of non-randomness. 
 

The prime test can be extended using the option \texttt{All} in the Python code. To do so, define the $L_4$ statistics as follows:
\begin{equation}
L_4(n)=\sum_{k=1}^n \chi(k), \quad L^*_4(n)=\frac{|L_4(n)|}{\sqrt{n\log\log n}}.\label{l4}
\end{equation}
Now with $L^*_4$ rather than $L^*_3$, the \texttt{Dirichlet4} sequence with the seed set to $3$ would dramatically fail the prime test,
 rather than just barely failing. It would reveal that despite the appearances, there is something definitely non random about this sequence. Indeed, it satisfies 
$\chi(4k+1)=1, \chi(4k+3)=-1$ and $\chi(2k)=0$.  The details of the $L^*_4$ version of the prime test still need to be worked out, thus I did not include it in this chapter. 

Finally, if you swap $-1 / +1$ in the $\{d(k)\}$ sequence, the new sequence may pass the test even if the original fails (or the other way around). This is the case for the sequence \texttt{SQRT[90,91]}. Also, the $L^*_3$ scale should be interpreted as an earthquake scale: an increase from $0.35$ to $0.45$, or from $1.3$ to $1.8$, represents a massive difference. A sequence with a low $L^*_3$ alternates too frequently compared to a random
sequence, resulting in a ratio $+1$ versus $-1$ too close to $50\%$ among the $d(k)$'s. The ratio in question corresponds to the 
 column labeled $P[d(k)=1]$ in Table~\ref{tabuchi}.



\begin{table}%[H]
\[
\begin{array}{lccccc}
\hline
\text{Sequence}	& \text{Seed}&	|L_3(n)|&	P[d(k)=1]&	L^*_3(n) & \text{Status}\\
\hline
\hline
\text{Base3} & 0.181517&239&49.49\%&1.1202 & \text{Pass}\\	
\text{Base3} & 0.72&81&49.93\%&0.3796 & \text{Fail}\\	
\hline
\text{CounterExample} & 1&137&49.69\%&0.6421 &  \text{Pass}\\	
\text{CounterExample} & 0&91&49.80\%&0.4265 &  \text{Fail}\\	
\hline
\text{Dirichlet4} & 1&113&50.11\%&0.7611 &  \text{Pass}\\	
\text{Dirichlet4} & 3&70&49.65\%&0.4715 &  \text{Fail}\\	
\hline
\text{Logistic} & 0.181517&115&49.82\%&0.539 &  \text{Pass}\\	
\text{Logistic} & 0.72&254&49.37\%&1.1905 &  \text{Pass}\\
\hline	
\text{Python} & 0&220&49.71\%&1.0311 &  \text{Pass}\\	
\text{Python} & 1&150&50.03\%&0.7031 &  \text{Pass}\\	
\text{Python} & 2&279&49.46\%&1.3077 &  \text{Pass}\\	
\text{Python} & 4&365&50.81\%&1.7108 & \text{Fail} \\	
\text{Python} & 100&386&49.10\%&1.8092 &  \text{Fail}\\	
\text{Python} & 200&922&52.29\%&4.3214 &  \text{Fail}\\	
\text{Python} & 500&258&49.67\%&1.2093 &  \text{Pass}\\	
\hline
\text{SQRT} & (2, 5)&146&49.63\%&0.6843 &  \text{Pass}\\	
\text{SQRT} & (90, 91)&1236&53.07\%&5.7932 & \text{Fail} \\
\hline
\end{array}
\]
\caption{\label{tabuchi} $L^*_3(n)$, for various sequences ($n=\num{20000}$); ``Fail" means failing the prime test}
\end{table}
%\vspace{1pt}
\begin{Exercise}\label{q23}{\bf -- Pseudo-random sequence generated by rational numbers}. Let $q_k=2^{-\{k\log_2 3\}}$ be a rational number ($k=1,2$ and so on), where the brackets represent 
the \textcolor{index}{fractional part function}\index{fractional part function} [\href{https://en.wikipedia.org/wiki/Fractional_part}{Wiki}]. 
For instance, $q_6=512/729$. Let $M_n$ be the median of
$\{q_1,\dots,q_n\}$. Thus if $n$ is odd, then $M_n$ is the middle term after rearranging the $q_k$'s in increasing order. Prove
 that (1) $M_n\rightarrow\sqrt{2}/2$ as $n\rightarrow\infty$, (2) the binary digit expansion of $q_k$ has period $2\cdot 3^{k-1}$ and (3) the proportion of  $0$ and $1$ among these digits, is exactly 50/50.
\vspace{1ex} \\
{\bf Solution} \vspace{1ex} \\
Solution to (2) and (3) is found \href{https://math.stackexchange.com/questions/3310862/what-is-the-period-of-the-fraction-1-3k-in-base-2-for-k-1-2-dots}{here}; (3) follows from the \textcolor{index}{equidistribution modulo $1$}\index{equidistribution modulo $1$} [\href{https://en.wikipedia.org/wiki/Equidistribution_theorem}{Wiki}] 
of the sequence $\{ k \log_2 3\}$.
%$\{k \og_2 3\}$. 
This implies that the $q_k$'s are distributed like $2^{-U}$ where $U$ is uniformly distributed on $[0, 1]$.
\end{Exercise}

\renewcommand{\arraystretch}{1.0} %%%
\renewcommand{\arraystretch}{1.4} %%%

\section{Python code}\label{pythonviz}

The code in section~\ref{zw23} focuses on big integers and computing the binary digits for a class of quadratic irrational numbers,
  using an original, not well-known recursion, possibly published here for the first time. This code is very short, and the description is accompanied 
 by pretty cool math. I recommend that you start  looking at it, before digging into the main program in section~\ref{prngpython}.

The main program deals with the prime test. Before that, section~\ref{fixp} discusses some generalities related to Python and other languages,
 pertaining to PRNG issues and fixes.  


 %--- 
\subsection{Fixes to the faulty random function in Python}\label{fixp}

The default Python function to generate pseudo-random numbers is \texttt{random.random()}, available in the \texttt{random} library.
It is based on the \textcolor{index}{Mersenne twister}\index{Mersenne twister}\index{pseudo-random numbers!Mersenne twister} [\href{https://en.wikipedia.org/wiki/Mersenne_Twister}{Wiki}] -- 
a popular 
\index{pseudo-random numbers!congruential PRGN}
 \textcolor{index}{congruential generator} -- 
and documented \href{https://docs.python.org/3/library/random.html}{here}. As discussed in section~\ref{twist}, it is not suitable for cryptographic and other applications where pure randomness is critical. Indeed, the documentation comes with the following warning: ``The pseudo-random generators of this module should not be used for security purposes". 

One way to improve  \texttt{random.random()} is to avoid particularly bad seeds, such as $200$ or $4$, in the \texttt{random.seed()} call. You may also use binary digits of some 
\textcolor{index}{quadratic irrational numbers}\index{quadratic irrational} [\href{https://en.wikipedia.org/wiki/Quadratic_irrational_number}{Wiki}], using the Python code in section~\ref{prngpython}. Again, it is a good idea to check, using the prime test proposed in this chapter, which irrational numbers to avoid. Also this method may be slow, as it involves working with very big integers. A workaround is to store large tables of pre-computed digits in a secure location. The number of quadratic irrationals you can choose from is infinite. Also, your digit sequence
 should never start with the first binary digit of such numbers, but rather at a random position, to make hacking more difficult. 

For instance, to generate your sequence $\{d(k)\}$, set $d(3k)$ to $\delta(g_1(k),\alpha_1)$, set 
 $d(3k+1)$ to $\delta(g_2(k),\alpha_2)$, and set  $d(3k+2)$ to $\delta(g_3(k),\alpha_3)$ where 
\begin{itemize}
\item The numbers $\alpha_1, \alpha_1, \alpha_3$ are three quadratic irrationals, say $\sqrt{2},\sqrt{10},\sqrt{41}$,
\item The number $(\delta(k,\alpha)+1)/2$ is the $k$-th binary digit of the quadratic irrational $\alpha$, 
\item The functions $g_1,g_2,g_3$ are used for scrambling: for instance, 
$g_1(k)=5\cdot 10^5 +2k$, $g_2(k)=3\cdot 10^5 +3k$, and $g_3(k)=7\cdot 10^6 -k$.
\end{itemize}
Another solution is to use for your sequence $\{d(k)\}$ a \textcolor{index}{bitwise XOR}\index{XOR operator} [\href{https://en.wikipedia.org/wiki/Bitwise_operation}{Wiki}] on two pseudo-random sequences: the binary digits of (say) $\sqrt{2}$ and $\sqrt{41}$, starting at arbitrary positions.

There are also Python libraries that provide solutions suitable for cryptographic applications. For instance, 
\texttt{os.urandom()} 
uses the operating system to create random sequences that can not be seeded, and are thus not replicable.
See \href{https://docs.python.org/3/library/os.html#os.urandom}{here} and 
\href{https://docs.python.org/3/library/secrets.html\#module-secrets}{here}.

\subsection{Prime test implementation to detect subtle flaws in PRNG's}\label{prngpython}

The code presented here performs the prime test, computing $L_3(n)$. The variable \texttt{nterms} represents $n$, and it is set to
 $\num{10000}$. Rather than directly computing $L_3(n)$, the code iteratively computes more granular statistics, namely \texttt{minL} and \texttt{maxL}; $L_3(n)$ is the maximum between \texttt{-minL} and \texttt{maxL}, obtained at the last iteration. 

The seeds and the sequences are initialized in the main part, at the bottom. The default category \texttt{nonSquare} is used for $L_3$. The other categories, \texttt{Prime} and \texttt{All}, are respectively for $L_1$ defined in Formula~(\ref{oopi}) and $L_4$ defined in Formula~(\ref{l4}). If you use the function \texttt{createSignHash()} rather 
than the default \texttt{createSignHash2()}, you can easily compute $L_2$. The code is somewhat long only because it covers all the options discussed 
 in section~\ref{twist}, and more. It heavily relies on hash tables (dictionaries in Python) rather than arrays, because the corresponding arrays would be  rather sparse, consume a lot of memory, and slow down the computations. In addition, the code can easily handle Beurling primes (non-integer primes) thanks to the hash tables. A lengthier version named \texttt{dirichletL.py}, computing the orbit of $L_P(z,\chi)$ for $z$ in the complex plane
 when $\sigma=\Re(z)\geq \frac{1}{2}$ is fixed, for any set of primes $P$ (finite or infinite) including Beurling primes, is available on GitHub, \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/dirichletL.py}{here}. 

The Python code does not use any exotic library other than \texttt{primePy}.  To install this library,
 type in the command \texttt{pip install primePy} on the Windows Command Prompt or its Unix equivalent, as you would to install any library.
There is a possibility that some older versions of Python would require the \texttt{BigNumber} library. The code was tested under Python 3.10.
The source code, featured below, is also on GitHub: look for \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/randomNumbersTesting.py}{\texttt{randomNumbersTesting.py}}.  \\

\begin{lstlisting}
# Test randomness of binary sequences via the law of the iterated logarithm
# By Vincent Granville, www.MLTechniques.com

import math
import random
import numpy as np
from primePy import primes

#--
def createRandomDigits(method,seed): 
  primeSign={}
  idx=0
  if method=='SQRT':
    y=seed[0]
    z=seed[1]
  elif method=='Python':
    random.seed(seed)
  else:
    x=seed
  start=2
  if method=='Dirichlet4':
    start=3
  for k in range(start,nterms):
    if k%2500==0:
      print(k,"/",nterms)  
    if primes.check(k):
      primeSign[k]=1
      if method=='SQRT':
        if z<2*y:   
          y=4*y-2*z
          z=2*z+3
        else:     
          y=4*y
          z=2*z-1
          primeSign[k]=-1
      elif method=='Dirichlet4':
        if k%4==seed:
          primeSign[k]=-1
      elif method=='CounterExample':
        idx=idx+1
        if idx%2==seed:
          primeSign[k]=-1
      elif method=='Python':
        x=random.random()
      elif method=='Logistic':
        x=4*x*(1-x)
      elif method=='Base3':
        x=3*x-int(3*x)
      if method in ('Python','Logistic','Base3') and x>0.5:
        primeSign[k]=-1
  return(primeSign)

#--
def createSignHash2():
  signHash={}
  signHash[1]=1
  for p in primeSign:
    oldSignHash={}
    for k in signHash:
      oldSignHash[k]=signHash[k]
    for k in oldSignHash:
      pp=1
      power=0
      localProduct=oldSignHash[k]  
      while k*p*pp<nterms:
        pp=p*pp
        power=power+1
        new_k=k*pp
        localProduct=localProduct*primeSign[p]
        signHash[new_k]=localProduct  
  return(signHash)

#--
def createSignHash():
  # same as createSignHash() but for square-free integers only
  signHash={}
  signHash[1]=1
  for p in primeSign:
    oldSignHash={}
    for k in signHash:
      oldSignHash[k]=signHash[k]
    for k in oldSignHash:
      if k*p<nterms:
        new_k=k*p 
        signHash[new_k]=oldSignHash[k]*primeSign[p]  
  return(signHash)
 
#--
def testRandomness(category):
  signHash=createSignHash2()
  isSquare={}
  sqr=int(math.sqrt(nterms))
  for k in range(sqr):
    isSquare[k*k]=1
  count=0
  count1=0
  sumL=0
  minL= 2*nterms
  maxL=-2*nterms
  argMin=-1
  argMax=-1
  for k in sorted(signHash):
    selected=False
    if category=='Prime' and k in primeSign:
      selected=True
    elif category=='nonSquare' and k not in isSquare:
      selected=True
    elif category=='All':
      selected=True
    if selected==True:
      if signHash[k]==1:
        count1=count1+1  
      count=count+1
      sumL=sumL+signHash[k]
      if sumL<minL:
        minL=sumL
        argMin=count
      if sumL>maxL:
        maxL=sumL
        argMax=count
  return(minL,argMin,maxL,argMax,count,count1)

#--
# Main Part. Requirements:
#   0 < seed < 1 for 'Base3' and 'Logistic'; rational numbers not random
#   seed=(y,z) with z>y, z!=2y, y!=2x and x,y>0 are integers for 'SQRT'
#   swapping -1/+1 for seed=(90,91) in 'SQRT' does well, the original does not

seedMethod={}
seedMethod['Python']=(0,1,2,4,100,200,500)
seedMethod['Logistic']=(0.181517,0.72)
seedMethod['Base3']=(0.181517,0.72)
seedMethod['SQRT']=((2,5),(90,91))
seedMethod['Dirichlet4']=(1,3)
seedMethod['CounterExample']=(1,0)
categoryList=('Prime','nonSquare','All')

nterms=10000

OUT=open("prngTest.txt", "w")
for method in seedMethod:
  for seed in seedMethod[method]:
    for category in categoryList:

      primeSign=createRandomDigits(method,seed)
      [minL,argMin,maxL,argMax,count,count1]=testRandomness(category)

      string1=("%14s %9s|%5d %5d|%5d %5d|%5d %5d|" % (method,category,\
        minL,maxL,argMin,argMax,count1,count))+str(seed) 
      print(string1)
      string2=("%s\t%s\t%d\t%d\t%d\t%d\t%d\t%d\t" % (method,category,\
        minL,maxL,argMin,argMax,count1,count))+str(seed)+'\n'
      OUT.write(string2)
    
OUT.close()
\end{lstlisting}

\subsection{Special formula to compute 10 million digits of $\sqrt{2}$}\label{zw23}

The purpose of this code is twofold: to show you how to process integers with millions of digits in Python, and to offer a simple mechanism to compute the binary digits of some quadratic irrational numbers such as $\sqrt{2}/2$. The first problem is solved transparently with no special code or library  
 in Python 3.10. In short, this is a non-issue. With older versions of Python, you might have to install the \texttt{BigNumber} library. See
 documentation \href{https://pypi.org/project/BigNumber/}{here}. Nevertheless, it would be a good idea to track the size of the integers that you are working with (\texttt{y} and \texttt{z} in my code), as eventually their size will become the bottleneck, slowing down the computations. 

As for the actual computation of the digits, it is limited here to $\num{10000}$ digits, but I compare these digits with those obtained from an external source: Sagemath, see \href{https://mltblog.com/3uMZQ4s}{here}.  It shows, as it should, that both methods produce the same digits, for the number 
 $\sqrt{2}/2$ in particular.

\noindent The special recursion used for the digit computation is as follows:  \vspace{1ex} \\
\noindent  \textcolor{white}{0000}{\bf If}  $ z_k  <2y_k$   {\bf then}   \\
  \textcolor{white}{000000}  $y_{k+1}=4y_k-2z_k$\\
 \textcolor{white}{000000} $z_{k+1}=2z_k+3$\\
 \textcolor{white}{000000} $d(k)=1$ \\
\textcolor{white}{0000}{\bf else} \\  
\textcolor{white}{000000} $ y_{k+1}=4y_k$\\
\textcolor{white}{000000} $ z_{k+1}=2z_k-1$\\
\textcolor{white}{000000} $ d(k)=0$. 

\noindent The bivariate seed (the initial condition) is determined by the values of $y_0$ and $z_0$. You need $z_0>y_0$ and $z_0\neq 2y_0$. Then the binary digits $d(k)$ are those of the number 
 $$x_0 = \frac{-(z_0-1) + \sqrt{(z_0-1)^2+8y_0}}{4},$$ 
see \href{https://mltblog.com/3REtOB9}{here}. In particular, if $y_0=2, z_0=5$, then $x_0=-1+\sqrt{2}$. Using the change of variables
  $u_k=2y_k-z_k$ and $v_k = 2z_k+3$, the recurrence can be rewritten as: \vspace{1ex} \\

%\pagebreak %%

\noindent  \textcolor{white}{0000}{\bf If}  $ u_k>0$   {\bf then}   \\
  \textcolor{white}{000000} $u_{k+1}=4u_k -v_k$ \\
 \textcolor{white}{000000} $v_{k+1} = 2v_k + 3$\\
 \textcolor{white}{000000} $d(k)=1$\\
\textcolor{white}{0000}{\bf else} \\  
\textcolor{white}{000000} $u_{k+1}=4u_k + v_k-2$\\
\textcolor{white}{000000} $v_{k+1} = 2v_k-5$\\
\textcolor{white}{000000} $ d(k)=0$. 

\noindent Now $v_k-5$ is divisible by $8$. Let $w_k=(v_k-5)/8$. We have $d(k)=1$ if $w_{k+1}$ is odd, otherwise $d(k)=0$. We also have the 
 following one-dimensional backward recursion, allowing you to compute the digits backward all the way down to the first digit:\vspace{1ex} \\
\noindent  \textcolor{white}{0000}{\bf If}  $w_{k+1}$ is odd,  {\bf then}   \\
 \textcolor{white}{000000} $v_{k} = (v_{k+1} -3)/2$\\
\textcolor{white}{000000} $d(k)=1$\\
\textcolor{white}{0000}{\bf else} \\  
\textcolor{white}{000000} $v_{k} = (v_{k+1}+5)/2$\\
\textcolor{white}{000000} $d(k)=0$.

\noindent These recursions are reminiscent of the unsolved \textcolor{index}{Collatz conjecture}\index{Collatz conjecture} [\href{https://en.wikipedia.org/wiki/Collatz_conjecture}{Wiki}]. Below is the source code, also available on GitHub: look for \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/randomNumbers-sqrt2.py}{\texttt{randomNumbers-sqrt2.py}}.  \\


\begin{lstlisting}
# Comparing binary digits of SQRT(2) obtained with two different methods

# Method 1:
# 10,000 binary digits of SQRT(2) obtained via https://mltblog.com/3uMZQ4s
# Using sagemath.org. Sagemath commmand: N(sqrt(2),prec=10000).str(base=2)

sqrt2='011010100000100111100110011001111111001110111100110010010000100010110010111110110\
0010011011001101110101010010101011111010011111000111010110111101100000101110101000100100\
1110111010100001001100111011010001011110101100100001011000001100110011100110010001010101\
0010101111110010000011000001000011101010111000101000101100001110101000101100011111111001\
1011111101110010000011110110110011100100001111011101001010100001011110010000111001110001\
1110110100101001111000000001001000011100110110001111011111101000100111011010001101001000\
1000000010111010000111010000101010111100011111010011100101001100000101100111000110000000\
0100011011110000110011011110111100101010110001101111001001000100010110100010000100010110\
0010100100011000001010101111000111001000101111011111000100111000110011110001101101010110\
1010001010001110001011101101111110100111011100110010110010101001100011010000110011000111\
1100111100100001001101111101010010111100010010000011111000001101101110010110000010111011\
1010101010010010100000100010011001000001000000110010100100101010000001001110010100101010\
1101101101100011111101000011101111110111110100110100111010000000101100111010111100100100\
1111100000110001000010011001001101101010111100110101010010100010110110010100011011100011\
0011110011010000011011011011111000001000110110110001110000000100000100110111000000000111\
1111100011001000110101001011110011001100101010010111101001111101111011110110100001111010\
1111111111011010100001101111100011111111001010001000100001001100000111110111101010000001\
1000100100000111110111101010101000000111000010110000011111110010111101110111101010001011\
1101111101110000110011000110001000001110001010001011101010111111110101111100111011001011\
0100100100111101001010011101100011111110101100101110001000001011111101111111100001011100\
0011111101001110110001111101111000000011111001101011001100111001000001011110010111111100\
1010000000010111000010010001100111100001011110100100101001010101110000001000110101111111\
0011111000111101111011110010100011111010100011001110110100110101111100011000001010010111\
1001100011001111100011111000010100100001011111001110110100100101001101100000101011110001\
1000001000010110101100001001111101101001000011111001001001001001111010110110111000111111\
0000010110111001101001010010000001101100000100111011110111010001001000100101001100000111\
1010100111010101010110100001110111010100011001000011111011101001000100111110100011010100\
1111110100100000011000010111110001000001111101101110110100101001110111110110101100011001\
0011001100001001100110111010111000010100001110110101001000001000101110000111101000100110\
0111010000001101000010000100011110101101110001110000011000000111101100100000001101110101\
0011101101000110100011101100110100001000111100101101100010111001101010110010111001011011\
1000000111111110011010101000000110000110100000100101001010000101101011001000000011000010\
0000001111011110110111111000110110111101001000100010100101000101011010011110010010010001\
1000110100010011100011000000000101110110100000010101000101101010110101100000100000111111\
1010101110111100110111000001101110100001100011001101101010010000011000111111110011111111\
1111111010101110101011111100100011100010000100110000000110011011011110101011100001001101\
0000100100001011101101001011110100100011011001111100010111100100000010110110111001001110\
0100101101110010111000111010110000010100001111110001000100011110000100010100000101001101\
1100001000000001100110011101101110000101100111001011011110110011000101011101110011100011\
1100100001011100010011010001101001101111010011000000111001011110010001000000010001101000\
1100001001111111111110000100010010001010011010000111001111010101001011110101001100110011\
0110110110010011110001111001110011100011111100101001101011000001001001010101100111000010\
0100000101010111000111001010101000011100000110101010101000100010110100010000110001100010\
1110111100011001111011000000011010100001101000000101111111010000101111100101001111011001\
0001011111001011100011100101011100000000111101011110101011101110001101110000010010110110\
0110000101000001110011110000011011101101010100100011100000010001100011010100111111100001\
1111000111100110010001110110011011000101000000111010010001001010110100010011100000010110\
1011010100000100000010001111101101110011000100111110110001110101000101001100100111010000\
1010001100110111100000011010000110000011110001000100010100000000100100001001101100101001\
1110100111110011011101100111101010010110011000111010100100010011101011101010010001100011\
0110101110001100011001111000100100000001100100101101011111001010100100110111111010111011\
0010110011001111000101101001101001011000100110111011010100001100111101111011000111001001\
0000000001010011111111010101000110010000001011100110101011001111100001010111010001111010\
0111100111010011101111111100000101111010001101101001101110101110111000100010111100000000\
1010111101101101001010110110111111010101111000110110000010111000010001011001000110101011\
1111110111010111101000001110111111111101100100101101101101110001111011101111000111111000\
0011100001010111011101111001110000110110100100111101011111101011010111101000100011000001\
0001000001010010101100010101101110100000001110010100111111100011011011010111100000010110\
1111110110000011011110001101110000010100110111011011011110001100111111110000101101100100\
1110100111000001000011000011011001110100001001101110101011100011011000101100101010010011\
0000111001111010010000100011100011010101010111001101001110110000000000111100101011110010\
1000100010110111100011000001010001111100000101001001111110110001001011010001110101101111\
0010100101111011100011100000010110101101001010001011101101010010100101100000100101001000\
1000000110010101011110010010100001110000111110000111101001001101111110100001111001110111\
1101000101011110110001100000101101001010011101000010111011100101000100010101100101000010\
0111111101101000000011011001001100000101001000101011101100000111011101000001101001101010\
1011000110110000001110111010001000101011001111010010110010000111110101010100010011111101\
0110111011101010111101000100000010100101001011101011011011011110100101010001000100111100\
0111101000010010010101110110001110001101000010101001000000011011100001011101001100110010\
1000111101100110111001011011110110110100000010111100010000110010010111110010101101100011\
0111001001001100101000100101011010000000100110000110011000110000001110101100100101000110\
1011001101010001001101101101111100010000110010011110011110101010111010100110001100100110\
1011001101010100011100001100010010101111010100100111010010101111010010110010111011101010\
1000111111110100000111101100000011011001100001100101110111010101000010011101111011100010\
0000011110100101101001011010101011110110110110000010101001110100101111100110111110000101\
1000111011111111110001010010010100011010011001000011011001010010100011100001110101011100\
1000001010110001011001111001110110111000110010010101101111000011110010000110001101000010\
0111000101110110111101111100111001100000101101101000110111101111111111000001011110001110\
1010100111111110011111011000000010000101011110111001000010110110011001001011001100101100\
1010010001010101000011110111000011001000011111000111101000101111000011100001111100011000\
0100101101101011001011110000000011000001101010101011011100000101111110101001110110100111\
1110011111000010101100010010010101101011011010100100011011000011000001011001011001010000\
0110001111101000100110010100010011110101101000110100110011011111101010101101111110111000\
0101001101001111100011110101001001111100100001001111001101010100001001011010100011000001\
0100000110101001001010010010101101101101011111001001101110001110011110111011111100101101\
1101110100000001010100111011111110000100010110110001101001110000101111010100010111011001\
1001000000000011101011101000000110000011001100100001100011110111111001010000100010110000\
1001000000001010010011110111011100010111010100000010000100111010010011101110000110000111\
1111111110111100001001000011010011110111110010000110100111110000011110100001010101010111\
1111011110111100101001100011111011101001011101011111101101111000010110110101111101111101\
1001110000000101110011001011011101101110111101110001111101110111000110011011011010010101\
1011000111001001011100111101111000000011110001011001001000011111000000001101001110100100\
0111110110011111011000011001000111000101110001001111101011001000111110110101010010000110\
1110110001010100110010010111001110001111101011101000101001101000100111111011011010100010\
1101111110001010110111110100001111010101110011010101001010110000111010011111011010100000\
0011111011010011011000101111011011001101011100000011101111110111010000100011110110101010\
0100100000101011101011111101110001111100101001111111001010110000101100101000110101110110\
0100000100000010110000101111000000001011001100101101000000001011011101101001111100111011\
0100111100101101111011000110011110010011101000100111111011011000001011100101100001110101\
1111110001101000000000110100011001011101001100111011101011010100010010001111110001001111\
0010110000000001010001111110101110000001001100110010001101001100101010101001010001101010\
0000000000011111111111000001001000010101011111110111100001011011000101001001010100010000\
0010100010110011110110100010010011110100101010011001100111011001011011000111100111101100\
0110111001110100111000100011100100111001111100100010101011001101100001110010111101100101\
0000010011110001010011111110000101110111000101110111100010001010001011011011101110010101\
0110000111101101110100101010101011101000100100011111001001111101110101110000000111101101\
0011111001101001011010100100101110001110110000011110011001110000000111011111001111011011\
0111000101011001110111011111010100011011011001111000111110011100010011011100100001010101\
0100101100110000100110101001101101110010101011000010010111011000101001010001010001001000\
1011111110011110100001111011011001101110101011111110000101000001010000111111100001001001\
1111110001000011000111101001011011000110000011100100000100100000100000001000100101100100\
0110010111100111000110001000000001100101111000111000011101010111000100101110001110111010\
1001111011110010110011011011101100101110110000101001011101011101000110101111001011110100\
0001000010010001011000111010101110011111101101100010011011111010001'

size=len(sqrt2)

# Method2:
# 10,000 binary digits of SQRT(2) obtained via formula at https://mltblog.com/3REtOB9
# Implicitly uses the BigNumber Python library (https://pypi.org/project/BigNumber/)

y=2
z=5
for k in range(0,size-1): 
    if z<2*y:
        y=4*y-2*z
        z=2*z+3
        digit=1 
    else:
        y=4*y
        z=2*z-1
        digit=0 
    print(k,digit,sqrt2[k])
\end{lstlisting}


\section{Military-grade PRNG Based on Quadratic Irrationals}\label{pivizintrobvbc}

If you produce simulations or create \textcolor{index}{synthetic data}\index{synthetic data} 
%\textcolor{index}{synthetic data} %%% 
 that requires billions or trillions of random numbers, you 
 need a pseudo-random number generator (PRNG) that is not only fast, but in some cases, truly emulates randomness.
 Usually you can't have both. Congruential PRNGs are very fast and can be pretty good at emulating randomness.
 The Mersenne twister available in Python and in other languages has a very large period, more than enough for any
 practical need. Yet depending on the seed, it has flaws caused by the lack of perfect randomness in the distribution of prime numbers. These were revealed by the \textcolor{index}{prime test}\index{prime test (of randomness)} in section~\ref{twist}. To the contrary, PRNGs based on irrational numbers exhibit
 stronger randomness if you skip the first few digits and carefully choose your numbers. But they tend to be very slow.

In this section I propose a new approach to obtain billions of trillions of digits  from combinations of 
\textcolor{index}{quadratic irrationals}\index{quadratic irrational} [\href{https://en.wikipedia.org/wiki/Quadratic_irrational_number}{Wiki}] such as
 $\sqrt{2}$ or $\sqrt{7583}$, based on the algorithm in section~\ref{zw23}. The goal is to produce replicable random numbers. If you want to use them for strong encryption, you need to use a seed that is hardware-generated so that the same seed is never used more than once. 

\subsection{Fast algorithm rooted in advanced analytic number theory}\label{nt6hg4xz}

The idea to get a fast algorithm is simple. Instead of producing $n$ digits from a single number, I generate 
 $r$ digits from $m$ different numbers, with $n=rm$. While the Python code relies only on basic additions and very
 few operations, the computation of $n$ binary digits of a single irrational number involves very large integers. The  
\textcolor{index}{computational  complexity}\index{computational complexity} is $O(n^2)$. If instead you
 generate $r$ digits from $m$ numbers, the computational complexity drops to $O(rm^2)$. In the most extreme and very interesting case where $r=n$ and $m=1$, the computational complexity is $O(n)$, just as fast as the Mersenne twister. 
The method is based on two deep results in number theory:
\begin{itemize}
\item The binary digits of a quadratic irrational behave as an infinite realization of independent Bernoulli trials with equal proportions of $0$ and $1$. This unproven conjecture is one of the most difficult unsolved problems in mathematics. Very strong empirical results involving trillions of digits, support this hypothesis.
\item Two sequences of digits from irrational numbers that are linearly independent over the set 
$\mathbb{Q}$ of rational numbers,
 have zero cross-correlation. The correlation is defined as the empirical correlation in this case.
\end{itemize}
The latter is a consequence of the following theorem: the correlation $\rho(p,q)$ between the sequences
 $(\{p b^k\alpha\})$ and $(\{q b^k\alpha\})$ indexed by $k=0,1$ and so on, where $p,q$ are positive integers with no common divisors, $b > 1$ is an integer, and $\alpha$ is a positive irrational, is equal to $\rho(p,q) = (pq)^{-1}$. Here $\{\cdot\}$ denotes the fractional part function. 

A proof (by \href{https://www.analysisandinference.com/team/william-a-huber-phd}{William Huber}) of this unpublished theorem  can be found \href{https://stats.stackexchange.com/questions/422354/correlations-between-two-sequences-of-irrational-numbers}{here}, with additional discussion on this topic, \href{https://stats.stackexchange.com/questions/450922/cross-correlations-in-digit-distributions}{here}. 
Note that $\lfloor b \cdot \{b^k \alpha\}\rfloor$ is the $k$-th digit of $\alpha$ in base $b$. The brackets represent the integer part function. Thus, if $\alpha_1=p\alpha$ and $\alpha_2=q\alpha$, the correlation between the sequences
 $(\{b^k \alpha_1\})$ and $(\{b^k \alpha_2\})$ is $\rho(p,q)=(pq)^{-1}$. If $\alpha_1,\alpha_2$ are irrational and 
 linearly independent over $\mathbb{Q}$,
 then the only way you can write $\alpha_1=p\alpha,\alpha_2=q\alpha$ is by letting $p,q$ tends to infinity, thus the correlation vanishes. It implies that the correlation between the digit sequences of $\alpha_1$ and $\alpha_2$ is zero.

There is more number theory involved. In particular, the method uses the new algorithm 
 described in section~\ref{zw23} to compute the binary digits of quadratic irrationals. It is also connected
 to \textcolor{index}{square-free integers}\index{square-free integer}, and approximations of irrational by rational numbers which is linked to continued fractions.
 Square-free integers [\href{https://en.wikipedia.org/wiki/Square-free_integer}{Wiki}] are also discussed in section~\ref{sprng}. They represent 
61\% of all positive integers: the exact proportion is $6/\pi^2$. 

\subsection{Fast PRNG: explanations}\label{5fdi75fg}

Each positive integer  $c$ can be written as $c=ab$, where $a$ is a square, and $b$ is square-free. For instance, if $c=3^5 \times 13^6 \times 19$, then $a=3^4\times 13^6$ and $b=3\times 19$. If $c$ is a square, then $c=a$ and $b=1$. 

The quadratic irrationals used here are characterized by a bivariate \textcolor{index}{seed}\index{seed (dynamical systems)} $(y_0, z_0)$. Given a seed, the successive iterations produce the binary digits of the number  
 \begin{equation}
x_0 = \frac{-(z_0-1) + \sqrt{(z_0-1)^2+8y_0}}{4}.\label{butaneneon}
\end{equation}

The connection to square-free integers is as follows: $c=(z_0-1)^2 +8y_0$ can not be a square. I use the notation
 $c=ab$ where $a$ is the square part of $c$, and $b$ is the square-free part. Thus, we must have $b>1$. I use a large number of seeds to generate a large number of quadratic irrationals. The cross-correlation between the two digit sequences in any pair of quadratic irrationals must be zero. Based on the theory in section~\ref{nt6hg4xz}, it means that once a seed produces a specific $b$, any future seed with the same $b$ must be rejected. This is accomplished using the variable \texttt{accepted} in the code, along with
 the \textcolor{index}{hash table}\index{hash table} (dictionary in Python) \texttt{squareFreeList}. The key for this hash table is actually $b$.

The number of digits produced for each quadratic irrational is specified by the parameter \texttt{size}. The total number of quadratic irrationals is determined by \texttt{Niter}. Not all of them are used: a few are rejected for the reason just mentioned. All the binary digits of all the accepted quadratic irrationals are stored in the hash table \texttt{digits}. For instance,
 \texttt{digits[(b,k)]} is the $k$-th digit of the quadratic irrational with square-free part $b$. The
 seed $(y_0,z_0)$ corresponding to this number is \texttt{squareFreeList[b]}.

Due to the particular choice of seeds in the Python code (with $y_0=1$), many quadratic irrationals are close to zero. More specifically, 
 Formula~(\ref{butaneneon}) yields the following approximation: $x_0\approx  y_0/(z_0-1)$.
 So the first few digits are biased and should be skipped. This is accomplished via the parameter \texttt{offset}. There are
 other problematic quadratic irrationals such as those with $z_0-1$ being a power of $2$. A future version of this algorithm can reject these quadratic irrationals, and also reject those that are too close to a number already in the hash table. However, increasing the parameter \texttt{offset} is the easiest option to eliminate these problems. The next step is to run 
 a standard battery of tests such as the \textcolor{index}{Diehard tests}\index{pseudo-random numbers!Diehard tests},  and check whether this PRNG passes all of them depending on the parameters and configuration. 

I haven't tested yet
 the algorithm with \texttt{size=1}: this is the fastest way to generate many digits (assuming \texttt{Niter} is increased
 accordingly), and possibly the best choice assuming \texttt{offset} is large enough. In particular, if you extract just one digit of each quadratic irrational, there is a faster way to do it, see section~\ref{imprsqrt2mnb}.
  
\subsection{Python code}\label{tr3te4}

Now that I explained all the details about the algorithm, here is the Python code. It is also available 
 on GitHub, \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/strongprng.py}{here}, under the name \texttt{stronprng.py}.    \vspace{1ex}

\begin{lstlisting}
# By Vincent Granville, www.MLTechniques.com
 
import time
import random
import numpy as np

size =  400              # number of binary digits in each number
Niter =  5000            # number of quadratic irrationals
start = 0                # first value of (y, z) is (1, start)
yseed = 1                # y = yseed
offset =  100            #    skip first offset digits (all zeroes) of each number
PRNG = 'Quadratic' # options: 'Quadratic' or 'Mersenne' 
output = True            # True to print results (slow)

squareFreeList = {}
digits = {}
accepted = 0  # number of accepted seeds

for iter in range(start, Niter): 

    y = yseed # you could use a non-fixed  y instead, depending on iter
    z = iter
    c = (z - 1)**2 + 8*y 

    # represent c as a * b where a is square and b is square-free
    d = int(np.sqrt(c))
    a = 1 
    for h in range(2, d+1):
        if c % (h*h) == 0:  # c divisible by squared h
            a = h*h
    b = c // a   # integer division

    if b > 1 and b not in squareFreeList:
        q = (-(z - 1) + np.sqrt(c)) / 4  # number associated to seed (y, z); ~ y/(z-1)
        squareFreeList[b]=(y,z)          # accept the seed (y, z)
        accepted += 1

start = time.time()

for b in squareFreeList: 

    y = squareFreeList[b][0]
    z = squareFreeList[b][1]
         
    for k in range(size): 

        # trick to make computations faster
        y2 = y + y   
        y4 = y2 + y2
        z2 = z + z

        # actual computations
        if z < y2:
            y = y4 - z2
            z = z2 + 3
            digit = 1 
        else:
            y = y4
            z = z2 - 1
            digit = 0
        if k >= offset:
            digits[(b,k)] = digit

end = time.time()
print("Time elapsed:",end-start)

if output == True:
    OUT=open("strong4.txt","w")
    separator="\t"  # could be "\t" or "" or "," or " "
    if PRNG == 'Mersenne':
        random.seed(205) 
    for b in squareFreeList:
        OUT.write("["+str(b)+"]")
        for k in range(offset, size):
            key = (b, k)
            if PRNG == 'Quadratic':
                bit = digits[key] 
            elif PRNG == 'Mersenne':
                bit = int(2*random.random())
            OUT.write(separator+str(bit))
        OUT.write("\n")
    OUT.close()

print("Accepted seeds:",accepted," out of",Niter)


\end{lstlisting}

\subsection{Computing a digit without generating the previous ones}\label{imprsqrt2mnb}

If \texttt{offset} is large, you spend time computing digits (at the beginning of the binary expansion of each quadratic irrational) that you will use only   to get the subsequent digits. 
 There is a more efficient approach: use a different seed $(y_0', z_0')$. The new seed has the benefit of keeping
 the square-free part $b$ unchanged. To get the digit in position $k$ in one shot (assuming the first position corresponds to $k=0$), use the 
 seed $y_0'=2^{2k} y_0, z_0'=2^k(z_0-1) + 1$. Then $x_0' = 2^k x_0$, so your quadratic irrational is multiplied by $2^k$. This is a direct consequence
of Formula~(\ref{butaneneon}). However, this won't work if $y'_0 \geq z'_0$, which is always the case when $k$ is large enough. 
So you need to work with a different $(y'_0, z'_0)$, one that leads to $x'_0 = \lfloor 2^k x_0 \rfloor$. I describe how to do it efficiently
in section~\ref{puosaw}. 

Finally, by digits, I mean the binary digits on the right starting after the decimal point. The position $k$ that you select for the starting digit may be randomized: it can depend on the quadratic irrational. This makes it
 very difficult to reverse-engineer your sequence of random digits.

\subsection{Security and comparison with other PRNGs}

The spreadsheet \href{https://github.com/VincentGranville/Experimental-Math-Number-Theory/blob/main/Source-Code/strongprng.xlsx}{\texttt{strongprng.xlsx}} (on GitHub) compares the quadratic irrational PRNG with the 
Mersenne twister. The test involves $\num{5000}$ quadratic irrationals, one per row. Based on the acceptation rule, only 4971 were used. For each of them, I computed 400 binary digits and skipped the first 100 as suggested in the
 methodology. So in total, there are 4971 blocks, each with 300 digits. The tab corresponding to the Mersenne twister has the same block structure with the same number of digits: it was produced using the option \texttt{PRNG='Mersenne'} in the Python code. The numbers in brackets in column D represent the block ID: the number $b$ in the case of the quadratic irrational PRNG, and nothing in particular in the case of the Mersenne twister.

I computed some summary statistics:  digit expectation and variance for each column and for each row, as well as cross-correlations between rows, and also between columns. The results are as expected and markedly similar when comparing the two PRNG's. You would expect any decent PRNG to pass these basic tests, so this is not a surprise. You need more sophisticated tests to detect hard-to-find departure from randomness; that was the purpose of section~\ref{twist} with the prime test.  Figure~\ref{fig:rn1digyt} shows 5000 correlation values. They are not statistically different from zero, and 
 independent despite the fact that they correspond to seeds produced in chronological order. Indeed, their joint distribution is identical to that produced with the Mersenne twister (not shown in the picture, but included in the spreadsheet).   

%-----------------------------vince/riemann2and3.mp4
\begin{figure}%[H]
\centering
\includegraphics[width=0.88\textwidth]{correldigitsB.png}  
\caption{Correlations are computed on sequences consisting of 300 binary digits}
\label{fig:rn1digyt}
\end{figure}
%-------------------------

Whether using the Mersenne twister or quadratic irrationals, the compression factor -- based on the zip tool applied to the 1.5 million raw digits -- is
 about the same and equal to almost 8. This is the worst achievable compression factor: each character (8 bits) representing a digit is
 turned into 1 bit of information. It means that the zip tool is unable to detect any pattern in the digits that would allow
 for any amount of real compression.   

\subsubsection{Important comments}

There are very few serious articles in the literature dealing with digits of irrational numbers  to build PRNG's. It seems that this idea was abandoned long ago due to the computational complexity and the erroneous belief that it defeats 
 the non-deterministic nature of randomness. It is my hope that my quadratic irrational PRNG debunks all these myths. Note that there has been attempts to use chaotic dynamical systems to build PRNG's. See for instance~\cite{loginew} and~\cite{expmdb2002}. My upcoming book on dynamical systems explores  many new related methods in great details. 

Also, my PRNG's, by combining thousands to billions of quadratic irrationals, present some similarity to 
 \textcolor{index}{combined linear congruential generators}\index{pseudo-random numbers!combined generators} [\href{https://en.wikipedia.org/wiki/Combined_linear_congruential_generator}{Wiki}]. It avoids the drawbacks that these generators are facing.  In the context of congruential generators, combining is used to increase the period and randomness. In the case of
 quadratic irrational PRNG's, the period is always infinite, and the goal is to increase security, but not randomness which is already maximum with just one number. Also using many quadratic irrationals each with a few digits runs a lot faster than one number with many digits. In addition, using many quadratic irrationals leads to a very efficient implementation using a
 distributed architecture.

Finally, while it sounds like a cave-man idea to publish a table of random digits, it servers a very important purpose that has been forgotten in modern scientific research and benchmarking: replicability. You are free to reuse my Excel spreadsheet if
 you want your research to be replicable. Of course you would get the same digits if you use the Python code with the same seeds. This is not true with the Mersenne twister: the digits may depend on which version of Python you use. Also, one advantage of the quadratic irrational PRNG is its portability, and the fact that you will get the same digits regardless of your programming language. Time permitting, I will publish a much larger table with at least a trillion digits. 
 In the meanwhile, if you need such a table, feel free to email me at vincentg@MLTechniques.com. 


%-------------------------
\chapter{Application: Synthetic Stock Exchange}{}\label{pouti}

In this chapter, I describe an original number guessing game played with real money. It can also be used
 as a synthetic stock exchange, leveraging the mathematics discussed in chapter~\ref{chapterPRNG}.  In particular, it is based on the \textcolor{index}{dyatic map}\index{dyadic map} and efficient algorithms to 
compute the \textcolor{index}{digits}\index{digit} attached to
\textcolor{index}{good seeds}\index{seed (dynamical systems)!good seed} in the dynamical systems involved.


If properly implemented, this system  cannot be manipulated: formulas to win numbers  are made public. Also, it simulates a neutral, efficient stock market. In short, for the participants, there is nothing random: everything is deterministic and fixed in advance, and known to all users. Yet it behaves in a way that looks perfectly random, and public algorithms offered to compute winning numbers require so much computing power, that for all purposes, they are useless -- except to comply with gaming laws and to establish trustworthiness. Any attempt at predicting winning numbers based on historical data, using machine learning techniques, will fail: the sequence of winning numbers appear to be totally random, despite the fact that it is deterministic.

I use private algorithms to determine the winning numbers, and while they produce the exact same results as the public algorithms, they are incredibly more efficient, by many orders of magnitude. Also, I mathematically prove that the public and private algorithms produce the same results. To prove it on a real example may not possible: the public algorithm requires too much computing time. 

The goal is to turn this application into a real platform where participants play with real money. I also discuss how it can be done given the legal barriers. The operator generates revenue via charging a fee per transaction, just like stock exchanges, and unlike lotteries that retain a portion of the payments to finance themselves. In short, all the money used by participants to purchase numbers, goes back to the participants. 

\section{Introduction}

The application discussed here requires a very large number of pseudo-random digits generated in real-time, obtained as fast as possible. The digits must emulate randomness extremely well. I use the quadratic irrational random number
 generator discussed in section~\ref{pivizintrobvbc}. 

The idea consists of creating a virtual, market-neutral stock market where people buy and sell stocks with tokens. In short, a synthetic stock market where you play with synthetic money: tokens, themselves purchased with real money. Another description is a lottery or number guessing game. You pay a fee per transaction, and each time you make a correct guess, you are paid a specific amount, also in real money. The participant can select different strategies ranging from conservative and limited to small gains and low volatility, to aggressive with a very small chance to win a lot of money.

The algorithm that computes the winning numbers is public; it requires some input information, also publicly published (the equivalent of a public key in cryptography). So you can use it to compute the next winning number and be certain to win each time, which would very quickly result in bankruptcy for the operator. However the public algorithm necessitates so much computing time to obtain any winning number with certainty, to make it useless. But you can guess the winning number: your odds of winning by pure chance (in a particular example) is 1/256. 

The operator uses a private algorithm 
that very efficiently computes the next winning numbers. From the public algorithm, it is impossible to tell -- even if you are the greatest computer scientist or mathematician in the world -- that there is an alternative that could make the computations a lot faster: the private algorithm is the equivalent of a private key in cryptography. The public algorithm takes as much time as breaking an encryption key (comparable to factoring a product of two very large primes), while the private version is equivalent to decoding a message if you have the private key (comparable to finding the second factor in  the number in question if you know one of the two factors -- the private key).

Thus, technically, this application is not a game of chance but a mathematical competition. But one that no one can win 
other than by luck, due to tremendous computing power required to find the winning number with certainty. The 
digits used in the system must be uniformly distributed. Even a tiny bias will quickly lead to either the bankruptcy of the operator, or the operator enriching itself and being accused of lying about the neutrality of the simulated market.
  


\section{Winning, sequences and customized ROI tables}

Rather than trading stocks or other financial instruments, participants (the users) purchase numbers. Sequences of winning numbers are generated all the time, and if you can predict the next winning number in a given sequence, your return is maximum. If your prediction is not too far from a winning number, you still make money, but not as much. The system has the following features:
\vspace{1ex}

\begin{itemize}
\item	The algorithms to find the winning numbers are public and regularly updated. Winning is not a question of chance, since all future winning numbers are known in advance and can be computed using the public algorithm.
\item 	The public algorithm, though very simple in appearance, is not easy to implement efficiently. In fact, it is hard enough that mathematicians or computer scientists do not have advantages over the layman, to find winning numbers.
\item 	To each public algorithm, corresponds a private version that runs much, much faster. I use the private version to compute the winning numbers, but both versions produce the exact same numbers.
\item 	Reverse-engineering the system to discover any of the private algorithms, is as difficult as breaking strong encryption.
\item 	The exact return is known in advance and specified in public ROI tables. It is based on how close you are to a winning number, no matter what that winning number is. Thus, your gains or losses are not influenced by the transactions of other participants.
\item 	The system is not rigged and cannot be manipulated, since winning numbers are known in advance.
\item 	The system is fair: it simulates a perfectly neutral stock market.
\item 	Participants can cancel a transaction at any time, even 5 minutes before the winning number is announced.
\item 	Trading on margin is allowed, depending on model parameters.
\item 	The money played by the participants is not used to fund the company or pay employees or executives. It goes back, in its entirety, to the participants. Participants pay a fee to participate.
\end{itemize}\vspace{1ex}

%---------
\noindent Comprehensive tables of previous winning numbers are published, well before a new sequence -- based on these past numbers -- is offered to players. It entices participants to design or improve strategies to find winning numbers, even though arbitraging is technically not possible, unless there is an unknown flaw in my method. Actually, past winning numbers are part of the public data that is needed to compute the next winning numbers, both for participants and the platform operators.

Various ROI tables are offered to participants, and you can even design your own. If you are conservative, you can choose one with a return of 10\% for a perfect guess, a 54\% chance of winning on any submitted number, and a maximum potential loss of 4\%. This table is safe enough that you could ``trade" on margin. It mimics
 a neutral stock market. The return is what you make or lose in one day percentagewise, on one sequence. New winning numbers are issued every day for each live sequence, so your return -- negative or positive -- gets compounded if you play frequently.
 A sequence is the equivalent of a specific stock in the stock market.

Another interesting ROI table offers a return of 330\% for a perfect guess, with the same 54\% chance of winning on any transaction, and  a maximum potential loss also of 4\%. However, the payoff when your guess is close to but different from the winning number, is a lot lower than in the previous example. 
If you are a risk taker, you may like a table offering a maximum return of 500\%, a 68\% chance of winning on any transaction, and a maximum potential loss of 60\%. Or another table with a maximum return of 600\%, a 80\% chance of winning, but a maximum potential loss of 100\%.  I discuss ROI tables and sequences in details later in this chapter, along with examples. By winning, I mean that your guess is close enough to the winning number so that you get financially rewarded. The reward in question might be small depending on the ROI table, compared to correctly guessing the winning number. 

All the sequences currently implemented consist of 8-bit numbers: each winning number -- a new one per day per sequence -- is an integer between 0 and 255. I am working on a generalization to 16-bit numbers, offering payoffs of the same caliber as lottery jackpots. By design, all ROI tables including customized ones, are neutral, with an average zero gain. This beats all casinos and lotteries where the average gain is negative. It applies
 to all sequences. Indeed, sequences and ROI tables are independent. The participant can test various strategies: for instance:
\vspace{1ex}
%----
\begin{itemize}
\item 	Try various ROI tables.
\item 	Play every day until you experience your first win.
\item 	Play every day until you experience your first loss.
\item 	Play until you have achieved a prespecified goal, or exit if your losses reaches some threshold, whatever comes first. 
\item 	Increase or decrease how much you spend depending on your results.
\item 	Look if you can find patterns in the winning numbers, then exploit them. 
\end{itemize}\vspace{1ex}

\noindent Any pattern in the winning numbers is likely short-lived and coincidental, in the same way that any purely random 
 time series exhibits a number of patterns, since the number of potential patterns is infinite.

\section{Implementation details with example}

Here I show an example of a typical sequence. It illustrates how the winning numbers are computed for the sequence in question. The purpose is to explain the mechanics for one of the 8-bit systems. The 32-bit version offers more flexibility, as well as potential returns that can beat those of a state lottery jackpot. The sample 8-bit sequence is defined by the public algorithm below.


\subsection{Seeds, public and private algorithms}\label{porcinired}

I now describe the public algorithm. It works as follows. Start with initial values $y_0$ and $z_0$ that are positive integers 
called \textcolor{index}{seeds}\index{seed (dynamical systems)}, with $z_0>y_0$. Then for $t = 0, 1, 2$ and so on, compute $y_{t+1}$ and $z_{t+1}$ iteratively as follows:\vspace{1ex}

\noindent  \textcolor{white}{0000}{\bf If}  $ z_t  <2y_t$   {\bf then}   \\
  \textcolor{white}{000000}  $y_{t+1}=4y_t-2z_t$\\
 \textcolor{white}{000000} $z_{t+1}=2z_t+3$\\
\textcolor{white}{0000}{\bf else} \\  
\textcolor{white}{000000} $ y_{t+1}=4y_t$\\
\textcolor{white}{000000} $ z_{t+1}=2z_t-1.$
\vspace{1ex}

\noindent I discuss the seeds in section~\ref{sredsa}. In one version of the system, the seeds are public but are extremely large integers with millions of even billions of digits. In another version, I guarantee the existence of working seeds $y_0, z_0$ smaller than $10^3$, but I keep them secret. Thus the participant would have to test up to $10^3 \times 10^3 = 10^6$ pairs of seeds to find the winning numbers. And that's just the easy part of the problem.

In this example, the public algorithm computes big numbers linked to the successive binary digits of some quadratic irrational denoted as $x_0$,
 without actually computing the digits. It is part of a family of algorithms described in section~\ref{zw23}.
The quadratic irrational $x_0$ depends on the seeds $y_0, z_0$.
The player is unaware of this fact, and certainly does not know which $x_0$ is being used. It also guarantees that the digits -- and thus the winning numbers -- are uniformly distributed and uncorrelated. In short, they constitute a perfect random sequence. 

The operator, aware of these facts, can pre-compute billions of binary digits of $x_0$ using a very fast method, or better,
 obtain these digits from an external source such as \href{https://sagecell.sagemath.org/}{Sagemath}, and store them in some lookup table. This constitutes the private algorithm. 
In Sagemath, the command to get the first $10^4$ digits of (say) $\sqrt{2}$ is 
\texttt{N(sqrt(2),prec=10000).str(base=2)}. It takes less than one second to execute. It requires more and more time the more digits you want. The
 runtime is proportional to the square of the number of digits needed.  

Some methods do not need to compute previous digits to get those starting at a given location: see section~\ref{puosaw}. This can significantly increase performance. It allows the operator to quickly start at iteration (say) $t=10^8$ when creating a new set of winning numbers, skipping the previous iterations and saving a significant amount of computing time. To the contrary, the participant, having no clue as to when the winning numbers start, must go through all the iterations when using the public algorithm.


%If 4x(t) + 1 < 2y(t) Then 
%    y(t+1) = 4y(t) - 8x(t) - 2
 %   x(t+1) = 2x(t) + 1 
%Else 
%    x(t+1) = 2x(t) 
%    y(t+1) = 4y(t). 

\subsection{Winning numbers and public data}\label{bvp0z1}

% xxxx %
The iterations in the public algorithm  represent the time. 
All the winning numbers for a particular sequence are successive 8-bit blocks starting at a specific machine-generated iteration $T$ in the public algorithm.
No one knows the starting time $T$, not even the platform operator nor its software engineers. Typically, $T > 10^8$ and can automatically be specified
 by the system,  potentially using a random value such as the actual time (in milliseconds) when it was created by the algorithm in production mode. A different $T$ is used for each sequence, and $T$ can be periodically reset to increase the security of the system.

In the 8-bit system, winning numbers are always integers between 0 and 255. In the public algorithm, they occur only at iterations $t = T, T+8, T+16, T+24$ and so on. 
The winning number at iteration 
$t\geq T$ is defined as $w_t = (z_t - 256\cdot z_{t-8} + 255)/4$. It is a positive integer. It consists of 8 successive digits in the binary expansion of the underlying quadratic irrational $x_0$ used in the private algorithm.
The reason for skipping 7 out of 8 numbers is to make sure that winning numbers are not auto-correlated.

%---------
\subsubsection{Using the public algorithm to win}

Before any winning sequence is made available to the public and participants are allowed to play, the operator publishes the previous 2000 winning numbers  attached to the sequence in question. Using the public algorithm, the participants can identify when these 2000 numbers appear in the sequence $\{w_t\}$, though they must first find the right seeds among several candidates publicly listed. Once the past winning numbers are found with the public algorithm and correct seeds, they player knows that the next number is a winning number: he can purchase that number and win with 100\% certainty, thus getting the maximum payout. 

In case multiple seeds $(y_0, z_0)$ lead to the same 2000 winning numbers at some point within the prespecified number of iterations (say, one trillion), the operator must still pay the full prize to the participant, regardless of which number the participant purchased and which seed he used.  As long as the participant can show how he found the 2000 numbers in question, by sharing the seeds that he used. So the operator should make sure that
 the probability of two different seeds yielding 2000 identical winning numbers in the right order, in less than a trillion iterations, is essentially zero, see section~\ref{uitres}.    

Because winning numbers have a random behavior, the chance of such a ``collision" is essentially zero anyway. The chance of finding the 2000 winning numbers using the exact same seed as the operator (after testing a large number of them using the public algorithm on potential seeds listed by the operator),  is considerably higher by many orders of magnitude. It is 
 essentially 100\%. However the limitation here is of a different nature: it requires years of computing time, and sequences (that is, seeds) and even algorithms are updated frequently to essentially make it impossible to win other than by luck. Given that a winning number has only 8 bits, the chance of winning by luck is $1/256$.  And even if your guess is close to the winning number, you still get a payout, albeit smaller.
%-----------

\subsubsection{Python code}\label{bornase}

The code below is also available on GitHub, \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/lottery.py}{here}.
 It covers more than the public algorithm. In particular, it computes the successive digits $d(t)$ of
 the quadratic irrational $x_0$ determined by the seeds $y_0,z_0$, for $t=1,2$ and so on. 
 Using results obtained in section~\ref{puosaw}, the implementation can be accelerated by several orders of magnitude
 when $T$ is within some known range, for instance if $\tau < T < \tau +10^4$ and $T>10^9$.  
The operator typically knows $\tau$ but not $T$, while the player knows none of them.
\vspace{1ex}

\begin{lstlisting}
# w is an 8-bit winning number if t >= T and (t - T) % 8 == 0

t = 0
y = 2          # seed: 1st component, y0 (for t = 0)
z = 5          # seed: 2nd component, z0 (for t = 0)
T = 43         # must be >= 10
max = 200    # maximum for t, must be >= T
buffer = {}  # to store 9 previous vales of z
x0 = 0          # irrational number represented by the digits 

for t in range(1, max): 

    if z < 2*y:
        y = 4*y - 2*z
        z = 2*z + 3
        d = 1        # t-th binary digit of x0  
    else:
        y = 4*y
        z = 2*z - 1
        d = 0        # t-th binary digit of x0
  
    x0 += d * 1/2**t 

    if t >= T - 8:
      buffer[t % 9] = z
      if t >= T:
          w = (z - 256*buffer[(t-8) % 9] + 255) >> 2  

    if t >= T and (t - T) % 8 == 0:
        print(t, w, d, z) 

print("\nNumber x0:", x0)
\end{lstlisting}
\vspace{1ex}

\noindent At iteration $t$, the array \texttt{buffer}, once updated, contains the 
preceding 8 values $z_{t-1}, z_{t-2},\dots, z_{t-8}$ as well as $z_t$, in circular order. Actually, rather than keeping these huge values
 in a buffer, it is sufficient to just keep the corresponding digits instead, and compute $w_t$ with the convolution formula
$$
w_t = \sum_{k=0}^7 2^k d(t-k).
$$ 
Note that $d(t) = (z_t - 2z_{t-1} + 1)/4$ is equal to either 0 or 1. It is represented by the variable  \texttt{d} in the code. 
In the end, the whole system is justified based on the following theorem.

\begin{theorem}\label{vgfd2}
If $x_0$ is an irrational number, then the corresponding sequence  
 $w_t,w_{t+8}, w_{t+16},\dots$  is free of auto-correlations regardless of $t>0$, and these numbers
 are uniformly distributed on $\{0, 1,\dots, 255\}$. 
Assuming $d(t) = (z_t - 2z_{t-1} + 1)/4$ and $z_0 > y_0$, we also have
\begin{align}
x_0 & = \sum_{t=1}^\infty \frac{d(t)}{2^t} = \frac{-(z_0 -1) + \sqrt{(z_0-1)^2+8y_0}}{4},  \label{popot2i}\\
w_t & = \sum_{k=0}^7 2^k d(t-k) =  \frac{z_t - 256\cdot z_{t-8} + 255}{4}  \label{popot2}
\end{align}
Now let's consider two sequences: one with seeds $y_0, z_0$ and quadratic irrational $x_0$, and another one with seeds $y'_0, z'_0$ and 
quadratic irrational $x'_0$. If $x_0, x'_0$ are irrational numbers linearly independent over the set of rational numbers, then
 the two sequences $w_t,w_{t+8}, w_{t+16},\dots$ and $w'_t,w'_{t+8}, w'_{t+16},\dots$ are free of cross-correlations. 

%Finally if for some integer $k\geq 0$, the seeds of both sequences are related by the mapping 
%\begin{equation}
%y'_0 = 2^{2k}y_0,\quad 
%z'_0 = 2^k(z_0-1)+1, \label{piurews}
%\end{equation}
%with $z'_0 > y'_0$, then the corresponding quadratic irrationals satisfy $x'_0 = 2^k x_0$. Thus 
% for $t=1,2$ and so on, we have:
%\begin{equation}
%d'(t) = d(t+k), \quad w'_t = w_{t+k}. \label{vce32}
%\end{equation}

\end{theorem}

\begin{proof} 
\quad \\
Formula~(\ref{popot2i}) is proved \href{https://math.stackexchange.com/questions/3537637/limit-associated-with-a-recursion-connection-to-normality-of-quadratic-irration/3553816}{here}. To prove~(\ref{popot2}), recursively use the fact
 that $d(t) = (z_t - 2z_{t-1} + 1)/4$. That is:   
\begin{align}
d(t) + 2 d(t-1) & = (z_t - 4z_{t-2} + 3)/4, \nonumber \\
d(t) + 2 d(t-1) + 4d(t-2) & = (z_t - 8z_{t-3} + 7)/4,  \nonumber \\
d(t) + 2 d(t-1) + 4d(t-2) + 8d(t-3) & = (z_t - 16z_{t-4} + 15)/4, \nonumber \\
d(t) + 2 d(t-1) + 4d(t-2) + 8d(t-3) + 16d(t-4) & = (z_t - 32z_{t-5} + 31)/4, \nonumber
\end{align}
and so on. From this, it is clear that the sequence $w_t, w_{t+8},w_{t+16},\dots$  consists of successive blocks of 8 bits in the binary digit representation of the
 number $x_0$. Thus assuming these digits are a realization of independent Bernoulli trials with same chance of fail/win (thus
 assuming $x_0$ is a \textcolor{index}{good seed}\index{seed (dynamical systems)!good seed} of the 
\textcolor{index}{dyadic map}\index{dyadic map}),
 then the winning numbers are independent with a uniform distribution on $\{0,\dots,255\}$. The context here is about the empirical (observed) joint distribution of the first $n$ digits, its limit when $n\rightarrow\infty$,  and the conjecture that $x_0$ is a  \textcolor{index}{strongly normal}\index{normal number!strongly normal} number. The concept of strong normality is defined in section~\ref{vcfprng}. See also section~\ref{cxcdsojhg} where correlation is defined. Also note that $0<x_0<1$ since
$y_0 < z_0$. The last statement about the absence of cross-correlations is true if both $x_0$ and $x_0'$ are strongly normal. 
See \href{https://stats.stackexchange.com/questions/450922/cross-correlations-in-digit-distributions}{here}, and 
Exercise~\ref{pqcorr}. \qed
\end{proof}\vspace{1ex}

\noindent Table~\ref{tabtres} illustrates some of the quantities discussed here. It corresponds to the output of the Python code
 in section~\ref{bornase}, assuming the winning numbers $w_t$ start at iteration $t = T = 43$. In this case,
 $x_0 = -1 +\sqrt{2}$. The number $z_t$, needed to compute $w_t$, grows by a factor 2 at each iteration.
 In practice, $T$ is above~$10^9$.


%\renewcommand{\arraystretch}{0.99999} %%%
\begin{table}%[H]
\small
\setlength\extrarowheight{-2pt}
\[
\begin{array}{rrrc}
\hline
t	&  w_t & d(t) & z_t\\ 
\hline
  43 &  157 &  1 & 49758216191605 \\
  51 &  230 &  0 & 12738103345051545 \\
  59 &   72 &  0 & 3260954456333195553 \\
  67 &   69 &  1 & 834804340821298061589 \\
  75 &  151 &  1 & 213709911250252303767133 \\
  83 &  216 &  0 & 54709737280064589764386657 \\
  91 &  155 &  1 & 14005692743696534979682984557 \\
  99 &   55 &  1 & 3585457342386312954798844046557 \\
 107 &   84 &  0 & 917877079650896116428504075918673 \\
 115 &  171 &  1 & 234976532390629405805697043435180717 \\
 123 &  233 &  1 & 60153992292001127886258443119406264229 \\
 131 &  241 &  1 & 15399422026752288738882161438568003643333 \\
 139 &  214 &  0 & 3942252038848585917153833328273408932693849 \\
 147 &  246 &  0 & 1009216521945237994791381332037992686769626073 \\
 155 &   11 &  1 & 258359429617980926666593621001726127813024274477 \\
 163 &  168 &  0 & 66140013982203117226647966976441888720134214266529 \\
 171 &  147 &  1 & 16931843579443998010021879545969123512354358852231757 \\
 179 &  186 &  0 & 4334551956337663490565601163768095619162715866171330281 \\
 187 &  132 &  0 & 1109645300822441853584793897924632478505655261739860552209 \\
 195 &  206 &  0 & 284069197010545114517707237868705914497447747005404301366073 \\
\hline
\end{array}
\]
\caption{\label{tabtres} Winning number $w_t$, digit $d(t)$, and $z_t$ at iteration $t$}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%


\subsection{Optimizing computations}\label{puosaw}

Regardless of $k\geq 0$, the change of seeds $y'_0 =  2^{2k}y_0, z'_0 = 2^k(z_0-1)+1$ leads to the new quadratic irrational $x'_0 = 2^k x_0$, according to formula~(\ref{popot2i}). Thus, in theory, it allows you to  get the digits of $x_0$ starting at location $k+1$ without  computing the first $k$ digits. And consequently, the corresponding winning numbers -- no matter how far in the sequence -- with little computational efforts. However, this is true as long as $y'_0 < z'_0$. 
In practice, when $k$ is large, this is never the case. 

A workaround consists in finding seeds $y_0', z_0'$ leading to $x_0' = 2^kx_0 - \lfloor 2^k x_0\rfloor$ where
 the brackets represent the integer part function. It involves computing 
\textcolor{index}{integer square roots}\index{integer square root} [\href{https://en.wikipedia.org/wiki/Integer_square_root}{Wiki}], that is, the integer part of the square root of very large integers. There are very efficient methods to accomplish this,
 especially to get the binary digits. The \textcolor{index}{mpmath}\index{mpmath (Python library)} and 
\textcolor{index}{gmpy2}\index{gmpy2 (Python library)} Python libraries offer specific functions for these
 computations. Indeed you can even use the \texttt{isqrt} function from the math library. Here I illustrate how to
 do it with the \texttt{isqrt} function from the gmpy2 library: it is implemented in C and possibly the fastest of all. 

The starting point is to reverse the problem. Given two large positive integers $\alpha,\beta$, you want to find the seeds 
 $y_0, z_0$ for the quadratic irrational 
$x_0 = -\alpha + \sqrt{\beta}$. 
The choice of $\alpha,\beta$ must result in $0<x_0<1$ for the private algorithm to work.
By virtue of~(\ref{popot2i}), this leads
\begin{equation}
\alpha = \lfloor \sqrt{\beta} \rfloor, \quad y_0 = 2(\beta - \alpha^2),\quad z_0 = 1 + 4\alpha. \label{patron}
\end{equation}
To skip the first $k$ digits, you need to find seeds $y_0',z_0'$ such that $x'_0 = -\alpha' +\sqrt{\beta'}$ with 
 $\beta'= 2^{2k}\beta$ and $\alpha'=\lfloor \sqrt{\beta'} \rfloor$. This leads to the same solution
 as~(\ref{patron}), but this time with $y_0,z_0,\alpha,\beta$ replaced respectively by
$y_0',z_0',\alpha',\beta'$. The main challenge is the computation of $\alpha'$, that is, the integer square root of $2^{2k}\beta$.

I provide the code below if you want to use the gmpy2 library. Here \texttt{square} represents $\beta'$. For $k=10^9$ and $\beta=3$, it took about one minute on my laptop, excluding the time needed to print the result. The integer square root \texttt{isqrt} is computed in base 16, which is very handy since winning numbers are 8-bit long, thus easy to encode in base 16: each winning number consists of two consecutive digits in base 16.  It would be even better to use \texttt{base=256} in the code snippet. The maximum base allowed
 when I tested my script, was~60.

For much larger values of $k$, the code needs to be modified, as you will run out of memory and require a parallel architecture at some point. The time required seems to be a sublinear function of $k$ until you use  up all the memory. I suggest running it in GPU. \vspace{1ex}

\begin{lstlisting}
import gmpy2

beta = 3
base = 16
k = 10**9
square = beta * (2**(2*k))
isqrt = gmpy2.isqrt(square).digits(base)
print(isqrt)
\end{lstlisting}


\subsection{Seeds with billions of digits and enhanced system}\label{sredsa}

The methodology also works when the seeds are not integer numbers: Formula~(\ref{popot2i}) and~(\ref{popot2}) remain valid even
 if $y_0, z_0$ are irrational numbers, as long as $0< y_0< z_0$. In particular, the winning numbers $w_t$ are still 8-bit integers. The public algorithm still works, but now $y_t$ and $z_t$ are no longer integers. As a result, it will quickly lead to numeral inaccuracy and completely erroneous numbers unless you use special techniques to handle very high precision float, with billions of digits after the decimal point. Think about $y_0 = \log(415)$ and
 $z_0 = \exp(\sqrt{7\pi})$ just to give an example of the possibilities. This makes it a lot harder to win for the player. But 
it does not  make it much more difficult for the operator since it knows $x_0$, and the player does not.

When $y_0$ and $z_0$ are integers, the public algorithm -- if implemented in Python -- automatically takes care of very large integers. This no longer works when the seeds are not integers. In the above example, the seeds are private although some minimum information about them is provided to the player, to guarantee that the set of winning numbers is uniquely defined given the public information, and that the player only has to try a finite set of seeds.  Should the winning numbers be reachable via multiple sets of seeds, it may increase the player's chance of winning. Thus the operator might want to make sure that there is only one path to the winning numbers, given the public information.

\subsubsection{Towards maximum security}\label{tms20}

What if some player figures out the connection between the public algorithm and Formula~(\ref{popot2i}) and~(\ref{popot2})? For instance, by reading this chapter. He still has to try many seeds, but he could relatively quickly compute (possibly in a number of weeks or months) billions of digits of all the potential candidates for $x_0$, and then get an hedge to find the winning numbers.
In order to make the system robust against such hacks,  there are different possibilities, in addition to frequently changing the seeds or using non-integer seeds: \vspace{1ex}
\begin{itemize}
\item Reparametrize $y$ and $z$ in the public algorithm, using a mapping $(x, y) \mapsto (y',z')$
 such as a linear transform. The new public algorithm will be  based on $y', z'$, using the updated recursion. The player may not see the connection between the new recursion and the version published in this chapter.
\item Use an entirely different recursion attached to more general algebraic numbers rather than quadratic irrationals. This is a work in progress.
\item The easiest solution is to keep the recursion as is, but instead use (say) 5 different seeds $(y_{0,k},z_{0,k})$ with 
$0\leq k < 5$. It leads to 5 different sequences, and the winning number at iteration $t$ comes from the $k$-th
 sequence, with $k = t \bmod 5$.
\end{itemize}\vspace{1ex}
The last option is simple and offers flexibility. For instance, you could use 200 rather than 5 sequences.

\subsubsection{Real example}
 
Here I illustrate the method with a simple example. In this case, the public data consists of two pairs of seeds
 $(y_{0,1}, z_{0,1})$, $(y_{0,2}, z_{0,2})$ and the past 2000 winning numbers. Can you find the next 500 winning numbers with this information? One of the two seeds leads to the winning numbers, the other one does not. Also available in the public information: you need less than one trillion iterations to identify the 2000 winning numbers in question. 
You may use the
 public algorithm in section~\ref{porcinired} to answer this question, or better, any hack of your own. The
 public information is available at the following locations: \vspace{1ex}

\begin{itemize}
\item The seeds $y_{0,1}, z_{0,1}, y_{0,2}, z_{0,2}$ are on GitHub. The links to the files are
 \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/lottery_seed_y10000000_2.txt}{$y_{0,1}$}, 
\href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/lottery_seed_z10000000_2.txt}{$z_{0,1}$}, 
 \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/lottery_seed_y10000000_3.txt}{$y_{0,2}$}, 
 \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/lottery_seed_z10000000_3.txt}{$z_{0,2}$}.
Each of them is about 3 megabytes (uncompressed).
\item  The file with the 2000 past winning numbers can be found \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/lottery_winning_numbers_123903793_2.txt}{here}. 
It also contains the 500 future winning numbers. Ignore them: these are the numbers that you are supposed to find. I included them so you that can check against the winning numbers that you come up with. 
\end{itemize}\vspace{1ex}

\noindent Note that the filenames contain information about times and seeds chosen in this test. In the file with the winning numbers,
 the first column represents the time $t$. In practice, this information is not available or encrypted. Here you can retrieve
 the winning numbers in a matter of minutes, by using this information, with the Python code in this section. Pretend that you
 don't have this information, and see if you can retrieve the winning numbers in a reasonable amount of time. The
 size for each seed is about 3 megabytes. In an industry-grade version, this size could be several gigabytes or even terabytes, and the secrete $\beta$ (see code) much larger than the one used in this test, and hard to find. 


Increasing the number of candidate seeds (with only one leading to winning numbers) may not significantly increase the security of the system, as hackers may pre-compute tables with billions of digits for as many integers ($\beta$ in the code)  
as they can~\cite{rkan92}. A much better solution is to
 interlace winning numbers from multiple seeds, as discussed in the last bullet item in section~\ref{tms20}. In this case, the hacker
 does not know which seed is used at any given iteration, as many combinations are possible. Also, working with trillions rather than billions of digits makes it considerably harder for hackers. To this day, only the first $10^{13}$ digits of $\sqrt{2}$ are known, see \href{https://en.wikipedia.org/wiki/Square_root_of_2#Records_in_computation}{here}.
Typically, such computations require
 months of computing time. 

The Python code below is private and not shared with the player. The operator uses it to generate the winning numbers. It is also on GitHub \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/lottery_fast.py}{here},
 and named \texttt{lottery\_fast.py}. The variables \texttt{y0\_1}, \texttt{z0\_1}, \texttt{beta\_1}, and 
\texttt{alpha\_1} in the code correspond respectively to $y_0', z_0', \beta'$ and $\alpha'$  in section~\ref{puosaw}.
In this case, the quadratic irrational is $x_0 = -\lfloor\sqrt{\beta}\rfloor  + \sqrt{\beta}$. You can skip
 the computation of the first $10^8$ digits and directly jump at iteration $1+ 10^8$ by
 setting \texttt{offset=10**8} in the code. The number of winning numbers produced is 
about one eighth of \texttt{n}.

The code allows you to determine the seed when starting at an arbitrary iteration \texttt{t+offset} rather
 than $t=1$. I use this functionality to create the large public seeds leading to winning numbers. Of course, \texttt{offset} is kept private. It also allows you to compute the winning numbers starting at an even much larger iteration $t=T$, with $T$ also kept secret. 
One issue is checking whether the digits produced are correct. In practice I use two different mechanisms to compute the digits, to double-check. I have seen implementations that are correct for the first few million digits, then fail later on in the sequence. \vspace{1ex}

 \begin{lstlisting}
import gmpy2

def big_seed(offset, beta):

    beta_1  = beta * (2**(2*offset)) 
    alpha_1 = int(gmpy2.isqrt(beta_1)) 
    y0_1 = 2 * (beta_1 - alpha_1*alpha_1)
    z0_1 = 1 + 4*alpha_1
    return(y0_1, z0_1)


n = 20000          # number of digits to compute 
offset = 10**7     # digits start at location 1 + offset
beta = 2
y0, z0 = big_seed(offset, beta)
y = y0
z = z0

digits = {}
winning_numbers = {}

for t in range(1, n): 
    if z < 2*y:
        y = 4*y - 2*z
        z = 2*z + 3
        digits[t + offset] = 1        
    else:
        y = 4*y
        z = 2*z - 1
        digits[t + offset] = 0   
    if t > 8 and t % 8 == 5:
        w = 0
        for k in range(8):
            w += digits[t + offset - k] * (2**k)
        winning_numbers[t + offset - k] = w
        print(t + offset, w)

filename = "lottery_seed_y" + str(offset) + "_" + str(beta) + ".txt"
OUT=open(filename,"w")
OUT.write(str(y0))
OUT.close()

filename = "lottery_seed_z" + str(offset) + "_" + str(beta) + ".txt"
OUT=open(filename,"w")
OUT.write(str(z0))
OUT.close()

filename = "lottery_winning_numbers_" + str(offset) + "_" + str(beta) + ".txt"
OUT=open(filename,"w")
for time in winning_numbers:
    number = winning_numbers[time]
    OUT.write(str(time) + "\t" + str(number) + "\n")
OUT.close()
\end{lstlisting}

\subsection{Collision risks}\label{uitres}


This problem is related to the probability of finding (or not) a substring in a string~\cite{noonan99}. In this context, the player may use the wrong  seeds yet by some incredible luck, find the past winning numbers in his own sequence. This is called a collision. Given the constraints of the system, I show that the chance of such a collision is incredibly close to zero.

Here the substring consists of the past 2000 winning numbers, in other words $m=8\times 2000 \approx 2^{14}$ bits. 
The string consists of numbers originating from a random-looking sequence, other than that used by the operator. In terms of string terminology, the associated alphabet -- the letters -- consists of two symbols: 0 and 1. 
Since the winning numbers are guaranteed to show up within the first trillion iterations, the size of the string (the long random-looking sequence) is about $N=2^{40}$ bits. 

Finally, if the operator publishes a set of $K + 1$ seeds with one of them leading to the past winning numbers, what is the chance that at least one of the $K$ remaining sequences also leads to the same past winning numbers, albeit (in all likelihood) starting at a different position? 
Now I give an approximate answer to this question. The probability of the $m$-bit subsequence of winning numbers being in a given larger $N$-bit sequence is
\begin{equation}
1- \Big(1- \frac{1}{2^m}\Big)^{N-m+1} \approx \frac{N-m+1}{2^m} \approx 2^{-15960}.\label{tortelle}
\end{equation}
So, even if $K=2^{300}$, it is still considerably smaller than $2^m$, and the risk of at least one collision across the $K$ sequences is 
  not larger than (\ref{tortelle}) multiplied by $K$. In short, the operator can offer a selection of $2^{300}$ seeds, claiming
 that only one of them leads to the winning numbers within a trillion iterations. 
 

\subsection{Exercises}

The following original exercises complement the theory. Several are good candidates as job interview questions for
 software engineers, or exam questions in computer science programs. They range in complexity from relatively simple (requiring less than one hour of work) to difficult. To help
 you decide which exercises to work on depending on your interests, I added a title to each of them.

\begin{Exercise}\label{kn5z21990fr} {\em Partial sums of binary digits} --  Let $s_t(x_0) = d(1,x_0) +\cdots + d(t,x_0)$ be the sum of the first $t$ binary digits of a real number $x_0\in [0, 1[$. Thus $0\leq s_t(x_0)\leq t$. Typically, it is easier to study these sums rather than the individual digits, as they are less volatile.  Show that $s_{t+1}(x_0/2) = s_t(x)$, for $t=0,1,2$ and so on.  By convention, $s_0(x_0)=0$. Also show that 
\begin{equation}
 2x_0 = \sum_{t=1}^\infty \frac{s_t(x_0)}{2^t}, \label{presaton}
\end{equation}
where $d(t,x_0)$ is the $t$-th binary digit of $x_0$. Finally, using a \textcolor{index}{greedy algorithm}\index{greedy algorithm}, for any real number $x_0\in[0, 1]$, show how to compute $s_t(x_0)$ without computing the individual digits.\vspace{1ex} \\ 
{\bf Solution} \\
Formula~(\ref{presaton}) is obtained by summing the left and right hand sides of the following equalities:
\begin{align}
x_0 & = \frac{d(1,x_0)}{2} +  \frac{d(2,x_0)}{4}+ \frac{d(3,x_0)}{8} + \cdots, \nonumber \\
\frac{x_0}{2} & =   \hspace{47pt}  \frac{d(1,x_0)}{4} +  \frac{d(2,x_0)}{8} + \cdots, \nonumber \\
\frac{x_0}{4} & =   \hspace{94pt}   \frac{d(1,x_0)}{8} + \cdots, \nonumber
\end{align}
and so on. As for the greedy algorithm, it works as follows. Let $0\leq x_0<1$ and
$$
h_n(x_0) = \sum_{t=1}^n \frac{\varphi_t(x_0)}{2^t}, \quad n=1,2,\dots
$$
Iteratively compute $h_n(x_0)$ for $n=1,2$ and so on, by defining
$\varphi_{n}(x_0)$ as the largest integer with $0\leq \varphi_{n}(x_0)\leq n$, such that $h_{n}(x_0) \leq 2x_0$. Then
 $\varphi_{t}(x_0)= s_t(x_0)$ for all $t$. Also, as $n\rightarrow\infty$, $h_n(x_0)\rightarrow 2x_0$. 

Note that if you replace 
the constraint $0\leq \varphi_{n}(x_0)\leq n$ by $0\leq \varphi_{n}(x_0)\leq 1$, 
then you get the binary digit representation of 
 $2x_0$ instead, with $\varphi_{t}(x_0)= d(t, 2x_0)$ for all $t$ and $h_n(x_0)\rightarrow 2x_0$, assuming $x_0<\frac{1}{2}$.  
\end{Exercise}


\begin{Exercise}\label{kn5es} {\em Autocorrelation in the sequence of winning numbers} -- Given a seed
 $(y_0,z_0)$, show that the autocorrelation in the sequence $\{w_t\}$ with $t=1,2$ and so on, is $\frac{1}{2}$.
To the contrary, the autocorrelation in the sequence $w_t, w_{t+8}, w_{t+16}$ and so on, is zero regardless of where you start. \vspace{1ex} \\ 
{\bf Solution} \\
   The sequence $w_t, w_{t+8}, w_{t+16},\dots$ consists of successive blocks of non-overlapping bits in the binary representation of $x_0$, or in other words, digits in base 256. These blocks are independent and thus uncorrelated assuming $x_0$ is a \textcolor{index}{strongly normal} number. To the contrary, the sequence $w_t, w_{t+1}, w_{t+2},\dots$ consists of overlapping
 blocks. For instance, $w_t$ and $w_{t+1}$ both have 8 bits, but share 7 bits. Thus the autocorrelation.
\end{Exercise}

\begin{Exercise}\label{kn5es} {\em Some digit sequences are more random than others} -- Identify some 
 quadratic irrationals that exhibit
 a less random behavior than others, in their binary digit sequence. Can you explain why? \vspace{1ex} \\ 
{\bf Solution} \\
 If you pick up a million quadratic irrationals and look at the first million digits for each of them, you are bound to find some extremes. Actually, failure to find such extremes would be an indication of lack of randomness. You would expect these numbers, at least some of them, given enough digits, to {\em locally} exhibit some patterns. 

In the end, the number of zeros and ones in a random sequence of $n$ digits is governed by the \textcolor{index}{law of the iterated logarithm}\index{law of the iterated logarithm}, see section~\ref{iterlawsd}. It is typically different from $n/2$, with a discrepancy 
in the order $\sqrt{n}$. The maximum discrepancy is of the order $\sqrt{n\log\log n}$. A lower discrepancy, say of order $n^{1/4}$, is actually a sign of non-randomness, contrarily to intuition: see section~\ref{azxa} for examples and discussion.

Other metrics such as the length of the maximum run, are governed by similar laws. 
 I looked at those statistics to identify outliers, and shared my results in Table~\ref{tabuchi}. For instance
 the seed $y_0=90, z_0=91$ leading to $x_0 = (-45 +\sqrt{2203})/2$, fails the 
\textcolor{index}{prime test of randomness}\index{prime test (of randomness)} discussed in section~\ref{vcfprng}, at least for the first $\num{20000}$ digits.

In general, it is a good idea to skip the first million digits, as increased randomness typically kicks in later in sequences that are otherwise initially less random.  
\end{Exercise}

\begin{Exercise}\label{knoy54} {\em Square-free integers and seeds to avoid} -- Numbers of the form $(z_0-1)^2 +8$ where $z_0>1$ is an integer, rarely contain a square factor. These numbers  appear in Formula~(\ref{popot2i}) with $y_0=1$, and thus satisfy $y_0 < z_0$
 as required in Theorem~\ref{vgfd2}. To the contrary, among all positive integers, only a proportion $6/\pi^2$ consists of square-free
 integers. Identify integers of the form $(z_0-1)^2 +8$ that are not square-free, and show how rare they are. \vspace{1ex} \\ 
{\bf Solution} \\
 You can use the Python code in section~\ref{tr3te4} to answer this question. Among the first \texttt{Niter} integers of the form
 $(z_0-1)^2 +8$, the variable \texttt{accepted} counts those that are square-free. Seeds $(y_0,z_0)$ leading to non square-free integers $(z_0-1)^2 +8y_0$ must be removed because they introduce cross-correlations among the various sequences of winning numbers across multiple seeds. The fact that these seeds are rare means that very few must be rejected.
\end{Exercise}

\begin{Exercise}\label{pqcorr} {\em Cross-correlation between binary digit sequences} -- This is a difficult exercise. Let 
 $X=x_0$ be a positive real number with random digits, each digit with probability $\frac{1}{2}$ of being one. If $p, q$ are two strictly positive co-prime integers, free of factors of the form $2^k$ for $k=1,2$ and so on, then the correlation between the binary digit sequences of $pX$ and $qX$, is equal to $(pq)^{-1}$. By digit sequences, I mean the digits starting after the decimal point.  
How would you use this fact to prove that the binary digits of $x_0= -1+\sqrt{2}$ and $x'_0=-2 +\sqrt{7}$ are
 uncorrelated? Also show that the correlation between the binary digits of $-4 + \sqrt{18}$ and $-7+\sqrt{50}$ is $1/15$.
\vspace{1ex} \\ 
{\bf Solution} \\
Use the Python code in Exercise~\ref{knoy55wc} to do some preliminary investigations, starting with $p=1$ and $q=3$, to empirically confirm that the correlation is $(pq)^{-1}$. 

If $x_0= -1+\sqrt{2}$ and $x'_0=-2 +\sqrt{7}$, then there is no real number $X$ such that
 $x_0=pX, x'_0=qX$ and $p, q$ are positive integers. Otherwise $x_0/x'_0 = p/q$ would be a rational number, and we know
 that $x_0/x'_0$ is irrational. Indeed, the only way to make $p/q$ irrational is to have $p, q\rightarrow\infty$. Then the correlation
  $(pq)^{-1}$ in question (for the digit sequences of $x_0$ and $x'_0$) is zero, the desired result. 

To prove that the correlation is $(pq)^{-1}$, for the digit sequences of $pX$ and $qX$ assuming $p,q$ are co-prime positive integers free of powers of 2,
 see \href{https://stats.stackexchange.com/questions/450922/cross-correlations-in-digit-distributions}{here}. The proof is unfinished. It uses the same notations as in the Python code in Exercise~\ref{knoy55wc}. It relies on analyzing the carry-over mechanism when doing the multiplications $pX$ and $qX$ using the grade school algorithm (implemented in the Python code). I invite you to finish my proof. 

Finally, $\sqrt{18} = 3\sqrt{2}$ and $\sqrt{50} = 5\sqrt{2}$. Using $X=\sqrt{2}, p=3$, and $q=5$, the correlation between the two corresponding digit sequences is $(pq)^{-1}= 1/15$. It can be verified empirically. This assumes that $X=\sqrt{2}$ is \textcolor{index}{strongly normal}\index{normal number!strongly normal}, a conjecture widely believed to be true.
\end{Exercise}

\begin{Exercise}\label{knoy55wc} {\em Grade school multiplication} -- Write a program that computes the binary digits of $pX$. Here $p$ is a positive integer, and $X$ is a positive real number in $[0, 1]$. Use this program to compute the correlation between the sequences of binary digits of $pX$ and $qX$, where $p,q$ are positive integers, and $X$ a number in $[0, 1]$ with random binary digits. Focus on the digits after the decimal point (ignore the other ones).
\vspace{1ex} \\ 
{\bf Solution} \\
Here is the Python code. It is also on GitHub, \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/lottery_correl.py}{here}. It creates the digits of $X$, then those of $pX$ and $qX$, starting backward with the last digits. Finally it computes the correlation in question, assuming the digits of $X$ are random. It works if $X$ has a finite number of digits, denoted as \texttt{kmax} in the code. By increasing \texttt{kmax}, you can approximate any $X$ with infinitely many digits, arbitrarily closely.
\end{Exercise}
\begin{lstlisting}
# Compute binary digits of X, p*X, q*X backwards (assuming X is random)
# Only digits after the decimal point (on the right) are computed
# Compute correlations between digits of p*X and q*X
# Include carry-over when performing grade school multiplication

import numpy as np

# main parameters
seed = 105
np.random.seed(seed)
kmax = 1000000
p  = 5
q  = 3

# local variables
X, pX, qX = 0, 0, 0
d1, d2, e1, e2 = 0, 0, 0, 0
prod, count = 0, 0 

# loop over digits in reverse order
for k in range(kmax): 

    b = np.random.randint(0, 2)  # digit of X
    X = b + X/2  

    c1 = p*b
    old_d1 = d1
    old_e1 = e1 
    d1 = (c1 + old_e1//2) %2  # digit of pX
    e1 = (old_e1//2) + c1 - d1
    pX = d1 + pX/2

    c2 = q*b
    old_d2 = d2
    old_e2 = e2 
    d2 = (c2 + old_e2//2) %2  #digit of qX
    e2 = (old_e2//2) + c2 - d2
    qX = d2 + qX/2

    prod  += d1*d2
    count += 1 
    correl = 4*prod/count - 1

    if k% 10000 == 0:  
        print("k = %7d, correl = %7.4f" % (k, correl))  

print("\np = %3d, q = %3d" %(p, q))
print("X = %12.9f, pX  = %12.9f, qX  = %12.9f" % (X, pX, qX))
print("X = %12.9f, p*X = %12.9f, q*X = %12.9f" % (X, p*X, q*X))    
print("Correl = %7.4f, 1/(p*q) = %7.4f" % (correl, 1/(p*q))) 
\end{lstlisting}

\section{Customized ROI tables and business model}

The platform may work like a stock exchange. Here I use the word participant or player interchangeably. The operator, sometimes called the house in casino terminology, is the company managing the platform. Buying a number (also called a guess) is sometimes referred to as making a bet. The purchase price is called a wager.

The participant pays a small transaction fee for each purchased number. Buying a number is similar to buying a stock on the stock market. If the number is a winning number or close to a winning number, your invested money (the wager) increases in value, generating a positive ROI. Otherwise, it decreases, generating a negative ROI. Once the winning number is announced, the player can redeem his money (wager plus gain or minus loss), or leave it on his account and use it for future bets. The operator may offer mechanisms to automatically buy numbers regularly so that the money invested by the player is actively ``traded".


If the participant purchases all the 256 potential numbers for a given sequence at a given time and places the same dollar amount on each of them, the return is zero. In other words, the operator does not make money by ``taxing" or taking a share on positive returns: the game is perfectly fair. The operator makes money via the fixed transaction fees only. 

Of course, the participant can pick up a number randomly, and has a 1/256 chance to win the largest return in the 8-bit system. Indeed, there is no way to do better than pure luck, in order to win. The reason is because the public algorithm requires too much computing time to find the future winning numbers, given a sequence of past winning numbers and a choice of public seeds. By the time you find them -- which could take years -- the sequence in question has been archived, and new sequences are offered. That said, the player can use any algorithm to find patterns in the data, and test them in the hope to increase her odds to get a positive return, as well as for learning purposes. In the end, the system is similar to the real stock market -- a mathematical competition -- except
 that in principle, you know in advance with absolute certainty which stocks  (in this case numbers)  will win at any given time.

How much ROI you can get depends on the strategy that you select. The operator offers different strategies, and the player can  also create and upload customized strategies, as long as they are perfectly fair (neutral). Templates are provided to create customized fair strategies.  A strategy is a ROI table that tells you how much you can win or lose depending on how close you are to the winning number. Some provide gains or losses comparable to trading major indexes, and are rather safe: your ROI on a single transaction may never be less than -10\% or more than 20\%. Some are very aggressive and comparable to playing the lottery (except that lotteries are not fair games). In this case you could lose 50\% of your wager in a worst case scenario -- if your number is very far from the winner -- or multiply it by a factor 10 if your number is a winner. 

A player can submit multiple numbers for a given sequence at any given time. Or she may decide to submit only one number, or continue to play on the same sequence (new winning numbers are issued regularly) until she wins some positive ROI, or until her loss reaches some threshold, whichever comes first. 

Thus, the core of the methodology, from a business standpoint, are the strategies and the public ROI tables attached to them. These tables allow you to choose a strategy that matches your risk tolerance. I now describe them in details. I
explain the 32-bit system later in section~\ref{32bitsre}.

\subsection{Bracketed and parametric ROI tables: examples}

In the 8-bit system, both winning numbers and guesses submitted by the player are integers between 0 and 255 inclusive. Let's define the distance between a winning number $w$, and the guess $w'$ made by a player, as
\begin{equation}
d(w, w') = \min\Big(|w - w'|, 256 - |w - w'|\Big).\label{uyr4bg}
\end{equation}
Thus the distance is an integer between 0 and 128 inclusive. Assuming guesses and winning numbers are perfectly random, the distance is a random variable $D$ with the following distribution:
\begin{eqnarray}\label{65obf}
  P(D = d) =
    \begin{cases}
      \frac{1}{256}, & \, d=0 \text{ or } 128 \vspace{1ex}\\
      \frac{1}{128}, & \, 0<d<128  
    \end{cases}       
\end{eqnarray}
Formula~(\ref{65obf}) is at the core of the business model. It allows you to compute expected profits and losses, as well as the variance in financial metrics, for the operator. 
 Before diving into this in section~\ref{pozwed}, I now define the closely related concept of ROI table. 
An ROI table is a file with three columns, generated by the operator and offered to the players. These columns are: \vspace{1ex}
\begin{itemize}
\item[1.] The distance $d$ between a guess $w'$ and a winning number $w$. It is an integer number between 0 and 128 inclusive, and the key of the ROI table, from a database standpoint.
\item[2.] The multiplier $m(d)$. If you bet one dollar on a number $w'$, and the distance to the  winning number $w$ 
  is  $d = d(w, w')$, then your one dollar is multiplied by $m(d)$. The multiplier is a positive real number. When smaller than one,
 you  lose some money. Otherwise you make money. The function $m(d)$ is decreasing, and maximum when $d=0$, that is, when you correctly guess the winning number.
\item[3.] The return $r(d)$ is the ROI on the guessed number. Thus, $r(d)=m(d)-1$. Positive returns of 500\% are possible
 depending on the ROI table. However, you can not lose more than the amount you put on an number. This is in contrast with the real stock market where shorting may lead to losses bigger than the principal.
\end{itemize} \vspace{1ex}

\noindent The second and third columns are redundant, so you may use only one of them. 
All the ROI tables offered by the operator are neutral (also called fair or unbiased) in the sense that $m(d)$ must satisfy
\begin{equation}
\sum_{d=0}^{128} m(d)\cdot P(D = d) = 1. \label{sapired}
\end{equation}
The operator generates revenue via a small transaction fee on each bet. Thus the return, for any player, is not influenced by the gains and losses of other players. This is contrast to the the real stock market or other systems where the collected money is simply redistributed
 to participants, minus a fixed portion kept by the operator.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{ROI1.png}  
\caption{Smooth neutral ROI table with moderate risk / reward}
\label{fig:rn1diur44}
\end{figure}

\subsubsection{Types of ROI tables, with examples}

Several examples of ROI tables are featured in my spreadsheet \texttt{lottery\_ROI\_tables.xlsx} available on GitHub,
 \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/lottery_ROI_tables.xlsx}{here}.
 They range from high risk / high reward with a small chance of winning big, similar to a lottery, to low risk / low reward for risk-adverse players, in this case similar to trading major indexes on the stock market. 

\begin{figure}[H]
\centering
\includegraphics[width=0.64\textwidth]{ROI2.png}  
\caption{Smooth neutral ROI table with high risk / high reward}
\label{fig:rn1digygtf43t}
\end{figure}

 In addition, the ROI curve may appear smooth
 as in Figures~\ref{fig:rn1diur44}--~\ref{fig:rn1ffdsdigyt}, or piecewise-linear with brackets as in Figure~\ref{fig:rn1dqwaswigyt}.
 The former is a called the parametric type because the ROI curve is a smooth decaying math function governed by one 
 or two parameters, such as $m(d) = \mu \lambda^{d}$ with $\lambda<1$. Depending on the parameters, it is designed for risk takers (Figure~\ref{fig:rn1digygtf43t}) or risk-adverse
 players (Figure~\ref{fig:rn1ffdsdigyt}). 

Again, all these tables are fair and designed for the 8-bit system. Tables for the 32-bit system are described
 in section~\ref{32bitsre}. The Y-axis in Figures~\ref{fig:rn1diur44}--~\ref{fig:rn1dqwaswigyt} represents $r(d)$, while the X-axis represents $d$.

\begin{figure}[H]
\centering
\includegraphics[width=0.64\textwidth]{ROI3.png}  
\caption{Smooth neutral ROI table with low risk / low reward}
\label{fig:rn1ffdsdigyt}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[width=0.64\textwidth]{ROI4.png}  
\caption{Bracketed neutral ROI table with about 20 brackets}
\label{fig:rn1dqwaswigyt}
\end{figure}


\subsubsection{The 32-bit system}\label{32bitsre}

In the 32-bit system, winning numbers consist of 4 successive winning numbers from the 8-bit system. Now the player makes
 a 32-bit guess, and uses 32-bit ROI tables provided by the operator. The distance function in~(\ref{uyr4bg}) and everything else easily generalize to 32-bit. 



\renewcommand{\arraystretch}{1.2} %%%
\begin{table}[H]
%\small
%\setlength\extrarowheight{-2pt}
\[
\begin{array}{lrr}
\hline
\text{distance } d \text{ to winning number}	&  \text{multiplier } m(d)  & \text{proportion of guesses}\\ 
\hline
  0 \text{ -- } 19                             &  \num{1000000}   &  0.00000091\% \\
  20 \text{ -- } \num{26065}                      &   1000 &   0.0012\% \\
 \num{26066} \text{ -- }  \num{11530000} &    10  &   0.54\%\\
 \num{11530001} \text{ -- }  \num{1100765000}  & 1.15 &  50.7\%     \\
 \num{1100765001} \text{ -- }  \num{2147483648}  & 0.70 &  48.7\%   \\
\hline
\end{array}
\]
\caption{\label{taxzqa} Example of a fair, bracketed 32-bit ROI table}
\end{table}
\renewcommand{\arraystretch}{1.0} %%%

\noindent This system allows the operator to create tables with a very small chance to win a very large payout,
 with a potential multiplier $m(d)=10^6$ for the winning number ($d=0$), compared to $10^3$ for aggressive tables in the 8-bit system. It is more risky both for the operator and the player, despite the average ROI still being zero. However, it may attract more users, but also more attention from regulatory agencies. Table~\ref{taxzqa} is an example of a fair ROI table in
 the 32-bit system, with extraordinary return when correctly guessing the 32-bit winning number. 

With table~\ref{taxzqa}, if your guess is within 19 points of the winning number (it will happen to about 39 people out of 4.3 billion) then your money (the amount that you bet)  is multiplied by a factor one million. About 48.7\% of the guesses result in a 30\% loss. The next bracket is a 15\% gain, and 50.7\% of all guesses fall in that category. About one in two hundred (0.54\%) results in a 900\% ROI. One in 100,000 would boost your wager by a factor 1000. 

\subsubsection{Free simulator, time to positive return}

The most basic ROI table works as follows. You place a bet on a number.  The multiplier function $m(d)$ is such that
 you either win \$1 with probability $\frac{1}{2}$, or lose \$1 with probability $\frac{1}{2}$. The cumulative gains or losses
 by repeating this experiment over time, follow a \textcolor{index}{symmetric random walk}\index{random walk!symmetric}. It is well known that
 the probability of eventually returning to zero -- that is, erasing all your losses at some point  if you are in a losing streak -- is 100\%.
 The random variable measuring the time to reach such an event (hitting a target) is known as an \textcolor{index}{hitting time}\index{random walk!first hitting time}. The topic is discussed in section~\ref{poyt}. 
See also chapter 12 on random walks in~\cite{baslmou}, and lecture 5 entitled ``Random walks -- advanced methods" in~\cite{tex19ram}.

%---
Indeed the random walk will almost surely hit a positive value or gain (and thus any arbitrary large gain)
 in finite time if you play long enough, see~\cite{martin91} page 102. The proof is based on the 
\textcolor{index}{Wald martingale}\index{martingale!Wald martingale} [\href{https://en.wikipedia.org/wiki/Wald%27s_martingale}{Wiki}]  derived from the random walk. However, the expected number of bets to reach any prespecified value (positive or negative) is infinite.
 %---
You would think that the operator will eventually go bankrupt (if there were no transaction fee), as each participant can play long enough until realizing a gain. But this is not the case: see section~\ref{pozwed}. Anyway, this makes for a great selling point to attract players.

Offering free simulations to participants, or a free trial, is another way to attract users. They can test various ROI tables and see what they could earn by playing with the platform for a while, without committing real money. Half of them are expected to be in positive territory by the end of the trial period. The free version (where you don't play with real money) could be monetized via advertising, attracting sponsors selling financial products, educational material, or gambling platforms. It could also offer the possibility to enter new numbers (or the same one) in real time across multiple sequences and/or multiple ROI tables.


\subsection{Daily payouts for the operator: Brownian motion}\label{pozwed}

It is impossible to predict the total amount to pay each day to the participants. On average, the amount 
 is zero when combining gains and losses, but the daily variations over thousands of bets can potentially be large depending on the ROI tables in use. The purpose of this section is to study these variations, and show how the aggregates follow
 a \textcolor{index}{Brownian motion}\index{Brownian motion} over time. 
 Properties of Brownian motions are discussed in chapter~\ref{ch1}. 

Without loss of generality, let us assume that the wager $v$ is always a fixed \$20. For an individual bet, 
 the player's payout is $v\cdot m(d)$, where $m(d)$ is the multiplier based on the ROI table, and $d$ the distance to the winning number. Thus the operator must pay back
     $v\cdot m(d)$ to the player. This dollar amount includes the initial wager. The profit (or loss) on this transaction is 
$v-v\cdot m(d)$ for the operator if we ignore transaction fees. On average it is zero when using fair ROI tables: see  Formula~(\ref{sapired}).

Assuming the number chosen by the participant is random, the variance for the operator's profit/loss on a 
 single bet in the 8-bit system, given a ROI table, is
\begin{align}
\sigma^2 & =  \text{Var}[v  - v \cdot m(D)] \nonumber \\
 & = v^2 \text{Var}[m(D)]\\
  & = v^2 \Big(\text{E}[m^2(D)] - \text{E}^2[m(D)]\Big) \nonumber \\ 
  & = v^2 \Big(\text{E}[m^2(D)] - 1\Big)  \nonumber \\
  & = v^2 \Bigg[\Big(\sum_{d=0}^{128} m^2(d) P(D=d)\Big) -1\Bigg], \label{p120jh}
\end{align}
where  $D$  -- a random variable -- is the distance to the winning number, 
 with $P(D=d)$ given by~(\ref{65obf}). Aggregated over $n$ bets in a single day (assuming to be independent),
 the standard deviation is thus $\sigma_n = \sqrt{n}\cdot\sigma$. Since these profits/losses follow a Gaussian distribution of zero mean when $n$ is large, 
 it is easy to compute the 99.99 percentile: a worst case occurring once every $\num{10000}$ days on average -- when multiple players win big on a single day. Or the 99.995 percentile: the extreme occurring once every $\num{20000}$ days on average.

The most aggressive ROI table with the highest $\sigma^2$ corresponds to $m(0)=256$ and $m(d)=0$ if $d>0$. 
 You win only when you correctly guess the winning number. The odds are 1/256 per guess. Then your wager $v$ is multiplied
 by 256. Otherwise you lose the money you placed on that number, in its entirety.  In this case, it is easy to verify that 
$\sigma^2_n= 255 \cdot v^2 n$. 
If the daily volume of transactions attached to this ROI table 
 is $\num{10000}$ bets at \$20 each, then $\sigma_n \approx \$\num{31937}$. The 99.99 percentile of a standard normal distribution is about 3.72. Thus the worst daily loss expected in 30 years is about $3.72 \times \$\num{31937} \approx \$\num{118805}$. Aggressive 32-bit ROI tables produce far worse extremes.

Finally, you need to decide on the transaction fee. At 1 cent on a \$20 wager, your average daily revenue
 in the above example would be $0.01 \times \num{10000}\times \$20 = \$2000$, minus the finance charges to process credit cards or other financial instruments used to accept payments. This is more than enough to offset potentially severe aggregated losses due to the growing volatility of the underlying Brownian motion attached to the payouts. It also 
 guarantees a strong linear revenue growth. Without the transaction fees, losses due to payouts may take millions of years to naturally recover on their own. They always do eventually, in the same way that players always reach a positive return at some point, absent of transaction fees. 




\begin{figure}[H]
\centering
\includegraphics[width=0.94\textwidth]{profit1.png}  
\caption{Cumulative profits (left) and distribution of daily profits (right), with a 0.025\% fee}
\label{fig:rn1dqprf1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.94\textwidth]{profit2.png}  
\caption{Cumulative profits (left) and distribution of daily profits (right), with no fee}
\label{fig:rn1dqprf2}
\end{figure}

\subsubsection{Example with truncated geometric distribution}

I now illustrate the profit for the operator over time, and the statistical distribution of daily profits, based on the 
 transaction fee and the type of ROI table. Examples in Figures~\ref{fig:rn1diur44}--~\ref{fig:rn1ffdsdigyt} involved a truncated geometric function
 for $m(d)$. In other words, $m(d) = \mu \lambda^d$ for $0\leq d \leq 128$, with $\mu$ a normalization factor so 
that~(\ref{sapired}) is satisfied, and $0<\lambda < 1$. I use the same function here. With $\lambda = 1$ and no transaction fee, there is zero profit or loss either for the operator or the player. The most volatile case, with maximum potential payout, 
 corresponds to $\lambda\rightarrow 0$. In that case, $m(d)=256$ if $d=0$, otherwise $m(d)=0$: the player loses the entirety of 
his wager.  

The cumulative profits over time are extremely sensitive to the seed when there is no transaction fee. Each seed leads to a
 different \textcolor{index}{Brownian motion}\index{Brownian motion} path. But the introduction of even a tiny transaction fee, completely changes the picture to the benefit
 of the operator. It turns the Brownian path into an almost straight line with positive slope, thus consistently accumulating profits. 
In figure~\ref{fig:rn1dqprf1}, the transaction fee is $0.025\%$ (half of a cent on a \$20 wager). The time period is
 $10^4$ days, that is, about 30 years. By contrast, there is no transaction fee in figure~\ref{fig:rn1dqprf2}.
The two right plots look identical, but they aren't: the one in Figure~\ref{fig:rn1dqprf1} is not centered at zero, but shifted by a miniscule amount, to the right. The impact of this shift on the left plot (cumulative profit) is dramatic.

The Brownian motion in Figure~\ref{fig:rn1dqprf2} (left plot) is going nowhere. It is unpredictable, and will oscillate infinitely
 many times between positive and negative values, with the daily standard deviation increasing linearly over time. I encourage you
 to play with the code, and try different transaction fees, different seeds, and different values for $\lambda$.




\subsubsection{Python code and algorithm to find optimum transaction fee}

For simplicity, rather than digits of quadratic irrationals, the Python code uses random numbers generated by the \textcolor{index}{Mersenne twister}\index{Mersenne twister} via the \texttt{randint} function. Thus, the number of potential seeds is limited to $2^{32}$. With quadratic irrationals, the number of potential seeds is infinite, making it harder to crack.

To determine the minimum transaction fee, I suggest the following approach: compute the 
\textcolor{index}{R-squared}\index{R-squared} $\rho^2$  between the random curve 
 on the left plot in Figures~\ref{fig:rn1dqprf1}--~\ref{fig:rn1dqprf2}, and the ``best fit" straight line. In short, perform a simple
 linear regression. You want to choose a transaction fee satisfying 
$\rho^2>\rho^2_{\text{min}}$,
 where 
$\rho^2_{\text{min}}$ is a prespecified threshold. This guarantees that the operator makes steady profits, while offering a very low transaction fee. 

Increasing the transaction fee will increase $\rho^2$. Even with a modest fee -- say 0.1\% instead of 0.025\% in my example --
 $\rho^2$ is extremely close to 1. In short, the curve (almost) becomes a straight line. The minimum transaction fee 
 depends on the ROI table, with aggressive tables (high $\sigma^2$) requiring higher minimum transaction fees to operate a 
predictably profitable business. 

In practice, you use much higher fees than the minimum required, to accumulate profits faster. However fees that are too high 
 will deter potential players. The solution is to do some price elasticity analysis, to find out the ideal fees that maximizes profits,
 for each ROI table.
 

In the Python code, variables starting with \texttt{arr\_} represent arrays.  The code is also on GitHub, \href{https://github.com/VincentGranville/Stochastic-Processes/blob/master/lottery_profits.py}{here}. \vspace{1ex}

\begin{lstlisting}
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import matplotlib.ticker as mtick

def set_ROI_table(llambda):

    mu = 0
    ROI_table = np.empty(129)

    for d in range(129):
        multiplier = llambda**d
    
        ROI_table[d] = multiplier
        if d == 0 or d == 128: 
            mu += multiplier/256
        else: 
            mu += multiplier/128

    ROI_table /= mu
    return(ROI_table)

def compute_profits(llambda, fee, seed):

    arr_profits = []
    arr_cumul_profits = []

    np.random.seed(seed)
    ROI_table = set_ROI_table(llambda)

    for day in range(ndays):
        collected_fees =  fee * v * nbets
        if day % 1000 == 0:
            print("seed: %4d day: %5d" % (seed, day)) 
        winner = np.random.randint(0, 256, nbets)
        guess  = np.random.randint(0, 256, nbets)
        delta = abs(winner - guess)
        d = np.minimum(delta, 256 - delta)
        d_unique, d_counts = np.unique(d, return_counts=True)
        d_nvals = len(d_unique)
        if d_nvals == 129: 
            profits = np.dot(d_counts, ROI_table) # fast computation
        else: 
            profits = 0
            for index in range(d_nvals):
                d_value = d_unique[index]
                profits += d_counts[index]*ROI_table[d_value]
        today_profit = collected_fees + v*nbets - v*profits
        arr_profits.append(today_profit)
        if day == 0:
            arr_cumul_profits.append(today_profit)
        else:
            yesterday_profit = arr_cumul_profits[day - 1]
            arr_cumul_profits.append(today_profit + yesterday_profit)

    return(arr_profits, arr_cumul_profits, ROI_table) 

#--- main part

ndays = 10000     # time period = ndays
nbets = 10000     # bets per day
v     = 20        # wager
llambda  = 0.30   # strictly between 0 and 1
transaction_fee = 0.005 
seed =  26  

arr_profits, arr_cumul_profits, ROI_table = compute_profits(llambda, 
    transaction_fee, seed)

#--- plot daily profit/loss distribution, and aggregated numbers over time

x = np.arange(ndays)
y = arr_cumul_profits

# custom tick functions

def currency_ticks_k(x, pos):
    x = int(x / 1000)  # plotted values will be in thousand $
    if x >= 0:
        return '${:,.0f}k'.format(x)
    else:
        return '-${:,.0f}k'.format(abs(x))

def currency_ticks_m(x, pos):
    x = int(x / 1000000)  # plotted values will be in million $
    if x >= 0:
        return '${:,.0f}m'.format(x)
    else:
        return '-${:,.0f}m'.format(abs(x))

mpl.rcParams['axes.linewidth'] = 0.5
mpl.rc('xtick', labelsize=8) 
mpl.rc('ytick', labelsize=8) 

fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize =(8, 3))
axes[0].tick_params(axis='both', which='major', labelsize=8)
axes[0].tick_params(axis='both', which='minor', labelsize=8)

# plot: y axis in millions of dollars
tick_m = mtick.FuncFormatter(currency_ticks_m)
axes[0].yaxis.set_major_formatter(tick_m)
axes[0].plot(x, y, linewidth=0.3, c='tab:green')

# histogram: x axis in thousands of dollars
tick_k = mtick.FuncFormatter(currency_ticks_k)
axes[1].xaxis.set_major_formatter(tick_k) 
plt.locator_params(axis='both', nbins=4)
axes[1].hist(arr_profits, bins = 100,  linewidth = 0.5,edgecolor = "red",
            color = 'bisque', stacked=True) 
plt.show()

#--- print ROI table

print ("ROI Table:")
for d in range(129):
    print("%3d %8.4f" %(d, ROI_table[d]))
print("Maximum multiplier: ", ROI_table[0])
\end{lstlisting}

\subsection{Legal engineering}

I do not provide legal advice here. Instead I offer different options about how to set up this type of business. You should discuss them with your lawyer, to find out the best solution to create a platform based on this system, depending on your local regulations. The business model may be perceived as a
 lottery, even though the winning numbers can -- theoretically -- be precomputed. It is in fact a mathematical contest that does not involve chance, but one that nobody can win given current computer hardware. 

The first step is to avoid keywords such as bet, number guessing, gambling, or wager. On the plus side, conservative ROI tables are not different from playing major indexes on the stock market, and the game is fair. Also, given enough time and trials, all players, even if making random guesses, will regularly end up in positive territory due to mathematical laws pertaining to symmetric random walks. To avoid paying taxes on what tax agencies might consider as disguised wagers, special care must be taken. I now discuss 
 several  options.\vspace{1ex}

\begin{itemize}
\item[1.] Partner with a company allowed to operate this type of business, to avoid most of the red tape. Use a licensing agreement, where you design and provide the winning numbers. 
\item[2.] Do it yourself with reliable third-party vendors. The platform that you choose to process transactions (accept online bets and so on) may also help you set up your business to meet legal requirements.
\item[3.] Find a trustworthy partner in a gambling-friendly country. Your partner will create a company in that country. Your separate, local company receive payments from this third-party entity. You pay local taxes on profits that your company makes (revenue from the foreign entity minus expenses), as any regularly business does. This strategy will protect you against being accused in your home country of operating an illegal lottery.  It is targeted at US citizens in particular.
\item[4.] Offer the free version only, with no real money being gambled. On occasions, offer a reward (cash prize) to attract participants,
 without  wagers or transaction fees. This option is interesting if you want to grow a website and make money via different means, for instance consulting or training. It will showcase your expertise. 
\item[5.] Use a subscription-based model, with a fixed monthly fee, offering data and other services. Allow paid subscribers to use
 a portion of the subscription fee to participate in the system, without transaction fees or wagers, yet with the same chances of earning the same real money. There is no penalty for losing: the cost is absorbed by the subscription fees.  Money paid to winners can be deducted by the operator as consulting expenses, to test the system. This is one way to grow your subscription base.
\item[6.] Players use the system for free,  yet they can make real money as in the paid version: there are no wagers or transaction fees. Monetization comes from sponsorship, typically in the form of advertising revenue.  
\item[7.] Make it a private venture. Participation is by invitation only, and approved players are listed as friends.
\end{itemize}\vspace{1ex}


% % add exercise with product numeration system and python code / prodbin3*.txt and https://mathoverflow.net/questions/445723/product-based-binary-numeration-system

\noindent I presented this system  at the Operations Research Society conference (INFORMS), in a session about algorithm biases. 
See the abstract \href{https://www.abstractsonline.com/pp8/#!/6818/presentation/6842}{here}. 


% title: a synthetic stock exchange played with real money
% add big section about casinos from links.txt
% how to make this legal
% public data
% legal aspects: make it a private club, by invitation only
   % show all winning numbers are random
   % token attached to each transaction / licensing fee
   % make it simulated trading platform
  % synthetic stock exchange
  % organize competition
  % advertise in forum for hackers


\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{refstats} % Entries are in the refs.bib file in same directory as the tex file
\printindex



\hypersetup{linkcolor=red} % red %
\hypersetup{linkcolor=red}



\end{document}